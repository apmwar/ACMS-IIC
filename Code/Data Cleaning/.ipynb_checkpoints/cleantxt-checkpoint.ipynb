{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk import corpus\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the text file into the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"S3 public access can not be set.\\r\\nI've edited the bucket policy, public access settings, but the bucket is not changed to public and access is denied.\\r\\n\\r\\nWhy do you see these symptoms?\\r\\n\\r\\nhttps://imgur.com/rV64vwd\\r\\n(Edit Public Access Settings)\\r\\n\\r\\nhttps://imgur.com/Y3pPAQJ\\r\\n(bucket policy)\", \"I can't figure out why S3 won't display my website on my domain\\r\\nError 403 and I am not sure how to troubleshoot the error or resolve it despite reading guides\\r\\nI've looked at a couple of forums and pages but they are either irrelevant or beyond my current understanding. I cannot figure out why I keep getting a 403. I have a public bucket policy which changed my previous error of not getting a connection to the site to a 403, just forbidding traffic. I am new to AWS, what am I missing?\\r\\nI only have one HTML file in the bucket and when I hit 'make public' it said access denied. Are there other objects that I have to find?\\r\\nI was able to get into the HTML file permissions and when I selected 'public access' 'read object' it said access denied.\\r\\nFor whatever reason, when accessing the domain I purchased, it says 403. When I go to the amazon s3 version of my website it works.. huh. Why doesn't it come up on my domain?\", \"Re: I can't figure out why S3 won't display my website on my domain\\r\\ntest, ignore\", \"My AWS S3 account is suspended without any Notification\\r\\nToday when I was ready to some work on my website I found all the components which are stored on Amazon S3 are missing. To check the issue I tried to log in to s3 console and found out that my account is suspended.\\r\\n\\r\\nI didn't find any email regarding the suspension and I have no payment dues in my billing options.\\r\\nI have also opened a support case with Case Id: 5866475801 and mailed the issue but still no reply.\\r\\n\\r\\nWhat the hell is happening ? Please atleast tell and help me to resolve the issue so that I can work further.\", 'Unable to read more than 1000 objects from a bucket\\r\\nI am using the REST APIs (not through SDK) to read the list of objects from a bucket. The bucket has more then 1000 objects. \\r\\n\\r\\nAs per https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html  documentation, I am extracting the \"IsTruncated\" and \"NextMarker\" values from the response and if \"IsTruncated\" is true, I pass the value of \"NextMarker\" as \"marker\" parameter in the subsequent call.\\r\\n\\r\\nWhenever, I add the \"marker\" to the query parameter I get \"SignatureDoesNotMatch\" error. I specify other parameters like \"max-keys\", \"prefix\", \"delimiter\" and they does cause any error and the call goes through fine. Of course, I did URI encode the parameters. Its only the \"marker\" parameter that is causing the error.\\r\\n\\r\\nI also tried to use the v2 version, https://docs.aws.amazon.com/AmazonS3/latest/API/v2-RESTBucketGET.html . I get the same \"SignatureDoesNotMatch\" error when using the \"continuation-token\" in the subsequent call. The \"list-type\" and \"max-keys\" parameters were ok.\\r\\n\\r\\nAny help is appreciated.', 'Adding Expires or Cache-Control header for folders in S3\\r\\nThe help page here is as always quite useless: \\r\\n\\r\\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html#expiration-individual-objects\\r\\n\\r\\nIt says that in a folder, I\\'d see \"properties\" and then \"Metadata\". \\r\\n\\r\\nNone of my folders in S3 have this \"Metadata\". Inside Properties, there are cards like Versioning, Server Logging, etc. \\r\\n\\r\\nWhere should I enter the Cache-Control setting? This is for CloudFront, which seems to be not getting this header from S3. I could set up this header in CloudFront, but that UI isn\\'t any more intuitive. \\r\\n\\r\\nThanks for any guidance.', 'Re: Adding Expires or Cache-Control header for folders in S3\\r\\nFound it in the \"Change Metadata\" for folders and files. The HELP should be updated, the documentation remains fairly hideous. Found the solution on Stack Overflow. Thanks.', 'where do I get the canonical user id\\r\\nHi,\\r\\nI am setting up a new bucket and need to assign specific IAM users to the bucket.\\r\\nIt is asking for the canonical user id.\\r\\nHow do I get the canonical user id for all my IAM users?\\r\\nThis is so confusing.\\r\\n\\r\\nThanks', 'Re: where do I get the canonical user id\\r\\nHello nicolas_briant, \\r\\n\\r\\nThank you for your post, I trust you are well. \\r\\n\\r\\nRegarding the Canonical User ID, you can find it in two ways:\\r\\n\\r\\n1) Logging in as root. In the top right of the console, choose your account name or number. Then choose My Security Credentials. You will see the Canonical User ID in the Account Identifiers section. \\r\\n\\r\\nFinding Your Account Canonical User ID:\\r\\n\\r\\nhttp://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#FindingCanonicalId\\r\\n\\r\\n2) By using a ListBuckets API call. When you perform a GET operation on the S3 service to get a listing of all the buckets you own, the response contains an Owner ID element which is your canonical user id. For example:\\r\\n\\r\\naws s3api list-buckets --output text\\r\\n\\r\\nlist-buckets: \\r\\n\\r\\nhttp://docs.aws.amazon.com/cli/latest/reference/s3api/list-buckets.html\\r\\n\\r\\nBest regards. \\r\\n\\r\\nJayd J.', 'Re: where do I get the canonical user id\\r\\nThanks Jayd. I was missing \"Logging in as root\".\\r\\nRegards,\\r\\nNicolas', \"Re: where do I get the canonical user id\\r\\nI wanted to post the following, but I wasn't allowed because my forum account had just been created in the past hour.  (Some kind of spam prevention?)\\r\\n\\r\\nI'm pretty new to using AWS.  I can't find my canonical ID and I don't understand your instructions for looking up my ID.  I have access to the AWS console via my personal account.  Isn't there a way to see my own canonical ID via the web interface, without using a CLI?\\r\\n\\r\\nI've created an S3 bucket under my organization's account.  I want to grant myself and another user access to the bucket, but I need our two canonical IDs to do so.\\r\\n\\r\\nI'd appreciate step-by-step instructions for getting the IDs.  Assume I know nothing about AWS.\\r\\n\\r\\nThanks in advance.\\r\\n\\r\\nWhile I'd still like to have explicit step-by-step instructions for finding canonical IDs, the delay in forum posting inspired me to try a different approach.  Or rather, the same approach over again.\\r\\n\\r\\nThat is, when I was creating the S3 bucket, I had entered my email address as one to be granted access to it.  The bucket was created, but I received an error message that I couldn't be granted access by email address for some reason.  (It was unclear as to why.)\\r\\n\\r\\nFor some reason, I decided to try using my email address again.  This time it worked and my canonical ID was automatically substituted!  I entered the email address of the other person that I wanted to give access to the bucket, and his canonical ID was filled in as well.  Now we both have access.\\r\\n\\r\\nHowever...  When we view the permissions of the S3 bucket, we only see canonical IDs.  There's no indication to whom each of the IDs refer!  That's not a useful UI!  It really looks lazy.\\r\\n\\r\\nSo, how can I tell which canonical ID goes with each person?\", 'Re: where do I get the canonical user id\\r\\nHello,\\r\\nHow can I find another user Canonical ID?\\r\\nI created new IAM user and want him to have access to S# bucket.\\r\\nThank you!', 'Re: where do I get the canonical user id\\r\\nEvery IAM user will not have their own canonical ID. There will be only one Canonical ID per AWS account. \\r\\n\\r\\nThe following link has step-by-step instructions on how to find the canonical ID associated with your AWS account --> https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#FindingCanonicalId\\r\\n\\r\\nUsing Obejct/Bucket ACLs, you can grant/restrict access to an object/bucket to your AWS account or to another AWS account. You cannot restrict access to bucket/object to an IAM user using Object/Bucket ACL.\\r\\n\\r\\nYou will have to use bucket policy if you want to provide/restrict access to bucket/object to an IAM user in your account then you will have to use S3 bucket policy instead of ACL.\\r\\n\\r\\nBucket Policy Examples --> https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html', 'Bug in permission evaluation?\\r\\nTo reproduce:\\r\\n\\r\\nCreate a role: testRole\\r\\nCreate a bucket, we\\'ll call it testBucket here\\r\\nCreate a user: testUser\\r\\nAdd permissions to testUser to sts:assumeRole to testRole, and to the trust relationship of testRole to allow the user to assume it\\r\\nAdd an inline policy to testRole to allow actions s3:* to testBucket and testBucket/*\\r\\nAdd an inline policy to testUser to allow actions s3:* to testBucket and testBucket/*\\r\\nUpload a file to the bucket (via ui is fine)\\r\\nCreate a bucket policy on testBucket:\\r\\n\\r\\n\\r\\n{\\r\\n    \"Version\": \"2012-10-17\",\\r\\n    \"Statement\": [\\r\\n        {\\r\\n            \"Sid\": \"deny-others\",\\r\\n            \"Effect\": \"Deny\",\\r\\n            \"NotPrincipal\": {\\r\\n                \"AWS\": [\\r\\n                    \"arn:aws:iam::<acct id>:role/service-role/testRole\",\\r\\n                    \"arn:aws:iam::<acct id>:root\",\\r\\n                    \"arn:aws:iam::<acct id>:user/testUser\",\\r\\n                ]\\r\\n            },\\r\\n            \"NotAction\": [\\r\\n                \"s3:PutObject\",\\r\\n                \"s3:PutObjectAcl\"\\r\\n            ],\\r\\n            \"Resource\": \"arn:aws:s3:::<bucket name eg testBucket>/*\"\\r\\n        }\\r\\n    ]\\r\\n}\\r\\n\\r\\n\\r\\nThe bucket policy is taken from an anonymous upload use case, but in essence its deny\\'ing access to everyone but root, our test role, and a test user to everything other than putobject and putobjectacl.\\r\\n\\r\\nSo I\\'d expect that from testUser if I assumed the role testRole, that I\\'d be able to, for example, call s3:GetObject on the file I uploaded.  But this gets access denied.  Meanwhile if I do the same thing with testUser, it is allowed, despite both of these having the exact same set of policies.\\r\\n\\r\\nPlease help!\\r\\nThanks.\\r\\n\\r\\nEdited by: davidericksonfn on Mar 6, 2019 7:27 PM', \"Accidentally expired entire bucket contents\\r\\nSo yesterday one of the files in our bucket wouldn't delete. I set a lifecycle rule to expire the object, at least I thought I did. What I actually did was name the rule the object I wanted to expire and set the scope to the entire bucket. Is there any way to recover the contents?\\r\\n\\r\\nEdited by: _bd on Mar 6, 2019 11:46 AM\", 'Pentest of AWS Cloud Application - S3\\r\\nHello,\\r\\n\\r\\nthe new policy for penetrationtests permits the penetrationtest of a list of AWS services, as seen on this site: https://aws.amazon.com/security/penetration-testing/\\r\\n\\r\\nS3 is not in the list. Since publicly accessible files on a S3 bucket are often a security relevant issue it should be part of a security assessment of a cloud infrastructure. I wonder if it is necessary to obtain a special permission of AWS to issue S3-related requests in the context of a penetration test or is it generally forbidden for a pentester to examine S3 buckets belonging to a tested application?\\r\\n\\r\\nGenerally speaking: What am I allowed to do as a pentester performing a pentest in the AWS Cloud regarding the S3 bucket?\\r\\n\\r\\nKind regards\\r\\nMichael', 'Preview Large Files in S3\\r\\nI have several thousand image files that are 1G in size each storaged in S3.  I  access them locally via Storage Gateway.  Paging through these image files locally is slow and cumbersome.  I am looking for a way to allow me to preview these files without having to download them one at a time to view them.  Is there any kind of viewer available in AWS that will allow me to view these files without downloading them locally?', 'Unexplained error setting up policy for S3 Cross-Region Replication\\r\\nHi, I\\'m following the instructions on this page for setting up the roles for CRR: https://docs.aws.amazon.com/AmazonS3/latest/dev/setting-repl-config-perm-overview.html\\r\\n\\r\\nI have a role (we\\'ll call it ReplRole) with the exact trust policy listed on that page, and an access policy that looks like this:\\r\\n{\\r\\n    \"Version\": \"2012-10-17\",\\r\\n    \"Statement\": [\\r\\n        {\\r\\n            \"Effect\": \"Allow\",\\r\\n            \"Action\": [\\r\\n                \"s3:GetReplicationConfiguration\",\\r\\n                \"s3:ListBucket\",\\r\\n                \"s3:GetObjectVersion\",\\r\\n                \"s3:GetObjectVersionAcl\",\\r\\n                \"s3:GetObjectVersionTagging\",\\r\\n                \"s3:ReplicateObject\",\\r\\n                \"s3:ReplicateDelete\",\\r\\n                \"s3:ReplicateTags\"\\r\\n            ],\\r\\n            \"Resource\": [\\r\\n                \"arn:aws:s3:::our-s3-bucket-prefix-*\",\\r\\n                \"arn:aws:s3:::our-s3-bucket-prefix-*/*\"\\r\\n            ]\\r\\n        }\\r\\n    ]\\r\\n}\\r\\n\\r\\n\\r\\nIt doesn\\'t look exactly like the policy provided, but the main difference is that I use wildcards for the resources specified. I also group the actions together. But if I understand IAM access policies correctly, this should suffice for the purpose.\\r\\n\\r\\nS3 allows me to create the Cross-Region Replication rule, but then I get this error message:\\r\\nThe CRR rule is saved, but it might not work.\\r\\nThere was an error with setting up the IAM policy for the selected IAM role GobsCrossRegionReplicationRole. Ensure that you have set up the correct policy, or select another role.\\r\\n\\r\\nWhat did I do wrong?', 'Re: Unexplained error setting up policy for S3 Cross-Region Replication\\r\\nWhat I found was that the cross-region replication was actually working, despite the error message. When I checked back the next morning, the objects were successfully replicated to the backup bucket.', \"Can't Export CloudWatch Logs To S3\\r\\nI'm trying to export a CloudWatch Log Group to S3 but I keep getting this error every time I click Export Data on the CloudWatch side:\\r\\n\\r\\nThe ACL permission for the selected bucket is not correct. The Amazon S3 bucket must reside in the same region as the log data that you want to export.\\r\\n\\r\\nThe CloudWatch Log is in us-east-1. I created a bucket in each of the two US East regions: N. Virginia and Ohio but I still get this error when I try to export to either one of them. Why?\", \"Re: Can't Export CloudWatch Logs To S3\\r\\nPLEASE RESPOND. THIS IS VERY IMPORTANT AND TIME SENSITIVE.\", \"Re: Can't Export CloudWatch Logs To S3\\r\\nI had the exact same problem. It turns out that I neglected to populate the 'S3 bucket prefix' field under the 'Advanced' section of the export dialog.\\r\\n\\r\\nEdited by: lbrooks on Nov 30, 2018 12:17 AM\", \"Re: Can't Export CloudWatch Logs To S3\\r\\nThis unfortunately still doesn't work for me either. \\r\\n\\r\\nWhat exactly did you enter as the prefix? Why can't we simple save the logs to any bucket that we own as a user aka admin?\", \"Re: Can't Export CloudWatch Logs To S3\\r\\nDid you ever solve this?  How do I tell which region my Cloudwatch logs are in?\\r\\n\\r\\nI am trying to do the same thing and am getting the same error.\", \"Re: Can't Export CloudWatch Logs To S3\\r\\nI had the same problem.\\r\\n\\r\\nThe solution I found was to select the bucket in the web portal, and along the top you then have a number of buttons:\\r\\nOverview\\r\\nProperties\\r\\nPermissions\\r\\nManagement\\r\\n\\r\\nIn permissions you can set the Bucket Policy to allow Cloudwatch exports as detailed here: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3ExportTasks.html\\r\\n\\r\\nHope this helps\", 'Re: Can\\'t Export CloudWatch Logs To S3\\r\\nIt took me 30min to figure the error. \\r\\nTheir example policy is wrong\\r\\n\\r\\nThe region in the example is the one you have to replace\\r\\nSimply replace it by: \\r\\n                \"Service\": \"logs.us-east-1.amazonaws.com\"\\r\\nDo it in both fields below.\\r\\n\\r\\n{\\r\\n    \"Version\": \"2012-10-17\",\\r\\n    \"Statement\": [\\r\\n      {\\r\\n          \"Action\": \"s3:GetBucketAcl\",\\r\\n          \"Effect\": \"Allow\",\\r\\n          \"Resource\": \"arn:aws:s3:::my-exported-logs\",\\r\\n          \"Principal\": { \"Service\": \"logs.us-east-1.amazonaws.com\" }\\r\\n      },\\r\\n      {\\r\\n          \"Action\": \"s3:PutObject\" ,\\r\\n          \"Effect\": \"Allow\",\\r\\n          \"Resource\": \"arn:aws:s3:::my-exported-logs/random-string/*\",\\r\\n          \"Condition\": { \"StringEquals\": { \"s3:x-amz-acl\": \"bucket-owner-full-control\" } },\\r\\n          \"Principal\": { \"Service\": \"logs.us-east-1.amazonaws.com\" }\\r\\n      }\\r\\n    ]\\r\\n}', 'Amazon S3 slow on large buckets\\r\\nHi,\\r\\nI got a big amazon S3 bucket around 8 GB and now things started to get slow. Uploading files (with IAM user, so the API) takes forever. Now I recreated a new bucket with the same properties and settings and API uploading to this bucket is way faster. Is there a connection between a large bucket and slow API uploading? Do I have to make new buckets all the time?\\r\\n\\r\\nTime to write an image of around 300 KB with API to large bucket:\\r\\n\\r\\nimageGet: 1260ms\\r\\nimageWrite: 67683ms\\r\\n\\r\\nTo a newly created bucket time:\\r\\n\\r\\nimageGet: 1275ms\\r\\nimageWrite: 822ms\\r\\n\\r\\nOn https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html it is stated:\\r\\n\\r\\nThere is no limit to the number of objects that can be stored in a bucket and no difference in performance whether you use many buckets or just a few. You can store all of your objects in a single bucket, or you can organize them across several buckets.\\r\\n\\r\\nFor me this does not seem the case, anyone?\\r\\n\\r\\nRegards,\\r\\n\\r\\nDick Goosen\\r\\n\\r\\nEdited by: dickgoosen on Feb 1, 2019 12:49 AM\\r\\n\\r\\nEdited by: dickgoosen on Feb 1, 2019 1:22 AM', 'Re: Amazon S3 slow on large buckets\\r\\nHi there!\\r\\n\\r\\nI have the same issue. The small image takes 2 min by uploading to S3 from EC2.', 'Unable to Set the ACL to public read properly on Objects stored in Bucket\\r\\nThe bucket permissions are set to public, and when I upload image files from the angular web application, I include in the header of the put request \"x-amz-acl\": \"public-read\" as described by the AWS documentation. When I attempt to view the images through the s3 link to the file however, a 403 forbidden error is returned. \\r\\nIf I go into the bucket and examine the image, it shows there are no access permissions, and if I try to make the image public, I get an \"Access Denied\" message. What can I do to resolve this issue?  \\r\\nI\\'ve attached some images to the post, the first one shows the headers included in the put request for uploading, the second shows the bucket policy, and the third image shows the current state of permissions for an image I uploaded.', 'Password protect S3 mount\\r\\nUsing storage gateway, we\\'ve created an S3 mount, which we connect to using the recommended: \"mount -o nolock...\" command.  I know that we can limit the policy so that only certain IAM users have access to the buckets, and we can then mount the buckets using something like TNTDrive and enter the IAM credentials that way to access the buckets, but are there any alternatives to TNTDrive that would allow us to do this?  That is, when we set up a mount, is there any command like \"mount -o nolock -username=\"username\" password=\"password\" that would allow us to protect the mount?', '\\'Access Denied\\' when access s3 from angular app with cognito user pool\\r\\nI have S3 bucket which I configured to manage access using cognito user pool, as described here https://docs.amazonaws.cn/en_us/IAM/latest/UserGuide/reference_policies_examples_s3_cognito-bucket.html:\\r\\n{\\r\\n    \"Version\": \"2012-10-17\",\\r\\n    \"Statement\": [\\r\\n        {\\r\\n            \"Effect\": \"Allow\",\\r\\n            \"Principal\": \"*\",\\r\\n            \"Action\": \"s3:ListBucket\",\\r\\n            \"Resource\": \"arn:aws:s3:::<bucket-name>\",\\r\\n            \"Condition\": {\\r\\n                \"StringLike\": {\\r\\n                    \"s3:prefix\": \"cognito/<app-name>/\"\\r\\n                }\\r\\n            }\\r\\n        },\\r\\n        {\\r\\n            \"Effect\": \"Allow\",\\r\\n            \"Principal\": \"*\",\\r\\n            \"Action\": [\\r\\n                \"s3:PutObject\",\\r\\n                \"s3:GetObject\",\\r\\n                \"s3:DeleteObject\"\\r\\n            ],\\r\\n            \"Resource\": [\\r\\n                \"arn:aws:s3:::<bucket-name>/cognito/<app-name>/${cognito-identity.amazonaws.com:sub}*\",\\r\\n                \"arn:aws:s3:::<bucket-name>/cognito/<app-name>/${cognito-identity.amazonaws.com:sub}/*\"\\r\\n            ]\\r\\n        }\\r\\n    ]\\r\\n}\\r\\n\\r\\n\\r\\nI have angular web app which authenticate users with cognito user pool, and I\\'m using S3 client to get object. I see a call to cognito service (https://cognito-identity.eu-central-1.amazonaws.com/) is made successfully, and an identity is returned as a response, but the immediate call afterwards to s3 is failing with status code 403:\\r\\n<Error><Code>AccessDenied</Code><Message>Access Denied</Message><RequestId>AAC2B5FC5C74C971</RequestId><HostId>l8AOygYbUT+Y1QhTjHydRJ9Uxc97ElSZ+l6H2RQlNglpQuZrqQPW532U6Pixil7YPZ4ugpreoSs=</HostId></Error>\\r\\n\\r\\n\\r\\nHere\\'s my code setting AWS creds:\\r\\n    buildCognitoCreds(idTokenJwt: string) {\\r\\n        let url = \\'cognito-idp.\\' + CognitoUtil._REGION.toLowerCase() + \\'.amazonaws.com/\\' + CognitoUtil._USER_POOL_ID;\\r\\n        if (environment.cognito_idp_endpoint) {\\r\\n            url = environment.cognito_idp_endpoint + \\'/\\' + CognitoUtil._USER_POOL_ID;\\r\\n        }\\r\\n        let logins: CognitoIdentity.LoginsMap = {};\\r\\n        logins[url] = idTokenJwt;\\r\\n        let params = {   \\r\\n            IdentityPoolId: CognitoUtil._IDENTITY_POOL_ID, /* required */\\r\\n            Logins: logins\\r\\n        };        \\r\\n        let serviceConfigs = <awsservice.ServiceConfigurationOptions>{};\\r\\n        if (environment.cognito_identity_endpoint) {\\r\\n            serviceConfigs.endpoint = environment.cognito_identity_endpoint;\\r\\n        }\\r\\n        let creds = new AWS.CognitoIdentityCredentials(params, serviceConfigs);\\r\\n        this.setCognitoCreds(creds);\\r\\n        return creds;\\r\\n    }\\r\\n\\r\\n\\r\\nWhat am I missing?\\r\\nNo matter what I try, I\\'m getting Access Denied.', 'S3 console shows Error in Access row\\r\\nAlso it can\\'t generate an URL for \\'Download As\\'. It shows \"An error occurred generating the download link for this object.\". This happens for all AWS accounts we have and for different people.', \"Re: S3 console shows Error in Access row\\r\\nHi,\\r\\n\\r\\nI understand that you are seeing Error in the in the S3 console and 'Download As' option is throwing error. This usually happens when you do not proper permissions to access the bucket or object.\\r\\n\\r\\nPlease ensure that you have all the necessary permissions. If you still see the same issue, please share the bucket name, object name and requester IAM ARN with me over private message to troubleshoot further.\\r\\n\\r\\nThanks,\", \"Re: S3 console shows Error in Access row\\r\\nRGumber, I've sent requested information via PM several days ago.\", \"Re: S3 console shows Error in Access row\\r\\nAny news? I don't have any issue or restriction when I use aws cli. It only happens with AWS Console.\\r\\n\\r\\nEdited by: DenisKot on Feb 28, 2019 2:28 AM\", 'Amazon S3 : \"Error retrieving access type\", can\\'t use bucket\\r\\nHello,\\r\\n\\r\\nI have a problem concerning my S3 buckets. Every bucket that I create (or that is created automatically by Elastic Beanstalk) show a \"Error\" in the \"Access\" column (short for \"Error retrieving access type\"). I am unable to do anything with the console : i can\\'t delete the bucket, or download/upload any file. Elastic Beanstalk as well is unable to use s3 and it makes impossible the deployment of new version of any web application.\\r\\n\\r\\nUsing the CLI, I can perform any operation I want.\\r\\n\\r\\nThere is no IAM on the account. I log directly with root access.\\r\\n\\r\\nDo you have any idea what the problem can be ? \\r\\n\\r\\nThank you,\\r\\n\\r\\nEdouard', \"Can't delete empty S3 bucket (cf-templates)\\r\\nI can't delete a S3 bucket that\\r\\n• is empty\\r\\n• has no versioning\\r\\n• has no policy\\r\\n\\r\\nThis S3 bucket has been created with some other service.\\r\\nWhen I attempt deleting the bucket in the AWS console, I get an error with no message. \\r\\nAnd then i tried to delete via AWS CLI with below command,\\r\\n\\r\\naws s3 rb s3://bucketname --force\\r\\n\\r\\nIt doesn't work and shows error msg like this,\\r\\n\\r\\nfatal error: An error occurred (NoSuchBucket) when calling the ListObjects operation: The specified bucket does not exist\\r\\n\\r\\nremove_bucket failed: Unable to delete all objects in the bucket, bucket will not be deleted.\\r\\n\\r\\nI searched on forum and tried to edit Bucket policy, but It also shows like this, \\r\\nError\\r\\nData not found\\r\\n\\r\\nHope anyone could help me with this issue.\\r\\nThanks for and have a nice day!\", 'Copy from source bucket to dest bucket - GetObject() stream problem\\r\\nHi all,\\r\\nMy name is Eliran and i new in aws.\\r\\nmy goal is to copy from one bucket (Ireland) to another bucket (N.Virginia)\\r\\ni will explain my work flow:\\r\\n1) i use the CLI command - aws s3 sync s3://sorce-bucket/ s3://dest-bucket/ --exclude \"logs/*\".\\r\\nthe sync completed after some time...\\r\\n2)in my app in .NET i use the AWS SDK and use the command GetObject() like this:\\r\\nAmazonS3Client s3client = new AmazonS3Client(Globals.AWSAccessKey,Globals.AWSSecretKey, RegionEndpoint.USEast1);\\r\\nStream rs = s3client.GetObject(new GetObjectRequest\\r\\n{\\r\\nBucketName = SourceContainer,\\r\\nKey = key}).ResponseStream;\\r\\n\\r\\nits work fine but the ResponeStream for some objects is MD5Stream (that what i need) and for some objects is CachingWarpperStream.... (its not good for me)\\r\\n\\r\\nif i use the source bucket from Ireland so all the request with GetObject on the same objects (like above) will return ResponeStream MD5Stream! \\r\\n\\r\\n*the settings and policies is the same in both buckets.\\r\\n\\r\\nwhat goes wrong? and how i can get always MD5Stream from my new bucket in N.Virginia.\\r\\n\\r\\nThanks a lot,\\r\\nEliran Kasif.', 'Stockholm S3 endpoint issue\\r\\nHello,\\r\\n\\r\\nI\\'m trying to connect to an S3 bucket in the newly available EU North 1 Region (Stockholm) through two Mac S3 compatible apps (Forklift and ChronoSync) without success.\\r\\n\\r\\nI\\'ve used \"s3.eu-north-1.amazonaws.com\" and \"s3-eu-north-1.amazonaws.com\" endpoints to no avail. I get the following error: The authorization header is malformed; the region \\'us-east-1\\' is wrong; expecting \\'eu-north-1\\'\\r\\n\\r\\nIs anyone experiencing the same issue? Thanks.\\r\\n\\r\\nRegards.', \"Re: Stockholm S3 endpoint issue\\r\\nI answer to myself: one of the mentioned apps (Forklift) has been updated recently and now it works with EU North 1 Region S3 buckets.\\r\\n\\r\\nI suppose the same will happen soon with ChronoSync app. It's actually a matter of the specific app that needs to be updated to support new regions.\", 'Issue reading S3 buckets (XML Parsing Error)\\r\\nWhen I access my list of buckets in the web browser, I get the following error in the console: \\r\\n\\r\\nXML Parsing Error: no root element found\\r\\nLocation: https://us-east-1.console.aws.amazon.com/s3/proxy\\r\\nLine Number 1, Column 1:\\r\\n\\r\\nI believe this is coming from a few buckets I had deleted but are stuck in my account. If I try to delete them again, nothing happens visually, and in the console, this error is hit again.\\r\\n\\r\\nIs this something that will resolve itself in time? Or does something need to be done to resolve it?\\r\\n\\r\\nThanks!', \"Strange issue granting unexpected permission to single object, how to fix?\\r\\nI have two objects in a bucket, and GetObject should 403 for both for Role A, except inexplicably one object is accessible.\\r\\n\\r\\nThe bucket configuration is:\\r\\nblank bucket policy, all 4 options under Public Access Settings are True, ACL is defaults\\r\\n\\r\\nRole config grants no permissions to S3\\r\\n\\r\\nPermissions for both objects appear identical (at least in console). \\r\\n\\r\\nIAM Policy Simulator indicates both objects will be denied, but again, in reality one object is allowed.\\r\\n\\r\\nI'm performing the getObject from javascript AWS SDK using federated identities & Cognito pool. Role A  is associated with the logged in user's group. \\r\\n\\r\\nI've tried running CloudTrail but getObject is not being recorded when executed via AWS SDK. It seems to log the event however when I manually download via console. \\r\\n\\r\\nPlease help!\\r\\n\\r\\nEdited by: davegravy on Feb 22, 2019 5:36 AM\", 'Does Amazon S3 or glacier has build-in fixity checking?\\r\\nHi\\r\\n\\r\\nSee https://dltj.org/article/oclc-digital-archive-vs-amazon-s3/ claimed that Amazon S3 does not provide fixity check.\\r\\n\\r\\nSee https://www.slideshare.net/AmazonWebServices/deep-dive-on-archiving-and-compliance page 12 from AWS presentation said that it has built-in fixity checking. \\r\\n\\r\\nI can\\'t find anything mention about fixity by simply searching \"fixity\" in \\r\\nhttps://docs.aws.amazon.com/s3/index.html#lang/en_us\\r\\nhttps://docs.aws.amazon.com/glacier/index.html#lang/en_us\\r\\n\\r\\nSo, does Amazon s3/glacier provides fixity checking or not?', \"Need suggestions on how to look up existing objects on S3\\r\\nHi, we have incoming files everyday, and we upload them to S3. The files can be duplicates from earlier days, so we need to check if they already exist before uploading.\\r\\n\\r\\nOur old way is to save the filename to SDB after uploading a new file, so we can use SDB query to look up existing files.\\r\\n\\r\\nWe recently want to change it to use S3 HEAD Object API to check existence.\\r\\n\\r\\nWe need suggestions:\\r\\n(1) if there's better way beside SDB and S3 HEAD API? any new S3 API to check existence in bulk?\\r\\n(3) is S3 HEAD API good enough for our use case (we need look up ~200 filenames every hour during the day)\\r\\n\\r\\nThanks in advance!\", \"Re: Need suggestions on how to look up existing objects on S3\\r\\nHi,\\r\\n\\r\\nI use S3 pretty extensively but I'm not employed by Amazon so take this as it is.\\r\\n\\r\\n That amount of HEAD requests per hour will be totally fine, it should not be anywhere close to stressing out the service.\\r\\n\\r\\nThere isn't really a better way to check for existence of a random key. However, if you have a lot of keys at once and the keys share a path-like structure, you could do better by executing a list request on the common prefix and checking the contents. \\r\\n\\r\\nKeep in mind that S3 may not be immediately consistent. You should read the docs to fully understand the impact to your particular use case, but a couple things stick out:\\r\\n1) if you do a GET/HEAD prior to uploading, then PUT, then GET/HEAD -- that response will be eventually consistent i.e. not guaranteed to return that the object does exist\\r\\n2) LIST requests are eventually consistent\\r\\n\\r\\nGiven that and that your expectations for number of objects to check, I would recommend keeping it simple and just doing HEAD - maybe with time-based retries to clear up the eventual consistency issue. If you make the wrong decision, you simply re-upload a duplicate, which is not data loss but just extra cost to you, so if this happens every once in a while, shouldn't be too big of a deal.\\r\\n\\r\\nHope this helps!\", 'Re: Need suggestions on how to look up existing objects on S3\\r\\nThank you. I agreed to keep it simple is the right way to start. Your suggestions are very much appreciated.', 'Can\\'t delete object and its deletion marker from both CLI and console.\\r\\nHi, While playing with S3 bucket that I own I uploaded some text files that are several kilobytes, and put a deletion marker afterwards.\\r\\n\\r\\nHowever, when I tried to remove the object version 1 (original file) and version 2 (deletion marker), the console just failed saying \"Delete object: Total objects:2, successful: 0(0%).\\r\\n\\r\\nI also tried cli command aws s3api delete-object --bucket storage.ik1ne --version-id NByr0gS5MAbrxhWHHgqqOSoJaC0OCzNia --key (filename).txt\\r\\n on both its version 1(upload) and 2(deletion marker), but it just outputs json output {\"VersionId\": \"version_I_previously_specified\"}.\\r\\n\\r\\nI tried this with Account that has AdministratorAccess but failed. Also, all modification I did to bucket policy was public/private settings(i.e. did not specify deletion policy, etc).\\r\\n\\r\\nEven the \"Empty Bucket\" command on console also fails.\\r\\n\\r\\nJust in case, the filename was \"면접질문.txt\" and \"면접질문리스트.txt\", which is 2-bit character. (Yes, actually I have two files with same symptoms).\\r\\n\\r\\nWhat am I missing? To make it sure, I uploaded another file to the same bucket and tried to reproduce the issue but that file works out exactly expected(both file and deletion marker gets deleted).', \"Re: Can't delete object and its deletion marker from both CLI and console.\\r\\nHi,\\r\\n\\r\\nI see that your bucket is now empty. In case you still face the same issue, please share the bucket name and object name with me over private message and I will be able to assist you further.\\r\\n\\r\\nAlso, you can setup a lifecycle policy to expire all (or filter) the objects in your S3 bucket.\\r\\n\\r\\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\\r\\n\\r\\nThanks,\", \"Re: Can't delete object and its deletion marker from both CLI and console.\\r\\nI tried this on chrome and it worked, so I think it was just safari webkit and Mac terminal encoding bug. Thank you.\", \"Unable to delete S3 event notification\\r\\nHi,\\r\\n\\r\\nI'm trying to delete an S3 event notification but I get this error:\\r\\n\\r\\nUnable to validate the following destination configurations. Not authorized to invoke function [arn:aws:lambda:eu-west-1:XXXXXXXX:function:myLambdaFunction]. (arn:aws:lambda:eu-west-1:XXXXXXXX:function:myLambdaFunction, null)\\r\\n\\r\\n\\r\\nI was using this S3 event as a trigger for a Lambda function that I have since deleted. I thought that might be the issue and recreated the Lambda function (with the same name), but this did not solve the issue.\\r\\n\\r\\nThank you.\", \"Re: Unable to delete S3 event notification\\r\\nHi,\\r\\n\\r\\nI understand that you are not able to delete S3 event notification and you are getting the above mentioned error. From the error message, it looks like you have another event rule with the destination as 'arn:aws:lambda:eu-west-1:XXXXXXXX:function:myLambdaFunction' and your S3 bucket does not have the permission to invoke that Lambda function.\\r\\n\\r\\nI would suggest you to please verify the same and if the issue persists, please share the bucket name and complete error message with me over private message and I would be able to troubleshoot further.\\r\\n\\r\\nThanks,\", \"S3 bucket with versioning: lifecycle expiration rules not applied\\r\\nWe have an S3 bucket with replication and versioning enabled, about 60K. A rule should delete previous versions older than 3 days, but I can still see weeks old versions on this source bucket. The same rule applied to the destination bucket works fine. The rule has been applied weeks ago to both buckets.\\r\\nIs there anything obvious I might have missed related to replication, versioning etc?\\r\\nSee attached configuration for expiration. No path set, entire bucket selected.\\r\\n\\r\\nThanks,\\r\\n\\r\\nLuigi\\r\\n\\r\\nEdited by: lclemente on Jan 29, 2019 12:42 AM\\r\\nAfter I posted this message the bucket lifecycle worked. I don't know if AWS fixed it.\", 'Re: S3 bucket with versioning: lifecycle expiration rules not applied\\r\\nHi Luigi,\\r\\n\\r\\nI understand that lifecycle rule was not executed for your bucket but it worked now.\\r\\n\\r\\nPlease note that when an object reaches the end of its lifetime, Amazon S3 queues it for removal and removes it asynchronously. There may be a delay between the expiration date and the date at which Amazon S3 removes an object. You are not charged for storage time associated with an object that has expired. \\r\\n\\r\\n To find when an object is scheduled to expire, use the HEAD Object https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html or the GET Object API https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html operations. These API operations return response headers that provide this information. \\r\\n\\r\\nPlease refer to the following document for more details: https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-expire-general-considerations.html\\r\\n\\r\\nThanks,', 'S3 presigned post URL fails with `405 Client Error: Method Not Allowed for`\\r\\nI\\'ve been struggling all day to try and get the \"presigned post url\" feature working with S3 but keep running into errors. I have no problem making the request, but it seems that the response from the API is pointing at a domain that doesn\\'t allow POSTs.', 'Re: S3 presigned post URL fails with `405 Client Error: Method Not Allowed for`\\r\\nHi,\\r\\n\\r\\nI would like to inform you that you cannot make POST request to Object endpoints eg. (https://bucketname.s3.amazonaws.com/object.ext). In order to make POST request, you will need to make POST request to bucket endpoint (eg. https://bucketname.s3.amazonaws.com/) and specify the object key name and other properties in the multipart/form-data encoded message body. Please refer to the following document for more details: https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html\\r\\n\\r\\nPlease refer to the following document for an example on browser-based upload using HTTP POST: https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-post-example.html\\r\\n\\r\\nThanks,', 'Glacier to Physical Medium\\r\\nDoes Amazon offer any service that would transfer some of our Glacier content to a physical, encrypted medium such as tape and ship it to us?  Or are there any third party companies that might?', 'AWS Lambda Write Image to S3 Access Denied\\r\\nI’m trying to get a DeepLens Lambda function to upload an image to S3:\\r\\n\\r\\nresponse = s3.put_object(ACL=\\'public-read\\', Body=jpg_data.tostring(),Bucket=‘MY-BUCKET-NAME’,Key=file_name)\\r\\n\\r\\n\\r\\nHowever, I keep getting the error:\\r\\n\\r\\nError in face detection lambda: An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\\r\\n\\r\\n\\r\\nI made an IAM role and attached it to the Deeplens lambda function and attached the following policies: AWSDeepLensLambdaFunctionAccessPolicy, AWSLambdaExecute, AWSDeepLensServiceRolePolicy, AmazonS3FullAccess, and a custom policy with the following JSON:\\r\\n\\r\\n{\\r\\n    \"Version\": \"2012-10-17\",\\r\\n    \"Statement\": [\\r\\n        {\\r\\n            \"Effect\": \"Allow\",\\r\\n            \"Action\": [\\r\\n                \"s3:PutObject\",\\r\\n                \"s3:PutObjectAcl\"\\r\\n            ],\\r\\n            \"Resource\": [\\r\\n                \"arn:aws:s3:::MY-BUCKET-NAME”,\\r\\n                \"arn:aws:s3:::MY-BUCKET-NAME/*\"\\r\\n            ]\\r\\n        }\\r\\n    ]\\r\\n}\\r\\n\\r\\n\\r\\nI even gave the bucket public access through the access control list and made the bucket policy public:\\r\\n\\r\\n{\\r\\n    \"Version\": \"2012-10-17\",\\r\\n    \"Id\": \"Policy1534108093104\",\\r\\n    \"Statement\": [\\r\\n        {\\r\\n            \"Sid\": \"Stmt1534108083533\",\\r\\n            \"Effect\": \"Allow\",\\r\\n            \"Principal\": \"*\",\\r\\n            \"Action\": \"s3:*\",\\r\\n            \"Resource\": \"arn:aws:s3:::MY-BUCKET-NAME”\\r\\n        }\\r\\n    ]\\r\\n}\\r\\n\\r\\n\\r\\nBut I’m still getting the AccessDenied error.', 'Re: AWS Lambda Write Image to S3 Access Denied\\r\\nDid you ever get this to work? I am having the same issue.\\r\\n\\r\\nI got my function to write to the S3 bucket by change the public policy of \"Block new public ACLs and uploading public objects\" to false but this is not an ideal setup.', 'S3 throws error with latest curl in amz1 when getObject\\r\\nGreetings!\\r\\n\\r\\nSince updating curl to version curl-7.61.1-7.91.amzn1.i686 getObject(SDK) throws the following error when retrieving a file with Content-Encoding=UTF-8 and Content-type=text/html. Does not happen with previous version curl-7.53.1-16.86.amzn1.i686 which works flawless:\\r\\n\\r\\n.....\\r\\nexception \\'Aws\\\\S3\\\\Exception\\\\S3Exception\\' with message \\'Error executing \"GetObject\" on \"XXXXXXXX\"; AWS HTTP error: cURL error 61: Unrecognized content encoding type. libcurl understands deflate, gzip content encodings. (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)  (server): 200 OK (Request-ID: CB1F86A65ABDFF78) - \\'\\r\\n....\\r\\nexception \\'GuzzleHttp\\\\Exception\\\\RequestException\\' with message \\'cURL error 61: Unrecognized content encoding type. libcurl understands deflate, gzip content encodings. (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)\\r\\n.....', \"Re: S3 throws error with latest curl in amz1 when getObject\\r\\nI suffer form the same issue. I succeeded in getting this issue work by using 'http'    => ['decode_content' => false],\\r\\n in the S3 client constructor, like $client = new S3Client([\\r\\n    'region'  => 'us-west-2',\\r\\n    'version' => 'latest',\\r\\n    'http'    => ['decode_content' => false],\\r\\n]);\", 'Re: S3 throws error with latest curl in amz1 when getObject\\r\\nThanks for the tip, it worked.', 'sync/copy objects from different cloud providers\\r\\nHi,\\r\\nIs there any tool that I can use to copy objects from other cloud providers for example IBM cloud object store to AWS S3?\\r\\nAFAIK rclone is one of the tool. I wanted to check if we still can achieve this using aws cli or s3cmd\\r\\n\\r\\nThanks,\\r\\nNithin', \"Storage help\\r\\nI am helping set up storage for our municipal document scanning and archiving department. We aren't big and I'm not the best. Amazon offers the storage I want but the interface is more than I'm trained on or have time to learn. I haven't found any good third party apps that I can use to transfer the data. I know asking this here is likely not the best, commercial plugs are likely not appreciated, but can anyone (maybe an Amazon employee) point me to a good secure app or a service for me to use?\\r\\n\\r\\nScott\", 'Need suggestions on renaming large amount of objects\\r\\nHi, we need to rename ~10 million objects (~2.5TB in total) on S3 in the same location, same bucket.\\r\\nIs there any better way than copying them to new names and deleting the original ones?\\r\\n\\r\\nthanks in advance!\\r\\n\\r\\nEdited by: xpli on Feb 19, 2019 8:55 AM', \"Extending the date on S3 object lock\\r\\nHi,\\r\\n\\r\\nThe documentation is unclear on how you can extend the object lock date with the API. I think the API guides have not been updated maybe?\\r\\n\\r\\nI've tried:\\r\\n1) HTTP POST on object with new x-amz-object-lock-retain-until-date header. \\r\\nthis is disallowed right off the bat, no POST allowed\\r\\n2) HTTP PUT with no length/data\\r\\nthis is disallowed, length required (I don't have the source data anymore)\\r\\n\\r\\nI see that in general, object metadata can be changed with a Copy request. I'm not sure if this applies to object lock metadata, but that isn't quite what we want; our use case is to extend retention on objects frequently even if the original retention has not yet expired. We only want to pay for one versions' worth of storage, of course...\\r\\n\\r\\nAny pointers are appreciated!\", \"S3 - how to properly exeed days that file is avaiable in S3 in lifecycle ru\\r\\nHello,\\r\\n\\r\\nI have S3 bucket with lifecycle rule that transists objects with proper tag to Amazon Glacier some days after creation.\\r\\nAfter object upload, I attach this tag to it.\\r\\nI would like to exeed that number of days for single object.\\r\\n\\r\\nWhat is the most efficient way of achiving this?\\r\\nCan I create lifecycle rule that is applied not for days after creation but let's say days after last touch/tag creation ?\\r\\nDo I need to create lifecycle rule for each day configuration?\\r\\n\\r\\nEdited by: Wyci on Feb 15, 2019 12:15 AM\", \"Account Suspended & Reactivation for Billing?\\r\\nHi -\\r\\n\\r\\nMy account was suspended\\xa0for billing purposes, we've paid the outstanding invoices.  I opened a support ticket yesterday morning (> 24hrs ago) requesting re-instatement, but haven't heard back.\\r\\n\\r\\nWhat do we need to do to get this turned back on?\", 'Re: Account Suspended & Reactivation for Billing?\\r\\nHi\\r\\n\\r\\nI have escalated your concerns to the billing department. \\r\\nYou will receive an update on your account status via the support ticket.\\r\\n\\r\\nI do apologize for any inconveniences caused.\\r\\n\\r\\nRegards,\\r\\nFrancois', 'Re: Account Suspended & Reactivation for Billing?\\r\\nFrancisco,\\r\\nThe same thing is happening to me. It\\'s been over 24 hours and haven\\'t heard back. It won\\'t let me access the account because it is \"suspended\" and my website is down, yet it still seems to be charging me for using it... And even worse, my support ticket has been unassigned for over 24 hours!\\r\\nPlease help!\\r\\nSaul', 'Re: Account Suspended & Reactivation for Billing?\\r\\nHi  Saul,\\r\\n\\r\\nI sincerely apologize for the delay in responding to your support case. I confirm that the account has been reinstated.\\r\\n\\r\\nPlease respond via support case 1348555031 if you have any questions or concerns.\\r\\n\\r\\nBest regards,\\r\\nKuda', \"Re: Account Suspended & Reactivation for Billing?\\r\\nSame issue, I've opened a service case for it. Still waiting for reply.\", \"Re: Account Suspended & Reactivation for Billing?\\r\\nSame issue, I've opened a service case for it. Still waiting for reply.\", 'Re: Account Suspended & Reactivation for Billing?\\r\\nHi neciboliks,\\r\\n\\r\\nI have replied to you support case # 1354368401, should you have any further queries relating to the this please reply to the case directly.\\r\\n\\r\\nYour account has successfully been reinstated.\\r\\n\\r\\nHave a good day.', 'Re: Account Suspended & Reactivation for Billing?\\r\\nHi,\\r\\nIt\\'s been three days since I made all required payments to unsuspend my account, but I\\'m still unable to login AWS Console. In support case # 1354368401 it is stated that my account is activated and all services will be available in 30 minutes, however I still cant login to aws console and services are not working for three days. \\r\\n\\r\\nThe following message displayed when I try to login AWS Console: \"Authentication failed because your account has been suspended. Please contact AWS customer support.\"', 'Re: Account Suspended & Reactivation for Billing?\\r\\nCan someone please help me with my account.  I have paid all the bills and the account is still suspended.  I urgently need it up and running.  I have made several requests since tuesday and still no response from anyone.  I really appreciate it.  Thanks', 'Re: Account Suspended & Reactivation for Billing?\\r\\nCan someone please help me with my account.  I have paid all the bills and the account is still suspended.  I urgently need it up and running.  I have made several requests since tuesday and still no response from anyone.  I really appreciate it.  Thanks', \"Re: Account Suspended & Reactivation for Billing?\\r\\nHello, the same thing happened to me as well, we haven't heard from Amazon, we opened two tickets, it is extremely important for us to reenable the account. Can you help, please? Our case number is 1360905041.\\r\\n\\r\\nThanks!\", 'Re: Account Suspended & Reactivation for Billing?\\r\\nSame deal, Please expedite the enabling of my account Case# 1361705261', 'Re: Account Suspended & Reactivation for Billing?\\r\\nI will be leaving AWS for encountering a similar such delay. 3 support tickets and 2 days later, yet still locked out of my account. \\r\\n\\r\\nWay to fail miserably at customer support Amazon, if you ever get around to reading this message!', 'Re: Account Suspended & Reactivation for Billing?\\r\\nHello, the same thing happened to me as well, extream urgent Can you help, please? Our case number is 1390544761', 'Re: Account Suspended & Reactivation for Billing?\\r\\nHi Concern,\\r\\n\\r\\n               My account has been suspended, I cleared the bill and opened a ticket to reinstate my\\r\\naccount. Since it is a mail server all our mails were down from two days. Please helpout ASAP.\\r\\n\\r\\nThanks & Regards,\\r\\nBharath', 'Re: Account Suspended & Reactivation for Billing?\\r\\nI have paid all the bills and the account is still suspended. I urgently need it up and running.\\r\\n676733943884\\r\\nPlease,help me!\\r\\n\\r\\nThanks\\r\\n\\r\\nAdriana', \"Re: Account Suspended & Reactivation for Billing?\\r\\nI have the same problem. It's been more than 24 hours. My case number is 1394053931. Thank you\", 'Re: Account Suspended & Reactivation for Billing?\\r\\nHi, I have the same problem.  I cant acces to my account, i already made the payment, i need the service a soon as posible', 'Account Login\\r\\n.', 'Re: Account Suspended & Reactivation for Billing?\\r\\nHi, I am having the same issue and having an open ticket since Friday night. Can somebody please help me?', 'Re: Account Suspended & Reactivation for Billing?\\r\\nHello all,\\r\\n\\r\\nI have my account suspended with case id #1516007081.\\r\\n\\r\\nCould someone please take a look at this?', 'Re: Account Suspended & Reactivation for Billing?\\r\\nSame here,\\r\\n\\r\\nWe have all our services down and we are wating for the reactivation. We have already paid all unpayed bills. Our case is: 1556450571\\r\\n\\r\\nPlease, we need a solution, we are losing money.\\r\\n\\r\\nThank you.', 'Re: Account Suspended & Reactivation for Billing?\\r\\nHI we had a bad credit card on file, we have since since updated billing couple days ago but account is still suspended but , opened few tickets and no response ?\\r\\n\\r\\nAccount Id:\\r\\n552859475310 \\r\\nAccount Name:\\r\\nMobile', \"Re: Account Suspended & Reactivation for Billing?\\r\\nHi\\r\\n\\r\\nMy account was suspended because the credit card failed to process, but I now payed the outstanding amount. \\r\\n\\r\\nCan you please reactivate my account asap? It's been over 6 hours and I still haven't heard back (Case 1635447631, Account Nr: 361721873029)\\r\\n\\r\\nThanks\\r\\n\\r\\nEdited by: coolasdf on Jan 27, 2016 7:10 PM\\r\\n\\r\\nEdited by: coolasdf on Jan 27, 2016 7:11 PM\", \"Re: Account Suspended & Reactivation for Billing?\\r\\nThe same thing happens with me. We've paid the outstanding invoices. I opened a support ticket requesting re-instatement, but haven't heard back. My ticket support/Case ID is 1640316161. \\r\\nWhat do we need to do to get this turned back on?\", \"Is S3 the correct Service for my project?\\r\\nHello Lads!\\r\\n\\r\\nI'm currently working on a schoolproject where i basically want to send data from a pi to a cloud storage once every eight hours (approximately). The data is pretty primitive ( probably just an array and a affiliated Timestamp). Furthermore the data should be stored in the cloud and it should just be possible for the sender of the data to look at the data. (probably from an mobile app i'll add on later)\\r\\n\\r\\nSo basically my question is:\\r\\n\\r\\nIs Amazon S3 the correct service for those requirements? Do i need another service to implement these functionalities? What is the simplest way to do so? \\r\\n\\r\\nThank you for your advice in advance.\", 'Re: Is S3 the correct Service for my project?\\r\\nHello\\r\\n\\r\\nfrom my understanding of what you want to do, you will need\\r\\n1. Api gateway \\r\\n2. Lambda\\r\\n3. S3 and/or DynamoDB\\r\\n\\r\\nApi gateway receives your api call and passes that to a lambda function which writes the data to S3 or dynamoDB.  If you save the data in S3, each time you write somethign, it will become an object.  If you save it to dynamodb, it will be a new entry in the db which is faster and easier to retrieve but has extra cost.\\r\\nto read, it will be api gateway, lambda, and read from S3 requires knowing the object name to get to the content.  To read from dynamo, you need the key to get it or get all entries in the db (which costs more)\\r\\n\\r\\nhope this helps,\\r\\nRT', \"Re: Is S3 the correct Service for my project?\\r\\nOk so i've set up my DynamoDB and can add entries with a java program. Can you tell me how i can make an api which downloads certain entries using lambda and api gateway? googled but couldnt really find an solution.\\r\\n\\r\\nEdited by: Bautista on Feb 14, 2019 1:05 PM\", 'Unable to check whether the bucket is public using API call\\r\\nHello,\\r\\nI am testing API operations on AWS S3 buckets and by calling GET BucketPolicyStatus API operation (https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETVersion.html) using Postman I am not able to get response saying whether my bucket is public or not.\\r\\nI tried these 2 requests:\\r\\nRequest 1:\\r\\nGET https://bucket-name.s3.us-east-1.amazonaws.com/?policyStatus\\r\\nResponse 1:\\r\\n<Error>\\r\\n    <Code>NoSuchBucketPolicy</Code>\\r\\n    <Message>The bucket policy does not exist</Message>\\r\\n    <BucketName>bucket-name</BucketName>\\r\\n    <RequestId>...</RequestId>\\r\\n    <HostId>...</HostId>\\r\\n</Error>\\r\\n\\r\\nRequest 2:\\r\\nGET https://bucket-name.s3.us-east-1.amazonaws.com/bucket-name?policyStatus\\r\\nResponse 2:\\r\\n<Error>\\r\\n    <Code>NoSuchKey</Code>\\r\\n    <Message>The specified key does not exist.</Message>\\r\\n    <Key>bucket-name</Key>\\r\\n    <RequestId>...</RequestId>\\r\\n    <HostId>...</HostId>\\r\\n</Error>\\r\\n\\r\\nCould you please explain me what am I doing wrong?\\r\\nThanks in advance.', 'Re: Unable to check whether the bucket is public using API call\\r\\nHello\\r\\nthe request should be\\r\\nGET /<bucket-name>?policyStatus HTTP/1.1\\r\\nHost: <bucket-name>.s3.amazonaws.com\\r\\nx-amz-date: <Thu, 15 Nov 2016 00:17:21 GMT>\\r\\nAuthorization: <signatureValue>\\r\\n\\r\\nnotice that there is no slash \"/\" before the ?policyStatus\\r\\n\\r\\nhere is the link for the example\\r\\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETPolicyStatus.html\\r\\n\\r\\nhope this helps\\r\\nRT', 'Re: Unable to check whether the bucket is public using API call\\r\\nHi rt-jaws,\\r\\nthe Request 2 seems pretty much the same to a request you provided, and that request throws an error for me.', 'Re: Unable to check whether the bucket is public using API call\\r\\nHello\\r\\nfrom the documentation, the request is a little bit different\\r\\nthis is your original request\\r\\nhttps://bucket-name.s3.us-east-1.amazonaws.com/bucket-name?policyStatus\\r\\n\\r\\nthe doc shows\\r\\nhttps://bucket-name.s3.amazonaws.com/bucket-name?policyStatus\\r\\n\\r\\nnotice that it does not use the region in the call\\r\\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETPolicyStatus.html\\r\\n\\r\\nRT', 'Re: Unable to check whether the bucket is public using API call\\r\\nI have tried it without the region as you suggested and I received NoSuchKey error just like I described in Request 2. So unless you managed to get the proper response with that request, then either I am wrong or the documentation is not correct in this case.\\r\\n\\r\\nEdited by: xidex on Feb 13, 2019 5:39 AM', 'What is any other used for \"log delivery groups\" ACL\\r\\nHello.\\r\\n\\r\\nI found document what \"Log Delivery Group\" ACL is used for S3 Access logging.\\r\\nDocument URL is \\r\\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html\\r\\n\\r\\nbut, ELB Access logs,Cloudtrail logs and VPC Flow logs does not used \"Log Delivery Group\"ACL?\\r\\n\\r\\nI can\\'t found what is \"Log Delivery Group\" ACL is used for other.\\r\\n\\r\\nTell me please.']\n"
     ]
    }
   ],
   "source": [
    "text = pd.read_csv('Amazon S3.tsv', delimiter='\\t', encoding='utf-8')\n",
    "\n",
    "cleantxt = []\n",
    "\n",
    "for i in text.description:\n",
    "    cleantxt.append(i)\n",
    "\n",
    "print(cleantxt[0:100]) # fraction of the full text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1) Convert all characters to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"s3 public access can not be set.\\r\\ni've edited the bucket policy, public access settings, but the bucket is not changed to public and access is denied.\\r\\n\\r\\nwhy do you see these symptoms?\\r\\n\\r\\nhttps://imgur.com/rv64vwd\\r\\n(edit public access settings)\\r\\n\\r\\nhttps://imgur.com/y3ppaqj\\r\\n(bucket policy)\", \"i can't figure out why s3 won't display my website on my domain\\r\\nerror 403 and i am not sure how to troubleshoot the error or resolve it despite reading guides\\r\\ni've looked at a couple of forums and pages but they are either irrelevant or beyond my current understanding. i cannot figure out why i keep getting a 403. i have a public bucket policy which changed my previous error of not getting a connection to the site to a 403, just forbidding traffic. i am new to aws, what am i missing?\\r\\ni only have one html file in the bucket and when i hit 'make public' it said access denied. are there other objects that i have to find?\\r\\ni was able to get into the html file permissions and when i selected 'public access' 'read object' it said access denied.\\r\\nfor whatever reason, when accessing the domain i purchased, it says 403. when i go to the amazon s3 version of my website it works.. huh. why doesn't it come up on my domain?\", \"re: i can't figure out why s3 won't display my website on my domain\\r\\ntest, ignore\", \"my aws s3 account is suspended without any notification\\r\\ntoday when i was ready to some work on my website i found all the components which are stored on amazon s3 are missing. to check the issue i tried to log in to s3 console and found out that my account is suspended.\\r\\n\\r\\ni didn't find any email regarding the suspension and i have no payment dues in my billing options.\\r\\ni have also opened a support case with case id: 5866475801 and mailed the issue but still no reply.\\r\\n\\r\\nwhat the hell is happening ? please atleast tell and help me to resolve the issue so that i can work further.\", 'unable to read more than 1000 objects from a bucket\\r\\ni am using the rest apis (not through sdk) to read the list of objects from a bucket. the bucket has more then 1000 objects. \\r\\n\\r\\nas per https://docs.aws.amazon.com/amazons3/latest/api/restbucketget.html  documentation, i am extracting the \"istruncated\" and \"nextmarker\" values from the response and if \"istruncated\" is true, i pass the value of \"nextmarker\" as \"marker\" parameter in the subsequent call.\\r\\n\\r\\nwhenever, i add the \"marker\" to the query parameter i get \"signaturedoesnotmatch\" error. i specify other parameters like \"max-keys\", \"prefix\", \"delimiter\" and they does cause any error and the call goes through fine. of course, i did uri encode the parameters. its only the \"marker\" parameter that is causing the error.\\r\\n\\r\\ni also tried to use the v2 version, https://docs.aws.amazon.com/amazons3/latest/api/v2-restbucketget.html . i get the same \"signaturedoesnotmatch\" error when using the \"continuation-token\" in the subsequent call. the \"list-type\" and \"max-keys\" parameters were ok.\\r\\n\\r\\nany help is appreciated.', 'adding expires or cache-control header for folders in s3\\r\\nthe help page here is as always quite useless: \\r\\n\\r\\nhttps://docs.aws.amazon.com/amazoncloudfront/latest/developerguide/expiration.html#expiration-individual-objects\\r\\n\\r\\nit says that in a folder, i\\'d see \"properties\" and then \"metadata\". \\r\\n\\r\\nnone of my folders in s3 have this \"metadata\". inside properties, there are cards like versioning, server logging, etc. \\r\\n\\r\\nwhere should i enter the cache-control setting? this is for cloudfront, which seems to be not getting this header from s3. i could set up this header in cloudfront, but that ui isn\\'t any more intuitive. \\r\\n\\r\\nthanks for any guidance.', 're: adding expires or cache-control header for folders in s3\\r\\nfound it in the \"change metadata\" for folders and files. the help should be updated, the documentation remains fairly hideous. found the solution on stack overflow. thanks.', 'where do i get the canonical user id\\r\\nhi,\\r\\ni am setting up a new bucket and need to assign specific iam users to the bucket.\\r\\nit is asking for the canonical user id.\\r\\nhow do i get the canonical user id for all my iam users?\\r\\nthis is so confusing.\\r\\n\\r\\nthanks', 're: where do i get the canonical user id\\r\\nhello nicolas_briant, \\r\\n\\r\\nthank you for your post, i trust you are well. \\r\\n\\r\\nregarding the canonical user id, you can find it in two ways:\\r\\n\\r\\n1) logging in as root. in the top right of the console, choose your account name or number. then choose my security credentials. you will see the canonical user id in the account identifiers section. \\r\\n\\r\\nfinding your account canonical user id:\\r\\n\\r\\nhttp://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#findingcanonicalid\\r\\n\\r\\n2) by using a listbuckets api call. when you perform a get operation on the s3 service to get a listing of all the buckets you own, the response contains an owner id element which is your canonical user id. for example:\\r\\n\\r\\naws s3api list-buckets --output text\\r\\n\\r\\nlist-buckets: \\r\\n\\r\\nhttp://docs.aws.amazon.com/cli/latest/reference/s3api/list-buckets.html\\r\\n\\r\\nbest regards. \\r\\n\\r\\njayd j.', 're: where do i get the canonical user id\\r\\nthanks jayd. i was missing \"logging in as root\".\\r\\nregards,\\r\\nnicolas', \"re: where do i get the canonical user id\\r\\ni wanted to post the following, but i wasn't allowed because my forum account had just been created in the past hour.  (some kind of spam prevention?)\\r\\n\\r\\ni'm pretty new to using aws.  i can't find my canonical id and i don't understand your instructions for looking up my id.  i have access to the aws console via my personal account.  isn't there a way to see my own canonical id via the web interface, without using a cli?\\r\\n\\r\\ni've created an s3 bucket under my organization's account.  i want to grant myself and another user access to the bucket, but i need our two canonical ids to do so.\\r\\n\\r\\ni'd appreciate step-by-step instructions for getting the ids.  assume i know nothing about aws.\\r\\n\\r\\nthanks in advance.\\r\\n\\r\\nwhile i'd still like to have explicit step-by-step instructions for finding canonical ids, the delay in forum posting inspired me to try a different approach.  or rather, the same approach over again.\\r\\n\\r\\nthat is, when i was creating the s3 bucket, i had entered my email address as one to be granted access to it.  the bucket was created, but i received an error message that i couldn't be granted access by email address for some reason.  (it was unclear as to why.)\\r\\n\\r\\nfor some reason, i decided to try using my email address again.  this time it worked and my canonical id was automatically substituted!  i entered the email address of the other person that i wanted to give access to the bucket, and his canonical id was filled in as well.  now we both have access.\\r\\n\\r\\nhowever...  when we view the permissions of the s3 bucket, we only see canonical ids.  there's no indication to whom each of the ids refer!  that's not a useful ui!  it really looks lazy.\\r\\n\\r\\nso, how can i tell which canonical id goes with each person?\", 're: where do i get the canonical user id\\r\\nhello,\\r\\nhow can i find another user canonical id?\\r\\ni created new iam user and want him to have access to s# bucket.\\r\\nthank you!', 're: where do i get the canonical user id\\r\\nevery iam user will not have their own canonical id. there will be only one canonical id per aws account. \\r\\n\\r\\nthe following link has step-by-step instructions on how to find the canonical id associated with your aws account --> https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#findingcanonicalid\\r\\n\\r\\nusing obejct/bucket acls, you can grant/restrict access to an object/bucket to your aws account or to another aws account. you cannot restrict access to bucket/object to an iam user using object/bucket acl.\\r\\n\\r\\nyou will have to use bucket policy if you want to provide/restrict access to bucket/object to an iam user in your account then you will have to use s3 bucket policy instead of acl.\\r\\n\\r\\nbucket policy examples --> https://docs.aws.amazon.com/amazons3/latest/dev/example-bucket-policies.html', 'bug in permission evaluation?\\r\\nto reproduce:\\r\\n\\r\\ncreate a role: testrole\\r\\ncreate a bucket, we\\'ll call it testbucket here\\r\\ncreate a user: testuser\\r\\nadd permissions to testuser to sts:assumerole to testrole, and to the trust relationship of testrole to allow the user to assume it\\r\\nadd an inline policy to testrole to allow actions s3:* to testbucket and testbucket/*\\r\\nadd an inline policy to testuser to allow actions s3:* to testbucket and testbucket/*\\r\\nupload a file to the bucket (via ui is fine)\\r\\ncreate a bucket policy on testbucket:\\r\\n\\r\\n\\r\\n{\\r\\n    \"version\": \"2012-10-17\",\\r\\n    \"statement\": [\\r\\n        {\\r\\n            \"sid\": \"deny-others\",\\r\\n            \"effect\": \"deny\",\\r\\n            \"notprincipal\": {\\r\\n                \"aws\": [\\r\\n                    \"arn:aws:iam::<acct id>:role/service-role/testrole\",\\r\\n                    \"arn:aws:iam::<acct id>:root\",\\r\\n                    \"arn:aws:iam::<acct id>:user/testuser\",\\r\\n                ]\\r\\n            },\\r\\n            \"notaction\": [\\r\\n                \"s3:putobject\",\\r\\n                \"s3:putobjectacl\"\\r\\n            ],\\r\\n            \"resource\": \"arn:aws:s3:::<bucket name eg testbucket>/*\"\\r\\n        }\\r\\n    ]\\r\\n}\\r\\n\\r\\n\\r\\nthe bucket policy is taken from an anonymous upload use case, but in essence its deny\\'ing access to everyone but root, our test role, and a test user to everything other than putobject and putobjectacl.\\r\\n\\r\\nso i\\'d expect that from testuser if i assumed the role testrole, that i\\'d be able to, for example, call s3:getobject on the file i uploaded.  but this gets access denied.  meanwhile if i do the same thing with testuser, it is allowed, despite both of these having the exact same set of policies.\\r\\n\\r\\nplease help!\\r\\nthanks.\\r\\n\\r\\nedited by: davidericksonfn on mar 6, 2019 7:27 pm', \"accidentally expired entire bucket contents\\r\\nso yesterday one of the files in our bucket wouldn't delete. i set a lifecycle rule to expire the object, at least i thought i did. what i actually did was name the rule the object i wanted to expire and set the scope to the entire bucket. is there any way to recover the contents?\\r\\n\\r\\nedited by: _bd on mar 6, 2019 11:46 am\", 'pentest of aws cloud application - s3\\r\\nhello,\\r\\n\\r\\nthe new policy for penetrationtests permits the penetrationtest of a list of aws services, as seen on this site: https://aws.amazon.com/security/penetration-testing/\\r\\n\\r\\ns3 is not in the list. since publicly accessible files on a s3 bucket are often a security relevant issue it should be part of a security assessment of a cloud infrastructure. i wonder if it is necessary to obtain a special permission of aws to issue s3-related requests in the context of a penetration test or is it generally forbidden for a pentester to examine s3 buckets belonging to a tested application?\\r\\n\\r\\ngenerally speaking: what am i allowed to do as a pentester performing a pentest in the aws cloud regarding the s3 bucket?\\r\\n\\r\\nkind regards\\r\\nmichael', 'preview large files in s3\\r\\ni have several thousand image files that are 1g in size each storaged in s3.  i  access them locally via storage gateway.  paging through these image files locally is slow and cumbersome.  i am looking for a way to allow me to preview these files without having to download them one at a time to view them.  is there any kind of viewer available in aws that will allow me to view these files without downloading them locally?', 'unexplained error setting up policy for s3 cross-region replication\\r\\nhi, i\\'m following the instructions on this page for setting up the roles for crr: https://docs.aws.amazon.com/amazons3/latest/dev/setting-repl-config-perm-overview.html\\r\\n\\r\\ni have a role (we\\'ll call it replrole) with the exact trust policy listed on that page, and an access policy that looks like this:\\r\\n{\\r\\n    \"version\": \"2012-10-17\",\\r\\n    \"statement\": [\\r\\n        {\\r\\n            \"effect\": \"allow\",\\r\\n            \"action\": [\\r\\n                \"s3:getreplicationconfiguration\",\\r\\n                \"s3:listbucket\",\\r\\n                \"s3:getobjectversion\",\\r\\n                \"s3:getobjectversionacl\",\\r\\n                \"s3:getobjectversiontagging\",\\r\\n                \"s3:replicateobject\",\\r\\n                \"s3:replicatedelete\",\\r\\n                \"s3:replicatetags\"\\r\\n            ],\\r\\n            \"resource\": [\\r\\n                \"arn:aws:s3:::our-s3-bucket-prefix-*\",\\r\\n                \"arn:aws:s3:::our-s3-bucket-prefix-*/*\"\\r\\n            ]\\r\\n        }\\r\\n    ]\\r\\n}\\r\\n\\r\\n\\r\\nit doesn\\'t look exactly like the policy provided, but the main difference is that i use wildcards for the resources specified. i also group the actions together. but if i understand iam access policies correctly, this should suffice for the purpose.\\r\\n\\r\\ns3 allows me to create the cross-region replication rule, but then i get this error message:\\r\\nthe crr rule is saved, but it might not work.\\r\\nthere was an error with setting up the iam policy for the selected iam role gobscrossregionreplicationrole. ensure that you have set up the correct policy, or select another role.\\r\\n\\r\\nwhat did i do wrong?', 're: unexplained error setting up policy for s3 cross-region replication\\r\\nwhat i found was that the cross-region replication was actually working, despite the error message. when i checked back the next morning, the objects were successfully replicated to the backup bucket.', \"can't export cloudwatch logs to s3\\r\\ni'm trying to export a cloudwatch log group to s3 but i keep getting this error every time i click export data on the cloudwatch side:\\r\\n\\r\\nthe acl permission for the selected bucket is not correct. the amazon s3 bucket must reside in the same region as the log data that you want to export.\\r\\n\\r\\nthe cloudwatch log is in us-east-1. i created a bucket in each of the two us east regions: n. virginia and ohio but i still get this error when i try to export to either one of them. why?\", \"re: can't export cloudwatch logs to s3\\r\\nplease respond. this is very important and time sensitive.\", \"re: can't export cloudwatch logs to s3\\r\\ni had the exact same problem. it turns out that i neglected to populate the 's3 bucket prefix' field under the 'advanced' section of the export dialog.\\r\\n\\r\\nedited by: lbrooks on nov 30, 2018 12:17 am\", \"re: can't export cloudwatch logs to s3\\r\\nthis unfortunately still doesn't work for me either. \\r\\n\\r\\nwhat exactly did you enter as the prefix? why can't we simple save the logs to any bucket that we own as a user aka admin?\", \"re: can't export cloudwatch logs to s3\\r\\ndid you ever solve this?  how do i tell which region my cloudwatch logs are in?\\r\\n\\r\\ni am trying to do the same thing and am getting the same error.\", \"re: can't export cloudwatch logs to s3\\r\\ni had the same problem.\\r\\n\\r\\nthe solution i found was to select the bucket in the web portal, and along the top you then have a number of buttons:\\r\\noverview\\r\\nproperties\\r\\npermissions\\r\\nmanagement\\r\\n\\r\\nin permissions you can set the bucket policy to allow cloudwatch exports as detailed here: https://docs.aws.amazon.com/amazoncloudwatch/latest/logs/s3exporttasks.html\\r\\n\\r\\nhope this helps\", 're: can\\'t export cloudwatch logs to s3\\r\\nit took me 30min to figure the error. \\r\\ntheir example policy is wrong\\r\\n\\r\\nthe region in the example is the one you have to replace\\r\\nsimply replace it by: \\r\\n                \"service\": \"logs.us-east-1.amazonaws.com\"\\r\\ndo it in both fields below.\\r\\n\\r\\n{\\r\\n    \"version\": \"2012-10-17\",\\r\\n    \"statement\": [\\r\\n      {\\r\\n          \"action\": \"s3:getbucketacl\",\\r\\n          \"effect\": \"allow\",\\r\\n          \"resource\": \"arn:aws:s3:::my-exported-logs\",\\r\\n          \"principal\": { \"service\": \"logs.us-east-1.amazonaws.com\" }\\r\\n      },\\r\\n      {\\r\\n          \"action\": \"s3:putobject\" ,\\r\\n          \"effect\": \"allow\",\\r\\n          \"resource\": \"arn:aws:s3:::my-exported-logs/random-string/*\",\\r\\n          \"condition\": { \"stringequals\": { \"s3:x-amz-acl\": \"bucket-owner-full-control\" } },\\r\\n          \"principal\": { \"service\": \"logs.us-east-1.amazonaws.com\" }\\r\\n      }\\r\\n    ]\\r\\n}', 'amazon s3 slow on large buckets\\r\\nhi,\\r\\ni got a big amazon s3 bucket around 8 gb and now things started to get slow. uploading files (with iam user, so the api) takes forever. now i recreated a new bucket with the same properties and settings and api uploading to this bucket is way faster. is there a connection between a large bucket and slow api uploading? do i have to make new buckets all the time?\\r\\n\\r\\ntime to write an image of around 300 kb with api to large bucket:\\r\\n\\r\\nimageget: 1260ms\\r\\nimagewrite: 67683ms\\r\\n\\r\\nto a newly created bucket time:\\r\\n\\r\\nimageget: 1275ms\\r\\nimagewrite: 822ms\\r\\n\\r\\non https://docs.aws.amazon.com/amazons3/latest/dev/bucketrestrictions.html it is stated:\\r\\n\\r\\nthere is no limit to the number of objects that can be stored in a bucket and no difference in performance whether you use many buckets or just a few. you can store all of your objects in a single bucket, or you can organize them across several buckets.\\r\\n\\r\\nfor me this does not seem the case, anyone?\\r\\n\\r\\nregards,\\r\\n\\r\\ndick goosen\\r\\n\\r\\nedited by: dickgoosen on feb 1, 2019 12:49 am\\r\\n\\r\\nedited by: dickgoosen on feb 1, 2019 1:22 am', 're: amazon s3 slow on large buckets\\r\\nhi there!\\r\\n\\r\\ni have the same issue. the small image takes 2 min by uploading to s3 from ec2.', 'unable to set the acl to public read properly on objects stored in bucket\\r\\nthe bucket permissions are set to public, and when i upload image files from the angular web application, i include in the header of the put request \"x-amz-acl\": \"public-read\" as described by the aws documentation. when i attempt to view the images through the s3 link to the file however, a 403 forbidden error is returned. \\r\\nif i go into the bucket and examine the image, it shows there are no access permissions, and if i try to make the image public, i get an \"access denied\" message. what can i do to resolve this issue?  \\r\\ni\\'ve attached some images to the post, the first one shows the headers included in the put request for uploading, the second shows the bucket policy, and the third image shows the current state of permissions for an image i uploaded.', 'password protect s3 mount\\r\\nusing storage gateway, we\\'ve created an s3 mount, which we connect to using the recommended: \"mount -o nolock...\" command.  i know that we can limit the policy so that only certain iam users have access to the buckets, and we can then mount the buckets using something like tntdrive and enter the iam credentials that way to access the buckets, but are there any alternatives to tntdrive that would allow us to do this?  that is, when we set up a mount, is there any command like \"mount -o nolock -username=\"username\" password=\"password\" that would allow us to protect the mount?', '\\'access denied\\' when access s3 from angular app with cognito user pool\\r\\ni have s3 bucket which i configured to manage access using cognito user pool, as described here https://docs.amazonaws.cn/en_us/iam/latest/userguide/reference_policies_examples_s3_cognito-bucket.html:\\r\\n{\\r\\n    \"version\": \"2012-10-17\",\\r\\n    \"statement\": [\\r\\n        {\\r\\n            \"effect\": \"allow\",\\r\\n            \"principal\": \"*\",\\r\\n            \"action\": \"s3:listbucket\",\\r\\n            \"resource\": \"arn:aws:s3:::<bucket-name>\",\\r\\n            \"condition\": {\\r\\n                \"stringlike\": {\\r\\n                    \"s3:prefix\": \"cognito/<app-name>/\"\\r\\n                }\\r\\n            }\\r\\n        },\\r\\n        {\\r\\n            \"effect\": \"allow\",\\r\\n            \"principal\": \"*\",\\r\\n            \"action\": [\\r\\n                \"s3:putobject\",\\r\\n                \"s3:getobject\",\\r\\n                \"s3:deleteobject\"\\r\\n            ],\\r\\n            \"resource\": [\\r\\n                \"arn:aws:s3:::<bucket-name>/cognito/<app-name>/${cognito-identity.amazonaws.com:sub}*\",\\r\\n                \"arn:aws:s3:::<bucket-name>/cognito/<app-name>/${cognito-identity.amazonaws.com:sub}/*\"\\r\\n            ]\\r\\n        }\\r\\n    ]\\r\\n}\\r\\n\\r\\n\\r\\ni have angular web app which authenticate users with cognito user pool, and i\\'m using s3 client to get object. i see a call to cognito service (https://cognito-identity.eu-central-1.amazonaws.com/) is made successfully, and an identity is returned as a response, but the immediate call afterwards to s3 is failing with status code 403:\\r\\n<error><code>accessdenied</code><message>access denied</message><requestid>aac2b5fc5c74c971</requestid><hostid>l8aoygybut+y1qhtjhydrj9uxc97elsz+l6h2rqlnglpquzrqqpw532u6pixil7ypz4ugpreoss=</hostid></error>\\r\\n\\r\\n\\r\\nhere\\'s my code setting aws creds:\\r\\n    buildcognitocreds(idtokenjwt: string) {\\r\\n        let url = \\'cognito-idp.\\' + cognitoutil._region.tolowercase() + \\'.amazonaws.com/\\' + cognitoutil._user_pool_id;\\r\\n        if (environment.cognito_idp_endpoint) {\\r\\n            url = environment.cognito_idp_endpoint + \\'/\\' + cognitoutil._user_pool_id;\\r\\n        }\\r\\n        let logins: cognitoidentity.loginsmap = {};\\r\\n        logins[url] = idtokenjwt;\\r\\n        let params = {   \\r\\n            identitypoolid: cognitoutil._identity_pool_id, /* required */\\r\\n            logins: logins\\r\\n        };        \\r\\n        let serviceconfigs = <awsservice.serviceconfigurationoptions>{};\\r\\n        if (environment.cognito_identity_endpoint) {\\r\\n            serviceconfigs.endpoint = environment.cognito_identity_endpoint;\\r\\n        }\\r\\n        let creds = new aws.cognitoidentitycredentials(params, serviceconfigs);\\r\\n        this.setcognitocreds(creds);\\r\\n        return creds;\\r\\n    }\\r\\n\\r\\n\\r\\nwhat am i missing?\\r\\nno matter what i try, i\\'m getting access denied.', 's3 console shows error in access row\\r\\nalso it can\\'t generate an url for \\'download as\\'. it shows \"an error occurred generating the download link for this object.\". this happens for all aws accounts we have and for different people.', \"re: s3 console shows error in access row\\r\\nhi,\\r\\n\\r\\ni understand that you are seeing error in the in the s3 console and 'download as' option is throwing error. this usually happens when you do not proper permissions to access the bucket or object.\\r\\n\\r\\nplease ensure that you have all the necessary permissions. if you still see the same issue, please share the bucket name, object name and requester iam arn with me over private message to troubleshoot further.\\r\\n\\r\\nthanks,\", \"re: s3 console shows error in access row\\r\\nrgumber, i've sent requested information via pm several days ago.\", \"re: s3 console shows error in access row\\r\\nany news? i don't have any issue or restriction when i use aws cli. it only happens with aws console.\\r\\n\\r\\nedited by: deniskot on feb 28, 2019 2:28 am\", 'amazon s3 : \"error retrieving access type\", can\\'t use bucket\\r\\nhello,\\r\\n\\r\\ni have a problem concerning my s3 buckets. every bucket that i create (or that is created automatically by elastic beanstalk) show a \"error\" in the \"access\" column (short for \"error retrieving access type\"). i am unable to do anything with the console : i can\\'t delete the bucket, or download/upload any file. elastic beanstalk as well is unable to use s3 and it makes impossible the deployment of new version of any web application.\\r\\n\\r\\nusing the cli, i can perform any operation i want.\\r\\n\\r\\nthere is no iam on the account. i log directly with root access.\\r\\n\\r\\ndo you have any idea what the problem can be ? \\r\\n\\r\\nthank you,\\r\\n\\r\\nedouard', \"can't delete empty s3 bucket (cf-templates)\\r\\ni can't delete a s3 bucket that\\r\\n• is empty\\r\\n• has no versioning\\r\\n• has no policy\\r\\n\\r\\nthis s3 bucket has been created with some other service.\\r\\nwhen i attempt deleting the bucket in the aws console, i get an error with no message. \\r\\nand then i tried to delete via aws cli with below command,\\r\\n\\r\\naws s3 rb s3://bucketname --force\\r\\n\\r\\nit doesn't work and shows error msg like this,\\r\\n\\r\\nfatal error: an error occurred (nosuchbucket) when calling the listobjects operation: the specified bucket does not exist\\r\\n\\r\\nremove_bucket failed: unable to delete all objects in the bucket, bucket will not be deleted.\\r\\n\\r\\ni searched on forum and tried to edit bucket policy, but it also shows like this, \\r\\nerror\\r\\ndata not found\\r\\n\\r\\nhope anyone could help me with this issue.\\r\\nthanks for and have a nice day!\", 'copy from source bucket to dest bucket - getobject() stream problem\\r\\nhi all,\\r\\nmy name is eliran and i new in aws.\\r\\nmy goal is to copy from one bucket (ireland) to another bucket (n.virginia)\\r\\ni will explain my work flow:\\r\\n1) i use the cli command - aws s3 sync s3://sorce-bucket/ s3://dest-bucket/ --exclude \"logs/*\".\\r\\nthe sync completed after some time...\\r\\n2)in my app in .net i use the aws sdk and use the command getobject() like this:\\r\\namazons3client s3client = new amazons3client(globals.awsaccesskey,globals.awssecretkey, regionendpoint.useast1);\\r\\nstream rs = s3client.getobject(new getobjectrequest\\r\\n{\\r\\nbucketname = sourcecontainer,\\r\\nkey = key}).responsestream;\\r\\n\\r\\nits work fine but the responestream for some objects is md5stream (that what i need) and for some objects is cachingwarpperstream.... (its not good for me)\\r\\n\\r\\nif i use the source bucket from ireland so all the request with getobject on the same objects (like above) will return responestream md5stream! \\r\\n\\r\\n*the settings and policies is the same in both buckets.\\r\\n\\r\\nwhat goes wrong? and how i can get always md5stream from my new bucket in n.virginia.\\r\\n\\r\\nthanks a lot,\\r\\neliran kasif.', 'stockholm s3 endpoint issue\\r\\nhello,\\r\\n\\r\\ni\\'m trying to connect to an s3 bucket in the newly available eu north 1 region (stockholm) through two mac s3 compatible apps (forklift and chronosync) without success.\\r\\n\\r\\ni\\'ve used \"s3.eu-north-1.amazonaws.com\" and \"s3-eu-north-1.amazonaws.com\" endpoints to no avail. i get the following error: the authorization header is malformed; the region \\'us-east-1\\' is wrong; expecting \\'eu-north-1\\'\\r\\n\\r\\nis anyone experiencing the same issue? thanks.\\r\\n\\r\\nregards.', \"re: stockholm s3 endpoint issue\\r\\ni answer to myself: one of the mentioned apps (forklift) has been updated recently and now it works with eu north 1 region s3 buckets.\\r\\n\\r\\ni suppose the same will happen soon with chronosync app. it's actually a matter of the specific app that needs to be updated to support new regions.\", 'issue reading s3 buckets (xml parsing error)\\r\\nwhen i access my list of buckets in the web browser, i get the following error in the console: \\r\\n\\r\\nxml parsing error: no root element found\\r\\nlocation: https://us-east-1.console.aws.amazon.com/s3/proxy\\r\\nline number 1, column 1:\\r\\n\\r\\ni believe this is coming from a few buckets i had deleted but are stuck in my account. if i try to delete them again, nothing happens visually, and in the console, this error is hit again.\\r\\n\\r\\nis this something that will resolve itself in time? or does something need to be done to resolve it?\\r\\n\\r\\nthanks!', \"strange issue granting unexpected permission to single object, how to fix?\\r\\ni have two objects in a bucket, and getobject should 403 for both for role a, except inexplicably one object is accessible.\\r\\n\\r\\nthe bucket configuration is:\\r\\nblank bucket policy, all 4 options under public access settings are true, acl is defaults\\r\\n\\r\\nrole config grants no permissions to s3\\r\\n\\r\\npermissions for both objects appear identical (at least in console). \\r\\n\\r\\niam policy simulator indicates both objects will be denied, but again, in reality one object is allowed.\\r\\n\\r\\ni'm performing the getobject from javascript aws sdk using federated identities & cognito pool. role a  is associated with the logged in user's group. \\r\\n\\r\\ni've tried running cloudtrail but getobject is not being recorded when executed via aws sdk. it seems to log the event however when i manually download via console. \\r\\n\\r\\nplease help!\\r\\n\\r\\nedited by: davegravy on feb 22, 2019 5:36 am\", 'does amazon s3 or glacier has build-in fixity checking?\\r\\nhi\\r\\n\\r\\nsee https://dltj.org/article/oclc-digital-archive-vs-amazon-s3/ claimed that amazon s3 does not provide fixity check.\\r\\n\\r\\nsee https://www.slideshare.net/amazonwebservices/deep-dive-on-archiving-and-compliance page 12 from aws presentation said that it has built-in fixity checking. \\r\\n\\r\\ni can\\'t find anything mention about fixity by simply searching \"fixity\" in \\r\\nhttps://docs.aws.amazon.com/s3/index.html#lang/en_us\\r\\nhttps://docs.aws.amazon.com/glacier/index.html#lang/en_us\\r\\n\\r\\nso, does amazon s3/glacier provides fixity checking or not?', \"need suggestions on how to look up existing objects on s3\\r\\nhi, we have incoming files everyday, and we upload them to s3. the files can be duplicates from earlier days, so we need to check if they already exist before uploading.\\r\\n\\r\\nour old way is to save the filename to sdb after uploading a new file, so we can use sdb query to look up existing files.\\r\\n\\r\\nwe recently want to change it to use s3 head object api to check existence.\\r\\n\\r\\nwe need suggestions:\\r\\n(1) if there's better way beside sdb and s3 head api? any new s3 api to check existence in bulk?\\r\\n(3) is s3 head api good enough for our use case (we need look up ~200 filenames every hour during the day)\\r\\n\\r\\nthanks in advance!\", \"re: need suggestions on how to look up existing objects on s3\\r\\nhi,\\r\\n\\r\\ni use s3 pretty extensively but i'm not employed by amazon so take this as it is.\\r\\n\\r\\n that amount of head requests per hour will be totally fine, it should not be anywhere close to stressing out the service.\\r\\n\\r\\nthere isn't really a better way to check for existence of a random key. however, if you have a lot of keys at once and the keys share a path-like structure, you could do better by executing a list request on the common prefix and checking the contents. \\r\\n\\r\\nkeep in mind that s3 may not be immediately consistent. you should read the docs to fully understand the impact to your particular use case, but a couple things stick out:\\r\\n1) if you do a get/head prior to uploading, then put, then get/head -- that response will be eventually consistent i.e. not guaranteed to return that the object does exist\\r\\n2) list requests are eventually consistent\\r\\n\\r\\ngiven that and that your expectations for number of objects to check, i would recommend keeping it simple and just doing head - maybe with time-based retries to clear up the eventual consistency issue. if you make the wrong decision, you simply re-upload a duplicate, which is not data loss but just extra cost to you, so if this happens every once in a while, shouldn't be too big of a deal.\\r\\n\\r\\nhope this helps!\", 're: need suggestions on how to look up existing objects on s3\\r\\nthank you. i agreed to keep it simple is the right way to start. your suggestions are very much appreciated.', 'can\\'t delete object and its deletion marker from both cli and console.\\r\\nhi, while playing with s3 bucket that i own i uploaded some text files that are several kilobytes, and put a deletion marker afterwards.\\r\\n\\r\\nhowever, when i tried to remove the object version 1 (original file) and version 2 (deletion marker), the console just failed saying \"delete object: total objects:2, successful: 0(0%).\\r\\n\\r\\ni also tried cli command aws s3api delete-object --bucket storage.ik1ne --version-id nbyr0gs5mabrxhwhhgqqosojac0ocznia --key (filename).txt\\r\\n on both its version 1(upload) and 2(deletion marker), but it just outputs json output {\"versionid\": \"version_i_previously_specified\"}.\\r\\n\\r\\ni tried this with account that has administratoraccess but failed. also, all modification i did to bucket policy was public/private settings(i.e. did not specify deletion policy, etc).\\r\\n\\r\\neven the \"empty bucket\" command on console also fails.\\r\\n\\r\\njust in case, the filename was \"면접질문.txt\" and \"면접질문리스트.txt\", which is 2-bit character. (yes, actually i have two files with same symptoms).\\r\\n\\r\\nwhat am i missing? to make it sure, i uploaded another file to the same bucket and tried to reproduce the issue but that file works out exactly expected(both file and deletion marker gets deleted).', \"re: can't delete object and its deletion marker from both cli and console.\\r\\nhi,\\r\\n\\r\\ni see that your bucket is now empty. in case you still face the same issue, please share the bucket name and object name with me over private message and i will be able to assist you further.\\r\\n\\r\\nalso, you can setup a lifecycle policy to expire all (or filter) the objects in your s3 bucket.\\r\\n\\r\\nhttps://docs.aws.amazon.com/amazons3/latest/dev/object-lifecycle-mgmt.html\\r\\n\\r\\nthanks,\", \"re: can't delete object and its deletion marker from both cli and console.\\r\\ni tried this on chrome and it worked, so i think it was just safari webkit and mac terminal encoding bug. thank you.\", \"unable to delete s3 event notification\\r\\nhi,\\r\\n\\r\\ni'm trying to delete an s3 event notification but i get this error:\\r\\n\\r\\nunable to validate the following destination configurations. not authorized to invoke function [arn:aws:lambda:eu-west-1:xxxxxxxx:function:mylambdafunction]. (arn:aws:lambda:eu-west-1:xxxxxxxx:function:mylambdafunction, null)\\r\\n\\r\\n\\r\\ni was using this s3 event as a trigger for a lambda function that i have since deleted. i thought that might be the issue and recreated the lambda function (with the same name), but this did not solve the issue.\\r\\n\\r\\nthank you.\", \"re: unable to delete s3 event notification\\r\\nhi,\\r\\n\\r\\ni understand that you are not able to delete s3 event notification and you are getting the above mentioned error. from the error message, it looks like you have another event rule with the destination as 'arn:aws:lambda:eu-west-1:xxxxxxxx:function:mylambdafunction' and your s3 bucket does not have the permission to invoke that lambda function.\\r\\n\\r\\ni would suggest you to please verify the same and if the issue persists, please share the bucket name and complete error message with me over private message and i would be able to troubleshoot further.\\r\\n\\r\\nthanks,\", \"s3 bucket with versioning: lifecycle expiration rules not applied\\r\\nwe have an s3 bucket with replication and versioning enabled, about 60k. a rule should delete previous versions older than 3 days, but i can still see weeks old versions on this source bucket. the same rule applied to the destination bucket works fine. the rule has been applied weeks ago to both buckets.\\r\\nis there anything obvious i might have missed related to replication, versioning etc?\\r\\nsee attached configuration for expiration. no path set, entire bucket selected.\\r\\n\\r\\nthanks,\\r\\n\\r\\nluigi\\r\\n\\r\\nedited by: lclemente on jan 29, 2019 12:42 am\\r\\nafter i posted this message the bucket lifecycle worked. i don't know if aws fixed it.\", 're: s3 bucket with versioning: lifecycle expiration rules not applied\\r\\nhi luigi,\\r\\n\\r\\ni understand that lifecycle rule was not executed for your bucket but it worked now.\\r\\n\\r\\nplease note that when an object reaches the end of its lifetime, amazon s3 queues it for removal and removes it asynchronously. there may be a delay between the expiration date and the date at which amazon s3 removes an object. you are not charged for storage time associated with an object that has expired. \\r\\n\\r\\n to find when an object is scheduled to expire, use the head object https://docs.aws.amazon.com/amazons3/latest/api/restobjecthead.html or the get object api https://docs.aws.amazon.com/amazons3/latest/api/restobjectget.html operations. these api operations return response headers that provide this information. \\r\\n\\r\\nplease refer to the following document for more details: https://docs.aws.amazon.com/amazons3/latest/dev/lifecycle-expire-general-considerations.html\\r\\n\\r\\nthanks,', 's3 presigned post url fails with `405 client error: method not allowed for`\\r\\ni\\'ve been struggling all day to try and get the \"presigned post url\" feature working with s3 but keep running into errors. i have no problem making the request, but it seems that the response from the api is pointing at a domain that doesn\\'t allow posts.', 're: s3 presigned post url fails with `405 client error: method not allowed for`\\r\\nhi,\\r\\n\\r\\ni would like to inform you that you cannot make post request to object endpoints eg. (https://bucketname.s3.amazonaws.com/object.ext). in order to make post request, you will need to make post request to bucket endpoint (eg. https://bucketname.s3.amazonaws.com/) and specify the object key name and other properties in the multipart/form-data encoded message body. please refer to the following document for more details: https://docs.aws.amazon.com/amazons3/latest/api/restobjectpost.html\\r\\n\\r\\nplease refer to the following document for an example on browser-based upload using http post: https://docs.aws.amazon.com/amazons3/latest/api/sigv4-post-example.html\\r\\n\\r\\nthanks,', 'glacier to physical medium\\r\\ndoes amazon offer any service that would transfer some of our glacier content to a physical, encrypted medium such as tape and ship it to us?  or are there any third party companies that might?', 'aws lambda write image to s3 access denied\\r\\ni’m trying to get a deeplens lambda function to upload an image to s3:\\r\\n\\r\\nresponse = s3.put_object(acl=\\'public-read\\', body=jpg_data.tostring(),bucket=‘my-bucket-name’,key=file_name)\\r\\n\\r\\n\\r\\nhowever, i keep getting the error:\\r\\n\\r\\nerror in face detection lambda: an error occurred (accessdenied) when calling the putobject operation: access denied\\r\\n\\r\\n\\r\\ni made an iam role and attached it to the deeplens lambda function and attached the following policies: awsdeeplenslambdafunctionaccesspolicy, awslambdaexecute, awsdeeplensservicerolepolicy, amazons3fullaccess, and a custom policy with the following json:\\r\\n\\r\\n{\\r\\n    \"version\": \"2012-10-17\",\\r\\n    \"statement\": [\\r\\n        {\\r\\n            \"effect\": \"allow\",\\r\\n            \"action\": [\\r\\n                \"s3:putobject\",\\r\\n                \"s3:putobjectacl\"\\r\\n            ],\\r\\n            \"resource\": [\\r\\n                \"arn:aws:s3:::my-bucket-name”,\\r\\n                \"arn:aws:s3:::my-bucket-name/*\"\\r\\n            ]\\r\\n        }\\r\\n    ]\\r\\n}\\r\\n\\r\\n\\r\\ni even gave the bucket public access through the access control list and made the bucket policy public:\\r\\n\\r\\n{\\r\\n    \"version\": \"2012-10-17\",\\r\\n    \"id\": \"policy1534108093104\",\\r\\n    \"statement\": [\\r\\n        {\\r\\n            \"sid\": \"stmt1534108083533\",\\r\\n            \"effect\": \"allow\",\\r\\n            \"principal\": \"*\",\\r\\n            \"action\": \"s3:*\",\\r\\n            \"resource\": \"arn:aws:s3:::my-bucket-name”\\r\\n        }\\r\\n    ]\\r\\n}\\r\\n\\r\\n\\r\\nbut i’m still getting the accessdenied error.', 're: aws lambda write image to s3 access denied\\r\\ndid you ever get this to work? i am having the same issue.\\r\\n\\r\\ni got my function to write to the s3 bucket by change the public policy of \"block new public acls and uploading public objects\" to false but this is not an ideal setup.', 's3 throws error with latest curl in amz1 when getobject\\r\\ngreetings!\\r\\n\\r\\nsince updating curl to version curl-7.61.1-7.91.amzn1.i686 getobject(sdk) throws the following error when retrieving a file with content-encoding=utf-8 and content-type=text/html. does not happen with previous version curl-7.53.1-16.86.amzn1.i686 which works flawless:\\r\\n\\r\\n.....\\r\\nexception \\'aws\\\\s3\\\\exception\\\\s3exception\\' with message \\'error executing \"getobject\" on \"xxxxxxxx\"; aws http error: curl error 61: unrecognized content encoding type. libcurl understands deflate, gzip content encodings. (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)  (server): 200 ok (request-id: cb1f86a65abdff78) - \\'\\r\\n....\\r\\nexception \\'guzzlehttp\\\\exception\\\\requestexception\\' with message \\'curl error 61: unrecognized content encoding type. libcurl understands deflate, gzip content encodings. (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)\\r\\n.....', \"re: s3 throws error with latest curl in amz1 when getobject\\r\\ni suffer form the same issue. i succeeded in getting this issue work by using 'http'    => ['decode_content' => false],\\r\\n in the s3 client constructor, like $client = new s3client([\\r\\n    'region'  => 'us-west-2',\\r\\n    'version' => 'latest',\\r\\n    'http'    => ['decode_content' => false],\\r\\n]);\", 're: s3 throws error with latest curl in amz1 when getobject\\r\\nthanks for the tip, it worked.', 'sync/copy objects from different cloud providers\\r\\nhi,\\r\\nis there any tool that i can use to copy objects from other cloud providers for example ibm cloud object store to aws s3?\\r\\nafaik rclone is one of the tool. i wanted to check if we still can achieve this using aws cli or s3cmd\\r\\n\\r\\nthanks,\\r\\nnithin', \"storage help\\r\\ni am helping set up storage for our municipal document scanning and archiving department. we aren't big and i'm not the best. amazon offers the storage i want but the interface is more than i'm trained on or have time to learn. i haven't found any good third party apps that i can use to transfer the data. i know asking this here is likely not the best, commercial plugs are likely not appreciated, but can anyone (maybe an amazon employee) point me to a good secure app or a service for me to use?\\r\\n\\r\\nscott\", 'need suggestions on renaming large amount of objects\\r\\nhi, we need to rename ~10 million objects (~2.5tb in total) on s3 in the same location, same bucket.\\r\\nis there any better way than copying them to new names and deleting the original ones?\\r\\n\\r\\nthanks in advance!\\r\\n\\r\\nedited by: xpli on feb 19, 2019 8:55 am', \"extending the date on s3 object lock\\r\\nhi,\\r\\n\\r\\nthe documentation is unclear on how you can extend the object lock date with the api. i think the api guides have not been updated maybe?\\r\\n\\r\\ni've tried:\\r\\n1) http post on object with new x-amz-object-lock-retain-until-date header. \\r\\nthis is disallowed right off the bat, no post allowed\\r\\n2) http put with no length/data\\r\\nthis is disallowed, length required (i don't have the source data anymore)\\r\\n\\r\\ni see that in general, object metadata can be changed with a copy request. i'm not sure if this applies to object lock metadata, but that isn't quite what we want; our use case is to extend retention on objects frequently even if the original retention has not yet expired. we only want to pay for one versions' worth of storage, of course...\\r\\n\\r\\nany pointers are appreciated!\", \"s3 - how to properly exeed days that file is avaiable in s3 in lifecycle ru\\r\\nhello,\\r\\n\\r\\ni have s3 bucket with lifecycle rule that transists objects with proper tag to amazon glacier some days after creation.\\r\\nafter object upload, i attach this tag to it.\\r\\ni would like to exeed that number of days for single object.\\r\\n\\r\\nwhat is the most efficient way of achiving this?\\r\\ncan i create lifecycle rule that is applied not for days after creation but let's say days after last touch/tag creation ?\\r\\ndo i need to create lifecycle rule for each day configuration?\\r\\n\\r\\nedited by: wyci on feb 15, 2019 12:15 am\", \"account suspended & reactivation for billing?\\r\\nhi -\\r\\n\\r\\nmy account was suspended\\xa0for billing purposes, we've paid the outstanding invoices.  i opened a support ticket yesterday morning (> 24hrs ago) requesting re-instatement, but haven't heard back.\\r\\n\\r\\nwhat do we need to do to get this turned back on?\", 're: account suspended & reactivation for billing?\\r\\nhi\\r\\n\\r\\ni have escalated your concerns to the billing department. \\r\\nyou will receive an update on your account status via the support ticket.\\r\\n\\r\\ni do apologize for any inconveniences caused.\\r\\n\\r\\nregards,\\r\\nfrancois', 're: account suspended & reactivation for billing?\\r\\nfrancisco,\\r\\nthe same thing is happening to me. it\\'s been over 24 hours and haven\\'t heard back. it won\\'t let me access the account because it is \"suspended\" and my website is down, yet it still seems to be charging me for using it... and even worse, my support ticket has been unassigned for over 24 hours!\\r\\nplease help!\\r\\nsaul', 're: account suspended & reactivation for billing?\\r\\nhi  saul,\\r\\n\\r\\ni sincerely apologize for the delay in responding to your support case. i confirm that the account has been reinstated.\\r\\n\\r\\nplease respond via support case 1348555031 if you have any questions or concerns.\\r\\n\\r\\nbest regards,\\r\\nkuda', \"re: account suspended & reactivation for billing?\\r\\nsame issue, i've opened a service case for it. still waiting for reply.\", \"re: account suspended & reactivation for billing?\\r\\nsame issue, i've opened a service case for it. still waiting for reply.\", 're: account suspended & reactivation for billing?\\r\\nhi neciboliks,\\r\\n\\r\\ni have replied to you support case # 1354368401, should you have any further queries relating to the this please reply to the case directly.\\r\\n\\r\\nyour account has successfully been reinstated.\\r\\n\\r\\nhave a good day.', 're: account suspended & reactivation for billing?\\r\\nhi,\\r\\nit\\'s been three days since i made all required payments to unsuspend my account, but i\\'m still unable to login aws console. in support case # 1354368401 it is stated that my account is activated and all services will be available in 30 minutes, however i still cant login to aws console and services are not working for three days. \\r\\n\\r\\nthe following message displayed when i try to login aws console: \"authentication failed because your account has been suspended. please contact aws customer support.\"', 're: account suspended & reactivation for billing?\\r\\ncan someone please help me with my account.  i have paid all the bills and the account is still suspended.  i urgently need it up and running.  i have made several requests since tuesday and still no response from anyone.  i really appreciate it.  thanks', 're: account suspended & reactivation for billing?\\r\\ncan someone please help me with my account.  i have paid all the bills and the account is still suspended.  i urgently need it up and running.  i have made several requests since tuesday and still no response from anyone.  i really appreciate it.  thanks', \"re: account suspended & reactivation for billing?\\r\\nhello, the same thing happened to me as well, we haven't heard from amazon, we opened two tickets, it is extremely important for us to reenable the account. can you help, please? our case number is 1360905041.\\r\\n\\r\\nthanks!\", 're: account suspended & reactivation for billing?\\r\\nsame deal, please expedite the enabling of my account case# 1361705261', 're: account suspended & reactivation for billing?\\r\\ni will be leaving aws for encountering a similar such delay. 3 support tickets and 2 days later, yet still locked out of my account. \\r\\n\\r\\nway to fail miserably at customer support amazon, if you ever get around to reading this message!', 're: account suspended & reactivation for billing?\\r\\nhello, the same thing happened to me as well, extream urgent can you help, please? our case number is 1390544761', 're: account suspended & reactivation for billing?\\r\\nhi concern,\\r\\n\\r\\n               my account has been suspended, i cleared the bill and opened a ticket to reinstate my\\r\\naccount. since it is a mail server all our mails were down from two days. please helpout asap.\\r\\n\\r\\nthanks & regards,\\r\\nbharath', 're: account suspended & reactivation for billing?\\r\\ni have paid all the bills and the account is still suspended. i urgently need it up and running.\\r\\n676733943884\\r\\nplease,help me!\\r\\n\\r\\nthanks\\r\\n\\r\\nadriana', \"re: account suspended & reactivation for billing?\\r\\ni have the same problem. it's been more than 24 hours. my case number is 1394053931. thank you\", 're: account suspended & reactivation for billing?\\r\\nhi, i have the same problem.  i cant acces to my account, i already made the payment, i need the service a soon as posible', 'account login\\r\\n.', 're: account suspended & reactivation for billing?\\r\\nhi, i am having the same issue and having an open ticket since friday night. can somebody please help me?', 're: account suspended & reactivation for billing?\\r\\nhello all,\\r\\n\\r\\ni have my account suspended with case id #1516007081.\\r\\n\\r\\ncould someone please take a look at this?', 're: account suspended & reactivation for billing?\\r\\nsame here,\\r\\n\\r\\nwe have all our services down and we are wating for the reactivation. we have already paid all unpayed bills. our case is: 1556450571\\r\\n\\r\\nplease, we need a solution, we are losing money.\\r\\n\\r\\nthank you.', 're: account suspended & reactivation for billing?\\r\\nhi we had a bad credit card on file, we have since since updated billing couple days ago but account is still suspended but , opened few tickets and no response ?\\r\\n\\r\\naccount id:\\r\\n552859475310 \\r\\naccount name:\\r\\nmobile', \"re: account suspended & reactivation for billing?\\r\\nhi\\r\\n\\r\\nmy account was suspended because the credit card failed to process, but i now payed the outstanding amount. \\r\\n\\r\\ncan you please reactivate my account asap? it's been over 6 hours and i still haven't heard back (case 1635447631, account nr: 361721873029)\\r\\n\\r\\nthanks\\r\\n\\r\\nedited by: coolasdf on jan 27, 2016 7:10 pm\\r\\n\\r\\nedited by: coolasdf on jan 27, 2016 7:11 pm\", \"re: account suspended & reactivation for billing?\\r\\nthe same thing happens with me. we've paid the outstanding invoices. i opened a support ticket requesting re-instatement, but haven't heard back. my ticket support/case id is 1640316161. \\r\\nwhat do we need to do to get this turned back on?\", \"is s3 the correct service for my project?\\r\\nhello lads!\\r\\n\\r\\ni'm currently working on a schoolproject where i basically want to send data from a pi to a cloud storage once every eight hours (approximately). the data is pretty primitive ( probably just an array and a affiliated timestamp). furthermore the data should be stored in the cloud and it should just be possible for the sender of the data to look at the data. (probably from an mobile app i'll add on later)\\r\\n\\r\\nso basically my question is:\\r\\n\\r\\nis amazon s3 the correct service for those requirements? do i need another service to implement these functionalities? what is the simplest way to do so? \\r\\n\\r\\nthank you for your advice in advance.\", 're: is s3 the correct service for my project?\\r\\nhello\\r\\n\\r\\nfrom my understanding of what you want to do, you will need\\r\\n1. api gateway \\r\\n2. lambda\\r\\n3. s3 and/or dynamodb\\r\\n\\r\\napi gateway receives your api call and passes that to a lambda function which writes the data to s3 or dynamodb.  if you save the data in s3, each time you write somethign, it will become an object.  if you save it to dynamodb, it will be a new entry in the db which is faster and easier to retrieve but has extra cost.\\r\\nto read, it will be api gateway, lambda, and read from s3 requires knowing the object name to get to the content.  to read from dynamo, you need the key to get it or get all entries in the db (which costs more)\\r\\n\\r\\nhope this helps,\\r\\nrt', \"re: is s3 the correct service for my project?\\r\\nok so i've set up my dynamodb and can add entries with a java program. can you tell me how i can make an api which downloads certain entries using lambda and api gateway? googled but couldnt really find an solution.\\r\\n\\r\\nedited by: bautista on feb 14, 2019 1:05 pm\", 'unable to check whether the bucket is public using api call\\r\\nhello,\\r\\ni am testing api operations on aws s3 buckets and by calling get bucketpolicystatus api operation (https://docs.aws.amazon.com/amazons3/latest/api/restbucketgetversion.html) using postman i am not able to get response saying whether my bucket is public or not.\\r\\ni tried these 2 requests:\\r\\nrequest 1:\\r\\nget https://bucket-name.s3.us-east-1.amazonaws.com/?policystatus\\r\\nresponse 1:\\r\\n<error>\\r\\n    <code>nosuchbucketpolicy</code>\\r\\n    <message>the bucket policy does not exist</message>\\r\\n    <bucketname>bucket-name</bucketname>\\r\\n    <requestid>...</requestid>\\r\\n    <hostid>...</hostid>\\r\\n</error>\\r\\n\\r\\nrequest 2:\\r\\nget https://bucket-name.s3.us-east-1.amazonaws.com/bucket-name?policystatus\\r\\nresponse 2:\\r\\n<error>\\r\\n    <code>nosuchkey</code>\\r\\n    <message>the specified key does not exist.</message>\\r\\n    <key>bucket-name</key>\\r\\n    <requestid>...</requestid>\\r\\n    <hostid>...</hostid>\\r\\n</error>\\r\\n\\r\\ncould you please explain me what am i doing wrong?\\r\\nthanks in advance.', 're: unable to check whether the bucket is public using api call\\r\\nhello\\r\\nthe request should be\\r\\nget /<bucket-name>?policystatus http/1.1\\r\\nhost: <bucket-name>.s3.amazonaws.com\\r\\nx-amz-date: <thu, 15 nov 2016 00:17:21 gmt>\\r\\nauthorization: <signaturevalue>\\r\\n\\r\\nnotice that there is no slash \"/\" before the ?policystatus\\r\\n\\r\\nhere is the link for the example\\r\\nhttps://docs.aws.amazon.com/amazons3/latest/api/restbucketgetpolicystatus.html\\r\\n\\r\\nhope this helps\\r\\nrt', 're: unable to check whether the bucket is public using api call\\r\\nhi rt-jaws,\\r\\nthe request 2 seems pretty much the same to a request you provided, and that request throws an error for me.', 're: unable to check whether the bucket is public using api call\\r\\nhello\\r\\nfrom the documentation, the request is a little bit different\\r\\nthis is your original request\\r\\nhttps://bucket-name.s3.us-east-1.amazonaws.com/bucket-name?policystatus\\r\\n\\r\\nthe doc shows\\r\\nhttps://bucket-name.s3.amazonaws.com/bucket-name?policystatus\\r\\n\\r\\nnotice that it does not use the region in the call\\r\\nhttps://docs.aws.amazon.com/amazons3/latest/api/restbucketgetpolicystatus.html\\r\\n\\r\\nrt', 're: unable to check whether the bucket is public using api call\\r\\ni have tried it without the region as you suggested and i received nosuchkey error just like i described in request 2. so unless you managed to get the proper response with that request, then either i am wrong or the documentation is not correct in this case.\\r\\n\\r\\nedited by: xidex on feb 13, 2019 5:39 am', 'what is any other used for \"log delivery groups\" acl\\r\\nhello.\\r\\n\\r\\ni found document what \"log delivery group\" acl is used for s3 access logging.\\r\\ndocument url is \\r\\nhttps://docs.aws.amazon.com/amazons3/latest/dev/acl-overview.html\\r\\n\\r\\nbut, elb access logs,cloudtrail logs and vpc flow logs does not used \"log delivery group\"acl?\\r\\n\\r\\ni can\\'t found what is \"log delivery group\" acl is used for other.\\r\\n\\r\\ntell me please.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(cleantxt)):\n",
    "    cleantxt[i] = cleantxt[i].lower()\n",
    "\n",
    "print(cleantxt[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2) Remove whitespace characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"s3 public access can not be set. i've edited the bucket policy, public access settings, but the bucket is not changed to public and access is denied. why do you see these symptoms? https://imgur.com/rv64vwd (edit public access settings) https://imgur.com/y3ppaqj (bucket policy)\", \"i can't figure out why s3 won't display my website on my domain error 403 and i am not sure how to troubleshoot the error or resolve it despite reading guides i've looked at a couple of forums and pages but they are either irrelevant or beyond my current understanding. i cannot figure out why i keep getting a 403. i have a public bucket policy which changed my previous error of not getting a connection to the site to a 403, just forbidding traffic. i am new to aws, what am i missing? i only have one html file in the bucket and when i hit 'make public' it said access denied. are there other objects that i have to find? i was able to get into the html file permissions and when i selected 'public access' 'read object' it said access denied. for whatever reason, when accessing the domain i purchased, it says 403. when i go to the amazon s3 version of my website it works.. huh. why doesn't it come up on my domain?\", \"re: i can't figure out why s3 won't display my website on my domain test, ignore\", \"my aws s3 account is suspended without any notification today when i was ready to some work on my website i found all the components which are stored on amazon s3 are missing. to check the issue i tried to log in to s3 console and found out that my account is suspended. i didn't find any email regarding the suspension and i have no payment dues in my billing options. i have also opened a support case with case id: 5866475801 and mailed the issue but still no reply. what the hell is happening ? please atleast tell and help me to resolve the issue so that i can work further.\", 'unable to read more than 1000 objects from a bucket i am using the rest apis (not through sdk) to read the list of objects from a bucket. the bucket has more then 1000 objects. as per https://docs.aws.amazon.com/amazons3/latest/api/restbucketget.html documentation, i am extracting the \"istruncated\" and \"nextmarker\" values from the response and if \"istruncated\" is true, i pass the value of \"nextmarker\" as \"marker\" parameter in the subsequent call. whenever, i add the \"marker\" to the query parameter i get \"signaturedoesnotmatch\" error. i specify other parameters like \"max-keys\", \"prefix\", \"delimiter\" and they does cause any error and the call goes through fine. of course, i did uri encode the parameters. its only the \"marker\" parameter that is causing the error. i also tried to use the v2 version, https://docs.aws.amazon.com/amazons3/latest/api/v2-restbucketget.html . i get the same \"signaturedoesnotmatch\" error when using the \"continuation-token\" in the subsequent call. the \"list-type\" and \"max-keys\" parameters were ok. any help is appreciated.', 'adding expires or cache-control header for folders in s3 the help page here is as always quite useless: https://docs.aws.amazon.com/amazoncloudfront/latest/developerguide/expiration.html#expiration-individual-objects it says that in a folder, i\\'d see \"properties\" and then \"metadata\". none of my folders in s3 have this \"metadata\". inside properties, there are cards like versioning, server logging, etc. where should i enter the cache-control setting? this is for cloudfront, which seems to be not getting this header from s3. i could set up this header in cloudfront, but that ui isn\\'t any more intuitive. thanks for any guidance.', 're: adding expires or cache-control header for folders in s3 found it in the \"change metadata\" for folders and files. the help should be updated, the documentation remains fairly hideous. found the solution on stack overflow. thanks.', 'where do i get the canonical user id hi, i am setting up a new bucket and need to assign specific iam users to the bucket. it is asking for the canonical user id. how do i get the canonical user id for all my iam users? this is so confusing. thanks', 're: where do i get the canonical user id hello nicolas_briant, thank you for your post, i trust you are well. regarding the canonical user id, you can find it in two ways: 1) logging in as root. in the top right of the console, choose your account name or number. then choose my security credentials. you will see the canonical user id in the account identifiers section. finding your account canonical user id: http://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#findingcanonicalid 2) by using a listbuckets api call. when you perform a get operation on the s3 service to get a listing of all the buckets you own, the response contains an owner id element which is your canonical user id. for example: aws s3api list-buckets --output text list-buckets: http://docs.aws.amazon.com/cli/latest/reference/s3api/list-buckets.html best regards. jayd j.', 're: where do i get the canonical user id thanks jayd. i was missing \"logging in as root\". regards, nicolas', \"re: where do i get the canonical user id i wanted to post the following, but i wasn't allowed because my forum account had just been created in the past hour. (some kind of spam prevention?) i'm pretty new to using aws. i can't find my canonical id and i don't understand your instructions for looking up my id. i have access to the aws console via my personal account. isn't there a way to see my own canonical id via the web interface, without using a cli? i've created an s3 bucket under my organization's account. i want to grant myself and another user access to the bucket, but i need our two canonical ids to do so. i'd appreciate step-by-step instructions for getting the ids. assume i know nothing about aws. thanks in advance. while i'd still like to have explicit step-by-step instructions for finding canonical ids, the delay in forum posting inspired me to try a different approach. or rather, the same approach over again. that is, when i was creating the s3 bucket, i had entered my email address as one to be granted access to it. the bucket was created, but i received an error message that i couldn't be granted access by email address for some reason. (it was unclear as to why.) for some reason, i decided to try using my email address again. this time it worked and my canonical id was automatically substituted! i entered the email address of the other person that i wanted to give access to the bucket, and his canonical id was filled in as well. now we both have access. however... when we view the permissions of the s3 bucket, we only see canonical ids. there's no indication to whom each of the ids refer! that's not a useful ui! it really looks lazy. so, how can i tell which canonical id goes with each person?\", 're: where do i get the canonical user id hello, how can i find another user canonical id? i created new iam user and want him to have access to s# bucket. thank you!', 're: where do i get the canonical user id every iam user will not have their own canonical id. there will be only one canonical id per aws account. the following link has step-by-step instructions on how to find the canonical id associated with your aws account --> https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#findingcanonicalid using obejct/bucket acls, you can grant/restrict access to an object/bucket to your aws account or to another aws account. you cannot restrict access to bucket/object to an iam user using object/bucket acl. you will have to use bucket policy if you want to provide/restrict access to bucket/object to an iam user in your account then you will have to use s3 bucket policy instead of acl. bucket policy examples --> https://docs.aws.amazon.com/amazons3/latest/dev/example-bucket-policies.html', 'bug in permission evaluation? to reproduce: create a role: testrole create a bucket, we\\'ll call it testbucket here create a user: testuser add permissions to testuser to sts:assumerole to testrole, and to the trust relationship of testrole to allow the user to assume it add an inline policy to testrole to allow actions s3:* to testbucket and testbucket/* add an inline policy to testuser to allow actions s3:* to testbucket and testbucket/* upload a file to the bucket (via ui is fine) create a bucket policy on testbucket: { \"version\": \"2012-10-17\", \"statement\": [ { \"sid\": \"deny-others\", \"effect\": \"deny\", \"notprincipal\": { \"aws\": [ \"arn:aws:iam::<acct id>:role/service-role/testrole\", \"arn:aws:iam::<acct id>:root\", \"arn:aws:iam::<acct id>:user/testuser\", ] }, \"notaction\": [ \"s3:putobject\", \"s3:putobjectacl\" ], \"resource\": \"arn:aws:s3:::<bucket name eg testbucket>/*\" } ] } the bucket policy is taken from an anonymous upload use case, but in essence its deny\\'ing access to everyone but root, our test role, and a test user to everything other than putobject and putobjectacl. so i\\'d expect that from testuser if i assumed the role testrole, that i\\'d be able to, for example, call s3:getobject on the file i uploaded. but this gets access denied. meanwhile if i do the same thing with testuser, it is allowed, despite both of these having the exact same set of policies. please help! thanks. edited by: davidericksonfn on mar 6, 2019 7:27 pm', \"accidentally expired entire bucket contents so yesterday one of the files in our bucket wouldn't delete. i set a lifecycle rule to expire the object, at least i thought i did. what i actually did was name the rule the object i wanted to expire and set the scope to the entire bucket. is there any way to recover the contents? edited by: _bd on mar 6, 2019 11:46 am\", 'pentest of aws cloud application - s3 hello, the new policy for penetrationtests permits the penetrationtest of a list of aws services, as seen on this site: https://aws.amazon.com/security/penetration-testing/ s3 is not in the list. since publicly accessible files on a s3 bucket are often a security relevant issue it should be part of a security assessment of a cloud infrastructure. i wonder if it is necessary to obtain a special permission of aws to issue s3-related requests in the context of a penetration test or is it generally forbidden for a pentester to examine s3 buckets belonging to a tested application? generally speaking: what am i allowed to do as a pentester performing a pentest in the aws cloud regarding the s3 bucket? kind regards michael', 'preview large files in s3 i have several thousand image files that are 1g in size each storaged in s3. i access them locally via storage gateway. paging through these image files locally is slow and cumbersome. i am looking for a way to allow me to preview these files without having to download them one at a time to view them. is there any kind of viewer available in aws that will allow me to view these files without downloading them locally?', 'unexplained error setting up policy for s3 cross-region replication hi, i\\'m following the instructions on this page for setting up the roles for crr: https://docs.aws.amazon.com/amazons3/latest/dev/setting-repl-config-perm-overview.html i have a role (we\\'ll call it replrole) with the exact trust policy listed on that page, and an access policy that looks like this: { \"version\": \"2012-10-17\", \"statement\": [ { \"effect\": \"allow\", \"action\": [ \"s3:getreplicationconfiguration\", \"s3:listbucket\", \"s3:getobjectversion\", \"s3:getobjectversionacl\", \"s3:getobjectversiontagging\", \"s3:replicateobject\", \"s3:replicatedelete\", \"s3:replicatetags\" ], \"resource\": [ \"arn:aws:s3:::our-s3-bucket-prefix-*\", \"arn:aws:s3:::our-s3-bucket-prefix-*/*\" ] } ] } it doesn\\'t look exactly like the policy provided, but the main difference is that i use wildcards for the resources specified. i also group the actions together. but if i understand iam access policies correctly, this should suffice for the purpose. s3 allows me to create the cross-region replication rule, but then i get this error message: the crr rule is saved, but it might not work. there was an error with setting up the iam policy for the selected iam role gobscrossregionreplicationrole. ensure that you have set up the correct policy, or select another role. what did i do wrong?', 're: unexplained error setting up policy for s3 cross-region replication what i found was that the cross-region replication was actually working, despite the error message. when i checked back the next morning, the objects were successfully replicated to the backup bucket.', \"can't export cloudwatch logs to s3 i'm trying to export a cloudwatch log group to s3 but i keep getting this error every time i click export data on the cloudwatch side: the acl permission for the selected bucket is not correct. the amazon s3 bucket must reside in the same region as the log data that you want to export. the cloudwatch log is in us-east-1. i created a bucket in each of the two us east regions: n. virginia and ohio but i still get this error when i try to export to either one of them. why?\", \"re: can't export cloudwatch logs to s3 please respond. this is very important and time sensitive.\", \"re: can't export cloudwatch logs to s3 i had the exact same problem. it turns out that i neglected to populate the 's3 bucket prefix' field under the 'advanced' section of the export dialog. edited by: lbrooks on nov 30, 2018 12:17 am\", \"re: can't export cloudwatch logs to s3 this unfortunately still doesn't work for me either. what exactly did you enter as the prefix? why can't we simple save the logs to any bucket that we own as a user aka admin?\", \"re: can't export cloudwatch logs to s3 did you ever solve this? how do i tell which region my cloudwatch logs are in? i am trying to do the same thing and am getting the same error.\", \"re: can't export cloudwatch logs to s3 i had the same problem. the solution i found was to select the bucket in the web portal, and along the top you then have a number of buttons: overview properties permissions management in permissions you can set the bucket policy to allow cloudwatch exports as detailed here: https://docs.aws.amazon.com/amazoncloudwatch/latest/logs/s3exporttasks.html hope this helps\", 're: can\\'t export cloudwatch logs to s3 it took me 30min to figure the error. their example policy is wrong the region in the example is the one you have to replace simply replace it by: \"service\": \"logs.us-east-1.amazonaws.com\" do it in both fields below. { \"version\": \"2012-10-17\", \"statement\": [ { \"action\": \"s3:getbucketacl\", \"effect\": \"allow\", \"resource\": \"arn:aws:s3:::my-exported-logs\", \"principal\": { \"service\": \"logs.us-east-1.amazonaws.com\" } }, { \"action\": \"s3:putobject\" , \"effect\": \"allow\", \"resource\": \"arn:aws:s3:::my-exported-logs/random-string/*\", \"condition\": { \"stringequals\": { \"s3:x-amz-acl\": \"bucket-owner-full-control\" } }, \"principal\": { \"service\": \"logs.us-east-1.amazonaws.com\" } } ] }', 'amazon s3 slow on large buckets hi, i got a big amazon s3 bucket around 8 gb and now things started to get slow. uploading files (with iam user, so the api) takes forever. now i recreated a new bucket with the same properties and settings and api uploading to this bucket is way faster. is there a connection between a large bucket and slow api uploading? do i have to make new buckets all the time? time to write an image of around 300 kb with api to large bucket: imageget: 1260ms imagewrite: 67683ms to a newly created bucket time: imageget: 1275ms imagewrite: 822ms on https://docs.aws.amazon.com/amazons3/latest/dev/bucketrestrictions.html it is stated: there is no limit to the number of objects that can be stored in a bucket and no difference in performance whether you use many buckets or just a few. you can store all of your objects in a single bucket, or you can organize them across several buckets. for me this does not seem the case, anyone? regards, dick goosen edited by: dickgoosen on feb 1, 2019 12:49 am edited by: dickgoosen on feb 1, 2019 1:22 am', 're: amazon s3 slow on large buckets hi there! i have the same issue. the small image takes 2 min by uploading to s3 from ec2.', 'unable to set the acl to public read properly on objects stored in bucket the bucket permissions are set to public, and when i upload image files from the angular web application, i include in the header of the put request \"x-amz-acl\": \"public-read\" as described by the aws documentation. when i attempt to view the images through the s3 link to the file however, a 403 forbidden error is returned. if i go into the bucket and examine the image, it shows there are no access permissions, and if i try to make the image public, i get an \"access denied\" message. what can i do to resolve this issue? i\\'ve attached some images to the post, the first one shows the headers included in the put request for uploading, the second shows the bucket policy, and the third image shows the current state of permissions for an image i uploaded.', 'password protect s3 mount using storage gateway, we\\'ve created an s3 mount, which we connect to using the recommended: \"mount -o nolock...\" command. i know that we can limit the policy so that only certain iam users have access to the buckets, and we can then mount the buckets using something like tntdrive and enter the iam credentials that way to access the buckets, but are there any alternatives to tntdrive that would allow us to do this? that is, when we set up a mount, is there any command like \"mount -o nolock -username=\"username\" password=\"password\" that would allow us to protect the mount?', '\\'access denied\\' when access s3 from angular app with cognito user pool i have s3 bucket which i configured to manage access using cognito user pool, as described here https://docs.amazonaws.cn/en_us/iam/latest/userguide/reference_policies_examples_s3_cognito-bucket.html: { \"version\": \"2012-10-17\", \"statement\": [ { \"effect\": \"allow\", \"principal\": \"*\", \"action\": \"s3:listbucket\", \"resource\": \"arn:aws:s3:::<bucket-name>\", \"condition\": { \"stringlike\": { \"s3:prefix\": \"cognito/<app-name>/\" } } }, { \"effect\": \"allow\", \"principal\": \"*\", \"action\": [ \"s3:putobject\", \"s3:getobject\", \"s3:deleteobject\" ], \"resource\": [ \"arn:aws:s3:::<bucket-name>/cognito/<app-name>/${cognito-identity.amazonaws.com:sub}*\", \"arn:aws:s3:::<bucket-name>/cognito/<app-name>/${cognito-identity.amazonaws.com:sub}/*\" ] } ] } i have angular web app which authenticate users with cognito user pool, and i\\'m using s3 client to get object. i see a call to cognito service (https://cognito-identity.eu-central-1.amazonaws.com/) is made successfully, and an identity is returned as a response, but the immediate call afterwards to s3 is failing with status code 403: <error><code>accessdenied</code><message>access denied</message><requestid>aac2b5fc5c74c971</requestid><hostid>l8aoygybut+y1qhtjhydrj9uxc97elsz+l6h2rqlnglpquzrqqpw532u6pixil7ypz4ugpreoss=</hostid></error> here\\'s my code setting aws creds: buildcognitocreds(idtokenjwt: string) { let url = \\'cognito-idp.\\' + cognitoutil._region.tolowercase() + \\'.amazonaws.com/\\' + cognitoutil._user_pool_id; if (environment.cognito_idp_endpoint) { url = environment.cognito_idp_endpoint + \\'/\\' + cognitoutil._user_pool_id; } let logins: cognitoidentity.loginsmap = {}; logins[url] = idtokenjwt; let params = { identitypoolid: cognitoutil._identity_pool_id, /* required */ logins: logins }; let serviceconfigs = <awsservice.serviceconfigurationoptions>{}; if (environment.cognito_identity_endpoint) { serviceconfigs.endpoint = environment.cognito_identity_endpoint; } let creds = new aws.cognitoidentitycredentials(params, serviceconfigs); this.setcognitocreds(creds); return creds; } what am i missing? no matter what i try, i\\'m getting access denied.', 's3 console shows error in access row also it can\\'t generate an url for \\'download as\\'. it shows \"an error occurred generating the download link for this object.\". this happens for all aws accounts we have and for different people.', \"re: s3 console shows error in access row hi, i understand that you are seeing error in the in the s3 console and 'download as' option is throwing error. this usually happens when you do not proper permissions to access the bucket or object. please ensure that you have all the necessary permissions. if you still see the same issue, please share the bucket name, object name and requester iam arn with me over private message to troubleshoot further. thanks,\", \"re: s3 console shows error in access row rgumber, i've sent requested information via pm several days ago.\", \"re: s3 console shows error in access row any news? i don't have any issue or restriction when i use aws cli. it only happens with aws console. edited by: deniskot on feb 28, 2019 2:28 am\", 'amazon s3 : \"error retrieving access type\", can\\'t use bucket hello, i have a problem concerning my s3 buckets. every bucket that i create (or that is created automatically by elastic beanstalk) show a \"error\" in the \"access\" column (short for \"error retrieving access type\"). i am unable to do anything with the console : i can\\'t delete the bucket, or download/upload any file. elastic beanstalk as well is unable to use s3 and it makes impossible the deployment of new version of any web application. using the cli, i can perform any operation i want. there is no iam on the account. i log directly with root access. do you have any idea what the problem can be ? thank you, edouard', \"can't delete empty s3 bucket (cf-templates) i can't delete a s3 bucket that • is empty • has no versioning • has no policy this s3 bucket has been created with some other service. when i attempt deleting the bucket in the aws console, i get an error with no message. and then i tried to delete via aws cli with below command, aws s3 rb s3://bucketname --force it doesn't work and shows error msg like this, fatal error: an error occurred (nosuchbucket) when calling the listobjects operation: the specified bucket does not exist remove_bucket failed: unable to delete all objects in the bucket, bucket will not be deleted. i searched on forum and tried to edit bucket policy, but it also shows like this, error data not found hope anyone could help me with this issue. thanks for and have a nice day!\", 'copy from source bucket to dest bucket - getobject() stream problem hi all, my name is eliran and i new in aws. my goal is to copy from one bucket (ireland) to another bucket (n.virginia) i will explain my work flow: 1) i use the cli command - aws s3 sync s3://sorce-bucket/ s3://dest-bucket/ --exclude \"logs/*\". the sync completed after some time... 2)in my app in .net i use the aws sdk and use the command getobject() like this: amazons3client s3client = new amazons3client(globals.awsaccesskey,globals.awssecretkey, regionendpoint.useast1); stream rs = s3client.getobject(new getobjectrequest { bucketname = sourcecontainer, key = key}).responsestream; its work fine but the responestream for some objects is md5stream (that what i need) and for some objects is cachingwarpperstream.... (its not good for me) if i use the source bucket from ireland so all the request with getobject on the same objects (like above) will return responestream md5stream! *the settings and policies is the same in both buckets. what goes wrong? and how i can get always md5stream from my new bucket in n.virginia. thanks a lot, eliran kasif.', 'stockholm s3 endpoint issue hello, i\\'m trying to connect to an s3 bucket in the newly available eu north 1 region (stockholm) through two mac s3 compatible apps (forklift and chronosync) without success. i\\'ve used \"s3.eu-north-1.amazonaws.com\" and \"s3-eu-north-1.amazonaws.com\" endpoints to no avail. i get the following error: the authorization header is malformed; the region \\'us-east-1\\' is wrong; expecting \\'eu-north-1\\' is anyone experiencing the same issue? thanks. regards.', \"re: stockholm s3 endpoint issue i answer to myself: one of the mentioned apps (forklift) has been updated recently and now it works with eu north 1 region s3 buckets. i suppose the same will happen soon with chronosync app. it's actually a matter of the specific app that needs to be updated to support new regions.\", 'issue reading s3 buckets (xml parsing error) when i access my list of buckets in the web browser, i get the following error in the console: xml parsing error: no root element found location: https://us-east-1.console.aws.amazon.com/s3/proxy line number 1, column 1: i believe this is coming from a few buckets i had deleted but are stuck in my account. if i try to delete them again, nothing happens visually, and in the console, this error is hit again. is this something that will resolve itself in time? or does something need to be done to resolve it? thanks!', \"strange issue granting unexpected permission to single object, how to fix? i have two objects in a bucket, and getobject should 403 for both for role a, except inexplicably one object is accessible. the bucket configuration is: blank bucket policy, all 4 options under public access settings are true, acl is defaults role config grants no permissions to s3 permissions for both objects appear identical (at least in console). iam policy simulator indicates both objects will be denied, but again, in reality one object is allowed. i'm performing the getobject from javascript aws sdk using federated identities & cognito pool. role a is associated with the logged in user's group. i've tried running cloudtrail but getobject is not being recorded when executed via aws sdk. it seems to log the event however when i manually download via console. please help! edited by: davegravy on feb 22, 2019 5:36 am\", 'does amazon s3 or glacier has build-in fixity checking? hi see https://dltj.org/article/oclc-digital-archive-vs-amazon-s3/ claimed that amazon s3 does not provide fixity check. see https://www.slideshare.net/amazonwebservices/deep-dive-on-archiving-and-compliance page 12 from aws presentation said that it has built-in fixity checking. i can\\'t find anything mention about fixity by simply searching \"fixity\" in https://docs.aws.amazon.com/s3/index.html#lang/en_us https://docs.aws.amazon.com/glacier/index.html#lang/en_us so, does amazon s3/glacier provides fixity checking or not?', \"need suggestions on how to look up existing objects on s3 hi, we have incoming files everyday, and we upload them to s3. the files can be duplicates from earlier days, so we need to check if they already exist before uploading. our old way is to save the filename to sdb after uploading a new file, so we can use sdb query to look up existing files. we recently want to change it to use s3 head object api to check existence. we need suggestions: (1) if there's better way beside sdb and s3 head api? any new s3 api to check existence in bulk? (3) is s3 head api good enough for our use case (we need look up ~200 filenames every hour during the day) thanks in advance!\", \"re: need suggestions on how to look up existing objects on s3 hi, i use s3 pretty extensively but i'm not employed by amazon so take this as it is. that amount of head requests per hour will be totally fine, it should not be anywhere close to stressing out the service. there isn't really a better way to check for existence of a random key. however, if you have a lot of keys at once and the keys share a path-like structure, you could do better by executing a list request on the common prefix and checking the contents. keep in mind that s3 may not be immediately consistent. you should read the docs to fully understand the impact to your particular use case, but a couple things stick out: 1) if you do a get/head prior to uploading, then put, then get/head -- that response will be eventually consistent i.e. not guaranteed to return that the object does exist 2) list requests are eventually consistent given that and that your expectations for number of objects to check, i would recommend keeping it simple and just doing head - maybe with time-based retries to clear up the eventual consistency issue. if you make the wrong decision, you simply re-upload a duplicate, which is not data loss but just extra cost to you, so if this happens every once in a while, shouldn't be too big of a deal. hope this helps!\", 're: need suggestions on how to look up existing objects on s3 thank you. i agreed to keep it simple is the right way to start. your suggestions are very much appreciated.', 'can\\'t delete object and its deletion marker from both cli and console. hi, while playing with s3 bucket that i own i uploaded some text files that are several kilobytes, and put a deletion marker afterwards. however, when i tried to remove the object version 1 (original file) and version 2 (deletion marker), the console just failed saying \"delete object: total objects:2, successful: 0(0%). i also tried cli command aws s3api delete-object --bucket storage.ik1ne --version-id nbyr0gs5mabrxhwhhgqqosojac0ocznia --key (filename).txt on both its version 1(upload) and 2(deletion marker), but it just outputs json output {\"versionid\": \"version_i_previously_specified\"}. i tried this with account that has administratoraccess but failed. also, all modification i did to bucket policy was public/private settings(i.e. did not specify deletion policy, etc). even the \"empty bucket\" command on console also fails. just in case, the filename was \"면접질문.txt\" and \"면접질문리스트.txt\", which is 2-bit character. (yes, actually i have two files with same symptoms). what am i missing? to make it sure, i uploaded another file to the same bucket and tried to reproduce the issue but that file works out exactly expected(both file and deletion marker gets deleted).', \"re: can't delete object and its deletion marker from both cli and console. hi, i see that your bucket is now empty. in case you still face the same issue, please share the bucket name and object name with me over private message and i will be able to assist you further. also, you can setup a lifecycle policy to expire all (or filter) the objects in your s3 bucket. https://docs.aws.amazon.com/amazons3/latest/dev/object-lifecycle-mgmt.html thanks,\", \"re: can't delete object and its deletion marker from both cli and console. i tried this on chrome and it worked, so i think it was just safari webkit and mac terminal encoding bug. thank you.\", \"unable to delete s3 event notification hi, i'm trying to delete an s3 event notification but i get this error: unable to validate the following destination configurations. not authorized to invoke function [arn:aws:lambda:eu-west-1:xxxxxxxx:function:mylambdafunction]. (arn:aws:lambda:eu-west-1:xxxxxxxx:function:mylambdafunction, null) i was using this s3 event as a trigger for a lambda function that i have since deleted. i thought that might be the issue and recreated the lambda function (with the same name), but this did not solve the issue. thank you.\", \"re: unable to delete s3 event notification hi, i understand that you are not able to delete s3 event notification and you are getting the above mentioned error. from the error message, it looks like you have another event rule with the destination as 'arn:aws:lambda:eu-west-1:xxxxxxxx:function:mylambdafunction' and your s3 bucket does not have the permission to invoke that lambda function. i would suggest you to please verify the same and if the issue persists, please share the bucket name and complete error message with me over private message and i would be able to troubleshoot further. thanks,\", \"s3 bucket with versioning: lifecycle expiration rules not applied we have an s3 bucket with replication and versioning enabled, about 60k. a rule should delete previous versions older than 3 days, but i can still see weeks old versions on this source bucket. the same rule applied to the destination bucket works fine. the rule has been applied weeks ago to both buckets. is there anything obvious i might have missed related to replication, versioning etc? see attached configuration for expiration. no path set, entire bucket selected. thanks, luigi edited by: lclemente on jan 29, 2019 12:42 am after i posted this message the bucket lifecycle worked. i don't know if aws fixed it.\", 're: s3 bucket with versioning: lifecycle expiration rules not applied hi luigi, i understand that lifecycle rule was not executed for your bucket but it worked now. please note that when an object reaches the end of its lifetime, amazon s3 queues it for removal and removes it asynchronously. there may be a delay between the expiration date and the date at which amazon s3 removes an object. you are not charged for storage time associated with an object that has expired. to find when an object is scheduled to expire, use the head object https://docs.aws.amazon.com/amazons3/latest/api/restobjecthead.html or the get object api https://docs.aws.amazon.com/amazons3/latest/api/restobjectget.html operations. these api operations return response headers that provide this information. please refer to the following document for more details: https://docs.aws.amazon.com/amazons3/latest/dev/lifecycle-expire-general-considerations.html thanks,', 's3 presigned post url fails with `405 client error: method not allowed for` i\\'ve been struggling all day to try and get the \"presigned post url\" feature working with s3 but keep running into errors. i have no problem making the request, but it seems that the response from the api is pointing at a domain that doesn\\'t allow posts.', 're: s3 presigned post url fails with `405 client error: method not allowed for` hi, i would like to inform you that you cannot make post request to object endpoints eg. (https://bucketname.s3.amazonaws.com/object.ext). in order to make post request, you will need to make post request to bucket endpoint (eg. https://bucketname.s3.amazonaws.com/) and specify the object key name and other properties in the multipart/form-data encoded message body. please refer to the following document for more details: https://docs.aws.amazon.com/amazons3/latest/api/restobjectpost.html please refer to the following document for an example on browser-based upload using http post: https://docs.aws.amazon.com/amazons3/latest/api/sigv4-post-example.html thanks,', 'glacier to physical medium does amazon offer any service that would transfer some of our glacier content to a physical, encrypted medium such as tape and ship it to us? or are there any third party companies that might?', 'aws lambda write image to s3 access denied i’m trying to get a deeplens lambda function to upload an image to s3: response = s3.put_object(acl=\\'public-read\\', body=jpg_data.tostring(),bucket=‘my-bucket-name’,key=file_name) however, i keep getting the error: error in face detection lambda: an error occurred (accessdenied) when calling the putobject operation: access denied i made an iam role and attached it to the deeplens lambda function and attached the following policies: awsdeeplenslambdafunctionaccesspolicy, awslambdaexecute, awsdeeplensservicerolepolicy, amazons3fullaccess, and a custom policy with the following json: { \"version\": \"2012-10-17\", \"statement\": [ { \"effect\": \"allow\", \"action\": [ \"s3:putobject\", \"s3:putobjectacl\" ], \"resource\": [ \"arn:aws:s3:::my-bucket-name”, \"arn:aws:s3:::my-bucket-name/*\" ] } ] } i even gave the bucket public access through the access control list and made the bucket policy public: { \"version\": \"2012-10-17\", \"id\": \"policy1534108093104\", \"statement\": [ { \"sid\": \"stmt1534108083533\", \"effect\": \"allow\", \"principal\": \"*\", \"action\": \"s3:*\", \"resource\": \"arn:aws:s3:::my-bucket-name” } ] } but i’m still getting the accessdenied error.', 're: aws lambda write image to s3 access denied did you ever get this to work? i am having the same issue. i got my function to write to the s3 bucket by change the public policy of \"block new public acls and uploading public objects\" to false but this is not an ideal setup.', 's3 throws error with latest curl in amz1 when getobject greetings! since updating curl to version curl-7.61.1-7.91.amzn1.i686 getobject(sdk) throws the following error when retrieving a file with content-encoding=utf-8 and content-type=text/html. does not happen with previous version curl-7.53.1-16.86.amzn1.i686 which works flawless: ..... exception \\'aws\\\\s3\\\\exception\\\\s3exception\\' with message \\'error executing \"getobject\" on \"xxxxxxxx\"; aws http error: curl error 61: unrecognized content encoding type. libcurl understands deflate, gzip content encodings. (see http://curl.haxx.se/libcurl/c/libcurl-errors.html) (server): 200 ok (request-id: cb1f86a65abdff78) - \\' .... exception \\'guzzlehttp\\\\exception\\\\requestexception\\' with message \\'curl error 61: unrecognized content encoding type. libcurl understands deflate, gzip content encodings. (see http://curl.haxx.se/libcurl/c/libcurl-errors.html) .....', \"re: s3 throws error with latest curl in amz1 when getobject i suffer form the same issue. i succeeded in getting this issue work by using 'http' => ['decode_content' => false], in the s3 client constructor, like $client = new s3client([ 'region' => 'us-west-2', 'version' => 'latest', 'http' => ['decode_content' => false], ]);\", 're: s3 throws error with latest curl in amz1 when getobject thanks for the tip, it worked.', 'sync/copy objects from different cloud providers hi, is there any tool that i can use to copy objects from other cloud providers for example ibm cloud object store to aws s3? afaik rclone is one of the tool. i wanted to check if we still can achieve this using aws cli or s3cmd thanks, nithin', \"storage help i am helping set up storage for our municipal document scanning and archiving department. we aren't big and i'm not the best. amazon offers the storage i want but the interface is more than i'm trained on or have time to learn. i haven't found any good third party apps that i can use to transfer the data. i know asking this here is likely not the best, commercial plugs are likely not appreciated, but can anyone (maybe an amazon employee) point me to a good secure app or a service for me to use? scott\", 'need suggestions on renaming large amount of objects hi, we need to rename ~10 million objects (~2.5tb in total) on s3 in the same location, same bucket. is there any better way than copying them to new names and deleting the original ones? thanks in advance! edited by: xpli on feb 19, 2019 8:55 am', \"extending the date on s3 object lock hi, the documentation is unclear on how you can extend the object lock date with the api. i think the api guides have not been updated maybe? i've tried: 1) http post on object with new x-amz-object-lock-retain-until-date header. this is disallowed right off the bat, no post allowed 2) http put with no length/data this is disallowed, length required (i don't have the source data anymore) i see that in general, object metadata can be changed with a copy request. i'm not sure if this applies to object lock metadata, but that isn't quite what we want; our use case is to extend retention on objects frequently even if the original retention has not yet expired. we only want to pay for one versions' worth of storage, of course... any pointers are appreciated!\", \"s3 - how to properly exeed days that file is avaiable in s3 in lifecycle ru hello, i have s3 bucket with lifecycle rule that transists objects with proper tag to amazon glacier some days after creation. after object upload, i attach this tag to it. i would like to exeed that number of days for single object. what is the most efficient way of achiving this? can i create lifecycle rule that is applied not for days after creation but let's say days after last touch/tag creation ? do i need to create lifecycle rule for each day configuration? edited by: wyci on feb 15, 2019 12:15 am\", \"account suspended & reactivation for billing? hi - my account was suspended for billing purposes, we've paid the outstanding invoices. i opened a support ticket yesterday morning (> 24hrs ago) requesting re-instatement, but haven't heard back. what do we need to do to get this turned back on?\", 're: account suspended & reactivation for billing? hi i have escalated your concerns to the billing department. you will receive an update on your account status via the support ticket. i do apologize for any inconveniences caused. regards, francois', 're: account suspended & reactivation for billing? francisco, the same thing is happening to me. it\\'s been over 24 hours and haven\\'t heard back. it won\\'t let me access the account because it is \"suspended\" and my website is down, yet it still seems to be charging me for using it... and even worse, my support ticket has been unassigned for over 24 hours! please help! saul', 're: account suspended & reactivation for billing? hi saul, i sincerely apologize for the delay in responding to your support case. i confirm that the account has been reinstated. please respond via support case 1348555031 if you have any questions or concerns. best regards, kuda', \"re: account suspended & reactivation for billing? same issue, i've opened a service case for it. still waiting for reply.\", \"re: account suspended & reactivation for billing? same issue, i've opened a service case for it. still waiting for reply.\", 're: account suspended & reactivation for billing? hi neciboliks, i have replied to you support case # 1354368401, should you have any further queries relating to the this please reply to the case directly. your account has successfully been reinstated. have a good day.', 're: account suspended & reactivation for billing? hi, it\\'s been three days since i made all required payments to unsuspend my account, but i\\'m still unable to login aws console. in support case # 1354368401 it is stated that my account is activated and all services will be available in 30 minutes, however i still cant login to aws console and services are not working for three days. the following message displayed when i try to login aws console: \"authentication failed because your account has been suspended. please contact aws customer support.\"', 're: account suspended & reactivation for billing? can someone please help me with my account. i have paid all the bills and the account is still suspended. i urgently need it up and running. i have made several requests since tuesday and still no response from anyone. i really appreciate it. thanks', 're: account suspended & reactivation for billing? can someone please help me with my account. i have paid all the bills and the account is still suspended. i urgently need it up and running. i have made several requests since tuesday and still no response from anyone. i really appreciate it. thanks', \"re: account suspended & reactivation for billing? hello, the same thing happened to me as well, we haven't heard from amazon, we opened two tickets, it is extremely important for us to reenable the account. can you help, please? our case number is 1360905041. thanks!\", 're: account suspended & reactivation for billing? same deal, please expedite the enabling of my account case# 1361705261', 're: account suspended & reactivation for billing? i will be leaving aws for encountering a similar such delay. 3 support tickets and 2 days later, yet still locked out of my account. way to fail miserably at customer support amazon, if you ever get around to reading this message!', 're: account suspended & reactivation for billing? hello, the same thing happened to me as well, extream urgent can you help, please? our case number is 1390544761', 're: account suspended & reactivation for billing? hi concern, my account has been suspended, i cleared the bill and opened a ticket to reinstate my account. since it is a mail server all our mails were down from two days. please helpout asap. thanks & regards, bharath', 're: account suspended & reactivation for billing? i have paid all the bills and the account is still suspended. i urgently need it up and running. 676733943884 please,help me! thanks adriana', \"re: account suspended & reactivation for billing? i have the same problem. it's been more than 24 hours. my case number is 1394053931. thank you\", 're: account suspended & reactivation for billing? hi, i have the same problem. i cant acces to my account, i already made the payment, i need the service a soon as posible', 'account login .', 're: account suspended & reactivation for billing? hi, i am having the same issue and having an open ticket since friday night. can somebody please help me?', 're: account suspended & reactivation for billing? hello all, i have my account suspended with case id #1516007081. could someone please take a look at this?', 're: account suspended & reactivation for billing? same here, we have all our services down and we are wating for the reactivation. we have already paid all unpayed bills. our case is: 1556450571 please, we need a solution, we are losing money. thank you.', 're: account suspended & reactivation for billing? hi we had a bad credit card on file, we have since since updated billing couple days ago but account is still suspended but , opened few tickets and no response ? account id: 552859475310 account name: mobile', \"re: account suspended & reactivation for billing? hi my account was suspended because the credit card failed to process, but i now payed the outstanding amount. can you please reactivate my account asap? it's been over 6 hours and i still haven't heard back (case 1635447631, account nr: 361721873029) thanks edited by: coolasdf on jan 27, 2016 7:10 pm edited by: coolasdf on jan 27, 2016 7:11 pm\", \"re: account suspended & reactivation for billing? the same thing happens with me. we've paid the outstanding invoices. i opened a support ticket requesting re-instatement, but haven't heard back. my ticket support/case id is 1640316161. what do we need to do to get this turned back on?\", \"is s3 the correct service for my project? hello lads! i'm currently working on a schoolproject where i basically want to send data from a pi to a cloud storage once every eight hours (approximately). the data is pretty primitive ( probably just an array and a affiliated timestamp). furthermore the data should be stored in the cloud and it should just be possible for the sender of the data to look at the data. (probably from an mobile app i'll add on later) so basically my question is: is amazon s3 the correct service for those requirements? do i need another service to implement these functionalities? what is the simplest way to do so? thank you for your advice in advance.\", 're: is s3 the correct service for my project? hello from my understanding of what you want to do, you will need 1. api gateway 2. lambda 3. s3 and/or dynamodb api gateway receives your api call and passes that to a lambda function which writes the data to s3 or dynamodb. if you save the data in s3, each time you write somethign, it will become an object. if you save it to dynamodb, it will be a new entry in the db which is faster and easier to retrieve but has extra cost. to read, it will be api gateway, lambda, and read from s3 requires knowing the object name to get to the content. to read from dynamo, you need the key to get it or get all entries in the db (which costs more) hope this helps, rt', \"re: is s3 the correct service for my project? ok so i've set up my dynamodb and can add entries with a java program. can you tell me how i can make an api which downloads certain entries using lambda and api gateway? googled but couldnt really find an solution. edited by: bautista on feb 14, 2019 1:05 pm\", 'unable to check whether the bucket is public using api call hello, i am testing api operations on aws s3 buckets and by calling get bucketpolicystatus api operation (https://docs.aws.amazon.com/amazons3/latest/api/restbucketgetversion.html) using postman i am not able to get response saying whether my bucket is public or not. i tried these 2 requests: request 1: get https://bucket-name.s3.us-east-1.amazonaws.com/?policystatus response 1: <error> <code>nosuchbucketpolicy</code> <message>the bucket policy does not exist</message> <bucketname>bucket-name</bucketname> <requestid>...</requestid> <hostid>...</hostid> </error> request 2: get https://bucket-name.s3.us-east-1.amazonaws.com/bucket-name?policystatus response 2: <error> <code>nosuchkey</code> <message>the specified key does not exist.</message> <key>bucket-name</key> <requestid>...</requestid> <hostid>...</hostid> </error> could you please explain me what am i doing wrong? thanks in advance.', 're: unable to check whether the bucket is public using api call hello the request should be get /<bucket-name>?policystatus http/1.1 host: <bucket-name>.s3.amazonaws.com x-amz-date: <thu, 15 nov 2016 00:17:21 gmt> authorization: <signaturevalue> notice that there is no slash \"/\" before the ?policystatus here is the link for the example https://docs.aws.amazon.com/amazons3/latest/api/restbucketgetpolicystatus.html hope this helps rt', 're: unable to check whether the bucket is public using api call hi rt-jaws, the request 2 seems pretty much the same to a request you provided, and that request throws an error for me.', 're: unable to check whether the bucket is public using api call hello from the documentation, the request is a little bit different this is your original request https://bucket-name.s3.us-east-1.amazonaws.com/bucket-name?policystatus the doc shows https://bucket-name.s3.amazonaws.com/bucket-name?policystatus notice that it does not use the region in the call https://docs.aws.amazon.com/amazons3/latest/api/restbucketgetpolicystatus.html rt', 're: unable to check whether the bucket is public using api call i have tried it without the region as you suggested and i received nosuchkey error just like i described in request 2. so unless you managed to get the proper response with that request, then either i am wrong or the documentation is not correct in this case. edited by: xidex on feb 13, 2019 5:39 am', 'what is any other used for \"log delivery groups\" acl hello. i found document what \"log delivery group\" acl is used for s3 access logging. document url is https://docs.aws.amazon.com/amazons3/latest/dev/acl-overview.html but, elb access logs,cloudtrail logs and vpc flow logs does not used \"log delivery group\"acl? i can\\'t found what is \"log delivery group\" acl is used for other. tell me please.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(cleantxt)):\n",
    "    cleantxt[i] = ' '.join(cleantxt[i].split())\n",
    "    \n",
    "print(cleantxt[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3) Remove punctuation and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"s3 public access can not be set i've edited the bucket policy public access settings but the bucket is not changed to public and access is denied why do you see these symptoms https://imgurcom/rv64vwd edit public access settings https://imgurcom/y3ppaqj bucket policy\", \"i can't figure out why s3 won't display my website on my domain error 403 and i am not sure how to troubleshoot the error or resolve it despite reading guides i've looked at a couple of forums and pages but they are either irrelevant or beyond my current understanding i cannot figure out why i keep getting a 403 i have a public bucket policy which changed my previous error of not getting a connection to the site to a 403 just forbidding traffic i am new to aws what am i missing i only have one html file in the bucket and when i hit 'make public' it said access denied are there other objects that i have to find i was able to get into the html file permissions and when i selected 'public access' 'read object' it said access denied for whatever reason when accessing the domain i purchased it says 403 when i go to the amazon s3 version of my website it works huh why doesn't it come up on my domain\", \"re: i can't figure out why s3 won't display my website on my domain test ignore\", \"my aws s3 account is suspended without any notification today when i was ready to some work on my website i found all the components which are stored on amazon s3 are missing to check the issue i tried to log in to s3 console and found out that my account is suspended i didn't find any email regarding the suspension and i have no payment dues in my billing options i have also opened a support case with case id: 5866475801 and mailed the issue but still no reply what the hell is happening  please atleast tell and help me to resolve the issue so that i can work further\", 'unable to read more than 1000 objects from a bucket i am using the rest apis not through sdk to read the list of objects from a bucket the bucket has more then 1000 objects as per https://docsawsamazoncom/amazons3/latest/api/restbucketgethtml documentation i am extracting the istruncated and nextmarker values from the response and if istruncated is true i pass the value of nextmarker as marker parameter in the subsequent call whenever i add the marker to the query parameter i get signaturedoesnotmatch error i specify other parameters like maxkeys prefix delimiter and they does cause any error and the call goes through fine of course i did uri encode the parameters its only the marker parameter that is causing the error i also tried to use the v2 version https://docsawsamazoncom/amazons3/latest/api/v2restbucketgethtml  i get the same signaturedoesnotmatch error when using the continuationtoken in the subsequent call the listtype and maxkeys parameters were ok any help is appreciated', \"adding expires or cachecontrol header for folders in s3 the help page here is as always quite useless: https://docsawsamazoncom/amazoncloudfront/latest/developerguide/expirationhtml#expirationindividualobjects it says that in a folder i'd see properties and then metadata none of my folders in s3 have this metadata inside properties there are cards like versioning server logging etc where should i enter the cachecontrol setting this is for cloudfront which seems to be not getting this header from s3 i could set up this header in cloudfront but that ui isn't any more intuitive thanks for any guidance\", 're: adding expires or cachecontrol header for folders in s3 found it in the change metadata for folders and files the help should be updated the documentation remains fairly hideous found the solution on stack overflow thanks', 'where do i get the canonical user id hi i am setting up a new bucket and need to assign specific iam users to the bucket it is asking for the canonical user id how do i get the canonical user id for all my iam users this is so confusing thanks', 're: where do i get the canonical user id hello nicolas_briant thank you for your post i trust you are well regarding the canonical user id you can find it in two ways: 1 logging in as root in the top right of the console choose your account name or number then choose my security credentials you will see the canonical user id in the account identifiers section finding your account canonical user id: http://docsawsamazoncom/general/latest/gr/acctidentifiershtml#findingcanonicalid 2 by using a listbuckets api call when you perform a get operation on the s3 service to get a listing of all the buckets you own the response contains an owner id element which is your canonical user id for example: aws s3api listbuckets output text listbuckets: http://docsawsamazoncom/cli/latest/reference/s3api/listbucketshtml best regards jayd j', 're: where do i get the canonical user id thanks jayd i was missing logging in as root regards nicolas', \"re: where do i get the canonical user id i wanted to post the following but i wasn't allowed because my forum account had just been created in the past hour some kind of spam prevention i'm pretty new to using aws i can't find my canonical id and i don't understand your instructions for looking up my id i have access to the aws console via my personal account isn't there a way to see my own canonical id via the web interface without using a cli i've created an s3 bucket under my organization's account i want to grant myself and another user access to the bucket but i need our two canonical ids to do so i'd appreciate stepbystep instructions for getting the ids assume i know nothing about aws thanks in advance while i'd still like to have explicit stepbystep instructions for finding canonical ids the delay in forum posting inspired me to try a different approach or rather the same approach over again that is when i was creating the s3 bucket i had entered my email address as one to be granted access to it the bucket was created but i received an error message that i couldn't be granted access by email address for some reason it was unclear as to why for some reason i decided to try using my email address again this time it worked and my canonical id was automatically substituted i entered the email address of the other person that i wanted to give access to the bucket and his canonical id was filled in as well now we both have access however when we view the permissions of the s3 bucket we only see canonical ids there's no indication to whom each of the ids refer that's not a useful ui it really looks lazy so how can i tell which canonical id goes with each person\", 're: where do i get the canonical user id hello how can i find another user canonical id i created new iam user and want him to have access to s# bucket thank you', 're: where do i get the canonical user id every iam user will not have their own canonical id there will be only one canonical id per aws account the following link has stepbystep instructions on how to find the canonical id associated with your aws account > https://docsawsamazoncom/general/latest/gr/acctidentifiershtml#findingcanonicalid using obejct/bucket acls you can grant/restrict access to an object/bucket to your aws account or to another aws account you cannot restrict access to bucket/object to an iam user using object/bucket acl you will have to use bucket policy if you want to provide/restrict access to bucket/object to an iam user in your account then you will have to use s3 bucket policy instead of acl bucket policy examples > https://docsawsamazoncom/amazons3/latest/dev/examplebucketpolicieshtml', \"bug in permission evaluation to reproduce: create a role: testrole create a bucket we'll call it testbucket here create a user: testuser add permissions to testuser to sts:assumerole to testrole and to the trust relationship of testrole to allow the user to assume it add an inline policy to testrole to allow actions s3:* to testbucket and testbucket/* add an inline policy to testuser to allow actions s3:* to testbucket and testbucket/* upload a file to the bucket via ui is fine create a bucket policy on testbucket: { version: 20121017 statement:  { sid: denyothers effect: deny notprincipal: { aws:  arn:aws:iam::<acct id>:role/servicerole/testrole arn:aws:iam::<acct id>:root arn:aws:iam::<acct id>:user/testuser  } notaction:  s3:putobject s3:putobjectacl  resource: arn:aws:s3:::<bucket name eg testbucket>/* }  } the bucket policy is taken from an anonymous upload use case but in essence its deny'ing access to everyone but root our test role and a test user to everything other than putobject and putobjectacl so i'd expect that from testuser if i assumed the role testrole that i'd be able to for example call s3:getobject on the file i uploaded but this gets access denied meanwhile if i do the same thing with testuser it is allowed despite both of these having the exact same set of policies please help thanks edited by: davidericksonfn on mar 6 2019 7:27 pm\", \"accidentally expired entire bucket contents so yesterday one of the files in our bucket wouldn't delete i set a lifecycle rule to expire the object at least i thought i did what i actually did was name the rule the object i wanted to expire and set the scope to the entire bucket is there any way to recover the contents edited by: _bd on mar 6 2019 11:46 am\", 'pentest of aws cloud application  s3 hello the new policy for penetrationtests permits the penetrationtest of a list of aws services as seen on this site: https://awsamazoncom/security/penetrationtesting/ s3 is not in the list since publicly accessible files on a s3 bucket are often a security relevant issue it should be part of a security assessment of a cloud infrastructure i wonder if it is necessary to obtain a special permission of aws to issue s3related requests in the context of a penetration test or is it generally forbidden for a pentester to examine s3 buckets belonging to a tested application generally speaking: what am i allowed to do as a pentester performing a pentest in the aws cloud regarding the s3 bucket kind regards michael', 'preview large files in s3 i have several thousand image files that are 1g in size each storaged in s3 i access them locally via storage gateway paging through these image files locally is slow and cumbersome i am looking for a way to allow me to preview these files without having to download them one at a time to view them is there any kind of viewer available in aws that will allow me to view these files without downloading them locally', \"unexplained error setting up policy for s3 crossregion replication hi i'm following the instructions on this page for setting up the roles for crr: https://docsawsamazoncom/amazons3/latest/dev/settingreplconfigpermoverviewhtml i have a role we'll call it replrole with the exact trust policy listed on that page and an access policy that looks like this: { version: 20121017 statement:  { effect: allow action:  s3:getreplicationconfiguration s3:listbucket s3:getobjectversion s3:getobjectversionacl s3:getobjectversiontagging s3:replicateobject s3:replicatedelete s3:replicatetags  resource:  arn:aws:s3:::ours3bucketprefix* arn:aws:s3:::ours3bucketprefix*/*  }  } it doesn't look exactly like the policy provided but the main difference is that i use wildcards for the resources specified i also group the actions together but if i understand iam access policies correctly this should suffice for the purpose s3 allows me to create the crossregion replication rule but then i get this error message: the crr rule is saved but it might not work there was an error with setting up the iam policy for the selected iam role gobscrossregionreplicationrole ensure that you have set up the correct policy or select another role what did i do wrong\", 're: unexplained error setting up policy for s3 crossregion replication what i found was that the crossregion replication was actually working despite the error message when i checked back the next morning the objects were successfully replicated to the backup bucket', \"can't export cloudwatch logs to s3 i'm trying to export a cloudwatch log group to s3 but i keep getting this error every time i click export data on the cloudwatch side: the acl permission for the selected bucket is not correct the amazon s3 bucket must reside in the same region as the log data that you want to export the cloudwatch log is in useast1 i created a bucket in each of the two us east regions: n virginia and ohio but i still get this error when i try to export to either one of them why\", \"re: can't export cloudwatch logs to s3 please respond this is very important and time sensitive\", \"re: can't export cloudwatch logs to s3 i had the exact same problem it turns out that i neglected to populate the 's3 bucket prefix' field under the 'advanced' section of the export dialog edited by: lbrooks on nov 30 2018 12:17 am\", \"re: can't export cloudwatch logs to s3 this unfortunately still doesn't work for me either what exactly did you enter as the prefix why can't we simple save the logs to any bucket that we own as a user aka admin\", \"re: can't export cloudwatch logs to s3 did you ever solve this how do i tell which region my cloudwatch logs are in i am trying to do the same thing and am getting the same error\", \"re: can't export cloudwatch logs to s3 i had the same problem the solution i found was to select the bucket in the web portal and along the top you then have a number of buttons: overview properties permissions management in permissions you can set the bucket policy to allow cloudwatch exports as detailed here: https://docsawsamazoncom/amazoncloudwatch/latest/logs/s3exporttaskshtml hope this helps\", \"re: can't export cloudwatch logs to s3 it took me 30min to figure the error their example policy is wrong the region in the example is the one you have to replace simply replace it by: service: logsuseast1amazonawscom do it in both fields below { version: 20121017 statement:  { action: s3:getbucketacl effect: allow resource: arn:aws:s3:::myexportedlogs principal: { service: logsuseast1amazonawscom } } { action: s3:putobject  effect: allow resource: arn:aws:s3:::myexportedlogs/randomstring/* condition: { stringequals: { s3:xamzacl: bucketownerfullcontrol } } principal: { service: logsuseast1amazonawscom } }  }\", 'amazon s3 slow on large buckets hi i got a big amazon s3 bucket around 8 gb and now things started to get slow uploading files with iam user so the api takes forever now i recreated a new bucket with the same properties and settings and api uploading to this bucket is way faster is there a connection between a large bucket and slow api uploading do i have to make new buckets all the time time to write an image of around 300 kb with api to large bucket: imageget: 1260ms imagewrite: 67683ms to a newly created bucket time: imageget: 1275ms imagewrite: 822ms on https://docsawsamazoncom/amazons3/latest/dev/bucketrestrictionshtml it is stated: there is no limit to the number of objects that can be stored in a bucket and no difference in performance whether you use many buckets or just a few you can store all of your objects in a single bucket or you can organize them across several buckets for me this does not seem the case anyone regards dick goosen edited by: dickgoosen on feb 1 2019 12:49 am edited by: dickgoosen on feb 1 2019 1:22 am', 're: amazon s3 slow on large buckets hi there i have the same issue the small image takes 2 min by uploading to s3 from ec2', \"unable to set the acl to public read properly on objects stored in bucket the bucket permissions are set to public and when i upload image files from the angular web application i include in the header of the put request xamzacl: publicread as described by the aws documentation when i attempt to view the images through the s3 link to the file however a 403 forbidden error is returned if i go into the bucket and examine the image it shows there are no access permissions and if i try to make the image public i get an access denied message what can i do to resolve this issue i've attached some images to the post the first one shows the headers included in the put request for uploading the second shows the bucket policy and the third image shows the current state of permissions for an image i uploaded\", \"password protect s3 mount using storage gateway we've created an s3 mount which we connect to using the recommended: mount o nolock command i know that we can limit the policy so that only certain iam users have access to the buckets and we can then mount the buckets using something like tntdrive and enter the iam credentials that way to access the buckets but are there any alternatives to tntdrive that would allow us to do this that is when we set up a mount is there any command like mount o nolock username=username password=password that would allow us to protect the mount\", \"'access denied' when access s3 from angular app with cognito user pool i have s3 bucket which i configured to manage access using cognito user pool as described here https://docsamazonawscn/en_us/iam/latest/userguide/reference_policies_examples_s3_cognitobuckethtml: { version: 20121017 statement:  { effect: allow principal: * action: s3:listbucket resource: arn:aws:s3:::<bucketname> condition: { stringlike: { s3:prefix: cognito/<appname>/ } } } { effect: allow principal: * action:  s3:putobject s3:getobject s3:deleteobject  resource:  arn:aws:s3:::<bucketname>/cognito/<appname>/${cognitoidentityamazonawscom:sub}* arn:aws:s3:::<bucketname>/cognito/<appname>/${cognitoidentityamazonawscom:sub}/*  }  } i have angular web app which authenticate users with cognito user pool and i'm using s3 client to get object i see a call to cognito service https://cognitoidentityeucentral1amazonawscom/ is made successfully and an identity is returned as a response but the immediate call afterwards to s3 is failing with status code 403: <error><code>accessdenied</code><message>access denied</message><requestid>aac2b5fc5c74c971</requestid><hostid>l8aoygybuty1qhtjhydrj9uxc97elszl6h2rqlnglpquzrqqpw532u6pixil7ypz4ugpreoss=</hostid></error> here's my code setting aws creds: buildcognitocredsidtokenjwt: string { let url = 'cognitoidp'  cognitoutil_regiontolowercase  'amazonawscom/'  cognitoutil_user_pool_id if environmentcognito_idp_endpoint { url = environmentcognito_idp_endpoint  '/'  cognitoutil_user_pool_id } let logins: cognitoidentityloginsmap = {} loginsurl = idtokenjwt let params = { identitypoolid: cognitoutil_identity_pool_id /* required */ logins: logins } let serviceconfigs = <awsserviceserviceconfigurationoptions>{} if environmentcognito_identity_endpoint { serviceconfigsendpoint = environmentcognito_identity_endpoint } let creds = new awscognitoidentitycredentialsparams serviceconfigs thissetcognitocredscreds return creds } what am i missing no matter what i try i'm getting access denied\", \"s3 console shows error in access row also it can't generate an url for 'download as' it shows an error occurred generating the download link for this object this happens for all aws accounts we have and for different people\", \"re: s3 console shows error in access row hi i understand that you are seeing error in the in the s3 console and 'download as' option is throwing error this usually happens when you do not proper permissions to access the bucket or object please ensure that you have all the necessary permissions if you still see the same issue please share the bucket name object name and requester iam arn with me over private message to troubleshoot further thanks\", \"re: s3 console shows error in access row rgumber i've sent requested information via pm several days ago\", \"re: s3 console shows error in access row any news i don't have any issue or restriction when i use aws cli it only happens with aws console edited by: deniskot on feb 28 2019 2:28 am\", \"amazon s3 : error retrieving access type can't use bucket hello i have a problem concerning my s3 buckets every bucket that i create or that is created automatically by elastic beanstalk show a error in the access column short for error retrieving access type i am unable to do anything with the console : i can't delete the bucket or download/upload any file elastic beanstalk as well is unable to use s3 and it makes impossible the deployment of new version of any web application using the cli i can perform any operation i want there is no iam on the account i log directly with root access do you have any idea what the problem can be  thank you edouard\", \"can't delete empty s3 bucket cftemplates i can't delete a s3 bucket that • is empty • has no versioning • has no policy this s3 bucket has been created with some other service when i attempt deleting the bucket in the aws console i get an error with no message and then i tried to delete via aws cli with below command aws s3 rb s3://bucketname force it doesn't work and shows error msg like this fatal error: an error occurred nosuchbucket when calling the listobjects operation: the specified bucket does not exist remove_bucket failed: unable to delete all objects in the bucket bucket will not be deleted i searched on forum and tried to edit bucket policy but it also shows like this error data not found hope anyone could help me with this issue thanks for and have a nice day\", 'copy from source bucket to dest bucket  getobject stream problem hi all my name is eliran and i new in aws my goal is to copy from one bucket ireland to another bucket nvirginia i will explain my work flow: 1 i use the cli command  aws s3 sync s3://sorcebucket/ s3://destbucket/ exclude logs/* the sync completed after some time 2in my app in net i use the aws sdk and use the command getobject like this: amazons3client s3client = new amazons3clientglobalsawsaccesskeyglobalsawssecretkey regionendpointuseast1 stream rs = s3clientgetobjectnew getobjectrequest { bucketname = sourcecontainer key = key}responsestream its work fine but the responestream for some objects is md5stream that what i need and for some objects is cachingwarpperstream its not good for me if i use the source bucket from ireland so all the request with getobject on the same objects like above will return responestream md5stream *the settings and policies is the same in both buckets what goes wrong and how i can get always md5stream from my new bucket in nvirginia thanks a lot eliran kasif', \"stockholm s3 endpoint issue hello i'm trying to connect to an s3 bucket in the newly available eu north 1 region stockholm through two mac s3 compatible apps forklift and chronosync without success i've used s3eunorth1amazonawscom and s3eunorth1amazonawscom endpoints to no avail i get the following error: the authorization header is malformed the region 'useast1' is wrong expecting 'eunorth1' is anyone experiencing the same issue thanks regards\", \"re: stockholm s3 endpoint issue i answer to myself: one of the mentioned apps forklift has been updated recently and now it works with eu north 1 region s3 buckets i suppose the same will happen soon with chronosync app it's actually a matter of the specific app that needs to be updated to support new regions\", 'issue reading s3 buckets xml parsing error when i access my list of buckets in the web browser i get the following error in the console: xml parsing error: no root element found location: https://useast1consoleawsamazoncom/s3/proxy line number 1 column 1: i believe this is coming from a few buckets i had deleted but are stuck in my account if i try to delete them again nothing happens visually and in the console this error is hit again is this something that will resolve itself in time or does something need to be done to resolve it thanks', \"strange issue granting unexpected permission to single object how to fix i have two objects in a bucket and getobject should 403 for both for role a except inexplicably one object is accessible the bucket configuration is: blank bucket policy all 4 options under public access settings are true acl is defaults role config grants no permissions to s3 permissions for both objects appear identical at least in console iam policy simulator indicates both objects will be denied but again in reality one object is allowed i'm performing the getobject from javascript aws sdk using federated identities & cognito pool role a is associated with the logged in user's group i've tried running cloudtrail but getobject is not being recorded when executed via aws sdk it seems to log the event however when i manually download via console please help edited by: davegravy on feb 22 2019 5:36 am\", \"does amazon s3 or glacier has buildin fixity checking hi see https://dltjorg/article/oclcdigitalarchivevsamazons3/ claimed that amazon s3 does not provide fixity check see https://wwwslidesharenet/amazonwebservices/deepdiveonarchivingandcompliance page 12 from aws presentation said that it has builtin fixity checking i can't find anything mention about fixity by simply searching fixity in https://docsawsamazoncom/s3/indexhtml#lang/en_us https://docsawsamazoncom/glacier/indexhtml#lang/en_us so does amazon s3/glacier provides fixity checking or not\", \"need suggestions on how to look up existing objects on s3 hi we have incoming files everyday and we upload them to s3 the files can be duplicates from earlier days so we need to check if they already exist before uploading our old way is to save the filename to sdb after uploading a new file so we can use sdb query to look up existing files we recently want to change it to use s3 head object api to check existence we need suggestions: 1 if there's better way beside sdb and s3 head api any new s3 api to check existence in bulk 3 is s3 head api good enough for our use case we need look up ~200 filenames every hour during the day thanks in advance\", \"re: need suggestions on how to look up existing objects on s3 hi i use s3 pretty extensively but i'm not employed by amazon so take this as it is that amount of head requests per hour will be totally fine it should not be anywhere close to stressing out the service there isn't really a better way to check for existence of a random key however if you have a lot of keys at once and the keys share a pathlike structure you could do better by executing a list request on the common prefix and checking the contents keep in mind that s3 may not be immediately consistent you should read the docs to fully understand the impact to your particular use case but a couple things stick out: 1 if you do a get/head prior to uploading then put then get/head  that response will be eventually consistent ie not guaranteed to return that the object does exist 2 list requests are eventually consistent given that and that your expectations for number of objects to check i would recommend keeping it simple and just doing head  maybe with timebased retries to clear up the eventual consistency issue if you make the wrong decision you simply reupload a duplicate which is not data loss but just extra cost to you so if this happens every once in a while shouldn't be too big of a deal hope this helps\", 're: need suggestions on how to look up existing objects on s3 thank you i agreed to keep it simple is the right way to start your suggestions are very much appreciated', \"can't delete object and its deletion marker from both cli and console hi while playing with s3 bucket that i own i uploaded some text files that are several kilobytes and put a deletion marker afterwards however when i tried to remove the object version 1 original file and version 2 deletion marker the console just failed saying delete object: total objects:2 successful: 00% i also tried cli command aws s3api deleteobject bucket storageik1ne versionid nbyr0gs5mabrxhwhhgqqosojac0ocznia key filenametxt on both its version 1upload and 2deletion marker but it just outputs json output {versionid: version_i_previously_specified} i tried this with account that has administratoraccess but failed also all modification i did to bucket policy was public/private settingsie did not specify deletion policy etc even the empty bucket command on console also fails just in case the filename was 면접질문txt and 면접질문리스트txt which is 2bit character yes actually i have two files with same symptoms what am i missing to make it sure i uploaded another file to the same bucket and tried to reproduce the issue but that file works out exactly expectedboth file and deletion marker gets deleted\", \"re: can't delete object and its deletion marker from both cli and console hi i see that your bucket is now empty in case you still face the same issue please share the bucket name and object name with me over private message and i will be able to assist you further also you can setup a lifecycle policy to expire all or filter the objects in your s3 bucket https://docsawsamazoncom/amazons3/latest/dev/objectlifecyclemgmthtml thanks\", \"re: can't delete object and its deletion marker from both cli and console i tried this on chrome and it worked so i think it was just safari webkit and mac terminal encoding bug thank you\", \"unable to delete s3 event notification hi i'm trying to delete an s3 event notification but i get this error: unable to validate the following destination configurations not authorized to invoke function arn:aws:lambda:euwest1:xxxxxxxx:function:mylambdafunction arn:aws:lambda:euwest1:xxxxxxxx:function:mylambdafunction null i was using this s3 event as a trigger for a lambda function that i have since deleted i thought that might be the issue and recreated the lambda function with the same name but this did not solve the issue thank you\", \"re: unable to delete s3 event notification hi i understand that you are not able to delete s3 event notification and you are getting the above mentioned error from the error message it looks like you have another event rule with the destination as 'arn:aws:lambda:euwest1:xxxxxxxx:function:mylambdafunction' and your s3 bucket does not have the permission to invoke that lambda function i would suggest you to please verify the same and if the issue persists please share the bucket name and complete error message with me over private message and i would be able to troubleshoot further thanks\", \"s3 bucket with versioning: lifecycle expiration rules not applied we have an s3 bucket with replication and versioning enabled about 60k a rule should delete previous versions older than 3 days but i can still see weeks old versions on this source bucket the same rule applied to the destination bucket works fine the rule has been applied weeks ago to both buckets is there anything obvious i might have missed related to replication versioning etc see attached configuration for expiration no path set entire bucket selected thanks luigi edited by: lclemente on jan 29 2019 12:42 am after i posted this message the bucket lifecycle worked i don't know if aws fixed it\", 're: s3 bucket with versioning: lifecycle expiration rules not applied hi luigi i understand that lifecycle rule was not executed for your bucket but it worked now please note that when an object reaches the end of its lifetime amazon s3 queues it for removal and removes it asynchronously there may be a delay between the expiration date and the date at which amazon s3 removes an object you are not charged for storage time associated with an object that has expired to find when an object is scheduled to expire use the head object https://docsawsamazoncom/amazons3/latest/api/restobjectheadhtml or the get object api https://docsawsamazoncom/amazons3/latest/api/restobjectgethtml operations these api operations return response headers that provide this information please refer to the following document for more details: https://docsawsamazoncom/amazons3/latest/dev/lifecycleexpiregeneralconsiderationshtml thanks', \"s3 presigned post url fails with `405 client error: method not allowed for` i've been struggling all day to try and get the presigned post url feature working with s3 but keep running into errors i have no problem making the request but it seems that the response from the api is pointing at a domain that doesn't allow posts\", 're: s3 presigned post url fails with `405 client error: method not allowed for` hi i would like to inform you that you cannot make post request to object endpoints eg https://bucketnames3amazonawscom/objectext in order to make post request you will need to make post request to bucket endpoint eg https://bucketnames3amazonawscom/ and specify the object key name and other properties in the multipart/formdata encoded message body please refer to the following document for more details: https://docsawsamazoncom/amazons3/latest/api/restobjectposthtml please refer to the following document for an example on browserbased upload using http post: https://docsawsamazoncom/amazons3/latest/api/sigv4postexamplehtml thanks', 'glacier to physical medium does amazon offer any service that would transfer some of our glacier content to a physical encrypted medium such as tape and ship it to us or are there any third party companies that might', \"aws lambda write image to s3 access denied i’m trying to get a deeplens lambda function to upload an image to s3: response = s3put_objectacl='publicread' body=jpg_datatostringbucket=‘mybucketname’key=file_name however i keep getting the error: error in face detection lambda: an error occurred accessdenied when calling the putobject operation: access denied i made an iam role and attached it to the deeplens lambda function and attached the following policies: awsdeeplenslambdafunctionaccesspolicy awslambdaexecute awsdeeplensservicerolepolicy amazons3fullaccess and a custom policy with the following json: { version: 20121017 statement:  { effect: allow action:  s3:putobject s3:putobjectacl  resource:  arn:aws:s3:::mybucketname” arn:aws:s3:::mybucketname/*  }  } i even gave the bucket public access through the access control list and made the bucket policy public: { version: 20121017 id: policy1534108093104 statement:  { sid: stmt1534108083533 effect: allow principal: * action: s3:* resource: arn:aws:s3:::mybucketname” }  } but i’m still getting the accessdenied error\", 're: aws lambda write image to s3 access denied did you ever get this to work i am having the same issue i got my function to write to the s3 bucket by change the public policy of block new public acls and uploading public objects to false but this is not an ideal setup', \"s3 throws error with latest curl in amz1 when getobject greetings since updating curl to version curl7611791amzn1i686 getobjectsdk throws the following error when retrieving a file with contentencoding=utf8 and contenttype=text/html does not happen with previous version curl75311686amzn1i686 which works flawless:  exception 'awss3exceptions3exception' with message 'error executing getobject on xxxxxxxx aws http error: curl error 61: unrecognized content encoding type libcurl understands deflate gzip content encodings see http://curlhaxxse/libcurl/c/libcurlerrorshtml server: 200 ok requestid: cb1f86a65abdff78  '  exception 'guzzlehttpexceptionrequestexception' with message 'curl error 61: unrecognized content encoding type libcurl understands deflate gzip content encodings see http://curlhaxxse/libcurl/c/libcurlerrorshtml \", \"re: s3 throws error with latest curl in amz1 when getobject i suffer form the same issue i succeeded in getting this issue work by using 'http' => 'decode_content' => false in the s3 client constructor like $client = new s3client 'region' => 'uswest2' 'version' => 'latest' 'http' => 'decode_content' => false \", 're: s3 throws error with latest curl in amz1 when getobject thanks for the tip it worked', 'sync/copy objects from different cloud providers hi is there any tool that i can use to copy objects from other cloud providers for example ibm cloud object store to aws s3 afaik rclone is one of the tool i wanted to check if we still can achieve this using aws cli or s3cmd thanks nithin', \"storage help i am helping set up storage for our municipal document scanning and archiving department we aren't big and i'm not the best amazon offers the storage i want but the interface is more than i'm trained on or have time to learn i haven't found any good third party apps that i can use to transfer the data i know asking this here is likely not the best commercial plugs are likely not appreciated but can anyone maybe an amazon employee point me to a good secure app or a service for me to use scott\", 'need suggestions on renaming large amount of objects hi we need to rename ~10 million objects ~25tb in total on s3 in the same location same bucket is there any better way than copying them to new names and deleting the original ones thanks in advance edited by: xpli on feb 19 2019 8:55 am', \"extending the date on s3 object lock hi the documentation is unclear on how you can extend the object lock date with the api i think the api guides have not been updated maybe i've tried: 1 http post on object with new xamzobjectlockretainuntildate header this is disallowed right off the bat no post allowed 2 http put with no length/data this is disallowed length required i don't have the source data anymore i see that in general object metadata can be changed with a copy request i'm not sure if this applies to object lock metadata but that isn't quite what we want our use case is to extend retention on objects frequently even if the original retention has not yet expired we only want to pay for one versions' worth of storage of course any pointers are appreciated\", \"s3  how to properly exeed days that file is avaiable in s3 in lifecycle ru hello i have s3 bucket with lifecycle rule that transists objects with proper tag to amazon glacier some days after creation after object upload i attach this tag to it i would like to exeed that number of days for single object what is the most efficient way of achiving this can i create lifecycle rule that is applied not for days after creation but let's say days after last touch/tag creation  do i need to create lifecycle rule for each day configuration edited by: wyci on feb 15 2019 12:15 am\", \"account suspended & reactivation for billing hi  my account was suspended for billing purposes we've paid the outstanding invoices i opened a support ticket yesterday morning > 24hrs ago requesting reinstatement but haven't heard back what do we need to do to get this turned back on\", 're: account suspended & reactivation for billing hi i have escalated your concerns to the billing department you will receive an update on your account status via the support ticket i do apologize for any inconveniences caused regards francois', \"re: account suspended & reactivation for billing francisco the same thing is happening to me it's been over 24 hours and haven't heard back it won't let me access the account because it is suspended and my website is down yet it still seems to be charging me for using it and even worse my support ticket has been unassigned for over 24 hours please help saul\", 're: account suspended & reactivation for billing hi saul i sincerely apologize for the delay in responding to your support case i confirm that the account has been reinstated please respond via support case 1348555031 if you have any questions or concerns best regards kuda', \"re: account suspended & reactivation for billing same issue i've opened a service case for it still waiting for reply\", \"re: account suspended & reactivation for billing same issue i've opened a service case for it still waiting for reply\", 're: account suspended & reactivation for billing hi neciboliks i have replied to you support case # 1354368401 should you have any further queries relating to the this please reply to the case directly your account has successfully been reinstated have a good day', \"re: account suspended & reactivation for billing hi it's been three days since i made all required payments to unsuspend my account but i'm still unable to login aws console in support case # 1354368401 it is stated that my account is activated and all services will be available in 30 minutes however i still cant login to aws console and services are not working for three days the following message displayed when i try to login aws console: authentication failed because your account has been suspended please contact aws customer support\", 're: account suspended & reactivation for billing can someone please help me with my account i have paid all the bills and the account is still suspended i urgently need it up and running i have made several requests since tuesday and still no response from anyone i really appreciate it thanks', 're: account suspended & reactivation for billing can someone please help me with my account i have paid all the bills and the account is still suspended i urgently need it up and running i have made several requests since tuesday and still no response from anyone i really appreciate it thanks', \"re: account suspended & reactivation for billing hello the same thing happened to me as well we haven't heard from amazon we opened two tickets it is extremely important for us to reenable the account can you help please our case number is 1360905041 thanks\", 're: account suspended & reactivation for billing same deal please expedite the enabling of my account case# 1361705261', 're: account suspended & reactivation for billing i will be leaving aws for encountering a similar such delay 3 support tickets and 2 days later yet still locked out of my account way to fail miserably at customer support amazon if you ever get around to reading this message', 're: account suspended & reactivation for billing hello the same thing happened to me as well extream urgent can you help please our case number is 1390544761', 're: account suspended & reactivation for billing hi concern my account has been suspended i cleared the bill and opened a ticket to reinstate my account since it is a mail server all our mails were down from two days please helpout asap thanks & regards bharath', 're: account suspended & reactivation for billing i have paid all the bills and the account is still suspended i urgently need it up and running 676733943884 pleasehelp me thanks adriana', \"re: account suspended & reactivation for billing i have the same problem it's been more than 24 hours my case number is 1394053931 thank you\", 're: account suspended & reactivation for billing hi i have the same problem i cant acces to my account i already made the payment i need the service a soon as posible', 'account login ', 're: account suspended & reactivation for billing hi i am having the same issue and having an open ticket since friday night can somebody please help me', 're: account suspended & reactivation for billing hello all i have my account suspended with case id #1516007081 could someone please take a look at this', 're: account suspended & reactivation for billing same here we have all our services down and we are wating for the reactivation we have already paid all unpayed bills our case is: 1556450571 please we need a solution we are losing money thank you', 're: account suspended & reactivation for billing hi we had a bad credit card on file we have since since updated billing couple days ago but account is still suspended but  opened few tickets and no response  account id: 552859475310 account name: mobile', \"re: account suspended & reactivation for billing hi my account was suspended because the credit card failed to process but i now payed the outstanding amount can you please reactivate my account asap it's been over 6 hours and i still haven't heard back case 1635447631 account nr: 361721873029 thanks edited by: coolasdf on jan 27 2016 7:10 pm edited by: coolasdf on jan 27 2016 7:11 pm\", \"re: account suspended & reactivation for billing the same thing happens with me we've paid the outstanding invoices i opened a support ticket requesting reinstatement but haven't heard back my ticket support/case id is 1640316161 what do we need to do to get this turned back on\", \"is s3 the correct service for my project hello lads i'm currently working on a schoolproject where i basically want to send data from a pi to a cloud storage once every eight hours approximately the data is pretty primitive  probably just an array and a affiliated timestamp furthermore the data should be stored in the cloud and it should just be possible for the sender of the data to look at the data probably from an mobile app i'll add on later so basically my question is: is amazon s3 the correct service for those requirements do i need another service to implement these functionalities what is the simplest way to do so thank you for your advice in advance\", 're: is s3 the correct service for my project hello from my understanding of what you want to do you will need 1 api gateway 2 lambda 3 s3 and/or dynamodb api gateway receives your api call and passes that to a lambda function which writes the data to s3 or dynamodb if you save the data in s3 each time you write somethign it will become an object if you save it to dynamodb it will be a new entry in the db which is faster and easier to retrieve but has extra cost to read it will be api gateway lambda and read from s3 requires knowing the object name to get to the content to read from dynamo you need the key to get it or get all entries in the db which costs more hope this helps rt', \"re: is s3 the correct service for my project ok so i've set up my dynamodb and can add entries with a java program can you tell me how i can make an api which downloads certain entries using lambda and api gateway googled but couldnt really find an solution edited by: bautista on feb 14 2019 1:05 pm\", 'unable to check whether the bucket is public using api call hello i am testing api operations on aws s3 buckets and by calling get bucketpolicystatus api operation https://docsawsamazoncom/amazons3/latest/api/restbucketgetversionhtml using postman i am not able to get response saying whether my bucket is public or not i tried these 2 requests: request 1: get https://bucketnames3useast1amazonawscom/policystatus response 1: <error> <code>nosuchbucketpolicy</code> <message>the bucket policy does not exist</message> <bucketname>bucketname</bucketname> <requestid></requestid> <hostid></hostid> </error> request 2: get https://bucketnames3useast1amazonawscom/bucketnamepolicystatus response 2: <error> <code>nosuchkey</code> <message>the specified key does not exist</message> <key>bucketname</key> <requestid></requestid> <hostid></hostid> </error> could you please explain me what am i doing wrong thanks in advance', 're: unable to check whether the bucket is public using api call hello the request should be get /<bucketname>policystatus http/11 host: <bucketname>s3amazonawscom xamzdate: <thu 15 nov 2016 00:17:21 gmt> authorization: <signaturevalue> notice that there is no slash / before the policystatus here is the link for the example https://docsawsamazoncom/amazons3/latest/api/restbucketgetpolicystatushtml hope this helps rt', 're: unable to check whether the bucket is public using api call hi rtjaws the request 2 seems pretty much the same to a request you provided and that request throws an error for me', 're: unable to check whether the bucket is public using api call hello from the documentation the request is a little bit different this is your original request https://bucketnames3useast1amazonawscom/bucketnamepolicystatus the doc shows https://bucketnames3amazonawscom/bucketnamepolicystatus notice that it does not use the region in the call https://docsawsamazoncom/amazons3/latest/api/restbucketgetpolicystatushtml rt', 're: unable to check whether the bucket is public using api call i have tried it without the region as you suggested and i received nosuchkey error just like i described in request 2 so unless you managed to get the proper response with that request then either i am wrong or the documentation is not correct in this case edited by: xidex on feb 13 2019 5:39 am', \"what is any other used for log delivery groups acl hello i found document what log delivery group acl is used for s3 access logging document url is https://docsawsamazoncom/amazons3/latest/dev/acloverviewhtml but elb access logscloudtrail logs and vpc flow logs does not used log delivery groupacl i can't found what is log delivery group acl is used for other tell me please\"]\n"
     ]
    }
   ],
   "source": [
    "punctuation = \"?,.\\\\()!\\\";[]+-\"\n",
    "\n",
    "for i in range(0, len(cleantxt)):\n",
    "    cleantxt[i] = cleantxt[i].translate(str.maketrans(\"\",\"\", punctuation))\n",
    "    \n",
    "print(cleantxt[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4) Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = corpus.stopwords.words('english')\n",
    "new_stop_words = ['something','i\\'m','please','thank','thanks', 'hello', 'hi', 're:', 'hey', 'i\\'ve', 'regards']\n",
    "stop_words.extend(new_stop_words)\n",
    "\n",
    "filtered_text = []\n",
    "\n",
    "# print(stop_words)\n",
    "\n",
    "for i in range(0, len(cleantxt)):\n",
    "    words = cleantxt[i].split(\" \")\n",
    "    # print(words)\n",
    "    filtered_sentence = []\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            filtered_sentence.append(w)\n",
    "            \n",
    "    filtered_text.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, write the output to a TSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Amazon S3_Cleaned.tsv', 'w', encoding='utf-8') as tsvfile:\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "    for item in filtered_text:\n",
    "        writer.writerow(['Amazon S3', item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s3', 'public', 'access', 'set', 'edited', 'bucket', 'policy', 'public', 'access', 'settings', 'bucket', 'changed', 'public', 'access', 'denied', 'see', 'symptoms', 'https://imgurcom/rv64vwd', 'edit', 'public', 'access', 'settings', 'https://imgurcom/y3ppaqj', 'bucket', 'policy']\n",
      "\n",
      "[\"can't\", 'figure', 's3', 'display', 'website', 'domain', 'error', '403', 'sure', 'troubleshoot', 'error', 'resolve', 'despite', 'reading', 'guides', 'looked', 'couple', 'forums', 'pages', 'either', 'irrelevant', 'beyond', 'current', 'understanding', 'cannot', 'figure', 'keep', 'getting', '403', 'public', 'bucket', 'policy', 'changed', 'previous', 'error', 'getting', 'connection', 'site', '403', 'forbidding', 'traffic', 'new', 'aws', 'missing', 'one', 'html', 'file', 'bucket', 'hit', \"'make\", \"public'\", 'said', 'access', 'denied', 'objects', 'find', 'able', 'get', 'html', 'file', 'permissions', 'selected', \"'public\", \"access'\", \"'read\", \"object'\", 'said', 'access', 'denied', 'whatever', 'reason', 'accessing', 'domain', 'purchased', 'says', '403', 'go', 'amazon', 's3', 'version', 'website', 'works', 'huh', 'come', 'domain']\n",
      "\n",
      "[\"can't\", 'figure', 's3', 'display', 'website', 'domain', 'test', 'ignore']\n",
      "\n",
      "['aws', 's3', 'account', 'suspended', 'without', 'notification', 'today', 'ready', 'work', 'website', 'found', 'components', 'stored', 'amazon', 's3', 'missing', 'check', 'issue', 'tried', 'log', 's3', 'console', 'found', 'account', 'suspended', 'find', 'email', 'regarding', 'suspension', 'payment', 'dues', 'billing', 'options', 'also', 'opened', 'support', 'case', 'case', 'id:', '5866475801', 'mailed', 'issue', 'still', 'reply', 'hell', 'happening', 'atleast', 'tell', 'help', 'resolve', 'issue', 'work']\n",
      "\n",
      "['unable', 'read', '1000', 'objects', 'bucket', 'using', 'rest', 'apis', 'sdk', 'read', 'list', 'objects', 'bucket', 'bucket', '1000', 'objects', 'per', 'https://docsawsamazoncom/amazons3/latest/api/restbucketgethtml', 'documentation', 'extracting', 'istruncated', 'nextmarker', 'values', 'response', 'istruncated', 'true', 'pass', 'value', 'nextmarker', 'marker', 'parameter', 'subsequent', 'call', 'whenever', 'add', 'marker', 'query', 'parameter', 'get', 'signaturedoesnotmatch', 'error', 'specify', 'parameters', 'like', 'maxkeys', 'prefix', 'delimiter', 'cause', 'error', 'call', 'goes', 'fine', 'course', 'uri', 'encode', 'parameters', 'marker', 'parameter', 'causing', 'error', 'also', 'tried', 'use', 'v2', 'version', 'https://docsawsamazoncom/amazons3/latest/api/v2restbucketgethtml', 'get', 'signaturedoesnotmatch', 'error', 'using', 'continuationtoken', 'subsequent', 'call', 'listtype', 'maxkeys', 'parameters', 'ok', 'help', 'appreciated']\n",
      "\n",
      "['adding', 'expires', 'cachecontrol', 'header', 'folders', 's3', 'help', 'page', 'always', 'quite', 'useless:', 'https://docsawsamazoncom/amazoncloudfront/latest/developerguide/expirationhtml#expirationindividualobjects', 'says', 'folder', \"i'd\", 'see', 'properties', 'metadata', 'none', 'folders', 's3', 'metadata', 'inside', 'properties', 'cards', 'like', 'versioning', 'server', 'logging', 'etc', 'enter', 'cachecontrol', 'setting', 'cloudfront', 'seems', 'getting', 'header', 's3', 'could', 'set', 'header', 'cloudfront', 'ui', 'intuitive', 'guidance']\n",
      "\n",
      "['adding', 'expires', 'cachecontrol', 'header', 'folders', 's3', 'found', 'change', 'metadata', 'folders', 'files', 'help', 'updated', 'documentation', 'remains', 'fairly', 'hideous', 'found', 'solution', 'stack', 'overflow']\n",
      "\n",
      "['get', 'canonical', 'user', 'id', 'setting', 'new', 'bucket', 'need', 'assign', 'specific', 'iam', 'users', 'bucket', 'asking', 'canonical', 'user', 'id', 'get', 'canonical', 'user', 'id', 'iam', 'users', 'confusing']\n",
      "\n",
      "['get', 'canonical', 'user', 'id', 'nicolas_briant', 'post', 'trust', 'well', 'regarding', 'canonical', 'user', 'id', 'find', 'two', 'ways:', 'logging', 'root', 'top', 'right', 'console', 'choose', 'account', 'name', 'number', 'choose', 'security', 'credentials', 'see', 'canonical', 'user', 'id', 'account', 'identifiers', 'section', 'finding', 'account', 'canonical', 'user', 'id:', 'http://docsawsamazoncom/general/latest/gr/acctidentifiershtml#findingcanonicalid', 'using', 'listbuckets', 'api', 'call', 'perform', 'get', 'operation', 's3', 'service', 'get', 'listing', 'buckets', 'response', 'contains', 'owner', 'id', 'element', 'canonical', 'user', 'id', 'example:', 'aws', 's3api', 'listbuckets', 'output', 'text', 'listbuckets:', 'http://docsawsamazoncom/cli/latest/reference/s3api/listbucketshtml', 'best', 'jayd']\n",
      "\n",
      "['get', 'canonical', 'user', 'id', 'jayd', 'missing', 'logging', 'root', 'nicolas']\n",
      "\n",
      "['get', 'canonical', 'user', 'id', 'wanted', 'post', 'following', 'allowed', 'forum', 'account', 'created', 'past', 'hour', 'kind', 'spam', 'prevention', 'pretty', 'new', 'using', 'aws', \"can't\", 'find', 'canonical', 'id', 'understand', 'instructions', 'looking', 'id', 'access', 'aws', 'console', 'via', 'personal', 'account', 'way', 'see', 'canonical', 'id', 'via', 'web', 'interface', 'without', 'using', 'cli', 'created', 's3', 'bucket', \"organization's\", 'account', 'want', 'grant', 'another', 'user', 'access', 'bucket', 'need', 'two', 'canonical', 'ids', \"i'd\", 'appreciate', 'stepbystep', 'instructions', 'getting', 'ids', 'assume', 'know', 'nothing', 'aws', 'advance', \"i'd\", 'still', 'like', 'explicit', 'stepbystep', 'instructions', 'finding', 'canonical', 'ids', 'delay', 'forum', 'posting', 'inspired', 'try', 'different', 'approach', 'rather', 'approach', 'creating', 's3', 'bucket', 'entered', 'email', 'address', 'one', 'granted', 'access', 'bucket', 'created', 'received', 'error', 'message', 'granted', 'access', 'email', 'address', 'reason', 'unclear', 'reason', 'decided', 'try', 'using', 'email', 'address', 'time', 'worked', 'canonical', 'id', 'automatically', 'substituted', 'entered', 'email', 'address', 'person', 'wanted', 'give', 'access', 'bucket', 'canonical', 'id', 'filled', 'well', 'access', 'however', 'view', 'permissions', 's3', 'bucket', 'see', 'canonical', 'ids', \"there's\", 'indication', 'ids', 'refer', \"that's\", 'useful', 'ui', 'really', 'looks', 'lazy', 'tell', 'canonical', 'id', 'goes', 'person']\n",
      "\n",
      "['get', 'canonical', 'user', 'id', 'find', 'another', 'user', 'canonical', 'id', 'created', 'new', 'iam', 'user', 'want', 'access', 's#', 'bucket']\n",
      "\n",
      "['get', 'canonical', 'user', 'id', 'every', 'iam', 'user', 'canonical', 'id', 'one', 'canonical', 'id', 'per', 'aws', 'account', 'following', 'link', 'stepbystep', 'instructions', 'find', 'canonical', 'id', 'associated', 'aws', 'account', 'https://docsawsamazoncom/general/latest/gr/acctidentifiershtml#findingcanonicalid', 'using', 'obejct/bucket', 'acls', 'grant/restrict', 'access', 'object/bucket', 'aws', 'account', 'another', 'aws', 'account', 'cannot', 'restrict', 'access', 'bucket/object', 'iam', 'user', 'using', 'object/bucket', 'acl', 'use', 'bucket', 'policy', 'want', 'provide/restrict', 'access', 'bucket/object', 'iam', 'user', 'account', 'use', 's3', 'bucket', 'policy', 'instead', 'acl', 'bucket', 'policy', 'examples', 'https://docsawsamazoncom/amazons3/latest/dev/examplebucketpolicieshtml']\n",
      "\n",
      "['bug', 'permission', 'evaluation', 'reproduce:', 'create', 'role:', 'testrole', 'create', 'bucket', \"we'll\", 'call', 'testbucket', 'create', 'user:', 'testuser', 'add', 'permissions', 'testuser', 'sts:assumerole', 'testrole', 'trust', 'relationship', 'testrole', 'allow', 'user', 'assume', 'add', 'inline', 'policy', 'testrole', 'allow', 'actions', 's3:*', 'testbucket', 'testbucket/*', 'add', 'inline', 'policy', 'testuser', 'allow', 'actions', 's3:*', 'testbucket', 'testbucket/*', 'upload', 'file', 'bucket', 'via', 'ui', 'fine', 'create', 'bucket', 'policy', 'testbucket:', 'version:', '20121017', 'statement:', 'sid:', 'denyothers', 'effect:', 'deny', 'notprincipal:', 'aws:', 'arn:aws:iam::<acct', 'id>:role/servicerole/testrole', 'arn:aws:iam::<acct', 'id>:root', 'arn:aws:iam::<acct', 'id>:user/testuser', 'notaction:', 's3:putobject', 's3:putobjectacl', 'resource:', 'arn:aws:s3:::<bucket', 'name', 'eg', 'testbucket>/*', 'bucket', 'policy', 'taken', 'anonymous', 'upload', 'use', 'case', 'essence', \"deny'ing\", 'access', 'everyone', 'root', 'test', 'role', 'test', 'user', 'everything', 'putobject', 'putobjectacl', \"i'd\", 'expect', 'testuser', 'assumed', 'role', 'testrole', \"i'd\", 'able', 'example', 'call', 's3:getobject', 'file', 'uploaded', 'gets', 'access', 'denied', 'meanwhile', 'thing', 'testuser', 'allowed', 'despite', 'exact', 'set', 'policies', 'help', 'edited', 'by:', 'davidericksonfn', 'mar', '2019', '7:27', 'pm']\n",
      "\n",
      "['accidentally', 'expired', 'entire', 'bucket', 'contents', 'yesterday', 'one', 'files', 'bucket', 'delete', 'set', 'lifecycle', 'rule', 'expire', 'object', 'least', 'thought', 'actually', 'name', 'rule', 'object', 'wanted', 'expire', 'set', 'scope', 'entire', 'bucket', 'way', 'recover', 'contents', 'edited', 'by:', '_bd', 'mar', '2019', '11:46']\n",
      "\n",
      "['pentest', 'aws', 'cloud', 'application', 's3', 'new', 'policy', 'penetrationtests', 'permits', 'penetrationtest', 'list', 'aws', 'services', 'seen', 'site:', 'https://awsamazoncom/security/penetrationtesting/', 's3', 'list', 'since', 'publicly', 'accessible', 'files', 's3', 'bucket', 'often', 'security', 'relevant', 'issue', 'part', 'security', 'assessment', 'cloud', 'infrastructure', 'wonder', 'necessary', 'obtain', 'special', 'permission', 'aws', 'issue', 's3related', 'requests', 'context', 'penetration', 'test', 'generally', 'forbidden', 'pentester', 'examine', 's3', 'buckets', 'belonging', 'tested', 'application', 'generally', 'speaking:', 'allowed', 'pentester', 'performing', 'pentest', 'aws', 'cloud', 'regarding', 's3', 'bucket', 'kind', 'michael']\n",
      "\n",
      "['preview', 'large', 'files', 's3', 'several', 'thousand', 'image', 'files', '1g', 'size', 'storaged', 's3', 'access', 'locally', 'via', 'storage', 'gateway', 'paging', 'image', 'files', 'locally', 'slow', 'cumbersome', 'looking', 'way', 'allow', 'preview', 'files', 'without', 'download', 'one', 'time', 'view', 'kind', 'viewer', 'available', 'aws', 'allow', 'view', 'files', 'without', 'downloading', 'locally']\n",
      "\n",
      "['unexplained', 'error', 'setting', 'policy', 's3', 'crossregion', 'replication', 'following', 'instructions', 'page', 'setting', 'roles', 'crr:', 'https://docsawsamazoncom/amazons3/latest/dev/settingreplconfigpermoverviewhtml', 'role', \"we'll\", 'call', 'replrole', 'exact', 'trust', 'policy', 'listed', 'page', 'access', 'policy', 'looks', 'like', 'this:', 'version:', '20121017', 'statement:', 'effect:', 'allow', 'action:', 's3:getreplicationconfiguration', 's3:listbucket', 's3:getobjectversion', 's3:getobjectversionacl', 's3:getobjectversiontagging', 's3:replicateobject', 's3:replicatedelete', 's3:replicatetags', 'resource:', 'arn:aws:s3:::ours3bucketprefix*', 'arn:aws:s3:::ours3bucketprefix*/*', 'look', 'exactly', 'like', 'policy', 'provided', 'main', 'difference', 'use', 'wildcards', 'resources', 'specified', 'also', 'group', 'actions', 'together', 'understand', 'iam', 'access', 'policies', 'correctly', 'suffice', 'purpose', 's3', 'allows', 'create', 'crossregion', 'replication', 'rule', 'get', 'error', 'message:', 'crr', 'rule', 'saved', 'might', 'work', 'error', 'setting', 'iam', 'policy', 'selected', 'iam', 'role', 'gobscrossregionreplicationrole', 'ensure', 'set', 'correct', 'policy', 'select', 'another', 'role', 'wrong']\n",
      "\n",
      "['unexplained', 'error', 'setting', 'policy', 's3', 'crossregion', 'replication', 'found', 'crossregion', 'replication', 'actually', 'working', 'despite', 'error', 'message', 'checked', 'back', 'next', 'morning', 'objects', 'successfully', 'replicated', 'backup', 'bucket']\n",
      "\n",
      "[\"can't\", 'export', 'cloudwatch', 'logs', 's3', 'trying', 'export', 'cloudwatch', 'log', 'group', 's3', 'keep', 'getting', 'error', 'every', 'time', 'click', 'export', 'data', 'cloudwatch', 'side:', 'acl', 'permission', 'selected', 'bucket', 'correct', 'amazon', 's3', 'bucket', 'must', 'reside', 'region', 'log', 'data', 'want', 'export', 'cloudwatch', 'log', 'useast1', 'created', 'bucket', 'two', 'us', 'east', 'regions:', 'virginia', 'ohio', 'still', 'get', 'error', 'try', 'export', 'either', 'one']\n",
      "\n",
      "[\"can't\", 'export', 'cloudwatch', 'logs', 's3', 'respond', 'important', 'time', 'sensitive']\n",
      "\n",
      "[\"can't\", 'export', 'cloudwatch', 'logs', 's3', 'exact', 'problem', 'turns', 'neglected', 'populate', \"'s3\", 'bucket', \"prefix'\", 'field', \"'advanced'\", 'section', 'export', 'dialog', 'edited', 'by:', 'lbrooks', 'nov', '30', '2018', '12:17']\n",
      "\n",
      "[\"can't\", 'export', 'cloudwatch', 'logs', 's3', 'unfortunately', 'still', 'work', 'either', 'exactly', 'enter', 'prefix', \"can't\", 'simple', 'save', 'logs', 'bucket', 'user', 'aka', 'admin']\n",
      "\n",
      "[\"can't\", 'export', 'cloudwatch', 'logs', 's3', 'ever', 'solve', 'tell', 'region', 'cloudwatch', 'logs', 'trying', 'thing', 'getting', 'error']\n",
      "\n",
      "[\"can't\", 'export', 'cloudwatch', 'logs', 's3', 'problem', 'solution', 'found', 'select', 'bucket', 'web', 'portal', 'along', 'top', 'number', 'buttons:', 'overview', 'properties', 'permissions', 'management', 'permissions', 'set', 'bucket', 'policy', 'allow', 'cloudwatch', 'exports', 'detailed', 'here:', 'https://docsawsamazoncom/amazoncloudwatch/latest/logs/s3exporttaskshtml', 'hope', 'helps']\n",
      "\n",
      "[\"can't\", 'export', 'cloudwatch', 'logs', 's3', 'took', '30min', 'figure', 'error', 'example', 'policy', 'wrong', 'region', 'example', 'one', 'replace', 'simply', 'replace', 'by:', 'service:', 'logsuseast1amazonawscom', 'fields', 'version:', '20121017', 'statement:', 'action:', 's3:getbucketacl', 'effect:', 'allow', 'resource:', 'arn:aws:s3:::myexportedlogs', 'principal:', 'service:', 'logsuseast1amazonawscom', 'action:', 's3:putobject', 'effect:', 'allow', 'resource:', 'arn:aws:s3:::myexportedlogs/randomstring/*', 'condition:', 'stringequals:', 's3:xamzacl:', 'bucketownerfullcontrol', 'principal:', 'service:', 'logsuseast1amazonawscom']\n",
      "\n",
      "['amazon', 's3', 'slow', 'large', 'buckets', 'got', 'big', 'amazon', 's3', 'bucket', 'around', 'gb', 'things', 'started', 'get', 'slow', 'uploading', 'files', 'iam', 'user', 'api', 'takes', 'forever', 'recreated', 'new', 'bucket', 'properties', 'settings', 'api', 'uploading', 'bucket', 'way', 'faster', 'connection', 'large', 'bucket', 'slow', 'api', 'uploading', 'make', 'new', 'buckets', 'time', 'time', 'write', 'image', 'around', '300', 'kb', 'api', 'large', 'bucket:', 'imageget:', '1260ms', 'imagewrite:', '67683ms', 'newly', 'created', 'bucket', 'time:', 'imageget:', '1275ms', 'imagewrite:', '822ms', 'https://docsawsamazoncom/amazons3/latest/dev/bucketrestrictionshtml', 'stated:', 'limit', 'number', 'objects', 'stored', 'bucket', 'difference', 'performance', 'whether', 'use', 'many', 'buckets', 'store', 'objects', 'single', 'bucket', 'organize', 'across', 'several', 'buckets', 'seem', 'case', 'anyone', 'dick', 'goosen', 'edited', 'by:', 'dickgoosen', 'feb', '2019', '12:49', 'edited', 'by:', 'dickgoosen', 'feb', '2019', '1:22']\n",
      "\n",
      "['amazon', 's3', 'slow', 'large', 'buckets', 'issue', 'small', 'image', 'takes', 'min', 'uploading', 's3', 'ec2']\n",
      "\n",
      "['unable', 'set', 'acl', 'public', 'read', 'properly', 'objects', 'stored', 'bucket', 'bucket', 'permissions', 'set', 'public', 'upload', 'image', 'files', 'angular', 'web', 'application', 'include', 'header', 'put', 'request', 'xamzacl:', 'publicread', 'described', 'aws', 'documentation', 'attempt', 'view', 'images', 's3', 'link', 'file', 'however', '403', 'forbidden', 'error', 'returned', 'go', 'bucket', 'examine', 'image', 'shows', 'access', 'permissions', 'try', 'make', 'image', 'public', 'get', 'access', 'denied', 'message', 'resolve', 'issue', 'attached', 'images', 'post', 'first', 'one', 'shows', 'headers', 'included', 'put', 'request', 'uploading', 'second', 'shows', 'bucket', 'policy', 'third', 'image', 'shows', 'current', 'state', 'permissions', 'image', 'uploaded']\n",
      "\n",
      "['password', 'protect', 's3', 'mount', 'using', 'storage', 'gateway', \"we've\", 'created', 's3', 'mount', 'connect', 'using', 'recommended:', 'mount', 'nolock', 'command', 'know', 'limit', 'policy', 'certain', 'iam', 'users', 'access', 'buckets', 'mount', 'buckets', 'using', 'like', 'tntdrive', 'enter', 'iam', 'credentials', 'way', 'access', 'buckets', 'alternatives', 'tntdrive', 'would', 'allow', 'us', 'set', 'mount', 'command', 'like', 'mount', 'nolock', 'username=username', 'password=password', 'would', 'allow', 'us', 'protect', 'mount']\n",
      "\n",
      "[\"'access\", \"denied'\", 'access', 's3', 'angular', 'app', 'cognito', 'user', 'pool', 's3', 'bucket', 'configured', 'manage', 'access', 'using', 'cognito', 'user', 'pool', 'described', 'https://docsamazonawscn/en_us/iam/latest/userguide/reference_policies_examples_s3_cognitobuckethtml:', 'version:', '20121017', 'statement:', 'effect:', 'allow', 'principal:', 'action:', 's3:listbucket', 'resource:', 'arn:aws:s3:::<bucketname>', 'condition:', 'stringlike:', 's3:prefix:', 'cognito/<appname>/', 'effect:', 'allow', 'principal:', 'action:', 's3:putobject', 's3:getobject', 's3:deleteobject', 'resource:', 'arn:aws:s3:::<bucketname>/cognito/<appname>/${cognitoidentityamazonawscom:sub}*', 'arn:aws:s3:::<bucketname>/cognito/<appname>/${cognitoidentityamazonawscom:sub}/*', 'angular', 'web', 'app', 'authenticate', 'users', 'cognito', 'user', 'pool', 'using', 's3', 'client', 'get', 'object', 'see', 'call', 'cognito', 'service', 'https://cognitoidentityeucentral1amazonawscom/', 'made', 'successfully', 'identity', 'returned', 'response', 'immediate', 'call', 'afterwards', 's3', 'failing', 'status', 'code', '403:', '<error><code>accessdenied</code><message>access', 'denied</message><requestid>aac2b5fc5c74c971</requestid><hostid>l8aoygybuty1qhtjhydrj9uxc97elszl6h2rqlnglpquzrqqpw532u6pixil7ypz4ugpreoss=</hostid></error>', \"here's\", 'code', 'setting', 'aws', 'creds:', 'buildcognitocredsidtokenjwt:', 'string', 'let', 'url', \"'cognitoidp'\", 'cognitoutil_regiontolowercase', \"'amazonawscom/'\", 'cognitoutil_user_pool_id', 'environmentcognito_idp_endpoint', 'url', 'environmentcognito_idp_endpoint', \"'/'\", 'cognitoutil_user_pool_id', 'let', 'logins:', 'cognitoidentityloginsmap', '{}', 'loginsurl', 'idtokenjwt', 'let', 'params', 'identitypoolid:', 'cognitoutil_identity_pool_id', '/*', 'required', '*/', 'logins:', 'logins', 'let', 'serviceconfigs', '<awsserviceserviceconfigurationoptions>{}', 'environmentcognito_identity_endpoint', 'serviceconfigsendpoint', 'environmentcognito_identity_endpoint', 'let', 'creds', 'new', 'awscognitoidentitycredentialsparams', 'serviceconfigs', 'thissetcognitocredscreds', 'return', 'creds', 'missing', 'matter', 'try', 'getting', 'access', 'denied']\n",
      "\n",
      "['s3', 'console', 'shows', 'error', 'access', 'row', 'also', \"can't\", 'generate', 'url', \"'download\", \"as'\", 'shows', 'error', 'occurred', 'generating', 'download', 'link', 'object', 'happens', 'aws', 'accounts', 'different', 'people']\n",
      "\n",
      "['s3', 'console', 'shows', 'error', 'access', 'row', 'understand', 'seeing', 'error', 's3', 'console', \"'download\", \"as'\", 'option', 'throwing', 'error', 'usually', 'happens', 'proper', 'permissions', 'access', 'bucket', 'object', 'ensure', 'necessary', 'permissions', 'still', 'see', 'issue', 'share', 'bucket', 'name', 'object', 'name', 'requester', 'iam', 'arn', 'private', 'message', 'troubleshoot']\n",
      "\n",
      "['s3', 'console', 'shows', 'error', 'access', 'row', 'rgumber', 'sent', 'requested', 'information', 'via', 'pm', 'several', 'days', 'ago']\n",
      "\n",
      "['s3', 'console', 'shows', 'error', 'access', 'row', 'news', 'issue', 'restriction', 'use', 'aws', 'cli', 'happens', 'aws', 'console', 'edited', 'by:', 'deniskot', 'feb', '28', '2019', '2:28']\n",
      "\n",
      "['amazon', 's3', 'error', 'retrieving', 'access', 'type', \"can't\", 'use', 'bucket', 'problem', 'concerning', 's3', 'buckets', 'every', 'bucket', 'create', 'created', 'automatically', 'elastic', 'beanstalk', 'show', 'error', 'access', 'column', 'short', 'error', 'retrieving', 'access', 'type', 'unable', 'anything', 'console', \"can't\", 'delete', 'bucket', 'download/upload', 'file', 'elastic', 'beanstalk', 'well', 'unable', 'use', 's3', 'makes', 'impossible', 'deployment', 'new', 'version', 'web', 'application', 'using', 'cli', 'perform', 'operation', 'want', 'iam', 'account', 'log', 'directly', 'root', 'access', 'idea', 'problem', 'edouard']\n",
      "\n",
      "[\"can't\", 'delete', 'empty', 's3', 'bucket', 'cftemplates', \"can't\", 'delete', 's3', 'bucket', 'empty', 'versioning', 'policy', 's3', 'bucket', 'created', 'service', 'attempt', 'deleting', 'bucket', 'aws', 'console', 'get', 'error', 'message', 'tried', 'delete', 'via', 'aws', 'cli', 'command', 'aws', 's3', 'rb', 's3://bucketname', 'force', 'work', 'shows', 'error', 'msg', 'like', 'fatal', 'error:', 'error', 'occurred', 'nosuchbucket', 'calling', 'listobjects', 'operation:', 'specified', 'bucket', 'exist', 'remove_bucket', 'failed:', 'unable', 'delete', 'objects', 'bucket', 'bucket', 'deleted', 'searched', 'forum', 'tried', 'edit', 'bucket', 'policy', 'also', 'shows', 'like', 'error', 'data', 'found', 'hope', 'anyone', 'could', 'help', 'issue', 'nice', 'day']\n",
      "\n",
      "['copy', 'source', 'bucket', 'dest', 'bucket', 'getobject', 'stream', 'problem', 'name', 'eliran', 'new', 'aws', 'goal', 'copy', 'one', 'bucket', 'ireland', 'another', 'bucket', 'nvirginia', 'explain', 'work', 'flow:', 'use', 'cli', 'command', 'aws', 's3', 'sync', 's3://sorcebucket/', 's3://destbucket/', 'exclude', 'logs/*', 'sync', 'completed', 'time', '2in', 'app', 'net', 'use', 'aws', 'sdk', 'use', 'command', 'getobject', 'like', 'this:', 'amazons3client', 's3client', 'new', 'amazons3clientglobalsawsaccesskeyglobalsawssecretkey', 'regionendpointuseast1', 'stream', 'rs', 's3clientgetobjectnew', 'getobjectrequest', 'bucketname', 'sourcecontainer', 'key', 'key}responsestream', 'work', 'fine', 'responestream', 'objects', 'md5stream', 'need', 'objects', 'cachingwarpperstream', 'good', 'use', 'source', 'bucket', 'ireland', 'request', 'getobject', 'objects', 'like', 'return', 'responestream', 'md5stream', '*the', 'settings', 'policies', 'buckets', 'goes', 'wrong', 'get', 'always', 'md5stream', 'new', 'bucket', 'nvirginia', 'lot', 'eliran', 'kasif']\n",
      "\n",
      "['stockholm', 's3', 'endpoint', 'issue', 'trying', 'connect', 's3', 'bucket', 'newly', 'available', 'eu', 'north', 'region', 'stockholm', 'two', 'mac', 's3', 'compatible', 'apps', 'forklift', 'chronosync', 'without', 'success', 'used', 's3eunorth1amazonawscom', 's3eunorth1amazonawscom', 'endpoints', 'avail', 'get', 'following', 'error:', 'authorization', 'header', 'malformed', 'region', \"'useast1'\", 'wrong', 'expecting', \"'eunorth1'\", 'anyone', 'experiencing', 'issue']\n",
      "\n",
      "['stockholm', 's3', 'endpoint', 'issue', 'answer', 'myself:', 'one', 'mentioned', 'apps', 'forklift', 'updated', 'recently', 'works', 'eu', 'north', 'region', 's3', 'buckets', 'suppose', 'happen', 'soon', 'chronosync', 'app', 'actually', 'matter', 'specific', 'app', 'needs', 'updated', 'support', 'new', 'regions']\n",
      "\n",
      "['issue', 'reading', 's3', 'buckets', 'xml', 'parsing', 'error', 'access', 'list', 'buckets', 'web', 'browser', 'get', 'following', 'error', 'console:', 'xml', 'parsing', 'error:', 'root', 'element', 'found', 'location:', 'https://useast1consoleawsamazoncom/s3/proxy', 'line', 'number', 'column', '1:', 'believe', 'coming', 'buckets', 'deleted', 'stuck', 'account', 'try', 'delete', 'nothing', 'happens', 'visually', 'console', 'error', 'hit', 'resolve', 'time', 'need', 'done', 'resolve']\n",
      "\n",
      "['strange', 'issue', 'granting', 'unexpected', 'permission', 'single', 'object', 'fix', 'two', 'objects', 'bucket', 'getobject', '403', 'role', 'except', 'inexplicably', 'one', 'object', 'accessible', 'bucket', 'configuration', 'is:', 'blank', 'bucket', 'policy', 'options', 'public', 'access', 'settings', 'true', 'acl', 'defaults', 'role', 'config', 'grants', 'permissions', 's3', 'permissions', 'objects', 'appear', 'identical', 'least', 'console', 'iam', 'policy', 'simulator', 'indicates', 'objects', 'denied', 'reality', 'one', 'object', 'allowed', 'performing', 'getobject', 'javascript', 'aws', 'sdk', 'using', 'federated', 'identities', 'cognito', 'pool', 'role', 'associated', 'logged', \"user's\", 'group', 'tried', 'running', 'cloudtrail', 'getobject', 'recorded', 'executed', 'via', 'aws', 'sdk', 'seems', 'log', 'event', 'however', 'manually', 'download', 'via', 'console', 'help', 'edited', 'by:', 'davegravy', 'feb', '22', '2019', '5:36']\n",
      "\n",
      "['amazon', 's3', 'glacier', 'buildin', 'fixity', 'checking', 'see', 'https://dltjorg/article/oclcdigitalarchivevsamazons3/', 'claimed', 'amazon', 's3', 'provide', 'fixity', 'check', 'see', 'https://wwwslidesharenet/amazonwebservices/deepdiveonarchivingandcompliance', 'page', '12', 'aws', 'presentation', 'said', 'builtin', 'fixity', 'checking', \"can't\", 'find', 'anything', 'mention', 'fixity', 'simply', 'searching', 'fixity', 'https://docsawsamazoncom/s3/indexhtml#lang/en_us', 'https://docsawsamazoncom/glacier/indexhtml#lang/en_us', 'amazon', 's3/glacier', 'provides', 'fixity', 'checking']\n",
      "\n",
      "['need', 'suggestions', 'look', 'existing', 'objects', 's3', 'incoming', 'files', 'everyday', 'upload', 's3', 'files', 'duplicates', 'earlier', 'days', 'need', 'check', 'already', 'exist', 'uploading', 'old', 'way', 'save', 'filename', 'sdb', 'uploading', 'new', 'file', 'use', 'sdb', 'query', 'look', 'existing', 'files', 'recently', 'want', 'change', 'use', 's3', 'head', 'object', 'api', 'check', 'existence', 'need', 'suggestions:', \"there's\", 'better', 'way', 'beside', 'sdb', 's3', 'head', 'api', 'new', 's3', 'api', 'check', 'existence', 'bulk', 's3', 'head', 'api', 'good', 'enough', 'use', 'case', 'need', 'look', '~200', 'filenames', 'every', 'hour', 'day', 'advance']\n",
      "\n",
      "['need', 'suggestions', 'look', 'existing', 'objects', 's3', 'use', 's3', 'pretty', 'extensively', 'employed', 'amazon', 'take', 'amount', 'head', 'requests', 'per', 'hour', 'totally', 'fine', 'anywhere', 'close', 'stressing', 'service', 'really', 'better', 'way', 'check', 'existence', 'random', 'key', 'however', 'lot', 'keys', 'keys', 'share', 'pathlike', 'structure', 'could', 'better', 'executing', 'list', 'request', 'common', 'prefix', 'checking', 'contents', 'keep', 'mind', 's3', 'may', 'immediately', 'consistent', 'read', 'docs', 'fully', 'understand', 'impact', 'particular', 'use', 'case', 'couple', 'things', 'stick', 'out:', 'get/head', 'prior', 'uploading', 'put', 'get/head', 'response', 'eventually', 'consistent', 'ie', 'guaranteed', 'return', 'object', 'exist', 'list', 'requests', 'eventually', 'consistent', 'given', 'expectations', 'number', 'objects', 'check', 'would', 'recommend', 'keeping', 'simple', 'head', 'maybe', 'timebased', 'retries', 'clear', 'eventual', 'consistency', 'issue', 'make', 'wrong', 'decision', 'simply', 'reupload', 'duplicate', 'data', 'loss', 'extra', 'cost', 'happens', 'every', 'big', 'deal', 'hope', 'helps']\n",
      "\n",
      "['need', 'suggestions', 'look', 'existing', 'objects', 's3', 'agreed', 'keep', 'simple', 'right', 'way', 'start', 'suggestions', 'much', 'appreciated']\n",
      "\n",
      "[\"can't\", 'delete', 'object', 'deletion', 'marker', 'cli', 'console', 'playing', 's3', 'bucket', 'uploaded', 'text', 'files', 'several', 'kilobytes', 'put', 'deletion', 'marker', 'afterwards', 'however', 'tried', 'remove', 'object', 'version', 'original', 'file', 'version', 'deletion', 'marker', 'console', 'failed', 'saying', 'delete', 'object:', 'total', 'objects:2', 'successful:', '00%', 'also', 'tried', 'cli', 'command', 'aws', 's3api', 'deleteobject', 'bucket', 'storageik1ne', 'versionid', 'nbyr0gs5mabrxhwhhgqqosojac0ocznia', 'key', 'filenametxt', 'version', '1upload', '2deletion', 'marker', 'outputs', 'json', 'output', '{versionid:', 'version_i_previously_specified}', 'tried', 'account', 'administratoraccess', 'failed', 'also', 'modification', 'bucket', 'policy', 'public/private', 'settingsie', 'specify', 'deletion', 'policy', 'etc', 'even', 'empty', 'bucket', 'command', 'console', 'also', 'fails', 'case', 'filename', '면접질문txt', '면접질문리스트txt', '2bit', 'character', 'yes', 'actually', 'two', 'files', 'symptoms', 'missing', 'make', 'sure', 'uploaded', 'another', 'file', 'bucket', 'tried', 'reproduce', 'issue', 'file', 'works', 'exactly', 'expectedboth', 'file', 'deletion', 'marker', 'gets', 'deleted']\n",
      "\n",
      "[\"can't\", 'delete', 'object', 'deletion', 'marker', 'cli', 'console', 'see', 'bucket', 'empty', 'case', 'still', 'face', 'issue', 'share', 'bucket', 'name', 'object', 'name', 'private', 'message', 'able', 'assist', 'also', 'setup', 'lifecycle', 'policy', 'expire', 'filter', 'objects', 's3', 'bucket', 'https://docsawsamazoncom/amazons3/latest/dev/objectlifecyclemgmthtml']\n",
      "\n",
      "[\"can't\", 'delete', 'object', 'deletion', 'marker', 'cli', 'console', 'tried', 'chrome', 'worked', 'think', 'safari', 'webkit', 'mac', 'terminal', 'encoding', 'bug']\n",
      "\n",
      "['unable', 'delete', 's3', 'event', 'notification', 'trying', 'delete', 's3', 'event', 'notification', 'get', 'error:', 'unable', 'validate', 'following', 'destination', 'configurations', 'authorized', 'invoke', 'function', 'arn:aws:lambda:euwest1:xxxxxxxx:function:mylambdafunction', 'arn:aws:lambda:euwest1:xxxxxxxx:function:mylambdafunction', 'null', 'using', 's3', 'event', 'trigger', 'lambda', 'function', 'since', 'deleted', 'thought', 'might', 'issue', 'recreated', 'lambda', 'function', 'name', 'solve', 'issue']\n",
      "\n",
      "['unable', 'delete', 's3', 'event', 'notification', 'understand', 'able', 'delete', 's3', 'event', 'notification', 'getting', 'mentioned', 'error', 'error', 'message', 'looks', 'like', 'another', 'event', 'rule', 'destination', \"'arn:aws:lambda:euwest1:xxxxxxxx:function:mylambdafunction'\", 's3', 'bucket', 'permission', 'invoke', 'lambda', 'function', 'would', 'suggest', 'verify', 'issue', 'persists', 'share', 'bucket', 'name', 'complete', 'error', 'message', 'private', 'message', 'would', 'able', 'troubleshoot']\n",
      "\n",
      "['s3', 'bucket', 'versioning:', 'lifecycle', 'expiration', 'rules', 'applied', 's3', 'bucket', 'replication', 'versioning', 'enabled', '60k', 'rule', 'delete', 'previous', 'versions', 'older', 'days', 'still', 'see', 'weeks', 'old', 'versions', 'source', 'bucket', 'rule', 'applied', 'destination', 'bucket', 'works', 'fine', 'rule', 'applied', 'weeks', 'ago', 'buckets', 'anything', 'obvious', 'might', 'missed', 'related', 'replication', 'versioning', 'etc', 'see', 'attached', 'configuration', 'expiration', 'path', 'set', 'entire', 'bucket', 'selected', 'luigi', 'edited', 'by:', 'lclemente', 'jan', '29', '2019', '12:42', 'posted', 'message', 'bucket', 'lifecycle', 'worked', 'know', 'aws', 'fixed']\n",
      "\n",
      "['s3', 'bucket', 'versioning:', 'lifecycle', 'expiration', 'rules', 'applied', 'luigi', 'understand', 'lifecycle', 'rule', 'executed', 'bucket', 'worked', 'note', 'object', 'reaches', 'end', 'lifetime', 'amazon', 's3', 'queues', 'removal', 'removes', 'asynchronously', 'may', 'delay', 'expiration', 'date', 'date', 'amazon', 's3', 'removes', 'object', 'charged', 'storage', 'time', 'associated', 'object', 'expired', 'find', 'object', 'scheduled', 'expire', 'use', 'head', 'object', 'https://docsawsamazoncom/amazons3/latest/api/restobjectheadhtml', 'get', 'object', 'api', 'https://docsawsamazoncom/amazons3/latest/api/restobjectgethtml', 'operations', 'api', 'operations', 'return', 'response', 'headers', 'provide', 'information', 'refer', 'following', 'document', 'details:', 'https://docsawsamazoncom/amazons3/latest/dev/lifecycleexpiregeneralconsiderationshtml']\n",
      "\n",
      "['s3', 'presigned', 'post', 'url', 'fails', '`405', 'client', 'error:', 'method', 'allowed', 'for`', 'struggling', 'day', 'try', 'get', 'presigned', 'post', 'url', 'feature', 'working', 's3', 'keep', 'running', 'errors', 'problem', 'making', 'request', 'seems', 'response', 'api', 'pointing', 'domain', 'allow', 'posts']\n",
      "\n",
      "['s3', 'presigned', 'post', 'url', 'fails', '`405', 'client', 'error:', 'method', 'allowed', 'for`', 'would', 'like', 'inform', 'cannot', 'make', 'post', 'request', 'object', 'endpoints', 'eg', 'https://bucketnames3amazonawscom/objectext', 'order', 'make', 'post', 'request', 'need', 'make', 'post', 'request', 'bucket', 'endpoint', 'eg', 'https://bucketnames3amazonawscom/', 'specify', 'object', 'key', 'name', 'properties', 'multipart/formdata', 'encoded', 'message', 'body', 'refer', 'following', 'document', 'details:', 'https://docsawsamazoncom/amazons3/latest/api/restobjectposthtml', 'refer', 'following', 'document', 'example', 'browserbased', 'upload', 'using', 'http', 'post:', 'https://docsawsamazoncom/amazons3/latest/api/sigv4postexamplehtml']\n",
      "\n",
      "['glacier', 'physical', 'medium', 'amazon', 'offer', 'service', 'would', 'transfer', 'glacier', 'content', 'physical', 'encrypted', 'medium', 'tape', 'ship', 'us', 'third', 'party', 'companies', 'might']\n",
      "\n",
      "['aws', 'lambda', 'write', 'image', 's3', 'access', 'denied', 'i’m', 'trying', 'get', 'deeplens', 'lambda', 'function', 'upload', 'image', 's3:', 'response', \"s3put_objectacl='publicread'\", 'body=jpg_datatostringbucket=‘mybucketname’key=file_name', 'however', 'keep', 'getting', 'error:', 'error', 'face', 'detection', 'lambda:', 'error', 'occurred', 'accessdenied', 'calling', 'putobject', 'operation:', 'access', 'denied', 'made', 'iam', 'role', 'attached', 'deeplens', 'lambda', 'function', 'attached', 'following', 'policies:', 'awsdeeplenslambdafunctionaccesspolicy', 'awslambdaexecute', 'awsdeeplensservicerolepolicy', 'amazons3fullaccess', 'custom', 'policy', 'following', 'json:', 'version:', '20121017', 'statement:', 'effect:', 'allow', 'action:', 's3:putobject', 's3:putobjectacl', 'resource:', 'arn:aws:s3:::mybucketname”', 'arn:aws:s3:::mybucketname/*', 'even', 'gave', 'bucket', 'public', 'access', 'access', 'control', 'list', 'made', 'bucket', 'policy', 'public:', 'version:', '20121017', 'id:', 'policy1534108093104', 'statement:', 'sid:', 'stmt1534108083533', 'effect:', 'allow', 'principal:', 'action:', 's3:*', 'resource:', 'arn:aws:s3:::mybucketname”', 'i’m', 'still', 'getting', 'accessdenied', 'error']\n",
      "\n",
      "['aws', 'lambda', 'write', 'image', 's3', 'access', 'denied', 'ever', 'get', 'work', 'issue', 'got', 'function', 'write', 's3', 'bucket', 'change', 'public', 'policy', 'block', 'new', 'public', 'acls', 'uploading', 'public', 'objects', 'false', 'ideal', 'setup']\n",
      "\n",
      "['s3', 'throws', 'error', 'latest', 'curl', 'amz1', 'getobject', 'greetings', 'since', 'updating', 'curl', 'version', 'curl7611791amzn1i686', 'getobjectsdk', 'throws', 'following', 'error', 'retrieving', 'file', 'contentencoding=utf8', 'contenttype=text/html', 'happen', 'previous', 'version', 'curl75311686amzn1i686', 'works', 'flawless:', 'exception', \"'awss3exceptions3exception'\", 'message', \"'error\", 'executing', 'getobject', 'xxxxxxxx', 'aws', 'http', 'error:', 'curl', 'error', '61:', 'unrecognized', 'content', 'encoding', 'type', 'libcurl', 'understands', 'deflate', 'gzip', 'content', 'encodings', 'see', 'http://curlhaxxse/libcurl/c/libcurlerrorshtml', 'server:', '200', 'ok', 'requestid:', 'cb1f86a65abdff78', 'exception', \"'guzzlehttpexceptionrequestexception'\", 'message', \"'curl\", 'error', '61:', 'unrecognized', 'content', 'encoding', 'type', 'libcurl', 'understands', 'deflate', 'gzip', 'content', 'encodings', 'see', 'http://curlhaxxse/libcurl/c/libcurlerrorshtml']\n",
      "\n",
      "['s3', 'throws', 'error', 'latest', 'curl', 'amz1', 'getobject', 'suffer', 'form', 'issue', 'succeeded', 'getting', 'issue', 'work', 'using', \"'http'\", '=>', \"'decode_content'\", '=>', 'false', 's3', 'client', 'constructor', 'like', '$client', 'new', 's3client', \"'region'\", '=>', \"'uswest2'\", \"'version'\", '=>', \"'latest'\", \"'http'\", '=>', \"'decode_content'\", '=>', 'false']\n",
      "\n",
      "['s3', 'throws', 'error', 'latest', 'curl', 'amz1', 'getobject', 'tip', 'worked']\n",
      "\n",
      "['sync/copy', 'objects', 'different', 'cloud', 'providers', 'tool', 'use', 'copy', 'objects', 'cloud', 'providers', 'example', 'ibm', 'cloud', 'object', 'store', 'aws', 's3', 'afaik', 'rclone', 'one', 'tool', 'wanted', 'check', 'still', 'achieve', 'using', 'aws', 'cli', 's3cmd', 'nithin']\n",
      "\n",
      "['storage', 'help', 'helping', 'set', 'storage', 'municipal', 'document', 'scanning', 'archiving', 'department', 'big', 'best', 'amazon', 'offers', 'storage', 'want', 'interface', 'trained', 'time', 'learn', 'found', 'good', 'third', 'party', 'apps', 'use', 'transfer', 'data', 'know', 'asking', 'likely', 'best', 'commercial', 'plugs', 'likely', 'appreciated', 'anyone', 'maybe', 'amazon', 'employee', 'point', 'good', 'secure', 'app', 'service', 'use', 'scott']\n",
      "\n",
      "['need', 'suggestions', 'renaming', 'large', 'amount', 'objects', 'need', 'rename', '~10', 'million', 'objects', '~25tb', 'total', 's3', 'location', 'bucket', 'better', 'way', 'copying', 'new', 'names', 'deleting', 'original', 'ones', 'advance', 'edited', 'by:', 'xpli', 'feb', '19', '2019', '8:55']\n",
      "\n",
      "['extending', 'date', 's3', 'object', 'lock', 'documentation', 'unclear', 'extend', 'object', 'lock', 'date', 'api', 'think', 'api', 'guides', 'updated', 'maybe', 'tried:', 'http', 'post', 'object', 'new', 'xamzobjectlockretainuntildate', 'header', 'disallowed', 'right', 'bat', 'post', 'allowed', 'http', 'put', 'length/data', 'disallowed', 'length', 'required', 'source', 'data', 'anymore', 'see', 'general', 'object', 'metadata', 'changed', 'copy', 'request', 'sure', 'applies', 'object', 'lock', 'metadata', 'quite', 'want', 'use', 'case', 'extend', 'retention', 'objects', 'frequently', 'even', 'original', 'retention', 'yet', 'expired', 'want', 'pay', 'one', \"versions'\", 'worth', 'storage', 'course', 'pointers', 'appreciated']\n",
      "\n",
      "['s3', 'properly', 'exeed', 'days', 'file', 'avaiable', 's3', 'lifecycle', 'ru', 's3', 'bucket', 'lifecycle', 'rule', 'transists', 'objects', 'proper', 'tag', 'amazon', 'glacier', 'days', 'creation', 'object', 'upload', 'attach', 'tag', 'would', 'like', 'exeed', 'number', 'days', 'single', 'object', 'efficient', 'way', 'achiving', 'create', 'lifecycle', 'rule', 'applied', 'days', 'creation', \"let's\", 'say', 'days', 'last', 'touch/tag', 'creation', 'need', 'create', 'lifecycle', 'rule', 'day', 'configuration', 'edited', 'by:', 'wyci', 'feb', '15', '2019', '12:15']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'account', 'suspended', 'billing', 'purposes', \"we've\", 'paid', 'outstanding', 'invoices', 'opened', 'support', 'ticket', 'yesterday', 'morning', '24hrs', 'ago', 'requesting', 'reinstatement', 'heard', 'back', 'need', 'get', 'turned', 'back']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'escalated', 'concerns', 'billing', 'department', 'receive', 'update', 'account', 'status', 'via', 'support', 'ticket', 'apologize', 'inconveniences', 'caused', 'francois']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'francisco', 'thing', 'happening', '24', 'hours', 'heard', 'back', 'let', 'access', 'account', 'suspended', 'website', 'yet', 'still', 'seems', 'charging', 'using', 'even', 'worse', 'support', 'ticket', 'unassigned', '24', 'hours', 'help', 'saul']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'saul', 'sincerely', 'apologize', 'delay', 'responding', 'support', 'case', 'confirm', 'account', 'reinstated', 'respond', 'via', 'support', 'case', '1348555031', 'questions', 'concerns', 'best', 'kuda']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'issue', 'opened', 'service', 'case', 'still', 'waiting', 'reply']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'issue', 'opened', 'service', 'case', 'still', 'waiting', 'reply']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'neciboliks', 'replied', 'support', 'case', '1354368401', 'queries', 'relating', 'reply', 'case', 'directly', 'account', 'successfully', 'reinstated', 'good', 'day']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'three', 'days', 'since', 'made', 'required', 'payments', 'unsuspend', 'account', 'still', 'unable', 'login', 'aws', 'console', 'support', 'case', '1354368401', 'stated', 'account', 'activated', 'services', 'available', '30', 'minutes', 'however', 'still', 'cant', 'login', 'aws', 'console', 'services', 'working', 'three', 'days', 'following', 'message', 'displayed', 'try', 'login', 'aws', 'console:', 'authentication', 'failed', 'account', 'suspended', 'contact', 'aws', 'customer', 'support']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'someone', 'help', 'account', 'paid', 'bills', 'account', 'still', 'suspended', 'urgently', 'need', 'running', 'made', 'several', 'requests', 'since', 'tuesday', 'still', 'response', 'anyone', 'really', 'appreciate']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'someone', 'help', 'account', 'paid', 'bills', 'account', 'still', 'suspended', 'urgently', 'need', 'running', 'made', 'several', 'requests', 'since', 'tuesday', 'still', 'response', 'anyone', 'really', 'appreciate']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'thing', 'happened', 'well', 'heard', 'amazon', 'opened', 'two', 'tickets', 'extremely', 'important', 'us', 'reenable', 'account', 'help', 'case', 'number', '1360905041']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'deal', 'expedite', 'enabling', 'account', 'case#', '1361705261']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'leaving', 'aws', 'encountering', 'similar', 'delay', 'support', 'tickets', 'days', 'later', 'yet', 'still', 'locked', 'account', 'way', 'fail', 'miserably', 'customer', 'support', 'amazon', 'ever', 'get', 'around', 'reading', 'message']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'thing', 'happened', 'well', 'extream', 'urgent', 'help', 'case', 'number', '1390544761']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'concern', 'account', 'suspended', 'cleared', 'bill', 'opened', 'ticket', 'reinstate', 'account', 'since', 'mail', 'server', 'mails', 'two', 'days', 'helpout', 'asap', 'bharath']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'paid', 'bills', 'account', 'still', 'suspended', 'urgently', 'need', 'running', '676733943884', 'pleasehelp', 'adriana']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'problem', '24', 'hours', 'case', 'number', '1394053931']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'problem', 'cant', 'acces', 'account', 'already', 'made', 'payment', 'need', 'service', 'soon', 'posible']\n",
      "\n",
      "['account', 'login']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'issue', 'open', 'ticket', 'since', 'friday', 'night', 'somebody', 'help']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'account', 'suspended', 'case', 'id', '#1516007081', 'could', 'someone', 'take', 'look']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'services', 'wating', 'reactivation', 'already', 'paid', 'unpayed', 'bills', 'case', 'is:', '1556450571', 'need', 'solution', 'losing', 'money']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'bad', 'credit', 'card', 'file', 'since', 'since', 'updated', 'billing', 'couple', 'days', 'ago', 'account', 'still', 'suspended', 'opened', 'tickets', 'response', 'account', 'id:', '552859475310', 'account', 'name:', 'mobile']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'account', 'suspended', 'credit', 'card', 'failed', 'process', 'payed', 'outstanding', 'amount', 'reactivate', 'account', 'asap', 'hours', 'still', 'heard', 'back', 'case', '1635447631', 'account', 'nr:', '361721873029', 'edited', 'by:', 'coolasdf', 'jan', '27', '2016', '7:10', 'pm', 'edited', 'by:', 'coolasdf', 'jan', '27', '2016', '7:11', 'pm']\n",
      "\n",
      "['account', 'suspended', 'reactivation', 'billing', 'thing', 'happens', \"we've\", 'paid', 'outstanding', 'invoices', 'opened', 'support', 'ticket', 'requesting', 'reinstatement', 'heard', 'back', 'ticket', 'support/case', 'id', '1640316161', 'need', 'get', 'turned', 'back']\n",
      "\n",
      "['s3', 'correct', 'service', 'project', 'lads', 'currently', 'working', 'schoolproject', 'basically', 'want', 'send', 'data', 'pi', 'cloud', 'storage', 'every', 'eight', 'hours', 'approximately', 'data', 'pretty', 'primitive', 'probably', 'array', 'affiliated', 'timestamp', 'furthermore', 'data', 'stored', 'cloud', 'possible', 'sender', 'data', 'look', 'data', 'probably', 'mobile', 'app', \"i'll\", 'add', 'later', 'basically', 'question', 'is:', 'amazon', 's3', 'correct', 'service', 'requirements', 'need', 'another', 'service', 'implement', 'functionalities', 'simplest', 'way', 'advice', 'advance']\n",
      "\n",
      "['s3', 'correct', 'service', 'project', 'understanding', 'want', 'need', 'api', 'gateway', 'lambda', 's3', 'and/or', 'dynamodb', 'api', 'gateway', 'receives', 'api', 'call', 'passes', 'lambda', 'function', 'writes', 'data', 's3', 'dynamodb', 'save', 'data', 's3', 'time', 'write', 'somethign', 'become', 'object', 'save', 'dynamodb', 'new', 'entry', 'db', 'faster', 'easier', 'retrieve', 'extra', 'cost', 'read', 'api', 'gateway', 'lambda', 'read', 's3', 'requires', 'knowing', 'object', 'name', 'get', 'content', 'read', 'dynamo', 'need', 'key', 'get', 'get', 'entries', 'db', 'costs', 'hope', 'helps', 'rt']\n",
      "\n",
      "['s3', 'correct', 'service', 'project', 'ok', 'set', 'dynamodb', 'add', 'entries', 'java', 'program', 'tell', 'make', 'api', 'downloads', 'certain', 'entries', 'using', 'lambda', 'api', 'gateway', 'googled', 'couldnt', 'really', 'find', 'solution', 'edited', 'by:', 'bautista', 'feb', '14', '2019', '1:05', 'pm']\n",
      "\n",
      "['unable', 'check', 'whether', 'bucket', 'public', 'using', 'api', 'call', 'testing', 'api', 'operations', 'aws', 's3', 'buckets', 'calling', 'get', 'bucketpolicystatus', 'api', 'operation', 'https://docsawsamazoncom/amazons3/latest/api/restbucketgetversionhtml', 'using', 'postman', 'able', 'get', 'response', 'saying', 'whether', 'bucket', 'public', 'tried', 'requests:', 'request', '1:', 'get', 'https://bucketnames3useast1amazonawscom/policystatus', 'response', '1:', '<error>', '<code>nosuchbucketpolicy</code>', '<message>the', 'bucket', 'policy', 'exist</message>', '<bucketname>bucketname</bucketname>', '<requestid></requestid>', '<hostid></hostid>', '</error>', 'request', '2:', 'get', 'https://bucketnames3useast1amazonawscom/bucketnamepolicystatus', 'response', '2:', '<error>', '<code>nosuchkey</code>', '<message>the', 'specified', 'key', 'exist</message>', '<key>bucketname</key>', '<requestid></requestid>', '<hostid></hostid>', '</error>', 'could', 'explain', 'wrong', 'advance']\n",
      "\n",
      "['unable', 'check', 'whether', 'bucket', 'public', 'using', 'api', 'call', 'request', 'get', '/<bucketname>policystatus', 'http/11', 'host:', '<bucketname>s3amazonawscom', 'xamzdate:', '<thu', '15', 'nov', '2016', '00:17:21', 'gmt>', 'authorization:', '<signaturevalue>', 'notice', 'slash', 'policystatus', 'link', 'example', 'https://docsawsamazoncom/amazons3/latest/api/restbucketgetpolicystatushtml', 'hope', 'helps', 'rt']\n",
      "\n",
      "['unable', 'check', 'whether', 'bucket', 'public', 'using', 'api', 'call', 'rtjaws', 'request', 'seems', 'pretty', 'much', 'request', 'provided', 'request', 'throws', 'error']\n",
      "\n",
      "['unable', 'check', 'whether', 'bucket', 'public', 'using', 'api', 'call', 'documentation', 'request', 'little', 'bit', 'different', 'original', 'request', 'https://bucketnames3useast1amazonawscom/bucketnamepolicystatus', 'doc', 'shows', 'https://bucketnames3amazonawscom/bucketnamepolicystatus', 'notice', 'use', 'region', 'call', 'https://docsawsamazoncom/amazons3/latest/api/restbucketgetpolicystatushtml', 'rt']\n",
      "\n",
      "['unable', 'check', 'whether', 'bucket', 'public', 'using', 'api', 'call', 'tried', 'without', 'region', 'suggested', 'received', 'nosuchkey', 'error', 'like', 'described', 'request', 'unless', 'managed', 'get', 'proper', 'response', 'request', 'either', 'wrong', 'documentation', 'correct', 'case', 'edited', 'by:', 'xidex', 'feb', '13', '2019', '5:39']\n",
      "\n",
      "['used', 'log', 'delivery', 'groups', 'acl', 'found', 'document', 'log', 'delivery', 'group', 'acl', 'used', 's3', 'access', 'logging', 'document', 'url', 'https://docsawsamazoncom/amazons3/latest/dev/acloverviewhtml', 'elb', 'access', 'logscloudtrail', 'logs', 'vpc', 'flow', 'logs', 'used', 'log', 'delivery', 'groupacl', \"can't\", 'found', 'log', 'delivery', 'group', 'acl', 'used', 'tell']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in filtered_text[0:100]:\n",
    "    print(item)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested improvements:\n",
    "1) Remove html URLs <br>\n",
    "2) Remove name and edited by section <br>\n",
    "3) Remove duplicate tokens in same word vector\n",
    "\n",
    "### Issues:\n",
    "1) Select * from queries are not distinguished uniquely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
