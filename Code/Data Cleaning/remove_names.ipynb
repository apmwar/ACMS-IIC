{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries, packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk import corpus\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title = [\"Amazon EC2\", \"Amazon RDS\", \"Amazon S3\", \"Amazon CloudFront\", \"Amazon VPC\", \"Amazon SNS\",\n",
    "            \"Amazon Elastic Beanstalk\", \"Amazon Lambda\"]\n",
    "\n",
    "fileloc = \"C:\\\\Users\\\\Aruna\\\\Documents\\\\ACMS-IID\\\\Scraped Files\\\\Forums\\\\\"\n",
    "\n",
    "text = pd.read_csv(fileloc + title[0] + '.tsv', delimiter='\\t', encoding='utf-8')\n",
    "# text['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1) Remove the question description and then the extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleantext = text['description'].tolist()\n",
    "\n",
    "for i in range(0, len(cleantext)):\n",
    "    if cleantext[i][0] == 'R' and cleantext[i][1] == 'e' and cleantext[i][2] == ':':\n",
    "        cleantext[i] = ' '.join(cleantext[i].splitlines()[1:])\n",
    "    cleantext[i] = ' '.join(cleantext[i].split())\n",
    "\n",
    "text['description'] = cleantext    \n",
    "    \n",
    "# text['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2) Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flag = 0\n",
    "txt = []\n",
    "\n",
    "for msg in text['description']:\n",
    "    tokens = word_tokenize(msg)\n",
    "    txt.append(tokens)\n",
    "    # print(flag)\n",
    "    flag = flag + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3) Remove (some) common names and proper nouns from text\n",
    "\n",
    "<i>Note: We are converting the names corpus to uppercase because otherwise checking is case sensitive </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('names')\n",
    "nltk.download('words')\n",
    "# names = corpus.names.words()\n",
    "words = corpus.words.words()\n",
    "\n",
    "names = ['JANUARY', 'FEBRUARY', 'MARCH', 'APRIL', 'MAY', 'JUNE', 'JULY', 'AUGUST', 'SEPTEMBER', 'OCTOBER', 'NOVEMBER',\n",
    "         'DECEMBER', 'JAN', 'FEB', 'MAR', 'APR', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC','MONDAY', 'TUESDAY',\n",
    "         'WEDNESDAY', 'THURSDAY', 'FRIDAY', 'SATURDAY', 'SUNDAY', 'MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN'\n",
    "        ]\n",
    "\n",
    "for name in corpus.names.words():\n",
    "    names.append(name.upper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flag = 0\n",
    "cleantext = []\n",
    "\n",
    "for item in txt:\n",
    "    mylist = []\n",
    "    pos = pos_tag(item)\n",
    "    for name, tag in pos:\n",
    "        if tag == 'NNP' and name.upper() in names:\n",
    "            # print(name)    \n",
    "            flag = flag + 1\n",
    "        else:\n",
    "            mylist.append(name)\n",
    "            \n",
    "    cleantext.append(mylist)\n",
    "            \n",
    "flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = len(cleantext)\n",
    "for i in range(0, m):\n",
    "    n = len(cleantext[i])\n",
    "    for j in range(0, n):\n",
    "        try:\n",
    "            if len(cleantext[i][j]) < 2:\n",
    "                del cleantext[i][j]\n",
    "                n = n - 1\n",
    "        except:\n",
    "            j = j - 1\n",
    "\n",
    "cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text['description'] = cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4) Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = corpus.stopwords.words('english')\n",
    "\n",
    "new_stop_words = ['something', \"'m\", 'please', 'thank', 'thanks', 'hello', 'hi', 'hey', \"'ve\", 'regards', \"ca\", 'I', \"'d\",\n",
    "                  \"n\\'t\", \"s3\", \"aws\", \"ec2\", \"regards\", \"'s\", \"help\", \"Edited\", \"sns\", \"cloudfront\", \"vpc\", \"rds\", \n",
    "                  \"beanstalk\", \"lambda\", \"Amazon\", 'wo', \"Elastic\"]\n",
    "stop_words.extend(new_stop_words)\n",
    "\n",
    "cleantext = []\n",
    "\n",
    "\n",
    "for i in range(0, len(stop_words)):\n",
    "    stop_words[i] = stop_words[i].upper()\n",
    "\n",
    "# print(stop_words)\n",
    "\n",
    "m = len(text['description'])\n",
    "\n",
    "for i in range(0, m):\n",
    "    n = len(text['description'][i])\n",
    "    \n",
    "    sentence = []\n",
    "    \n",
    "    for j in range(0, n):\n",
    "        if text['description'][i][j].upper() not in stop_words:\n",
    "            sentence.append(text['description'][i][j])\n",
    "    \n",
    "    cleantext.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text['description'] = cleantext\n",
    "\n",
    "text['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5) Remove all the 'words' that are only symbols and convert all words to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleantext = []\n",
    "\n",
    "for msg in text['description']:\n",
    "    sent = []\n",
    "    for word in msg:\n",
    "        if word.isalnum() == True:\n",
    "            sent.append(word.lower())\n",
    "    cleantext.append(sent)\n",
    "\n",
    "cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6) Remove repeated words in the same sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text['description'] = cleantext\n",
    "\n",
    "cleantext = []\n",
    "\n",
    "for msg in text['description']:\n",
    "    sent = []\n",
    "    for word in msg:\n",
    "        if word not in sent:\n",
    "            sent.append(word.lower())\n",
    "    cleantext.append(sent)\n",
    "\n",
    "cleantext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to the data.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Aruna\\\\Documents\\\\ACMS-IID\\\\Code\\\\Data Cleaning\\\\Data.csv', 'a', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['label', 'description'])\n",
    "    for item in cleantext:\n",
    "        if len(item) > 0:\n",
    "            writer.writerow([title[0], item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
