label	description

Amazon Simple Storage Service (S3)	"S3 console shows Error in Access row
Also it can't generate an URL for 'Download As'. It shows ""An error occurred generating the download link for this object."". This happens for all AWS accounts we have and for different people."

Amazon Simple Storage Service (S3)	"Re: S3 console shows Error in Access row
Hi,

I understand that you are seeing Error in the in the S3 console and 'Download As' option is throwing error. This usually happens when you do not proper permissions to access the bucket or object.

Please ensure that you have all the necessary permissions. If you still see the same issue, please share the bucket name, object name and requester IAM ARN with me over private message to troubleshoot further.

Thanks,"

Amazon Simple Storage Service (S3)	"Re: S3 console shows Error in Access row
RGumber, I've sent requested information via PM several days ago."

Amazon Simple Storage Service (S3)	"where do I get the canonical user id
Hi,
I am setting up a new bucket and need to assign specific IAM users to the bucket.
It is asking for the canonical user id.
How do I get the canonical user id for all my IAM users?
This is so confusing.

Thanks"

Amazon Simple Storage Service (S3)	"Re: where do I get the canonical user id
Hello nicolas_briant, 

Thank you for your post, I trust you are well. 

Regarding the Canonical User ID, you can find it in two ways:

1) Logging in as root. In the top right of the console, choose your account name or number. Then choose My Security Credentials. You will see the Canonical User ID in the Account Identifiers section. 

Finding Your Account Canonical User ID:

http://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#FindingCanonicalId

2) By using a ListBuckets API call. When you perform a GET operation on the S3 service to get a listing of all the buckets you own, the response contains an Owner ID element which is your canonical user id. For example:

aws s3api list-buckets --output text

list-buckets: 

http://docs.aws.amazon.com/cli/latest/reference/s3api/list-buckets.html

Best regards. 

Jayd J."

Amazon Simple Storage Service (S3)	"Re: where do I get the canonical user id
Thanks Jayd. I was missing ""Logging in as root"".
Regards,
Nicolas"

Amazon Simple Storage Service (S3)	"Re: where do I get the canonical user id
I wanted to post the following, but I wasn't allowed because my forum account had just been created in the past hour.  (Some kind of spam prevention?)

I'm pretty new to using AWS.  I can't find my canonical ID and I don't understand your instructions for looking up my ID.  I have access to the AWS console via my personal account.  Isn't there a way to see my own canonical ID via the web interface, without using a CLI?

I've created an S3 bucket under my organization's account.  I want to grant myself and another user access to the bucket, but I need our two canonical IDs to do so.

I'd appreciate step-by-step instructions for getting the IDs.  Assume I know nothing about AWS.

Thanks in advance.

While I'd still like to have explicit step-by-step instructions for finding canonical IDs, the delay in forum posting inspired me to try a different approach.  Or rather, the same approach over again.

That is, when I was creating the S3 bucket, I had entered my email address as one to be granted access to it.  The bucket was created, but I received an error message that I couldn't be granted access by email address for some reason.  (It was unclear as to why.)

For some reason, I decided to try using my email address again.  This time it worked and my canonical ID was automatically substituted!  I entered the email address of the other person that I wanted to give access to the bucket, and his canonical ID was filled in as well.  Now we both have access.

However...  When we view the permissions of the S3 bucket, we only see canonical IDs.  There's no indication to whom each of the IDs refer!  That's not a useful UI!  It really looks lazy.

So, how can I tell which canonical ID goes with each person?"

Amazon Simple Storage Service (S3)	"Re: where do I get the canonical user id
Hello,
How can I find another user Canonical ID?
I created new IAM user and want him to have access to S# bucket.
Thank you!"

Amazon Simple Storage Service (S3)	"Copy from source bucket to dest bucket - GetObject() stream problem
Hi all,
My name is Eliran and i new in aws.
my goal is to copy from one bucket (Ireland) to another bucket (N.Virginia)
i will explain my work flow:
1) i use the CLI command - aws s3 sync s3://sorce-bucket/ s3://dest-bucket/ --exclude ""logs/*"".
the sync completed after some time...
2)in my app in .NET i use the AWS SDK and use the command GetObject() like this:
AmazonS3Client s3client = new AmazonS3Client(Globals.AWSAccessKey,Globals.AWSSecretKey, RegionEndpoint.USEast1);
Stream rs = s3client.GetObject(new GetObjectRequest
{
BucketName = SourceContainer,
Key = key}).ResponseStream;

its work fine but the ResponeStream for some objects is MD5Stream (that what i need) and for some objects is CachingWarpperStream.... (its not good for me)

if i use the source bucket from Ireland so all the request with GetObject on the same objects (like above) will return ResponeStream MD5Stream! 

*the settings and policies is the same in both buckets.

what goes wrong? and how i can get always MD5Stream from my new bucket in N.Virginia.

Thanks a lot,
Eliran Kasif."

Amazon Simple Storage Service (S3)	"Stockholm S3 endpoint issue
Hello,

I'm trying to connect to an S3 bucket in the newly available EU North 1 Region (Stockholm) through two Mac S3 compatible apps (Forklift and ChronoSync) without success.

I've used ""s3.eu-north-1.amazonaws.com"" and ""s3-eu-north-1.amazonaws.com"" endpoints to no avail. I get the following error: The authorization header is malformed; the region 'us-east-1' is wrong; expecting 'eu-north-1'

Is anyone experiencing the same issue? Thanks.

Regards."

Amazon Simple Storage Service (S3)	"Re: Stockholm S3 endpoint issue
I answer to myself: one of the mentioned apps (Forklift) has been updated recently and now it works with EU North 1 Region S3 buckets.

I suppose the same will happen soon with ChronoSync app. It's actually a matter of the specific app that needs to be updated to support new regions."

Amazon Simple Storage Service (S3)	"Issue reading S3 buckets (XML Parsing Error)
When I access my list of buckets in the web browser, I get the following error in the console: 

XML Parsing Error: no root element found
Location: https://us-east-1.console.aws.amazon.com/s3/proxy
Line Number 1, Column 1:

I believe this is coming from a few buckets I had deleted but are stuck in my account. If I try to delete them again, nothing happens visually, and in the console, this error is hit again.

Is this something that will resolve itself in time? Or does something need to be done to resolve it?

Thanks!"

Amazon Simple Storage Service (S3)	"Strange issue granting unexpected permission to single object, how to fix?
I have two objects in a bucket, and GetObject should 403 for both for Role A, except inexplicably one object is accessible.

The bucket configuration is:
blank bucket policy, all 4 options under Public Access Settings are True, ACL is defaults

Role config grants no permissions to S3

Permissions for both objects appear identical (at least in console). 

IAM Policy Simulator indicates both objects will be denied, but again, in reality one object is allowed.

I'm performing the getObject from javascript AWS SDK using federated identities & Cognito pool. Role A  is associated with the logged in user's group. 

I've tried running CloudTrail but getObject is not being recorded when executed via AWS SDK. It seems to log the event however when I manually download via console. 

Please help!

Edited by: davegravy on Feb 22, 2019 5:36 AM"

Amazon Simple Storage Service (S3)	"Does Amazon S3 or glacier has build-in fixity checking?
Hi

See https://dltj.org/article/oclc-digital-archive-vs-amazon-s3/ claimed that Amazon S3 does not provide fixity check.

See https://www.slideshare.net/AmazonWebServices/deep-dive-on-archiving-and-compliance page 12 from AWS presentation said that it has built-in fixity checking. 

I can't find anything mention about fixity by simply searching ""fixity"" in 
https://docs.aws.amazon.com/s3/index.html#lang/en_us
https://docs.aws.amazon.com/glacier/index.html#lang/en_us

So, does Amazon s3/glacier provides fixity checking or not?"

Amazon Simple Storage Service (S3)	"Need suggestions on how to look up existing objects on S3
Hi, we have incoming files everyday, and we upload them to S3. The files can be duplicates from earlier days, so we need to check if they already exist before uploading.

Our old way is to save the filename to SDB after uploading a new file, so we can use SDB query to look up existing files.

We recently want to change it to use S3 HEAD Object API to check existence.

We need suggestions:
(1) if there's better way beside SDB and S3 HEAD API? any new S3 API to check existence in bulk?
(3) is S3 HEAD API good enough for our use case (we need look up ~200 filenames every hour during the day)

Thanks in advance!"

Amazon Simple Storage Service (S3)	"Re: Need suggestions on how to look up existing objects on S3
Hi,

I use S3 pretty extensively but I'm not employed by Amazon so take this as it is.

 That amount of HEAD requests per hour will be totally fine, it should not be anywhere close to stressing out the service.

There isn't really a better way to check for existence of a random key. However, if you have a lot of keys at once and the keys share a path-like structure, you could do better by executing a list request on the common prefix and checking the contents. 

Keep in mind that S3 may not be immediately consistent. You should read the docs to fully understand the impact to your particular use case, but a couple things stick out:
1) if you do a GET/HEAD prior to uploading, then PUT, then GET/HEAD -- that response will be eventually consistent i.e. not guaranteed to return that the object does exist
2) LIST requests are eventually consistent

Given that and that your expectations for number of objects to check, I would recommend keeping it simple and just doing HEAD - maybe with time-based retries to clear up the eventual consistency issue. If you make the wrong decision, you simply re-upload a duplicate, which is not data loss but just extra cost to you, so if this happens every once in a while, shouldn't be too big of a deal.

Hope this helps!"

Amazon Simple Storage Service (S3)	"Re: Need suggestions on how to look up existing objects on S3
Thank you. I agreed to keep it simple is the right way to start. Your suggestions are very much appreciated."

Amazon Simple Storage Service (S3)	"Can't delete object and its deletion marker from both CLI and console.
Hi, While playing with S3 bucket that I own I uploaded some text files that are several kilobytes, and put a deletion marker afterwards.

However, when I tried to remove the object version 1 (original file) and version 2 (deletion marker), the console just failed saying ""Delete object: Total objects:2, successful: 0(0%).

I also tried cli command aws s3api delete-object --bucket storage.ik1ne --version-id NByr0gS5MAbrxhWHHgqqOSoJaC0OCzNia --key (filename).txt
 on both its version 1(upload) and 2(deletion marker), but it just outputs json output {""VersionId"": ""version_I_previously_specified""}.

I tried this with Account that has AdministratorAccess but failed. Also, all modification I did to bucket policy was public/private settings(i.e. did not specify deletion policy, etc).

Even the ""Empty Bucket"" command on console also fails.

Just in case, the filename was ""면접질문.txt"" and ""면접질문리스트.txt"", which is 2-bit character. (Yes, actually I have two files with same symptoms).

What am I missing? To make it sure, I uploaded another file to the same bucket and tried to reproduce the issue but that file works out exactly expected(both file and deletion marker gets deleted)."

Amazon Simple Storage Service (S3)	"Re: Can't delete object and its deletion marker from both CLI and console.
Hi,

I see that your bucket is now empty. In case you still face the same issue, please share the bucket name and object name with me over private message and I will be able to assist you further.

Also, you can setup a lifecycle policy to expire all (or filter) the objects in your S3 bucket.

https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html

Thanks,"

Amazon Simple Storage Service (S3)	"Re: Can't delete object and its deletion marker from both CLI and console.
I tried this on chrome and it worked, so I think it was just safari webkit and Mac terminal encoding bug. Thank you."

Amazon Simple Storage Service (S3)	"Unable to delete S3 event notification
Hi,

I'm trying to delete an S3 event notification but I get this error:

Unable to validate the following destination configurations. Not authorized to invoke function [arn:aws:lambda:eu-west-1:XXXXXXXX:function:myLambdaFunction]. (arn:aws:lambda:eu-west-1:XXXXXXXX:function:myLambdaFunction, null)


I was using this S3 event as a trigger for a Lambda function that I have since deleted. I thought that might be the issue and recreated the Lambda function (with the same name), but this did not solve the issue.

Thank you."

Amazon Simple Storage Service (S3)	"Re: Unable to delete S3 event notification
Hi,

I understand that you are not able to delete S3 event notification and you are getting the above mentioned error. From the error message, it looks like you have another event rule with the destination as 'arn:aws:lambda:eu-west-1:XXXXXXXX:function:myLambdaFunction' and your S3 bucket does not have the permission to invoke that Lambda function.

I would suggest you to please verify the same and if the issue persists, please share the bucket name and complete error message with me over private message and I would be able to troubleshoot further.

Thanks,"

Amazon Simple Storage Service (S3)	"S3 bucket with versioning: lifecycle expiration rules not applied
We have an S3 bucket with replication and versioning enabled, about 60K. A rule should delete previous versions older than 3 days, but I can still see weeks old versions on this source bucket. The same rule applied to the destination bucket works fine. The rule has been applied weeks ago to both buckets.
Is there anything obvious I might have missed related to replication, versioning etc?
See attached configuration for expiration. No path set, entire bucket selected.

Thanks,

Luigi

Edited by: lclemente on Jan 29, 2019 12:42 AM
After I posted this message the bucket lifecycle worked. I don't know if AWS fixed it."

Amazon Simple Storage Service (S3)	"Re: S3 bucket with versioning: lifecycle expiration rules not applied
Hi Luigi,

I understand that lifecycle rule was not executed for your bucket but it worked now.

Please note that when an object reaches the end of its lifetime, Amazon S3 queues it for removal and removes it asynchronously. There may be a delay between the expiration date and the date at which Amazon S3 removes an object. You are not charged for storage time associated with an object that has expired. 

 To find when an object is scheduled to expire, use the HEAD Object https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html or the GET Object API https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html operations. These API operations return response headers that provide this information. 

Please refer to the following document for more details: https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-expire-general-considerations.html

Thanks,"

Amazon Simple Storage Service (S3)	"S3 presigned post URL fails with `405 Client Error: Method Not Allowed for`
I've been struggling all day to try and get the ""presigned post url"" feature working with S3 but keep running into errors. I have no problem making the request, but it seems that the response from the API is pointing at a domain that doesn't allow POSTs."

Amazon Simple Storage Service (S3)	"Re: S3 presigned post URL fails with `405 Client Error: Method Not Allowed for`
Hi,

I would like to inform you that you cannot make POST request to Object endpoints eg. (https://bucketname.s3.amazonaws.com/object.ext). In order to make POST request, you will need to make POST request to bucket endpoint (eg. https://bucketname.s3.amazonaws.com/) and specify the object key name and other properties in the multipart/form-data encoded message body. Please refer to the following document for more details: https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html

Please refer to the following document for an example on browser-based upload using HTTP POST: https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-post-example.html

Thanks,"

Amazon Simple Storage Service (S3)	"Glacier to Physical Medium
Does Amazon offer any service that would transfer some of our Glacier content to a physical, encrypted medium such as tape and ship it to us?  Or are there any third party companies that might?"

Amazon Simple Storage Service (S3)	"AWS Lambda Write Image to S3 Access Denied
I’m trying to get a DeepLens Lambda function to upload an image to S3:

response = s3.put_object(ACL='public-read', Body=jpg_data.tostring(),Bucket=‘MY-BUCKET-NAME’,Key=file_name)


However, I keep getting the error:

Error in face detection lambda: An error occurred (AccessDenied) when calling the PutObject operation: Access Denied


I made an IAM role and attached it to the Deeplens lambda function and attached the following policies: AWSDeepLensLambdaFunctionAccessPolicy, AWSLambdaExecute, AWSDeepLensServiceRolePolicy, AmazonS3FullAccess, and a custom policy with the following JSON:

{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Action"": [
                ""s3:PutObject"",
                ""s3:PutObjectAcl""
            ],
            ""Resource"": [
                ""arn:aws:s3:::MY-BUCKET-NAME”,
                ""arn:aws:s3:::MY-BUCKET-NAME/*""
            ]
        }
    ]
}


I even gave the bucket public access through the access control list and made the bucket policy public:

{
    ""Version"": ""2012-10-17"",
    ""Id"": ""Policy1534108093104"",
    ""Statement"": [
        {
            ""Sid"": ""Stmt1534108083533"",
            ""Effect"": ""Allow"",
            ""Principal"": ""*"",
            ""Action"": ""s3:*"",
            ""Resource"": ""arn:aws:s3:::MY-BUCKET-NAME”
        }
    ]
}


But I’m still getting the AccessDenied error."

Amazon Simple Storage Service (S3)	"Re: AWS Lambda Write Image to S3 Access Denied
Did you ever get this to work? I am having the same issue.

I got my function to write to the S3 bucket by change the public policy of ""Block new public ACLs and uploading public objects"" to false but this is not an ideal setup."

Amazon Simple Storage Service (S3)	"S3 throws error with latest curl in amz1 when getObject
Greetings!

Since updating curl to version curl-7.61.1-7.91.amzn1.i686 getObject(SDK) throws the following error when retrieving a file with Content-Encoding=UTF-8 and Content-type=text/html. Does not happen with previous version curl-7.53.1-16.86.amzn1.i686 which works flawless:

.....
exception 'Aws\S3\Exception\S3Exception' with message 'Error executing ""GetObject"" on ""XXXXXXXX""; AWS HTTP error: cURL error 61: Unrecognized content encoding type. libcurl understands deflate, gzip content encodings. (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)  (server): 200 OK (Request-ID: CB1F86A65ABDFF78) - '
....
exception 'GuzzleHttp\Exception\RequestException' with message 'cURL error 61: Unrecognized content encoding type. libcurl understands deflate, gzip content encodings. (see http://curl.haxx.se/libcurl/c/libcurl-errors.html)
....."

Amazon Simple Storage Service (S3)	"Re: S3 throws error with latest curl in amz1 when getObject
I suffer form the same issue. I succeeded in getting this issue work by using 'http'    => ['decode_content' => false],
 in the S3 client constructor, like $client = new S3Client([
    'region'  => 'us-west-2',
    'version' => 'latest',
    'http'    => ['decode_content' => false],
]);"

Amazon Simple Storage Service (S3)	"Re: S3 throws error with latest curl in amz1 when getObject
Thanks for the tip, it worked."

Amazon Simple Storage Service (S3)	"sync/copy objects from different cloud providers
Hi,
Is there any tool that I can use to copy objects from other cloud providers for example IBM cloud object store to AWS S3?
AFAIK rclone is one of the tool. I wanted to check if we still can achieve this using aws cli or s3cmd

Thanks,
Nithin"

Amazon Simple Storage Service (S3)	"Storage help
I am helping set up storage for our municipal document scanning and archiving department. We aren't big and I'm not the best. Amazon offers the storage I want but the interface is more than I'm trained on or have time to learn. I haven't found any good third party apps that I can use to transfer the data. I know asking this here is likely not the best, commercial plugs are likely not appreciated, but can anyone (maybe an Amazon employee) point me to a good secure app or a service for me to use?

Scott"

Amazon Simple Storage Service (S3)	"Need suggestions on renaming large amount of objects
Hi, we need to rename ~10 million objects (~2.5TB in total) on S3 in the same location, same bucket.
Is there any better way than copying them to new names and deleting the original ones?

thanks in advance!

Edited by: xpli on Feb 19, 2019 8:55 AM"

Amazon Simple Storage Service (S3)	"Can't Export CloudWatch Logs To S3
I'm trying to export a CloudWatch Log Group to S3 but I keep getting this error every time I click Export Data on the CloudWatch side:

The ACL permission for the selected bucket is not correct. The Amazon S3 bucket must reside in the same region as the log data that you want to export.

The CloudWatch Log is in us-east-1. I created a bucket in each of the two US East regions: N. Virginia and Ohio but I still get this error when I try to export to either one of them. Why?"

Amazon Simple Storage Service (S3)	"Re: Can't Export CloudWatch Logs To S3
PLEASE RESPOND. THIS IS VERY IMPORTANT AND TIME SENSITIVE."

Amazon Simple Storage Service (S3)	"Re: Can't Export CloudWatch Logs To S3
I had the exact same problem. It turns out that I neglected to populate the 'S3 bucket prefix' field under the 'Advanced' section of the export dialog.

Edited by: lbrooks on Nov 30, 2018 12:17 AM"

Amazon Simple Storage Service (S3)	"Re: Can't Export CloudWatch Logs To S3
This unfortunately still doesn't work for me either. 

What exactly did you enter as the prefix? Why can't we simple save the logs to any bucket that we own as a user aka admin?"

Amazon Simple Storage Service (S3)	"Re: Can't Export CloudWatch Logs To S3
Did you ever solve this?  How do I tell which region my Cloudwatch logs are in?

I am trying to do the same thing and am getting the same error."

Amazon Simple Storage Service (S3)	"Extending the date on S3 object lock
Hi,

The documentation is unclear on how you can extend the object lock date with the API. I think the API guides have not been updated maybe?

I've tried:
1) HTTP POST on object with new x-amz-object-lock-retain-until-date header. 
this is disallowed right off the bat, no POST allowed
2) HTTP PUT with no length/data
this is disallowed, length required (I don't have the source data anymore)

I see that in general, object metadata can be changed with a Copy request. I'm not sure if this applies to object lock metadata, but that isn't quite what we want; our use case is to extend retention on objects frequently even if the original retention has not yet expired. We only want to pay for one versions' worth of storage, of course...

Any pointers are appreciated!"

Amazon Simple Storage Service (S3)	"S3 - how to properly exeed days that file is avaiable in S3 in lifecycle ru
Hello,

I have S3 bucket with lifecycle rule that transists objects with proper tag to Amazon Glacier some days after creation.
After object upload, I attach this tag to it.
I would like to exeed that number of days for single object.

What is the most efficient way of achiving this?
Can I create lifecycle rule that is applied not for days after creation but let's say days after last touch/tag creation ?
Do I need to create lifecycle rule for each day configuration?

Edited by: Wyci on Feb 15, 2019 12:15 AM"

Amazon Simple Storage Service (S3)	"Account Suspended & Reactivation for Billing?
Hi -

My account was suspended for billing purposes, we've paid the outstanding invoices.  I opened a support ticket yesterday morning (> 24hrs ago) requesting re-instatement, but haven't heard back.

What do we need to do to get this turned back on?"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Hi

I have escalated your concerns to the billing department. 
You will receive an update on your account status via the support ticket.

I do apologize for any inconveniences caused.

Regards,
Francois"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Francisco,
The same thing is happening to me. It's been over 24 hours and haven't heard back. It won't let me access the account because it is ""suspended"" and my website is down, yet it still seems to be charging me for using it... And even worse, my support ticket has been unassigned for over 24 hours!
Please help!
Saul"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Hi  Saul,

I sincerely apologize for the delay in responding to your support case. I confirm that the account has been reinstated.

Please respond via support case 1348555031 if you have any questions or concerns.

Best regards,
Kuda"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Same issue, I've opened a service case for it. Still waiting for reply."

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Same issue, I've opened a service case for it. Still waiting for reply."

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Hi neciboliks,

I have replied to you support case # 1354368401, should you have any further queries relating to the this please reply to the case directly.

Your account has successfully been reinstated.

Have a good day."

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Hi,
It's been three days since I made all required payments to unsuspend my account, but I'm still unable to login AWS Console. In support case # 1354368401 it is stated that my account is activated and all services will be available in 30 minutes, however I still cant login to aws console and services are not working for three days. 

The following message displayed when I try to login AWS Console: ""Authentication failed because your account has been suspended. Please contact AWS customer support."""

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Can someone please help me with my account.  I have paid all the bills and the account is still suspended.  I urgently need it up and running.  I have made several requests since tuesday and still no response from anyone.  I really appreciate it.  Thanks"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Can someone please help me with my account.  I have paid all the bills and the account is still suspended.  I urgently need it up and running.  I have made several requests since tuesday and still no response from anyone.  I really appreciate it.  Thanks"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Hello, the same thing happened to me as well, we haven't heard from Amazon, we opened two tickets, it is extremely important for us to reenable the account. Can you help, please? Our case number is 1360905041.

Thanks!"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Same deal, Please expedite the enabling of my account Case# 1361705261"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
I will be leaving AWS for encountering a similar such delay. 3 support tickets and 2 days later, yet still locked out of my account. 

Way to fail miserably at customer support Amazon, if you ever get around to reading this message!"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Hello, the same thing happened to me as well, extream urgent Can you help, please? Our case number is 1390544761"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Hi Concern,

               My account has been suspended, I cleared the bill and opened a ticket to reinstate my
account. Since it is a mail server all our mails were down from two days. Please helpout ASAP.

Thanks & Regards,
Bharath"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
I have paid all the bills and the account is still suspended. I urgently need it up and running.
676733943884
Please,help me!

Thanks

Adriana"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
I have the same problem. It's been more than 24 hours. My case number is 1394053931. Thank you"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Hi, I have the same problem.  I cant acces to my account, i already made the payment, i need the service a soon as posible"

Amazon Simple Storage Service (S3)	"Account Login
."

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Hi, I am having the same issue and having an open ticket since Friday night. Can somebody please help me?"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Hello all,

I have my account suspended with case id #1516007081.

Could someone please take a look at this?"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Same here,

We have all our services down and we are wating for the reactivation. We have already paid all unpayed bills. Our case is: 1556450571

Please, we need a solution, we are losing money.

Thank you."

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
HI we had a bad credit card on file, we have since since updated billing couple days ago but account is still suspended but , opened few tickets and no response ?

Account Id:
552859475310 
Account Name:
Mobile"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
Hi

My account was suspended because the credit card failed to process, but I now payed the outstanding amount. 

Can you please reactivate my account asap? It's been over 6 hours and I still haven't heard back (Case 1635447631, Account Nr: 361721873029)

Thanks

Edited by: coolasdf on Jan 27, 2016 7:10 PM

Edited by: coolasdf on Jan 27, 2016 7:11 PM"

Amazon Simple Storage Service (S3)	"Re: Account Suspended & Reactivation for Billing?
The same thing happens with me. We've paid the outstanding invoices. I opened a support ticket requesting re-instatement, but haven't heard back. My ticket support/Case ID is 1640316161. 
What do we need to do to get this turned back on?"

Amazon Simple Storage Service (S3)	"Is S3 the correct Service for my project?
Hello Lads!

I'm currently working on a schoolproject where i basically want to send data from a pi to a cloud storage once every eight hours (approximately). The data is pretty primitive ( probably just an array and a affiliated Timestamp). Furthermore the data should be stored in the cloud and it should just be possible for the sender of the data to look at the data. (probably from an mobile app i'll add on later)

So basically my question is:

Is Amazon S3 the correct service for those requirements? Do i need another service to implement these functionalities? What is the simplest way to do so? 

Thank you for your advice in advance."

Amazon Simple Storage Service (S3)	"Re: Is S3 the correct Service for my project?
Hello

from my understanding of what you want to do, you will need
1. Api gateway 
2. Lambda
3. S3 and/or DynamoDB

Api gateway receives your api call and passes that to a lambda function which writes the data to S3 or dynamoDB.  If you save the data in S3, each time you write somethign, it will become an object.  If you save it to dynamodb, it will be a new entry in the db which is faster and easier to retrieve but has extra cost.
to read, it will be api gateway, lambda, and read from S3 requires knowing the object name to get to the content.  To read from dynamo, you need the key to get it or get all entries in the db (which costs more)

hope this helps,
RT"

Amazon Simple Storage Service (S3)	"Re: Is S3 the correct Service for my project?
Ok so i've set up my DynamoDB and can add entries with a java program. Can you tell me how i can make an api which downloads certain entries using lambda and api gateway? googled but couldnt really find an solution.

Edited by: Bautista on Feb 14, 2019 1:05 PM"

Amazon Simple Storage Service (S3)	"Unable to check whether the bucket is public using API call
Hello,
I am testing API operations on AWS S3 buckets and by calling GET BucketPolicyStatus API operation (https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETVersion.html) using Postman I am not able to get response saying whether my bucket is public or not.
I tried these 2 requests:
Request 1:
GET https://bucket-name.s3.us-east-1.amazonaws.com/?policyStatus
Response 1:
<Error>
    <Code>NoSuchBucketPolicy</Code>
    <Message>The bucket policy does not exist</Message>
    <BucketName>bucket-name</BucketName>
    <RequestId>...</RequestId>
    <HostId>...</HostId>
</Error>

Request 2:
GET https://bucket-name.s3.us-east-1.amazonaws.com/bucket-name?policyStatus
Response 2:
<Error>
    <Code>NoSuchKey</Code>
    <Message>The specified key does not exist.</Message>
    <Key>bucket-name</Key>
    <RequestId>...</RequestId>
    <HostId>...</HostId>
</Error>

Could you please explain me what am I doing wrong?
Thanks in advance."

Amazon Simple Storage Service (S3)	"Re: Unable to check whether the bucket is public using API call
Hello
the request should be
GET /<bucket-name>?policyStatus HTTP/1.1
Host: <bucket-name>.s3.amazonaws.com
x-amz-date: <Thu, 15 Nov 2016 00:17:21 GMT>
Authorization: <signatureValue>

notice that there is no slash ""/"" before the ?policyStatus

here is the link for the example
https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETPolicyStatus.html

hope this helps
RT"

Amazon Simple Storage Service (S3)	"Re: Unable to check whether the bucket is public using API call
Hi rt-jaws,
the Request 2 seems pretty much the same to a request you provided, and that request throws an error for me."

Amazon Simple Storage Service (S3)	"Re: Unable to check whether the bucket is public using API call
Hello
from the documentation, the request is a little bit different
this is your original request
https://bucket-name.s3.us-east-1.amazonaws.com/bucket-name?policyStatus

the doc shows
https://bucket-name.s3.amazonaws.com/bucket-name?policyStatus

notice that it does not use the region in the call
https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETPolicyStatus.html

RT"

Amazon Simple Storage Service (S3)	"Re: Unable to check whether the bucket is public using API call
I have tried it without the region as you suggested and I received NoSuchKey error just like I described in Request 2. So unless you managed to get the proper response with that request, then either I am wrong or the documentation is not correct in this case.

Edited by: xidex on Feb 13, 2019 5:39 AM"

Amazon Simple Storage Service (S3)	"What is any other used for ""log delivery groups"" ACL
Hello.

I found document what ""Log Delivery Group"" ACL is used for S3 Access logging.
Document URL is 
https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html

but, ELB Access logs,Cloudtrail logs and VPC Flow logs does not used ""Log Delivery Group""ACL?

I can't found what is ""Log Delivery Group"" ACL is used for other.

Tell me please."

Amazon Simple Storage Service (S3)	"Upload request size
I am not able to find out how to work out the cost of uploading data on S3 & Glacier. I uploaded 25 files with a total size of 982 KB and am seeing 42 requests in my Cost Explorer report.

How does AWS calculate the upload & download request size. I have a lot of customers who want to use the low cost storage but am not able to provide an estimate on the upload charges & transition charges from S3 to Glacier.

Is there a way to calculate this? Any bets practice recommendations on the same."

Amazon Simple Storage Service (S3)	"Re: Upload request size
Hello
to answer your question, there is no cost associated with data uploads to S3 and there is no charge for the transition between S3 to glacier (but there is from glacier to S3 data retrieval)

A bit more info:
I'm also new to this and had some issues with identifying actual costs, so here is what I have found

AWS recommends using the simple monthly calculator to get an idea
https://calculator.s3.amazonaws.com/index.html

the S3 prices can be seen here, but keep in mind that it is for the file size. If you keep it in IA or glacier, there is a penalty for moving the files out in less than 30days
https://aws.amazon.com/s3/pricing/

Also there are costs associated with data transfer.  If you make copies or move the data between regions, availability zones in the same region, access through other aws services, etc. However, there is no cost for uploading data into AWS or transition (the actual move from s3 standard to glacier)
https://blog.cloudability.com/aws-data-transfer-costs-what-they-are-and-how-to-minimize-them/
https://medium.com/@datapath_io/aws-data-transfer-costs-and-how-to-minimize-them-eb56482c29b5

and finally some regions have different costs
hope this helps
RT"

Amazon Simple Storage Service (S3)	"S3: Infinitely retrying lambda event
I have an s3 event listener set on my bucket looking for PUT and POST actions on .wav files. In the past couple of weeks I've noticed that every once in a while, if a 0mb .wav file is uploaded my lambda gets called THOUSANDS of times. This is currently going on right now with no way to call off the s3 event. I've tried to delete the event, but it continues to call my lambda. Is there any way to prevent this from happening?"

Amazon Simple Storage Service (S3)	"AWS Simple Compute
wouldnt it be great if there is a better version of S3 as SimpleCompute instances based on Lambda but works like an instance

SC1 : can host html + js with S3 as storage and DynamoDB
SC2 : can host .net onecore with S3 as storage and Arora/DynamoDB
SC3 : can host lamp stack with S3 as storage and Aurora/DynamoDB
SC4 : GoogleFirebase alternate which can run .netonecore and serve as a backend for apps
the cost should be the same as S3"

Amazon Simple Storage Service (S3)	"Create SNS for Glacier Restores in S3
I setup a lifecycle rule to move objects from S3 to Glacier.  How do I setup SNS to notify me when my ""initiate restore"" request is completed when I restore my object back from Glacier?

Thanks

Edited by: witherspoon on Feb 14, 2013 7:27 AM"

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
Hello,

Currently there are 2 types of notifications that can be sent to SNS from Glacier, Archive Retrieval Job Complete and Vault Inventory Retrieval Job Complete.

This can be accessed from the Glacier console by selecting the vault and then the notifications tabs.  Here you can enable or disable the SNS notifications and specify/create the topic.

Regards,
Rob."

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
Hi Rob, since I am pushing objects to Glacier through S3 using the lifecycle rules I didn't have to create a vault in Glacier.  Maybe there's vault created for me on the backend, but I can't see that via the S3 console.

Any ideas?"

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
witherspoon - Today Amazon S3 doesn't have a mechanism to send SNS notifications on a completed S3 object restore. 

We would be interested to understand your usecase though.

Some specific questions I have are:
 - Would you be ok to get notification for every single object restore or do you prefer some grouping? 

What kind of reliability guarantees would you need for this?
Is it ok to get duplicate notifications?


Any other requirements? You can either send me a private message or post it in the forums"

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
We have a similar use case : tons of content that we archive after a specific time-to-live.  Occasionally we need to restore an archive to access it and a SNS notification would be a lot more convenient than having to poll the status every couple of minutes.  Do you know if this feature might ever be added to S3?  Or is there any better solution?

Thanks."

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
nlewis40,

Thanks for adding your feedback as well. We have captured SNS notification on Glacier retrieval as a feature request for our backlog. 

Xing"

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
We'd also really love to receive SNS notifications when an S3 object has been restored. We have a bunch of business logic that must execute after an object is restored to S3, and the only way to do this in a timely manner at the moment is via polling S3 (which is pretty crappy both for us and for AWS). 

Any idea on when this feature might be added? I see there are SNS notifications on S3 buckets when a reduced redundancy object is lost, perhaps something similar for restored objects would make sense?"

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
Hello

Is there any news regarding this feature?

Regards
Herman de Jong"

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
Are there any updates on this feature. 

We currently restore content from Glacier to S3 as part of a larger workflow and would like to use notifications to progress the workflow once the restore is complete instead of having to poll."

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
We're also looking for a similar feature. Are there any updates on this feature request?"

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
We currently use Glacier directly as part of our workflow and rely on SNS messages to trigger subsequent steps.  We would like to transition to use the S3 storage class but we need a notification mechanism.  Any update on this feature?"

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
I too would like to +1 this feature, we use Glacier restore rarely (that's the point, right?) but when we do, automating further processes and notifying the person waiting on the restore would be super helpful - this is especially true in a bucket which contains a large number of objects making listing them to determine status of one or two items being restored much more difficult."

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
I'd like to add a +1 for this feature. 

I'm a little surprised that this thread is so old and still not implemented"

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
here's anther +1
We would find it would be very useful"

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
Xing@AWS,

 5 years after your feedback...  Do you have now this feature implemented?"

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
Same here. I am looking for this exact feature. Some kind of SNS notification to let me know when an object or group of objects have been restored from Glacier to S3."

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
I'd like to +1 this as well.

The only way that I currently use Glacier is through S3.  I rarely need to restore objects back to S3 (of course, that's why they are migrated to Glacier), and when I do I am rarely in enough of a hurry to use an Expedited restore.  In fact, it can usually wait a day, so I initiate a Bulk restore and then check back every few hours.

SNS events for restore would mean that I can just let the restore happen, and have some other logic run when the restore is complete, without having to keep checking back.

In terms of grouping of notifications, I don't know how S3 keeps track of the ""jobs"" when I restore several objects.  Ideally, I would love to get a notification when such a job is complete.

But if that is difficult to achieve for a general use case, then another alternative for me would be for each object to send an SNS event when a restore is initiated, and when it is completed. I could then use Lambda to marshal all the ""start"" notifications, decide which ones constitute a ""job"", and keep track of the progress as ""completion"" notifications arrive.  In this case, duplicate notifications would not be be a problem, since the Lambda functions would be written to be idempotent."

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
I, too, would love this to be integrated (5 years later). Seems like it shouldn't be that hard since hte basics of sending the notification from glacier are already integrated into AWS services, it's just adding a notification on storage class change. 

This would be so useful for staging asynchronous restore operations that automatically pull things from a queue and copy them somewhere else."

Amazon Simple Storage Service (S3)	"Re: Create SNS for Glacier Restores in S3
This can be done now with the ""S3 Events"" feature:
https://docs.aws.amazon.com/AmazonS3/latest/dev/ways-to-add-notification-config-to-bucket.html

Set up an SNS topic with the appropriate policy and you can then set it to fire on events including start of Glacier restore to S3 as well as completion of the restore (among other options)."

Amazon Simple Storage Service (S3)	"http/2 upload to s3
How can I enable http/2 for a s3 to upload files ?

Thanks."

Amazon Simple Storage Service (S3)	"Change metadata of objects in S3 while keeping ACL / other data
Hi there!

I need to change a metadata attribute for a lot of different S3 objects.
I found out that you apparently need to copy the file over itself to change any of the attributes, this gets rid of all metadata, ACL and other information.
What I do currently using the node aws sdk:
Get all the current information
const metaData: AWS.S3.HeadObjectOutput = s3.headObject(...);
const acl: AWS.S3.Types.GetObjectAclOutput = s3.getObjectAcl(...);


and then add them to the parameters for the s3.copyObject
 with    MetadataDirective: 'REPLACE' 

But this seems very error prone to me, what if there are some properties missing?
There surely must be another way?

Very thankful for any help."

Amazon Simple Storage Service (S3)	"Call Recordings
Our recorded calls are being saved as a spreadsheet and not WAV. How do I fix this?

Edited by: gymnasticsusa on Feb 7, 2019 1:58 PM"

Amazon Simple Storage Service (S3)	"Unable to set up bucket object lock configuration using API call
Hello,
I am testing API operations on buckets and I am not able to execute the PUT Bucket object lock configuration (https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html) successfully.
According to documentation (https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html#object-lock-bucket-config):
""Before you lock any objects, you have to configure a bucket to use Amazon S3 Object Lock. To configure a bucket for S3 Object Lock, you specify when you create the bucket that you want to enable S3 Object Lock. Once you configure a bucket for S3 Object Lock, you can lock objects in that bucket with retention periods, legal holds, or both.
Note
You can only enable S3 Object Lock for new buckets. If you need to turn on S3 Object Lock for an existing bucket, please contact AWS Support.""

I tried to call this operation right after creating a bucket using Postman like this:
HTTP method: PUT
Endpoint: https://bucket-name.s3.us-east-1.amazonaws.com/?object-lock=0
Headers:
-Content-Type: application/xml
-Content-MD5 (with no value)
Request body (raw - application/xml):
<?xml version=""1.0"" encoding=""UTF-8""?>
<ObjectLockConfiguration>
   <ObjectLockEnabled>Enabled</ObjectLockEnabled>
   <Rule>
      <DefaultRetention>
         <Mode>GOVERNANCE</Mode>
         <Days>1</Days>
      </DefaultRetention>
   </Rule>
</ObjectLockConfiguration>

but it returns with the InvalidBucketState error saying:
""Object Lock configuration cannot be enabled on existing buckets""
I also checked the documentation for PUT Bucket operation to see whether I missed any hidden option to configure bucket object lock at creation time, but with no luck. 
So I would like to ask you, if it is even possible to set up bucket object lock using the PUT Bucket object lock configuration API call, or If I missed something or did something wrong.
Thanks in advance."

Amazon Simple Storage Service (S3)	"Re: Unable to set up bucket object lock configuration using API call
xidex wrote:
I also checked the documentation for PUT Bucket operation to see whether I missed any hidden option to configure bucket object lock at creation time, but with no luck. 

It looks like the API docs have not been updated with the new option yet. I'm sorry for the inconvenience; I've asked the team to update them. In the meantime, you should be able to create a bucket that supports S3 Object Lock by using the AWS CLI or SDKs: https://docs.aws.amazon.com/cli/latest/reference/s3api/create-bucket.html
aws s3api create-bucket --bucket my-bucket --region us-east-1 --object-lock-enabled-for-bucket

-Miles"

Amazon Simple Storage Service (S3)	"Re: Unable to set up bucket object lock configuration using API call
Thanks Miles for pointing me in the right direction. I found out the correct format for the missing header in PUT Bucket request. 
For anyone wondering how to enable object lock on bucket during its creation using API request, here it is:
PUT https://bucket-name.s3.us-east-1.amazonaws.com
Headers:

x-amz-bucket-object-lock-enabled: true"

Amazon Simple Storage Service (S3)	"robots.txt => where is my root domain? (sorry rookie question ^^)
Hi there,

I've got a rookie question, I've looked it up but found no satisfactory answer!
Where should I put the robots.tx file on S3 to prevent crawling? 
I know it is at the root of the domain but what/where is the domain root on S3? 

Thanks for your help!

Edited by: AdrienJ on Feb 6, 2019 11:09 AM

Edited by: AdrienJ on Feb 6, 2019 11:09 AM"

Amazon Simple Storage Service (S3)	"Re: robots.txt => where is my root domain? (sorry rookie question ^^)
Hello

it would be in the same location where you put your index.html file.
bucket/robots.txt

RT"

Amazon Simple Storage Service (S3)	"Re: robots.txt => where is my root domain? (sorry rookie question ^^)
Thanks A LOT for taking the time to answer me, I put the robots.txt file inside each of my bucket, hopefully it will solve my problem!"

Amazon Simple Storage Service (S3)	"impossible to set policy to bucket using IAM user after setting perms
Hello, 

 I have created an IAM user, and granted it with the following permission through 2 groups:

AdministratorAccess

AWS managed policy from group Administratos


AmazonS3FullAccess

AWS managed policy from group S3Admin

Permissions boundary (not set)

However when trying to set up an anonymus view perms as per the AWS serverless tutorial, I get Access Denied both from CLI and Web console.

(venv0) sivan@titan:~$ aws s3api put-bucket-policy --bucket wilderydes-sivan --policy file://policy.json

An error occurred (AccessDenied) when calling the PutBucketPolicy operation: Access Denied

This is the policy:

{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"", 
            ""Principal"": ""*"", 
            ""Action"": ""s3:GetObject"", 
            ""Resource"": ""arn:aws:s3:::wilderydes-sivan/*"" 
        } 
    ] 
}

(I do realize the tutorial has loads of errors and is perhaps not been updated for a while, but the policy doc seems right as per other resources on the AWS docs).

Your help shall be immensely appreciated!

-Sivan"

Amazon Simple Storage Service (S3)	"Re: impossible to set policy to bucket using IAM user after setting perms
Hi Sivan,

I understand that you are trying to set a policy on your S3 bucket that allows public access, however you are seeing an access denied error.

Amazon S3 has introduced 'Public Access Settings'.
Amazon S3 block public access prevents the application of any settings that allow public access to data within S3 buckets. By default, new S3 bucket settings do not allow public access, but you can modify these settings to grant public access using policies or object-level permissions.

To edit the block public access settings, please refer:
https://docs.aws.amazon.com/AmazonS3/latest/user-guide/block-public-access.html

If you still face the access denied error, please refer:
https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-bucket-policy/

Best regards."

Amazon Simple Storage Service (S3)	"Download multiple files at a time from S3
I am new to S3 and am trying to download copies of some of our files. When I go into the management console and select more than one file, however, the ""download"" option is grayed out, only appearing when a single file is selected. What am I doing wrong? Thanks!"

Amazon Simple Storage Service (S3)	"Re: Download multiple files at a time from S3
The S3 service API -- and consequently the console -- does not implement multiple-object downloads.  (Of course, S3 does support a large number of simultaneous/parallel downloads, but not on a single connection or using a standard web browser.)

You'll need to download the objects individually, or use another utility to accomplish this.

One option is the AWS CLI.

""aws s3 cp"" https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html
""aws s3 sync"" https://docs.aws.amazon.com/cli/latest/reference/s3/sync.html"

Amazon Simple Storage Service (S3)	"Re: Download multiple files at a time from S3
Hi,

In order to download multiple objects from S3, please use the AWS CLI or the AWS SDKs.

You may download multiple files from the AWS CLI using below command:
aws s3 cp s3://bucketname/path/to/folder/ local-path --recursive 
(This command will download an entire folder for you).

https://aws.amazon.com/cli/
https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html

Best Regards."

Amazon Simple Storage Service (S3)	"S3 ""Error Access Denied"" when trying to change permissions
I'm trying to update the permissions on a particular S3 bucket.  I am using this bucket as a container for a CDN and presently the CDN is returning 403 errors when I try and access it.  The documentation suggested that the bucket policy might need to allow public access to the S3 bucket in order to use it as a CDN.  When I attempt to add read-only public access to my bucket, the request fails with an unauthorized error message.  Im using the root AWS account.

What needs to be done to get my CDN working?"

Amazon Simple Storage Service (S3)	"Re: S3 ""Error Access Denied"" when trying to change permissions
Hello,

Can you paste your policy here, with removed sensitive data ofc ?"

Amazon Simple Storage Service (S3)	"Re: S3 ""Error Access Denied"" when trying to change permissions
Hi,

I understand that you are trying to set a policy on your S3 bucket that allows public access, however you are seeing an access denied error.

Amazon S3 has introduced 'Public Access Settings'. 
Amazon S3 block public access prevents the application of any settings that allow public access to data within S3 buckets. By default, new S3 bucket settings do not allow public access, but you can modify these settings to grant public access using policies or object-level permissions.

To edit the block public access settings, please refer:
https://docs.aws.amazon.com/AmazonS3/latest/user-guide/block-public-access.html

If you still face the access denied error, please refer:
https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-bucket-policy/"

Amazon Simple Storage Service (S3)	"Objects disappeared from bucket on storage change
Hey,
I have a bucket which had a policy to move to Glacier after N time.
Recently the content of the bucket became in need and I have moved the policy to IA standard.
I have also checked the entire bucket and chose to move to IA standard.
I have returned to it today (2 days later) and the bucket is completely empty 😱 with no objects, no version - nothing... like everything is deleted!
Is there anything I can do to restore? Or to retrieve the objects?
What did I do wrong?"

Amazon Simple Storage Service (S3)	"S3 Presigned URL to PUT object with additional user params not working
I am trying to generate a presigned url to put files into s3 with some additional metadata with it
GeneratePresignedUrlRequest generatePresignedUrlRequest =
    new GeneratePresignedUrlRequest(bucket, objectKey)
        .withMethod(httpMethod)
        .withExpiration(expiration);
if (params != null) {
  params.forEach(
      (k, v) ->
          generatePresignedUrlRequest.addRequestParameter(
              Headers.S3_USER_METADATA_PREFIX + k.toLowerCase(), v));
} 


where params is a Map<String, String>

but after uploading file when I try to get the object using
 AmazonS3.getObjectMetadata(bucketName, key).getUserMetadata() 

returns an empty map.

Also tried generatePresignedUrlRequest.putCustomRequestHeader(key, value) 
 But I see that in the generated url string the header values are not being sent.
I guess this requires the client code to know the headers values.

I want to add the metadata from the service which  generates the pre signed url.
and the client code would just need to PUT file into the url"

Amazon Simple Storage Service (S3)	"Re: S3 Presigned URL to PUT object with additional user params not working
I guess this requires the client code to know the headers values.

That is correct.  The purpose of s pre-signed URL is to provide a signature to authorize the exact request that the client  will be making... not to modify the attributes of the client's request.  

Headers and query string parameters are not the same thing and are rarely interchangeable.

It does not appear possible to do what you are attempting."

Amazon Simple Storage Service (S3)	"Re: S3 Presigned URL to PUT object with additional user params not working
Is there any way we can enforce the consumer of the signed url to add those headers along with the request ?
eg: throw a bad request when needed headers are not there ?"

Amazon Simple Storage Service (S3)	"Owner name
Several years ago I registered on the seller forums.   For some reason, S3 is using the username I registered there as the owner name in the ACL.  I've tried changing my nickname/pen name and public profile name.  Nothing seems to change this.

Is there any way to change this?  It's really annoying.  The forums display my updated nickname except at the top where it says ""Welcome, username""

Edited by: RJButler on Jul 2, 2011 11:15 AM"

Amazon Simple Storage Service (S3)	"Re: Owner name
Same issue here - any ideas how to change the name that shows up as owner?

The bucket owner name should be the name of the account - which is fine, but in case of one of my accounts the name is something different and I cannot seem to find the place where I can change that.

Edited by: maciej-valtech on May 22, 2015 3:13 PM"

Amazon Simple Storage Service (S3)	"Re: Owner name
Same here! I was told to try posting in this forum, so I'm hoping that a single post will ""reset"" the system. I also tried making an account on the AWS seller forum, just in case..."

Amazon Simple Storage Service (S3)	"Please explain region-TimedStorage-GlacierStaging
Hello Everyone,

I've uploaded some large backups to my s3 bucket recently. Having realized the standard storage class costs, I've modified the storage class from the web console. The result is that in the costs reports (grouped by ""usage type"") the value region-TimedStorage-GlacierStaging increased from non-existent to a daily value of USD 0.24. The web console states that the storage class is now Glacier, so I could initiate a restore. Meanwhile when I open the CloudWatch metrics for the bucket it tells me that I have 338GB of space used in GlacierStagingStorage (see: https://imgur.com/9JtYr6G). I have versioning enabled on my bucket and I also did some manual storage transition (from Infrequently Access class to Glacier) with the aws cli (the web console was unable to do the transition for ~200GB files):
aws s3 cp s3://$BUCKET/$FOLDER/$OBJECT s3://$BUCKET/$FOLDER/$OBJECT --storage-class GLACIER


After the above command finished, I removed the previous object versions (ones with storage class: IA). Is there a way to determine which objects are stuck? And is there a way to move them from the Glacier Stagin to Glacier?

Thank you in advance!

Regards,
Liet"

Amazon Simple Storage Service (S3)	"Trying to delete a bucket, but fails du to Swedish characters
I am trying to delete a bucket, from the AWS management console, without success. I have narrowed it down to one remaining file. This file has Swedish characters åäö in it's name.

No specific error message besides ""Operation failed"".

Should that really be a problem?"

Amazon Simple Storage Service (S3)	"Creating an archive (compressed zip) of multiple S3 files
In a step towards GDPR compliance, I'd like to vend clients a single archive of all of their data in our service.  On a per-user basis, we have a list of their S3 keys.  My understanding is that this is best achieved through downloading all of the objects to an EC2 instance (with EBS attached as necessary), but it seems like a very common use case.  Does a service exist to achieve this?"

Amazon Simple Storage Service (S3)	"Cannot delete file/folder
I would like to simply delete a folder from the top level of a bucket. However there's a 0kb file named, ""icon"" that cannot be deleted from S3 Management Console or any client I've tried. This appears to prevent the folder from actually being deleted. 

If I navigate to the folder within the S3 Management Console, select the file and then select Delete from the pulldown menu, the progress bar reports complete and deleted and the file disappears from the file listing. If I then move up to the root of the bucket and delete the folder, same thing... progress bar reports success and the folder disappears from the listing at left. If I then logout and reset my browser (clearing cookies, local storage and cache), when I log back in to the S3 Management Console the folder is still there as is the 0kb, ""icon"" file located inside it. 

What can a guy do here? Is there an equivalent of rm -rf * to clear the contents of a bucket? Any help would be appreciated."

Amazon Simple Storage Service (S3)	"Re: Cannot delete file/folder
Hello,

Could you please provide me with the bucket name and object inside that bucket that will not delete?

Regards,

Matt J"

Amazon Simple Storage Service (S3)	"Re: Cannot delete file/folder
Hi,

If you would like to use third party S3 Tool, then can try Bucket Explorer and use its Delete and Quick.

How to delete file/folder ?
How to quick delete (batch delete)

Get your Bucket Explorer copy from here

Thanks

Tej Kiran
Bucket Explorer"

Amazon Simple Storage Service (S3)	"Re: Cannot delete file/folder
The bucket name is jeffrogers.I need to delete the folder named test.sparsebundle and all its contents."

Amazon Simple Storage Service (S3)	"Re: Cannot delete file/folder
I'm having the exact same issue. 

I'm trying to delete my bucket ""akulbe"" and all its contents. I've got two very long filenames that won't let me delete them."

Amazon Simple Storage Service (S3)	"Re: Cannot delete file/folder
Hello,

So this is what we see jeffrogers/Documents/Test.sparsebundle/Icon%0d You need to delete the Icon object first before you can delete the rest.

If the Console will not delete it you can try a 3rd party program.

Regards,

Matt J"

Amazon Simple Storage Service (S3)	"Re: Cannot delete file/folder
Thanks. I may not have spoke clearly in the initial email, but I have tried to delete that file from both the management console and from a number of 3rd party clients. Although in both cases the files appears to be deleted, next time I log back in, it's back."

Amazon Simple Storage Service (S3)	"Re: Cannot delete file/folder
Here is a screencast of me deleting the files and them showing up again after the next login: http://www.kg6bgj.com/S3_ISSUE/"

Amazon Simple Storage Service (S3)	"Re: Cannot delete file/folder
Hi,

We are continuing to investigate the cause of your issue. I'll update this thread again when I have more information. 

Best Regards,
Adam O."

Amazon Simple Storage Service (S3)	"Re: Cannot delete file/folder
Thank you."

Amazon Simple Storage Service (S3)	"Re: Cannot delete file/folder
Hello,

So what it looks like so far is that you should set the ""folder"" to be deleted with object expiration, and then this will cause it to be deleted automatically.

Give object expiration a shot and let me know how it works for you: http://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectExpiration.html

Regards,

Matt J"

Amazon Simple Storage Service (S3)	"Re: Cannot delete file/folder
OK. That was a long way to go to delete a couple files and folders, but it worked. Any idea why I couldn't just use a 3rd party application or the management console to delete these items? I'm concerned this could happen again and it makes me reluctant to use the service.

Edited by: Jeffrey Rogers on Jan 7, 2013 3:51 PM"

Amazon Simple Storage Service (S3)	"Re: Cannot delete file/folder
I too started having this same problem... Any idea why?

Chad"

Amazon Simple Storage Service (S3)	"Re: Cannot delete file/folder
I also just discovered this same bug!  AWS has known about it since 2012 and it's still unresolved?  Hmm..."

Amazon Simple Storage Service (S3)	"Amazon S3 slow on large buckets
Hi,
I got a big amazon S3 bucket around 8 GB and now things started to get slow. Uploading files (with IAM user, so the API) takes forever. Now I recreated a new bucket with the same properties and settings and API uploading to this bucket is way faster. Is there a connection between a large bucket and slow API uploading? Do I have to make new buckets all the time?

Time to write an image of around 300 KB with API to large bucket:

imageGet: 1260ms
imageWrite: 67683ms

To a newly created bucket time:

imageGet: 1275ms
imageWrite: 822ms

On https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html it is stated:

There is no limit to the number of objects that can be stored in a bucket and no difference in performance whether you use many buckets or just a few. You can store all of your objects in a single bucket, or you can organize them across several buckets.

For me this does not seem the case, anyone?

Regards,

Dick Goosen

Edited by: dickgoosen on Feb 1, 2019 12:49 AM

Edited by: dickgoosen on Feb 1, 2019 1:22 AM"

Amazon Simple Storage Service (S3)	"How to simulate clock-skew scenario
I met this clock-skew correction error(https://aws.amazon.com/blogs/developer/clock-skew-correction/) I'm getting from S3 getobject call from Lambda, and I have made a fix to the issue(by specifying correctClockSkew setting and a retry function, this is in Nodejs)and right now is writing the unit test for the function.

Basically I'm trying to simulate the scenario where the client has a time difference of 15+ mins with S3, I have searched for few S3 mock up libraries, but have no clue how to do so.

I'm using mocha as the test framework

Thanks,

Edited by: yoli on Jan 31, 2019 1:24 PM"

Amazon Simple Storage Service (S3)	"RequestTimeTooSkewed
Why would an AWS S3 server respond with a 'Date' header from >48 hours ago? Is this a common and expected response from S3? Should I be trying to catch and retry 403s?

I use boto2 to do testing for one of my projects. This morning one of my CreateBucket requests failed with a 'RequestTimeTooSkewed' error. But the rest (~60) succeeded.

Here are the request headers, response headers, and response body (logged from boto2):
Final headers:
{'User-Agent': 'Boto/2.48.0 Python/2.7.15 Linux/4.3.5-smp-816.23.0.0',
 'Date': 'Tue, 29 Jan 2019 18:49:37 GMT',
 'Content-Length': '0',
 'Authorization': u'***'}

Response headers:
{'x-amz-id-2': '***',
 'content-type': 'application/xml',
 'server': 'AmazonS3',
 'date': 'Sun, 27 Jan 2019 18:00:01 GMT',
 'x-amz-request-id': '***'}

S3ResponseError: S3ResponseError: 403 Forbidden
<?xml version=""1.0"" encoding=""UTF-8""?>
<Error>
  <Code>RequestTimeTooSkewed</Code>
  <Message>The difference between the request time and the current time is too large.</Message>
  <RequestTime>Tue, 29 Jan 2019 18:49:37 GMT</RequestTime>
  <ServerTime>2019-01-27T18:00:02Z</ServerTime>
  <MaxAllowedSkewMilliseconds>900000</MaxAllowedSkewMilliseconds>
  <RequestId>***</RequestId> 
<HostId>mwVzD5q2JsH4sAs5AU8/8OW753T97VXUytsfsfb2G09f8jKlmmd3WArfqBqlwk2RQS/UHQfzxGs=</HostId>
</Error>


Another request at virtually the same time succeeded:
Final headers:
{'User-Agent': 'Boto/2.48.0 Python/2.7.15 Linux/4.3.5-smp-816.23.0.0',
 'Content-Length': '0',
 'Authorization': u'***',
 'Date': 'Tue, 29 Jan 2019 18:49:45 GMT'}

Response headers:
{'x-amz-id-2': '***',
 'content-type': 'application/xml',
 'x-amz-bucket-region': 'us-east-1',
 'date': 'Tue, 29 Jan 2019 18:49:46 GMT',
 'server': 'AmazonS3',
 'x-amz-request-id': '***'}


I know that Amazon uses NTP to synchronize clocks: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/set-time.html. But this appears to be more than simple clock-skew, the first server responded with a 'Date' header from >48 hours ago!"

Amazon Simple Storage Service (S3)	"Re: RequestTimeTooSkewed
This is neither common nor expected.  This sounds like a node lost its time reference, and that's nothing I ever remember hearing reported, before.  A 403 error is not typically expected to work on  retry, but in my own work I typically do retry 4xx errors once, because anything can go wrong in any system at any time.

(Technically, it's still ""clock skew"" since that phrase is used to mean ""unacceptably large clock discrepancy.""  The service, of course, assumes that the client's clock is in error, not its own.)

You might want to edit your ""RequestId"" back into the question so that support can trace down this exact request -- according to the documentation, both that value and the ""HostId"" may be needed.  Neither of these values provides any sensitive information -- they are also return in the headers of each response (success or failure, they appear as as x-amz-request-id and x-amz-id-2) and only have meaning to the engineers.  

The request ID can also be found in your bucket logs.  If you can find that request there, it might be interesting to see the timestamp on the log entry.

https://aws.amazon.com/premiumsupport/knowledge-center/s3-request-id-values/"

Amazon Simple Storage Service (S3)	"AWS S3 Part number error when transitioning to Glacier
I was able to upload a 172.3 GB file to S3 STANDARD-IA. However, when I then tried to transition the file to Glacier I received this error: ""Part number must be an integer between 1 and 10000, inclusive""

It seems like the code running the transition process is attempting to break the file up into chunks that are too small for the process to stay within the maximum number of chunks. Is there a workaround for this?

Cheers,
Mike

Edited by: MikeFinlayson on Jan 23, 2019 11:58 AM"

Amazon Simple Storage Service (S3)	"Re: AWS S3 Part number error when transitioning to Glacier
Hi Mike,

Can you please clarify how you're transitioning the file? Officially, the only way to ""transition"" objects (without overwriting or creating a new version of the object) is via a lifecycle policy:
https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html
https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html#sc-howtoset

It sounds like you are doing something different, such as copying the file in-place to a different storage class, using the AWS CLI, AWS Console, or an SDK client. What are you getting the error from, and if it is the CLI or SDK, how are you calling it?

-Miles"

Amazon Simple Storage Service (S3)	"Re: AWS S3 Part number error when transitioning to Glacier
Hi Miles,

Thanks for the response. When I received this error I was working in the AWS S3 Management Console. I had clicked on the file in the Overview tab of one of my buckets. I then clicked Actions > Change storage class > Glacier > Save. The error was then reported from a clickable progress bar widget at the bottom of the page.

Cheers,
Mike"

Amazon Simple Storage Service (S3)	"Re: AWS S3 Part number error when transitioning to Glacier
The console is apparently only able to modify (e.g. change storage class) objects under a certain size. Thanks for reporting this issue, I've forwarded this to the appropriate team.

There are a couple of workarounds you can try for changing the storage class of larger objects:

1. Set up an appropriate lifecycle rule for the bucket that targets the object whose storage class you want to change. If there are other objects in the bucket that you don't want to transition, you should be able to specify a prefix or tag filter to target the rule more specifically. https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html

2. You can use the AWS CLI with a larger chunksize to copy the object and change the storage class.
The documentation to increase the chunksize can be found here: https://docs.aws.amazon.com/cli/latest/topic/s3-config.html 

You can then change the storage class of an object by running something like this:
aws s3 cp s3://bucket/key s3://bucket/key --storage-class GLACIER"

Amazon Simple Storage Service (S3)	"Can objects in Amazon S3 access each other? Like Html file needing js files
Hi, I have a question regarding Amazon S3.

We currently store our Phaser game in S3 and we create a presigned url through the SDK to access the index.html file, which is the start point of the game. The presigned link works but we have a problem.
The problem is that it does not seem allow the index.html to have access to its css and js files in the same folder, which it needs to run.

Can we somehow give access to index.html to see the other files in the same folder on S3 or is that out of the scope of S3?

Edited by: codingcow on Jan 29, 2019 6:24 AM"

Amazon Simple Storage Service (S3)	"Re: Can objects in Amazon S3 access each other? Like Html file needing js files
Hello

I dont think that is possible because when you create a presign url, it is for just one object.
even if you create presign urls for all the files that the one file would access, the signature would be different for each, i tried just in case but it did not work.
Since you are building a game, it would be better if you use cognito to authorize your users to access the folder.  that way it is not per object.

hope this helps,
RT"

Amazon Simple Storage Service (S3)	"Re: Can objects in Amazon S3 access each other? Like Html file needing js files
Is is possible to make a presigned link for a folder on S3 or how do you propose we could do the access to the folder?"

Amazon Simple Storage Service (S3)	"Re: Can objects in Amazon S3 access each other? Like Html file needing js files
No, i'm saying do not use presign urls try making your users login(cognito). Once they are authenticated, you can grant them access to that folder and all its contents with a policy
RT"

Amazon Simple Storage Service (S3)	"How to secure objects uploaded by clients are not accessible to each other
Our portal allows logged in users to upload secure objects to s3, using bucket policy we were able to ensure the files are not accessed until the referrer is app.ourdomain.com - but if a user knows the object name of another user then they can simply use browser console to change object name and access another users object. 

Most objects are images, considering the load time we do not want to download images to server and then send it to user... we would like the images to be loaded directly from s3. What is the best way to secure these images between our logged in users."

Amazon Simple Storage Service (S3)	"Re: How to secure objects uploaded by clients are not accessible to each other
Hello
the easiest way would be to create different buckets for each user.
if that is not possible, you could try to create different folders for each user and create a policy to grant users access to their own folders.  
check this
https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html

hope this helps,
RT"

Amazon Simple Storage Service (S3)	"How to upload a file to S3 from static webpage using API Gateway and Lambda
I am new to aws and trying to create simple webpage to upload file in S3 and maintain metadata in DynamoDB. I am able to create record in DynamoDB but not able to load file in S3 from my webpage. I get the following error,

""No such file or directory: 'C:\\fakepath2018_pirl_replacement_notes.pdf': OSError Traceback (most recent call last): File ""/var/task/lambda_function.py"", line 33, in lambda_handler s3.upload_file.......""

I created index.html to create the web form which allows user to enter metadata and select the file to upload. It triggers lambda function where I want to store file in S3 bucket and metadata in DynamoDB. 

This is how I am passing the File through webpage,

 File:  
        <input type=""file"" name=""file"" id=""file""/>


In Lambda I am using 

import boto3
import os
from contextlib import closing
from boto3.dynamodb.conditions import Key, Attr

def lambda_handler(event, context):

    publicationtype = event
    formnumber = event
    catalognumber = event
    productcategory = event
    description = event
    file = event

    head, tail = os.path.split(file)
    fileName = tail
    print('File Name: ' + fileName)

    #Uploading file to S3 bucket
    s3 = boto3.client('s3')
    s3.upload_file(file, os.environ, fileName)
    s3.put_object_acl(ACL='public-read',
      Bucket=os.environ, 
      Key= fileName)

    location = s3.get_bucket_location(Bucket=os.environ)
    region = location

    if region is None:
        url_begining = ""https://s3.amazonaws.com/""
    else:
        url_begining = ""https://s3-"" + str(region) + "".amazonaws.com/"" \

    url = url_begining \
             str(os.environ) \
             ""/"" \
             str(fileName)

    print('S3 File URL: ' + url)

Can you let me know what I am missing or doing wrong?

Thanks"

Amazon Simple Storage Service (S3)	"S3 redirect not forwarding query strings
In my account I have a bucket setup for static web hosting. www.test.agencyaccess.com
I have a cloudfront distribution used to forward http requests to https, with forward query strings enabled.
I also have redirect rules setup on the bucket for specific pages.

http://www.test.agencyaccess.com will redirect correctly to https://www.test.agencyaccess.com
because of the cloudfront redirect to https.

http://www.test.agencyaccess.com/services/database/lp?fid=45 will redirect correctly to https://www.test.agencyaccess.com/services/database/lp?fid=45 and the query string is kept.

https://www.test.agencyaccess.com/database will correctly redirect to https://www.test.agencyaccess.com/services/database/lp

based on the rule:
  <RoutingRule>
    <Condition>
      <KeyPrefixEquals>database</KeyPrefixEquals>
    </Condition>
    <Redirect>
      <Protocol>https</Protocol>
      <HostName>www.agencyaccess.com</HostName>
      <ReplaceKeyPrefixWith>services/database/lp</ReplaceKeyPrefixWith>
    </Redirect>
  </RoutingRule>

The problem is:
https://www.test.agencyaccess.com/database?fid=45 redirects without the query string to https://www.test.agencyaccess.com/services/database/lp

How can i have the above correctly redirect forwarding the query string to https://www.test.agencyaccess.com/services/database/lp?fid=45

Thank you for your help."

Amazon Simple Storage Service (S3)	"Re: S3 redirect not forwarding query strings
I'm curious to hear how you were able to get query parameters preserved at all.

In my experience...
https://www.my-domain.com/some-page?param=555
...will redirect to...
https://www.my-domain.com/some-page/
...dropping all params entirely.

Does it perhaps require CloudFront to format query strings?"

Amazon Simple Storage Service (S3)	"Re: S3 redirect not forwarding query strings
Same here, took us a while to identify this behavior, lost tons of data in the meantime..."

Amazon Simple Storage Service (S3)	"links not staying in pdf when download
Hi. I'm very new to AWS and really struggle with it. I have a pdf file that I upload on my website using the S3 services so my followers can download it as a freebie. In this file I have url links that they can click on to go to certain websites if they desire. It is a secured document and I've never had any problems with the links working. However, I have noticed that now that it is an AWS file to download, it doesn't have those url links on it anymore when my followers download it. Why is that? How do I fix it? Is it in my settings? (I don't understand any of those settings when I upload files!).

Thank you so much!"

Amazon Simple Storage Service (S3)	"Cannot access my S3 bucket through https, only http
Hi all,

Problem: if I try to access my newly created static webpage hosted on S3 though https: then the browser comes back in error after some time. But I can access it normally using http: only. This is not a replication issue, the problem has been like this for a couple of weeks now.

The new domain was puchased on GoDaddy.
I have defined the S3 bucket on AWS.
I have setup the hosted zone with Route53. I then received 4 NS server names which I entered on GoDaddy side.
I then used AWS certificate manager to issue certificate for my S3 bucket. From certificate manager i was offered to configure Route53 automatically, which I did. I now see 2 CNAME entries in my hosted zone.

If I check my domain with dnschecker.org, NS comes back fine, CNAME comes back in error.

Not sure what else is needed to make this work with https."

Amazon Simple Storage Service (S3)	"Re: Cannot access my S3 bucket through https, only http
Hi,
I understand that you have a static website hosting on S3 and you are unable to access this website through https. This is because S3 static website endpoint does not supports https. If you want to serve your website over https, you can use CloudFront ahead of S3 and set viewer protocol policy as redirect from http to https. Please refer to the following link.

https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html

Best Regards"

Amazon Simple Storage Service (S3)	"Re: Cannot access my S3 bucket through https, only http
Hi,

thanks a lot for your reply above. I did read now the documentation you link to (although it's quite extensive) and I have a few questions, because it's still not working over https after I deployed my CloudFront, but I probably misconfigured some fileds.


You say ""set viewer protocol policy as redirect from http to https"". I've done this now in CloudFront configuration
When requesting my certificate, I have used the automated configuration service of AWS to define my DNS configuration. It created 2 CNAMES entries, which I have not changed. But I still have a doubt. On the first CNAME entry, under value, I entered mydomain.org (where mydomain is the name of my S3 bucket and domain name i want my user to use). Is this correct ? Then in the second CNAME entry, in the value field, i see a long number starting with _ and finishing with .hkvuiqjoua.acm-validations.aws. Is this correct ?
Then in CloudFront: I have now a distribution created with Status Deployed and State Enabled. In its configuration I see:
Origin Domain Name: mydomain.org.s3.amazonaws.com (here i can ony select this value anyway)
Origin Path: i left empty / Restrct Bucket access: No / no custom headers.
Under Behaviors tab: I set viwer protocol policy to Redirect HTTP to HTTPS


There must be a field or two that I have not set right, because HTTP works but not HTTPS.

Edited by: HuguesL on Jan 26, 2019 9:21 AM"

Amazon Simple Storage Service (S3)	"Access Denied performing putObject to S3 in Dropzone.js with a signed URL
I'm using the aws-sdk for Node to call getSignedUrl(), in order to allow a user on my site to upload a file directly to S3. The bucket is set up to be private, with a CORS configuration as shown below.

I have an IAM User, with the user policy below.  I use the access key and secret for this user to generate the signed URL. I know that the user has the necessary access to PUT and GET objects, as I've successfully tested both actions using the AWS CLI from my machine.

I've tested the returned signed URL using my dropzone code in a browser, using a cURL request and directly from POSTMAN, and in all three I get a 403 response and an 'access denied' error, as below. I've specified the 'Content-Type' header in each PUT to the signed URL. No other headers were explicitly added by me. The pre-flight request in the browser returns a 200 status.

I've also tested giving the IAM user S3FullAccess, and by putting a wildcard in AllowedOrigin in my CORS configuration. No dice with either of those.

I think this narrows it down to the way that I've created the signature. I've tried various combinations of the parameters below, over a few days, but without success.

SIGNING CODE

const S3Client = require('aws-sdk/clients/s3');
    
    const s3 = new S3Client({
      region: 'eu-west-2',
      accessKeyId: process.env.IAM_ACCESS_KEY,
      secretAccessKey: process.env.IAM_SECRET_ACCESS_KEY,
      signatureVersion: 'v4'
    });
    
    const getSignedUploadUrl = function(req, res, next) {
    
      const urlParams = {
        Bucket: process.env.S3_BUCKETNAME,
        Key: req.body.filename,
        Expires: 60 * 60 * 2,
        ContentType: req.body.contenttype
      };
      s3.getSignedUrl('putObject', urlParams, function(err, signedURL) {
        if(err){
          console.log(err);
        } else {
          res.status(200).json({
            url: signedURL
          });
        }
      });
    };


ERROR

<?xml version=\""1.0\"" encoding=\""UTF-8\""?>
      <Error>
        <Code>AccessDenied</Code>
        <Message>Access Denied</Message>
        <RequestId>*****</RequestId>
        <HostId>*****</HostId>
      </Error>


CORS CONFIG

<?xml version=""1.0"" encoding=""UTF-8""?>
    <CORSConfiguration xmlns=""http://s3.amazonaws.com/doc/2006-03-01/"">
      <CORSRule>
        <AllowedOrigin>https://*.mydomain.io</AllowedOrigin>
        <AllowedMethod>PUT</AllowedMethod>
        <AllowedMethod>GET</AllowedMethod>
        <AllowedMethod>DELETE</AllowedMethod>
        <AllowedMethod>HEAD</AllowedMethod>
        <AllowedHeader>*</AllowedHeader>
      </CORSRule>
    </CORSConfiguration>


IAM USER POLICY

{
        ""Version"": ""2012-10-17"",
        ""Statement"": [
            {
                ""Effect"": ""Allow"",
                ""Action"": [
                    ""s3:ListBucket"",
                    ""s3:GetBucketPolicy"",
                    ""s3:GetBucketCORS""
                ],
                ""Resource"": ""arn:aws:s3:::my-bucket""
            },
            {
                ""Effect"": ""Allow"",
                ""Action"": ""s3:ListAllMyBuckets"",
                ""Resource"": ""*""
            },
            {
                ""Effect"": ""Allow"",
                ""Action"": [
                    ""s3:PutObject"",
                    ""s3:GetObject"",
                    ""s3:DeleteObject"",
                    ""s3:PutObjectAcl""
                ],
                ""Resource"": [
                    ""arn:aws:s3:::my-bucket"",
                    ""arn:aws:s3:::my-bucket/*""
                ]
            }
        ]
    }"

Amazon Simple Storage Service (S3)	"Error in bucket policy to restrict access with http referrer or IP address
I want to restrict access to a bucket and have tried setting up using http referrer, and ip addresses. I have followed the documentations and the policies were applied and saved through Cloudberry. However I am getting this message for both attempts:

This XML file does not appear to have any style information associated with it. If you get the following message in your web browser; This XML file does not appear to have any style information associated with it. The document tree is shown below...

It has been a frustrating search since I cannot seem to find a clear answer about the nature of this problem. Any assistance will be most welcome.

Celito"

Amazon Simple Storage Service (S3)	"What Encryption Module does S3 Utilize?
I know that AWS S3 buckets user server side encryption to encrypt data stored on it with AES-256 for storage and such, and the keys can either be S3 managed, or managed with KMS (and therefore accessible by users), however, I am trying to determine the exact module that AWS uses for the S3 server-side encryption to help determine if the module itself is FIPS validated.
Is anyone from AWS able to confirm what S3 server-side encryption module is utilized and/or whether or not its FIPS validated? Or possibly even provide a location of where such AWS documentation is located so that I can reference it?
I have done lots of research and I have only been able to find information that the KMS service is FIPS validated, but this does not mean that the actual encryption performed on the data being stored on S3 is FIPS validated."

Amazon Simple Storage Service (S3)	"Using S3 like Time Machine
I'm a new subscriber to S3 and I've been using Time Machine on my Mac to backup an entire volume of video work. I've outgrown my hard drive storage - I want to back up an entire volume of work (12 TB) 

I'm thinking that I should perform backups monthly- Can I create a Folder on my Mac for each month of data and upload an entire folder to a bucket in one step? The folder sizes could likely be about 5-10 TB each month or so. Is that a really insane amount of data to try to upload at once? I work for an archival institution with many hours o longform video and everything really needs to be saved here and backed up."

Amazon Simple Storage Service (S3)	"Re: Using S3 like Time Machine
Hello

I believe it is doable but it all depends on your internet connection speed.

here is a tutorial on how to do something like that
https://aws.amazon.com/getting-started/tutorials/backup-to-s3-cli/

hope this helps,
RT"

Amazon Simple Storage Service (S3)	"Re: Using S3 like Time Machine
Hi, 
I understand that you want to transfer 5-10 TB of data in S3 bucket from your local machine. One way is directly uploading this data to S3 using CLI but it can take a longer time. Therefore, I would recommend you to use Snowball service as it accelerates transferring large amounts of data in and out of AWS.

https://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html

Also, as you have mentioned that you want to backup this data, you can use Glacier storage class as it will be cost effective solution but it will also take sometime to retrieve data from Glacier.

https://aws.amazon.com/glacier/features/

Best Regards"

Amazon Simple Storage Service (S3)	"bucket and file permissions
the permissions menu is not clear how to make a file publicly readable only can someone tell me what the permissions are doing and at what level I just want a couple json files to be readable along with an mp4 file for an alexa skill"

Amazon Simple Storage Service (S3)	"Re: bucket and file permissions
Hello
If you want individual files to be public, click on the file, click on actions and then click on make public.
this will make it public for both read and write, so be careful if that is really what you need.

the recommended action would be to put objects in a folder and then grant public read access to that folder by creating a policy for it, something like
{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Principal"": ""*"",
            ""Action"": ""s3:GetObject"",
            ""Resource"": ""arn:aws:s3:::bucket-name/folder-name/*""
        }
    ]
}

hope this helps
RT"

Amazon Simple Storage Service (S3)	"Re: bucket and file permissions
Hi,
I understand that you want to know about how to make a file publicly readable. This can be done easily through public ACLs in S3. Just login to your AWS console, click on the object, click on ""Make public"" button. Please make sure that you have selected ""False"" in your bucket's public access settings and your account's public access settings which will block public access through public ACLs.

Block public ACL - https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html

Best Reagrds"

Amazon Simple Storage Service (S3)	"Connection reset by peer on file upload to s3 presigned post
Hi All, I'm very new to AWS and plan to use s3 to manage and store large files that users will upload to my application. These can be 1-5 gb in many cases. I am testing a web app and a react native app that both need upload functionality. I also have a server that uses boto3 to generate presigned post urls and returns them to client, which then calls them.
When a user uploads a file, the app calls the server and gets the url and fields, then the app makes a post to the url to upload the file.

Right now, uploading files from the react js web app works perfectly. Uploading from the react native android app fails with an error: Write error: ssl=0x92f8748: I/O error during system call, Connection reset by peer. All code works if I replace the presigned url with code to upload the file to my local server.

The react native app needs to be able to upload files in background, otherwise users have to keep it open for several minutes. The only stable solution I found for this was react-native-background-upload (https://github.com/Vydia/react-native-background-upload), which uses OkHttp for its android uploads. I suspect this may be the problem, after reading this: https://stackoverflow.com/a/48503726

Thanks in advance for any help or insight! I've shared code below.

Here is my server code:
@app.route('/getpresignedurl', methods=['POST'])
def getpresignedurl():
            s3 = boto3.client('s3', aws_access_key_id=CONFIG.accesskey,
                              aws_secret_access_key=CONFIG.secretkey)
 
            post = s3.generate_presigned_post(
                Bucket=CONFIG.bucket,
                Key=CONFIG.key,
                ExpiresIn=600
            )
            return jsonify(url=post['url'],
                           AWSAccessKeyId=post['fields']['AWSAccessKeyId'],
                           policy=post['fields']['policy'],
                           key=post['fields']['key'],
                           signature=post['fields']['signature']
                           )


React js code (WORKING):
Response data contains response from server call to /getpresignedurl
const config = {
            headers: {
                'Content-Type': 'video/mp4'
            },
            onUploadProgress: progressEvent => console.log(progressEvent.loaded / progressEvent.total)
        }
...
                var b = JSON.parse(responseData)
                var form = new FormData()
                form.append('AWSAccessKeyId', b.AWSAccessKeyId)
                form.append('policy', b.policy)
                form.append('key', b.key)
                form.append('signature', b.signature)
                //File that has been selected by image/media picker
                form.append('file', this.state.selectedFile)
 
                axios.post(b.url, form, config).catch(function (error) {
                    console.log(JSON.stringify(error))
                });


React Native Code (not working):
I am using a react native library for uploading in background, because otherwise the user has to keep their app open for several minutes to upload a large file.
https://github.com/Vydia/react-native-background-upload
It uses OkHttp for uploading, which I suspect is the problem.


.then((responseData) => {
                var b = JSON.parse(responseData)
                var form = new FormData()
                form.append('AWSAccessKeyId', b.AWSAccessKeyId)
                form.append('policy', b.policy)
                form.append('key', b.key)
                form.append('signature', b.signature)
                form.append('file', uri)
                var params = {
                    'AWSAccessKeyId' : b.AWSAccessKeyId,
                    'policy' : b.policy,
                    'key' : b.key,
                    'signature' : b.signature,
                    'file' : uri
                }
                var options = {
                    url: b.url,
                    path: uri,
                    method: 'POST',
                    type: 'multipart',
                    headers: {
                        'Content-Type': 'multipart/form-data',
                    },
                    field: 'file',
                    parameters: params,
                    // Below are options only supported on Android
                    notification: {
                        enabled: true
                    }
                }
 
                console.log(responseData)
                Upload.startUpload(options).then((uploadId) => {
                    console.log('Upload started')
                    Upload.addListener('progress', uploadId, (data) => {
                        console.log(`Progress: ${data.progress}%`)
                        this.setState({
                            imageuri: `Progress: ${data.progress}%`
                        });
                    })
                    Upload.addListener('error', uploadId, (data) => {
                        console.log(`Error: ${data.error}%`)
                        console.log(`Error: ${data.error.stack}%`)
                        console.trace(data.error)
 
                        this.setState({
                            imageuri: `Error: ${data.error}%`
                        });
                    })
                    Upload.addListener('cancelled', uploadId, (data) => {
                        console.log(`Cancelled!`)
                        this.setState({
                            imageuri: `Cancelled!`
                        });
                    })
                    Upload.addListener('completed', uploadId, (data) => {
                        // data includes responseCode: number and responseBody: Object
                        console.log('Completed!')
                        this.setState({
                            imageuri: 'Upload Complete!'
                        });
                    })
                }).catch((err) => {
                    console.log('Upload error!', err)
                    console.log(err.trace)
                    console.log(err.stack)
                    console.trace(err)
                    this.setState({
                        imageuri: 'Upload error!' + err
                    });
                })
 
            })


Edited by: iocuydi on Jan 23, 2019 9:21 AM

Edited by: iocuydi on Jan 23, 2019 9:23 AM

Edited by: iocuydi on Jan 23, 2019 9:28 AM"

Amazon Simple Storage Service (S3)	"Boto3/Botocore certificate expiration
I'm using the boto3/botocore library to upload/download to/from S3.
Internally I'm using the certificate file provided by botocore for https-connections.
The certificate file 'cacert.pem' is located in the botocore directory 'vendored\requests' and includes up to 132 different certificates.
Now I'm wondering how long I can use these certificates ... 
Is it determined by the last expiration date? (which is 2046-10-06 in my version)"

Amazon Simple Storage Service (S3)	"Correct permissions and ACLs for an S3 bucket as a static website?
I'm not really clear on the correct permissions for an S3 bucket set up for a static website. With what I have set now on the bucket, I can view the .html files in a browser, and I can sync files from my localhost server to the bucket using the aws cli.

But I don't know if the overall bucket policies and ACLs are too permissive for a static site. This is what I have set:

Manage public access control lists (ACLs)
Block new public ACLs and uploading public objects (Recommended)
False
Remove public access granted through public ACLs (Recommended)
True

Manage public bucket policies
Block new public bucket policies (Recommended)
True
Block public and cross-account access if bucket has public policies (Recommended)
False"

Amazon Simple Storage Service (S3)	"AWS S3 Root user abruptly lost access to 'update bucket policy'
Issue :As a root user cannot update S3 bucket policy . get ""access denied "" error. I have been using AWS for over a year now but never experienced such issue .
How it happened :
I had multiple AWS console windows open with S3 ,IAM and Cognito.
While updating something in IAM or Cognito I got error something like IRC ""Something went wrong , please log in again. If issue still persists try to clear the browser cookies "" Then AWS logged me out of all tabs. Ever since I think I have lost access to some parts of S3. I switched browser from MS Edge to Mozilla , still cannot update S3 bucket policy.
I have screenshot of static html page in S3 that I was testing , but after this issue I cannot access that same html page .I can create a new bucket though.
Here is Credential Report from the IAM dashboard:
user <root_account>
arn arn:aws:iam::888888888888:root
user_creation_time 2017-88888888888
password_enabled not_supported
+password_last_used 2019-01-22T09:02:45+00:00+
password_last_changed not_supported
password_next_rotation not_supported
mfa_active TRUE
access_key_1_active TRUE
+access_key_1_last_rotated 2017-09-18T11:06:12+00:00+
+access_key_1_last_used_date 2019-01-20T07:41:00+00:00+
access_key_1_last_used_region us-east-1
access_key_1_last_used_service cognito-idp
access_key_2_active FALSE
access_key_2_last_rotated N/A
access_key_2_last_used_date N/A
access_key_2_last_used_region N/A
access_key_2_last_used_service N/A
cert_1_active FALSE
cert_1_last_rotated N/A
cert_2_active FALSE
cert_2_last_rotated N/A

Kindly help to restore my S3 bucket policy update root access so that I can resume my project
Thanks"

Amazon Simple Storage Service (S3)	"Upload fails for SVGs that contain xlink to a url
Hi,
I'm unable to upload any SVGs into a bucket where the SVGs contains an xlink to a url. All other SVGs upload no issue. Has anyone experienced this? Any ways to resolve this?
Thanks
Mario

Copy of an SVG in question
<?xml version=""1.0"" encoding=""UTF-8""?>
<svg xmlns=""http://www.w3.org/2000/svg"" xmlns:xlink=""http://www.w3.org/1999/xlink"" version=""1.1"" width=""24"" height=""24"" viewBox=""0 0 24 24"" id=""instagramIcon"">
	<style type=""text/css"">
		.background {
			fill: #ff0000;
			fill-opacity: 0;
			stroke: none;
		}

		.socialMediaIcon {
			fill: #233d4d;
		}

		.socialMediaIconGroup:hover .socialMediaIcon {
			fill: #ffffff;
		}
	</style>
	<g class=""socialMediaIconGroup"">
		<title id=""titleInstagram"">Instagram Icon</title>

	</g>
</svg>"

Amazon Simple Storage Service (S3)	"Re: Upload fails for SVGs that contain xlink to a url
""Unable to upload"" ... meaning, what, exactly?

Do you get an error when you try to upload the file?  An error when you try to use the file?  S3 does not do any content analysis or processing when you store an object -- it just stores the bytes you send, so there is no obviois reason why this file would specifically be a problem."

Amazon Simple Storage Service (S3)	"Re: Upload fails for SVGs that contain xlink to a url
SQLBOT,

Funny thing the < a href / > in the sample I provided in my original post was removed by the forum. Sorry just noticed that.

Yes I do get an error message but no description. Might need to track down the actual error logs instead of clicking on the error message after the upload fail.

The bucket is being used for a static website, could this be an issue?

Edited by: MarioMadunic on Jan 18, 2019 6:26 PM

Edited by: MarioMadunic on Jan 19, 2019 8:45 AM"

Amazon Simple Storage Service (S3)	"Re: Upload fails for SVGs that contain xlink to a url
Found other objects that were not uploaded. I'm using Light Gallery and the share javascript modules were not uploaded also.

Decided not to use the interactive SVGs I created. They were just social media icons and the linking was done within the SVG. So am now using Font Awesome and so far no issues.

If I still had my dev support I'd post it there and hope it is investigated by them.

Marking as answered hoping others might find it of interest.

Mario"

Amazon Simple Storage Service (S3)	"Elastic IP address not attached to a running instance per hour
Hi,

My Billing Details show that i am being charged for the Elastic IP address which is not associated to any instance. I checked multiple regions (the ones which i have used) but i am not able to see any elastic IP address anywhere.

How do i stop this charge?"

Amazon Simple Storage Service (S3)	"Re: Elastic IP address not attached to a running instance per hour
Hello,

I have checked your account but I don't see any EIPs in it.

However I can see that you released 1 EIP from ap-south-1 on 2019-01-16 and 4 EIPs from ap-southeast-1 (1 on 2019-01-15 and 3 on 2019-01-17). I assume that your issue has been resolved by now but please let us know if you need any further help.

Regards,
awstomas"

Amazon Simple Storage Service (S3)	"Re: Elastic IP address not attached to a running instance per hour
Thanks for the response. I believe I am not charged on the EIP anymore now. However, I am not sure why my put requests are increasing when I am not even using any S3 bucket. Can you please help me with that as well? Is there any such service which I am using which uses S3 bucket in background?

Thanks,
Garima"

Amazon Simple Storage Service (S3)	"Cant change redirect
i have a few things running on EC2 and i wanted to log into them directly without having to enter a port in the browser. So i followed this tutorial as the issue was basically the same as mine.
https://stackoverflow.com/questions/19349287/route-53-record-set-on-different-port 

One of them works perfectly fine and redirects to the proper port when Subdomain.domain.com is entered into the browser.

The other was my first one setting up and i was still learning what i was doing. I must have entered something wrong in the redirect box in S3 because it redirects to https//(elastic ip of ec2):port notice there is no colon between https and the backslashes.

i did change this in s3 to www.domain.com:port just like the one that works just a different port.
however its been 2 days and it still redirects to https//(elastic ip of ec2):port same as before.

I did create a new record in R53 basically subdomain2.domain.com and linked it to a new bucket same as before and it works fine. but thats not the subdomain i want to use.

All three have the same domain in s3 and the records in r53 are set up the same.
any help is appreciated.

Edited by: drmachinegun on Jan 19, 2019 2:59 AM

Edited by: drmachinegun on Jan 19, 2019 3:00 AM"

Amazon Simple Storage Service (S3)	"Error configuring a S3 bucket for a website hosted with a different vendor
I have my website domain hosted with another provider.

I am trying to create S3 buckets for my http:<domain>.net as well as http://www.<domain>.net, as per the instructions provided for AWS S3.

I got the A-record from my domain provider, which happens to be an IPv4 IP address.

In the S3 bucket's Hosted Zone's Record Set, I entered that IP address as the A-record, with 'Alias Target' turned off. I left the NS and SOA records as is. The record was accepted and saved.

Then, I tried pinging both my domain.net and www.domain.net, which fails.

What am I missing or doing wrong?  I would very much appreciate your response.  Thanks much!"

Amazon Simple Storage Service (S3)	"Re: Error configuring a S3 bucket for a website hosted with a different vendor
Hi,

Request a reply to my issue. Please note that I have updated my DNS servers
at the domain provider as Amazon's DNS servers.  Please let me know if you
need any further information. Thanks."

Amazon Simple Storage Service (S3)	"files sometimes are immediately disappear from s3 bucket after saved.
Hi guys,
I'm using s3 for store some video (include the original 264 and audio data) and images of the linked cameras.
Everything is work perfectly before, but few days ago, there have some upload data start to disappear from s3 bucket in suddenly.
I'm sure that I have no use the lifecycle policy, and the files are disappear when I just save the data.
(cameras upload -> server handle the upload data -> store to s3 -> finish -> file disappear)
I also try to use the log of s3 to check, but those logs just show ""Access denied Access denied ....."".
-
I'm using the EC2 as a middle server, and catch the upload data from cameras.
s3 is mount on the EC2, and store the upload data by fwrite. (They are under the same aws account.)
-
Is there any suggestion for this issue?"

Amazon Simple Storage Service (S3)	"Re: files sometimes are immediately disappear from s3 bucket after saved.
I also try to use the log of s3 to check, but those logs just show ""Access denied Access denied ....."".

It sounds like you are having problems accessing the log files. The actual file contents should not show an ""Access denied"" error message. Please be sure that you are accessing the logs using an authenticated request by a user that has permission to access them (which will generally need to belong to the same account as the bucket owner, unless you have an unusual setup). One thing that you may be doing is trying to open the logs in your browser using the object ""URL"", which is unauthenticated, instead of using the ""Open"" or ""Download"" functionality of the AWS Console.

-Miles"

Amazon Simple Storage Service (S3)	"Re: files sometimes are immediately disappear from s3 bucket after saved.
Hi miles, thanks for useful reply.
I will try to use the 'Download' button for downloading the log file on s3.
But for the disappear files, have any other suggestion?"

Amazon Simple Storage Service (S3)	"Re: files sometimes are immediately disappear from s3 bucket after saved.
OK guys,

After I checked the logs on S3, I can't find anything of the (delete /access) log of the disappear files.

But I found that when I remove a part code of my periodic delete file lambda process, seems the file won't be missing.
So now Im check the lambda code.
If I got any result, I will post on this thread, or if anyone have the other suggestion, please let me know.

Thanks~"

Amazon Simple Storage Service (S3)	"Re: files sometimes are immediately disappear from s3 bucket after saved.
Hi gus,

Finally, I found that the files missing are cause by my periodic delete file lambda.
There have a little mistake in my code.
After fix the code, now all the program are perfectly working.
-
Also thanks miles give the helpful reply.
Thank you~"

Amazon Simple Storage Service (S3)	"Storage Pricing infrequent is per object or acumulative storage?
Hi, I have a doubt about how is the pricing calculate for Infrequent storage classes, for both of them. For example:

Let's say that I have 2 objects in Standard Infrequent Access (S3 Standard-IA), 64Kb each. So it has a 128Kb at total. 

How much it is the cost, C1 or C2? 

C1 = 128Kb / (1024x1024) * 0.0125

C2 = 2 * 128Kb / (1024x1024) * 0.0125

My confusion is because on the pricing says they have a minimum billable object size of 128KB. 

What would be the right price?

Edited by: grovercampos on Jan 18, 2019 8:50 AM"

Amazon Simple Storage Service (S3)	"Re: Storage Pricing infrequent is per object or acumulative storage?
The minimum billable object size applies to every object stored with the Standard-IA storage class, so with two 64KB Standard-IA objects you would be billed for 256KB worth of storage.

-Miles"

Amazon Simple Storage Service (S3)	"All but one file deleted, 300 suspicious files uploaded
It appears that on Dec 11, all but one of our files were deleted from our bucket, and 300 files with names like ""code_2018-12-11-20-33-17-895BE3803819B578""  (the last part varies) were uploaded.

Has anyone seen this?  Has our account been hacked?  Could this be caused by having the wrong Public Access Settings?  The bucket is supposed to be public for reading but not for writing.

All the old files can be viewed and downloaded by pressing Versions: Show, but I cannot undelete them nor do ""Restore from Glacier"".  Do I have any options besides downloading and re-uploading each file?

We've got a free Basic account."

Amazon Simple Storage Service (S3)	"Re: All but one file deleted, 300 suspicious files uploaded
Hello,

Let's start point by point:

1) How were the files uploaded initially and how many of them were uploaded ? 
2) Where were they uploaded ? To S3 or to Glacier ? 
3) You are mentioning restore from Glacier, were they transferred there via life-cycle policy or by hand ?"

Amazon Simple Storage Service (S3)	"Process hang sometimes whiles changing the storage class
Hi,

I tried to change the storage class for some of my backups to glacier. If i change 4 files classes to Glacier, for three files the progress bar will complete 100%. For the last file sometimes the progress bar hang on 73% or 84% (Random). I am sure the s3 console is not timeout. I tried different browser."

Amazon Simple Storage Service (S3)	"Re: Process hang sometimes whiles changing the storage class
Hello,

Try using CloudBerry Explorer instead, it is free and will allow you to do that without any problems. It is surely not an AWS console replacement, but will definitely make your life much easier in some cases.

You can find it here: https://www.cloudberrylab.com/explorer/amazon-s3.aspx"

Amazon Simple Storage Service (S3)	"Error in creating S3 bucket
Hello,

After I login to my AWS account and navigate to S3, an error message ""An unexpected error occurred"" is displayed. While trying to create a bucket, I tried using a random combination of letters and numbers (ranging from 3 to 63 letters), but every time I get an error message stating that bucket name already exists. Before the error in S3 appeared, I was able to create and modify a bucket and its settings. Any help would be really appreciated. 

Thank you"

Amazon Simple Storage Service (S3)	"Re: Error in creating S3 bucket
Hello,

Try to approach the issue from a different angle a bit, for example download CloudBerry Explorer and try creating a bucket from it, it is free so you won't need to pay anything. If the error will happen again we check out the logs of the software regarding the reply from AWS in that case, it might shed some light on the issue.

Link on the software: https://www.cloudberrylab.com/explorer/amazon-s3.aspx

It would be really useful to know also, if you are using a sub-account or a root one, because this will help us understand from which part of the service the issue might be coming from."

Amazon Simple Storage Service (S3)	"S3 Put Object with Presigned URL Returns 500 Internal Error
I'm encountering an issue that I have an application (called APP below) which uploads files to S3 as a proxy. User specifies a file and send request to APP, then APP will generate a presigned url for this file and returns back to user, then user uploads file through this presigned url.
Flow like this:

USER -> APP -> <Preigned URL> -> USER -> S3 

The flow works like charm and work for many months without any problem. However, I found that it's not working in these few days from about a week ago. The presigned urls were fine but user cannot upload file through these urls, and the responses from S3 were look like this:

<?xml version=""1.0"" encoding=""UTF-8""?>
<Error><Code>InternalError</Code><Message>We encountered an internal error. Please try again.</Message><RequestId>77FB2F60284B28A7</RequestId><HostId>31mDgOSi4m0cycpvwPhoFVSDk45J+DDQDiAYuFKG+ZQmvtotEQ2R7x2R7SatpYYanN7fjn4/mRE=</HostId></Error>

The http response code from S3 was 500, and I'm sure that the server time and url expired time were correct, the issuing time was not expired. There's a retry on the same url but failed, so it's been a week that I cannot upload any file to S3.

Any idea on this issue? Anything I can do for fixing this?
Since this issue occurs on my production server, so I need to get rid of this asap.

Huge thanks for any suggestion."

Amazon Simple Storage Service (S3)	"Unable to use lifecycle policy to transition objects to Intelligent Tiering
Hey there,

I'm having trouble configuring a lifecycle policy to transition objects to the Intelligent Tiering storage class. I've tried a few different configurations, but no matter how I set it up, the objects won't transition to Intelligent Tiering after the number of days has elapsed.

As an example, I have a test bucket at s3://testing-lifecycle-policy-it. The bucket has a lifecycle policy that should transition objects to Intelligent Tiering one day after creation:

{
    ""Rules"": [
        {
            ""ID"": ""intelligent-tiering"",
            ""Status"": ""Enabled"",
            ""Transition"": {
                ""Days"": 1,
                ""StorageClass"": ""INTELLIGENT_TIERING""
            },
            ""NoncurrentVersionTransition"": {
                ""NoncurrentDays"": 1,
                ""StorageClass"": ""INTELLIGENT_TIERING""
            }
        }
    ]
}


I uploaded an object called test-1-day-lifecycle-policy-transition.txt to this bucket using the Standard storage class about 4.5 days ago, and it's still in the Standard storage class. I know that it sometimes takes S3 longer than the configured number of days to enact a transition since it only checks for transitions once per day, but 4.5 days seems safely outside of this window.

Have I configured the lifecycle policy incorrectly, or is there something specific to Intelligent Tiering that's preventing me from transitioning objects? Any guidance you can provide would be appreciated."

Amazon Simple Storage Service (S3)	"Re: Unable to use lifecycle policy to transition objects to Intelligent Tiering
Intelligent Tiering is not available for objects smaller than 128KB.  Are your test objects smaller than this?"

Amazon Simple Storage Service (S3)	"Re: Unable to use lifecycle policy to transition objects to Intelligent Tiering
Good suggestion, sqlbot. To be a little more explicit, objects smaller than 128KB can be directly stored using the INTELLIGENT_TIERING storage class, but they are not auto-tiered and are always considered ""frequently accessed"", so S3 lifecycle rules will not transition objects smaller than 128KB from STANDARD to INTELLIGENT_TIERING.

This behavior is described here:
https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html
https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html

-Miles"

Amazon Simple Storage Service (S3)	"Re: Unable to use lifecycle policy to transition objects to Intelligent Tiering
Small filesizes are almost certainly the culprit. All of my test files were very small. I'll try another test with files larger than 128kb. Thanks for the help, Miles and sqlbot!"

Amazon Simple Storage Service (S3)	"Unable delete bucket: ObjectLockConfigurationNotFoundError
I have a bucket that created by terraform time ago.

There were thousands files. I've tried to remove all of them. All of them were removed but left only one.
When I request info about files in bucket by cli or S3 UI in browser I see the left file. Then I try to delete one and nothing happens.

CLI returns response with versions of files and Delete marks. I don't know why.

I opened developer console to trace requests on s3 and mentioned the first POST request after button Delete pushed with the next response:

/proxy
code 404

response: 

<?xml version=""1.0"" encoding=""UTF-8""?>
<Error>
   <Code>ObjectLockConfigurationNotFoundError</Code>
   <Message>Object Lock configuration does not exist for this bucket</Message>
   <BucketName>my-bucket-name</BucketName>
   <RequestId>*****</RequestId>
   <HostId>********</HostId>
</Error>

I need delete the bucket but i can't. Are any ideas what to do? Thanks.

Edited by: TheML9i on Jan 17, 2019 3:36 AM

Edited by: TheML9i on Jan 17, 2019 3:37 AM"

Amazon Simple Storage Service (S3)	"MediaConvert Bug. Outputting frame_captures using  Reduced redundancy
Hey Guys/Gals.

We are having an issue where some of our out frame_captures are being set to use ""Reduced redundancy"" storage on S3. 
It seems to be complete random. Anyone else seeing this issue?

Thanks in advance"

Amazon Simple Storage Service (S3)	"S3 not using Custom KMS key with Default Encryption enabled
I have an S3 bucket that has default encryption with a custom KMS ARN (not aws-s3 default key). 

My bucket policy denies any PUTs that aren't KMS.
{
            ""Sid"": ""DenyUnEncryptedObjectUploads"",
            ""Effect"": ""Deny"",
            ""Principal"": ""*"",
            ""Action"": ""s3:PutObject"",
            ""Resource"": ""arn:aws:s3:::my-bucket/*"",
            ""Condition"": {
                ""StringNotEquals"": {
                    ""s3:x-amz-server-side-encryption"": ""aws:kms""
                }
            }
}

This prevents me from being able to PUT a file using AES256, which is great, since I want encryption to be consistent across the bucket with the custom KMS key. 

My problem is that I am allowed to use other KMS keys, other than the custom KMS key set with the bucket's Default Encryption.

If I do a simple aws CLI test with cp command:
aws s3 cp temp.txt s3://my-bucket --sse aws:kms

It will PUT the file in my bucket but using the default ""aws-s3"" key. 

I am wondering if this is a bug? I was under the impression that it would be using the buckets associated default encryption key, not the amazon aws-s3 key. I know that I can also add this header to the bucket policy to enforce the custom KMS key that I want: x-amz-server-side-encryption-aws-kms-key-id 

But am I always required to explicitly pass the KMS key id on PUT requests? I was hoping that with Default Encryption enabled with a custom KMS key that it would always use that key implicitly if nothing else was specified...

Edited by: ponyboy on Jan 15, 2019 12:35 PM"

Amazon Simple Storage Service (S3)	"Re: S3 not using Custom KMS key with Default Encryption enabled
It looks like I was able to get this behavior to work with the bucket's default encryption custom KMS key by removing the previously mentioned Deny that rejected anything other than ""s3:x-amz-server-side-encryption"": ""aws:kms"",

Now I just have a deny on the bucket policy for anything thats not the bucket's default encryption custom kms key:

        {
            ""Sid"": ""DenyIncorrectKeyIdHeader"",
            ""Effect"": ""Deny"",
            ""Principal"": ""*"",
            ""Action"": ""s3:PutObject"",
            ""Resource"": ""arn:aws:s3:::my-bucket/*"",
            ""Condition"": {
                ""StringNotEquals"": {
                    ""s3:x-amz-server-side-encryption-aws-kms-key-id"": ""my-custom-kms-arn""
                }
            }
        } 

so now when I use a cp command without passing anything, it uses the default encryption custom kms key set on the bucket and not the aws-s3 master key.   
aws s3 cp temp.txt s3://$AMAZON_S3_BUCKET

passing the correct key id works perfectly
aws s3 cp temp.txt s3://$AMAZON_S3_BUCKET --sse aws:kms --sse-kms-key-id $custom_key_id

AES256 is rejected 
aws s3 cp temp.txt s3://$AMAZON_S3_BUCKET --sse
aws s3 cp temp.txt s3://$AMAZON_S3_BUCKET --sse AES256

this command previously would assume encrypt with the default aws-s3 master key but now its also rejected
aws s3 cp temp.txt s3://$AMAZON_S3_BUCKET --sse aws:kms

hope this helps anyone else

Edited by: ponyboy on Jan 16, 2019 7:02 AM"

Amazon Simple Storage Service (S3)	"Unable to upload specific files using aws cli - Bad Gateway error
Hello,

I am trying to upload a number of different files to my bucket using the aws cli. I can upload most files just fine, but when it comes to uploading Linux kernel images (uImage and zImage files), I get the following error:
upload failed: ./uImage to s3://<bucket>/test/uImage An error occurred (502) when calling the PutObject operation (reached max retries: 4): Bad Gateway


I have only ever seen this error trying to upload Linux uImage and zImage binaries, even though other binary files work fine. Text and video files also seem to upload fine.

Any ideas why I cannot upload specific types of files? I have attached the relevant parts of the debug log.

Thanks,
Patryk"

Amazon Simple Storage Service (S3)	"UserKeyMustBeSpecified error when deleting multiple objects
Hello,

I have been experiencing UserKeyMustBeSpecified errors lately when deleting multiple objects from s3, using keys without version.

A few requests that fail with this error are K4mEo4MTufgJk1C0w33+eDc6TYF8uWBOo7eDphhevMtR5KPZ9e6spVURX7KVTJM8KZtt805RuQg=
 or USQ8tpBljHjL4BMAZ8aIzon4AVkNfQX+L88FDg7v2bLMIIx8hA740d/ztx+k3aaSDxAKERGJbZc=
.

The operation is performed in a Java lambda function, which uses the following code:

public class S3Dao {
 
    private final AmazonS3 s3;
    private Logger logger;
 
    public S3Dao() {
        BasicAWSCredentials creds = new BasicAWSCredentials(accessKey, secretKey);
        ClientConfiguration config = new ClientConfiguration();
        config.setConnectionTimeout(220_000);
        config.setClientExecutionTimeout(220_000);
        this.s3 = AmazonS3ClientBuilder.standard()
                .withClientConfiguration(config)
                .withCredentials(new AWSStaticCredentialsProvider(creds))
                .build();
    }
 
    public void deleteKeys(Collection<String> s3keysToDelete) {
        logger.log(""Deleting S3 "" + s3keysToDelete.size() + "" keys"");
        if (s3keysToDelete.isEmpty()) {
            return;
        }
        DeleteObjectsRequest deleteRequest = new DeleteObjectsRequest(bucketName)
                .withKeys(s3keysToDelete.toArray(new String[] {}));
        DeleteObjectsResult deleteObjectsResult = s3.deleteObjects(deleteRequest);
        logger.log(""Deleted "" + deleteObjectsResult.getDeletedObjects().size() + "" s3 objects"");
    }
}


Would you be able to help me?

Thanks a lot,

Sébastien Tromp"

Amazon Simple Storage Service (S3)	"Re: UserKeyMustBeSpecified error when deleting multiple objects
In case that helps - everything was fine until 2018-07-08, and the error has been happening for all requests since."

Amazon Simple Storage Service (S3)	"Re: UserKeyMustBeSpecified error when deleting multiple objects
I got an answer from support - one of the keys I was trying to delete was null. Thanks!"

Amazon Simple Storage Service (S3)	"index.html objects for subdirectories in CloudFront
I'd like these two URLs to return the index.html file within the directory:

https://www.example.com/directory
https://www.example.com/directory/

More here: https://stackoverflow.com/questions/54164128/serve-index-file-instead-of-download-prompt"

Amazon Simple Storage Service (S3)	"Re: index.html objects for subdirectories in CloudFront
Hello

if this https://www.example.com/directory points to a bucket, you can set the properties for the bucket to static website hosting.  Once you do that, you willhave the option to select the index.html file to be returned when the bucket (directory) is accessed.

hope this helps,
RT"

Amazon Simple Storage Service (S3)	"S3 deleted bucket appear in the console without region
I have deleted four buckets from the console a few hours ago but they appear without an assigned region and I cant delete them.

When I access them, it appears

Error
Data not found

And when trying to delete them again, 404 error occurs when opening

https://eu-west-1.console.aws.amazon.com/s3/proxy

Request Payload

{region: ""eu-west-1"",…}
headers: {Content-Type: ""application/x-amz-json-1.1"", X-Amz-Date: ""Sun, 13 Jan 2019 19:46:27 GMT""}
method: ""HEAD""
operation: ""HeadBucket""
path: ""***bucket name***""
region: ""eu-west-1""

Edited by: JGrinon on Jan 13, 2019 1:51 PM"

Amazon Simple Storage Service (S3)	"S3 bucket disappeared
Today I notice that the main bucket of my account 'xxxxxxxxxxxxxx' was missing. And I'm here, desperate for a solution. I created another bucket in my account with the same name, but, of course, without any file in it. 

Guys from Amazon, please, please help me to solve this problem. The most frustrating thing is that I don't know why did this happened and I'd like you to help me understand. 

Please."

Amazon Simple Storage Service (S3)	"Re: S3 bucket disappeared
Hi Julio, 

I'm sorry about the concerns caused by this. 

We're unable to provide additional information via this platform for privacy reasons, but we've replied to your case; follow up with us in the Support Center if you have any more questions or comments: http://amzn.to/aws-support-center. 

Thanks
-Mickey N"

Amazon Simple Storage Service (S3)	"Downloading S3 file names and image URL in CSV Format
My company is moving over our website domain and we were wondering if there was a way to download just the name of the images we have saved on S3 along with the image URL. We do not need the actual image, just the name. We want to preferably have it downloaded into an excel sheet/CSV file.

Is there anyway to do this? I only see an option to download the actual picture in JPG forum. 

Any possible solutions or suggestions would be much appreciated.

Thanks!"

Amazon Simple Storage Service (S3)	"Re: Downloading S3 file names and image URL in CSV Format
Hello
You can export the list of objects from your bucket to an excel file in your computer by using the CLI

 aws s3api list-objects-v2 --bucket your-bucket-name --query ""Contents[].{Key: Key, Size: Size}"" --output table >> fileName.csv

once you have your text file with the list of all the objects in it, open it in excel and make the separator a ""|"" then you can manually create the URL link to each as the bucket is always in the format
https://s3-REGION.amazonaws.com/object 
which you can create for one and then paste to the rest of the entries.

hope this helps
RT"

Amazon Simple Storage Service (S3)	"Export to S3 Permissions Error
When we try to export a VM to our S3 bucket, we get an error saying ""Error occured (AuthFailure) when calling the CreateInstanceExportTask operation: vm-import-export@amazon.com must have WRITE and READ_ACL permissions on the S3 bucket"" Now I have seen several forums where it suggests that we need to add the account vm-import-export@amazon.com to our list of accounts with permission to our s3 bucket for it to work. Unfortunately, when we go to add that account, it says ""The request contained an unsupported argument."" The bucket gives permissions to everyone for list, write, and read ACL. Further, our VM was created from an imported VHD snapshot. So we should be able to export this VM to our S3 bucket.

Anyone have any updated information on how to solve this issue? Thanks in advance."

Amazon Simple Storage Service (S3)	"Can S3 expose a HTTP URL
Hi Team ,

I a newbie to AWS .  Just to provide you with some background -- > We are offloading our server logs events to SPUNK . 

SPLUNK exposes a HTTPS URL which can accept HTTPs POST of the various log events from our server.
We then configure this SPLUNK URL in our server so that the server know to which URL the log events need to be offloaded. 

We are trying to offload our log events to S3 . From my research - I believe S3 cannot expose a HTTP URL that can continuously accept JSON requests . Is this right ?"

Amazon Simple Storage Service (S3)	"Re: Can S3 expose a HTTP URL
Hello

I found a couple of Splunk links that may give you more information
https://docs.splunk.com/Documentation/Hunk/6.4.10/Hunk/ArchivingSplunkindexestoS3
https://www.aditumpartners.com/splunk-archive-aws-s3/

this may not be exactly what you are looking for but may point you in the right direction.
hope it helps,
RT"

Amazon Simple Storage Service (S3)	"Re: Can S3 expose a HTTP URL
You are correct, S3 cannot be used to capture the contents of HTTP POST requests.

For this, consider one of these options:

API Gateway with a Lambda Proxy Integration.  This service supports processing request bodies of up to approximately 6MB (slightly less, since the HTTP headers and other request attributes are also part of this limit).  The body is passed to a Lambda function that can be written in any language that Lambda supports. 

https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html

... or ...

CloudFront with a Lambda@Edge Origin Request trigger with request body access enabled.  This supports request bodies up to 1MB, and Lambda functions written in Node.JS to process the information.

https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-include-body-access.html

----

Either of these will accomplish your purpose.  The API Gateway option seems to offer substantially more flexibility (larger payload and many more programming language choices), so the natural question might be why you would want to choose the CloudFront + Lambda@Edge option.  The answer, there, is cost.  If Lambda@Edge is suitable to your purposes, the cost per invocation is significantly lower and the minimum Lambda billable runtime per invocation is 50ms, compared to 100ms with a Lambda function running behind API Gateway.

Both solutions involve a Lambda function, where the actual logic would be implemented to do whatever needs to be done in order to process or store the incoming payload.  Lambda, if you are unfamiliar with it, is the serverless compute service offering from AWS.  It allows you to write simple (or complex) modular function code that runs, securely, on a pool of servers managed and administered by AWS... but you don't pay for the servers.  You only pay for as much or as little runtime as each function invocation requires, and as much or as little memory as you choose to make available.  Each invocation of the function (triggered by an incoming request) runs in its own ""container"" -- which for all practical purposes is an individual virtual machine."

Amazon Simple Storage Service (S3)	"Error deleting S3 bucket
Hello

I'm trying to delete an S3 bucket. Have used web and CLI with no luck. I've looked through the forums but none of those suggestions work for me. Can someone point me in the right direction? 

Here are the commands and results from command line: 
aws s3 rb s3://akiai24kx5awpu4rhchqcomhaystacksoftwarearq --force
fatal error: Unable to locate credentials

remove_bucket failed: Unable to delete all objects in the bucket, bucket will not be deleted.

s3 rm s3://akiai24kx5awpu4rhchqcomhaystacksoftwarearq/doc --recursive
fatal error: Unable to locate credentials

This is a zombie bucket. I don't need it or the data, just want it off my plate. Thanks in advance."

Amazon Simple Storage Service (S3)	"Re: Error deleting S3 bucket
Hello

the error you got from the CLI(command line interface) is not about deleting the bucket but about your CLI configuration.  It needs your secret key and access key to be able to perform whatever command you need on AWS.
On the CLI, type AWS configure and then type in (or copy and paste) 
AWS Access key ID:   
AWS secret key ID:
Default region Name:
Default output Format:

if you dont have a secret access key, 
1. Open AWS
2. go to IAM
3. Click on users, 
4. Click the user that will need to access the CLI
5. Click on the security credentials tab
6. click on create access key

copy the access key id and the secret access key.  Note that the secret access key is only shown when you create it.


hope this helps,
RT"

Amazon Simple Storage Service (S3)	"Re: Error deleting S3 bucket
Thanks RT

That was helpful but now getting another error, ""fatal error: Could not connect to the endpoint URL."" 

I've got one user showing in the AWS Manager. The name refers to a Bitnami server I used several years ago. However the endpoint URL I want to delete includes a reference to Haystack Software, makers of Arq, which at one time was the backup software for my local desktops. 

I'm guessing that the Arq user has been deleted as I can't access the bucket from Arq. Is there a way to delete the bucket when the creating user has been deleted?

Thanks
ND"

Amazon Simple Storage Service (S3)	"Re: Error deleting S3 bucket
Hello
I think the error you are getting is still related to the CLI configuration.  Check the configuration one more time and see if the region is the same region as your S3 bucket.

If you are sure it is correct, before trying to delete do a:
aws s3 ls s3://bucket-name
this should list all objects in that bucket if you have access to it.

if you need to delete a bucket that you dont have access to, you will need to go back to IAM.
1. select the user
2. click on permission
3. add permission
4. if your company manages users with groups, just add the user to the group that has access to that bucket (usual in most companies)  
5. if not, attach existing policy directly.  Found a policy that grants access to that bucket or create a new policy that grants access to that bucket.

this should do the trick; I would recommend you talk with other administrators in your company before doing this because it may impact the work of others.
hope this helps,
RT

here is a link to the CLI configuration just in case
https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html

and here is a link to IAM policies
https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html

Edited by: rt-jsaws on Jan 9, 2019 10:37 AM - retyped the s3 ls command"

Amazon Simple Storage Service (S3)	"Why expired object cannot be deleted from S3 automatically
Hi, 

I have created a bucket which applied 7 days expirion lifecycle rule. I am able to see the correct expire date after objects have been uploaded into that bucket,  but they were never deleted after their expired date were passed, and now the longest expired date is at least 6 months ago.  

Would someone please tell me how I could let S3 delete those expired objects automatically?  Is there any other cofiguration need to be set in the bucket?  Thanks a lot.

Best regards,
Brian"

Amazon Simple Storage Service (S3)	"AWS withholds its phone number = can't verify account
I'm trying to sign up for Amazon Web Services in order to use S3 for my organisation's bulk storage needs. However, I've been stymied by the AWS verification procedure that requires me to give a phone number that AWS then calls and expects me to input a PIN provided on the sign-up page. The problem is that AWS withholds its phone number; my phone service is configured to challenge all callers who withhold their number and require them to enter a valid phone number, and diverts those who don't to voicemail. So I never get the chance to enter the PIN.

Is there an alternative verification process, or am I doomed to never be able to use AWS?

TIA,

Geoff"

Amazon Simple Storage Service (S3)	"Re: AWS withholds its phone number = can't verify account
Hello Geoff,

I'm sorry for the trouble you've had activating your account with our phone verification system. To help get you up and running as quickly as possible, we can manually verify your phone with our ""Call me now"" form.

Please click the link below and fill in the requested information to request a call back.

https://aws-portal.amazon.com/gp/aws/developer/account/index.html?&action=aws-click-to-call

An AWS customer service representative will call you at the provided phone number.

I hope this helps.
-TK"

Amazon Simple Storage Service (S3)	"Re: AWS withholds its phone number = can't verify account
Thanks for your reply.

Unfortunately, your suggestion has the same issue as the 'standard' verification. I entered my phone number, clicked ""Call Me"", and waited. The UI said that my call had been connected. A couple of minutes later, the UI reported that my call with you had ended and I should click a link to place another call if I needed to speak with you again. A few minutes later a voicemail message arrived in my inbox. This had some piano music, followed by a guy named 'Anthony' who asked for my AWS account number. He seemed perplexed that I wasn't talking with him, said that he couldn' t hear me, then hung up. IOW, your call was from a withheld number, your system didn't properly respond to my system's challenge, and so your call got dumped to voicemail.

If phone verification is going to work then you need to call me from a line with disclosed CLI or else get a real person to do the dialing and enter a valid number when challenged. Can this be arranged, is there another way for me to complete sign-up, or must I look elsewhere for a storage solution?

For info, blocking calls from withheld numbers is a popular option in UK -- and so your verification process has to be losing you customers. The issue could be easily resolved by AWS providing CLI. FWIW, you might not have known about this issue since the only way that would-be customers can get to talk to your customer support dept is via your flawed call-back system, which this issue renders useless!

HTH & TIA,

Geoff

Edited on Jun 1 to add: I've just tried to get a call-back to a friend's (UK) mobile phone. At first, your system didn't place the call and I had to retry four or five times before successful connection. I did manage to 'pass' identity verification. However, I'm leaving this question as unanswered because the issue remains and I won't be able to contact AWS by phone until it is resolved."

Amazon Simple Storage Service (S3)	"Re: AWS withholds its phone number = can't verify account
I got similar problem when I'd tried to verify my phone number and get full access to my account.
I live in Belarus and when I'm trying to pass check of my phone number after an account creating I can't do it. I tried to input my phone number with or without region code. But all is wrong. When I tried receive SMS nothing happens while 10 or more minutes. When I wanted to take a phone call I got next message: ""We are having difficulty contacting you or verifying the PIN you entered. You can retry the automated phone verification process or contact Customer Service to speak with a representative."" 
I'd opened the case in the service support but don't receive any response yet
Could you help me?"

Amazon Simple Storage Service (S3)	"Re: AWS withholds its phone number = can't verify account
Do you not have access to a single phone number that can take a regular incoming call? A cell phone? A home phone? Google Voice? Can you temporarily change the behavior of the number you're trying to use? Can a friend/colleague take the call for you? 

You don't have to keep this a permanent contact number for AWS. AFAIK, it's just to prevent abuse such as spinning up many accounts to take advantage of the free tier."

Amazon Simple Storage Service (S3)	"Re: AWS withholds its phone number = can't verify account
I tried to use two another phone numbers but get not success. I think I live in ""wrong"" country."

Amazon Simple Storage Service (S3)	"Re: AWS withholds its phone number = can't verify account
I tried to change the web browser, I tried to change the OS (linux → windows XP), but I have no results.
I'm so upset and surprised at the level of AWS customer support.
My support case hangs out two days now without any answer."

Amazon Simple Storage Service (S3)	"Re: AWS withholds its phone number = can't verify account
Finally my case was resolved by Customer's Support Service. One polite and intelligent guy called me and now all's OK."

Amazon Simple Storage Service (S3)	"Enforcing SSE-C on a bucket with a policy
Hi,
Is it possible to enforce SSE-C on a bucket using policy or any other means? 

Thank you,
Mateusz"

Amazon Simple Storage Service (S3)	"Your usage has exceeded your free tier usage limit
So I had signed up a month ago.  Haven't done anything with my account until 12/31/18 - at least not much up till them.  I went thru one of the tutorials with the unicorn website and location api.  Set it all up and tested.  The next morning got up to an email saying I have used all my posts for the month of January - 2000 of them.  How?  So during the wee morning hours there were 2000 Put Requests of Amazon S3.  Glad the requests stopped there and didn't cost me something but I am trying to figure out how this would have happened.  Any help is appreciated!"

Amazon Simple Storage Service (S3)	"Re: Your usage has exceeded your free tier usage limit
Hello

I think i have done the same example you mention with the unicorns (WildRydes)
The example has  a website with many objects (html, jpg, js, etc) so depending on how many times you uploaded the content, it might have used the posts.
Remember each time you modified something, you had to upload it again. If you were uploading folders instead of individual files, that could be the cause.

the email is not sent right away so even if you used all the posts, the email would have been delivered at a later time.
if you want to be super sure about this, you could enable cloudtrail on your bucket (but it has a cost)

if you think someone was adding objects to your bucket overnight, check the permissions.  The example asks for the bucket to be public read but maybe it is set as public write.

hope this helps,
RT"

Amazon Simple Storage Service (S3)	"Re: Your usage has exceeded your free tier usage limit
Thanks RT that helps!  It's weird that is stopped right at 2000 lol - and hasn't move up since.  I did delete the project.  Thanks again!"

Amazon Simple Storage Service (S3)	"Error during upload. (duplicate hostname)
Hi, 
As of yesterday every time I try to upload a file to my S3 bucket via the S3 Javascript SDK I get the following error:

error during upload to s3 { Error https://forums.aws.amazon.com/: Hostname/IP does not match certificate's altnames: Host: thumbnails-xxxx-com.thumbnails-xxxx-com.s3.eu-central-1.amazonaws.com. is not in the cert's altnames

It seems the problem is that the host name is being put in there twice. I've tested with multiple different files and can't seem to find a way around this.

Any help would be great."

Amazon Simple Storage Service (S3)	"Account compromised - Huge bill generated and its going on increasing
My Account number - 638478281407

Yesterday my account got compromised - as soon as I received that email from AWS in the morning, I opened my Bill-dashboard - saw a huge bill being charged to my account - Its absolutely not mine. 

So per the advice in that email from AWS, I deleted that ""user5"" ( the one which has taken hold of my account )  from IAM and also deleted the only single s3-bucket that I had in my account. 

I have created two issue ( Case ID - 5469130611, 5467924411 ). Am waiting for almost four hours now after I created those cases and having received NO reply yet. And in between this time seeing infront of my eyes the charge got increased. 

Thank you,
Rohan"

Amazon Simple Storage Service (S3)	"Re: Account compromised - Huge bill generated and its going on increasing
Same here. It happened to me over the month of December 2018. I opened a ticket too."

Amazon Simple Storage Service (S3)	"what is the best way to move data from standard-ia back to standard
I know it is unusual but we come cross a situation some SIA data need to be back to standard, due to the retrieve charges added on SIA. Has anybody had similar experience? there doesn't seem to be any AWS official way, any suggestions would be appreciated.

Thanks in advance"

Amazon Simple Storage Service (S3)	"Intelligent Tier - what counts as object access?
I've transitioned all objects in a bucket to the new Intelligent Tier using a 0 day Lifecycle. I did this on Dec 1st. I'm pretty sure the majority of my objects have not been accessed in the past 30 days, though I can't confirm this since AWS doesn't expose last accessed date. I haven't seen any decrease in my S3 costs though, so it appears S3 is treating my objects like they have been accessed. 

Does the lifecycle transition count as object access, and I should expect to start to see savings in the new year assuming nothing has been accessing these objects?

Regards,

David"

Amazon Simple Storage Service (S3)	"Re: Intelligent Tier - what counts as object access?
I'm interested too.
For AWS moderators: any news? The documentation ( https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html#sc-dynamic-data-access ) should provide more details about this topic, I've opened an issue, https://github.com/awsdocs/amazon-s3-developer-guide/issues/40 , +1 if you're interested as well."

Amazon Simple Storage Service (S3)	"Re: Intelligent Tier - what counts as object access?
I'm just starting to see intelligent tier - infrequent access fees on my statement now. It's been 30 days since i transitioned my objects, so it would appear transitions count as access (unsurprising).

Would still be good to know (as francescocambiaso77 mentioned) if things like metadata access count. Perhaps a listing of API operations that cause ""access"" in this context could be provided."

Amazon Simple Storage Service (S3)	"Uploads to S3 fail with OptionsRequestDenied error
With the change of the backend in S3, I cannot upload .html files anymore. Error code is OptionsRequestDenied which I cannot find any support for. 

Please advice"

Amazon Simple Storage Service (S3)	"Re: Uploads to S3 fail with OptionsRequestDenied error
I'm also getting this when using the new console UI for specific directories.  However, if using the AWS CLI (""aws s3 cp ..."") it works fine."

Amazon Simple Storage Service (S3)	"Re: Uploads to S3 fail with OptionsRequestDenied error
Really weird. I have couple different accounts. One is still with old interface and another with new. Both allow me to upload .html files successfully.

Have you tried using CLI or any other tools? Say, Cloudberry Explorer, it has free version.(https://www.cloudberrylab.com/free)

Try it. Hope it will help.

Edited by: AlViKo on Mar 3, 2017 5:14 AM"

Amazon Simple Storage Service (S3)	"Re: Uploads to S3 fail with OptionsRequestDenied error
It still fails to upload html files onto S3. Now it doesn't even show an error code but just ""100% fails"", please see screenshot. 

I am not working with CLI, also cannot test it since the Cloudberry tip is a Windows solution.

I'd really just want to figure out what's wrong within the new UI in S3 :-/"

Amazon Simple Storage Service (S3)	"Re: Uploads to S3 fail with OptionsRequestDenied error
I'm also encountering this error with the new UI.  CLI works fine."

Amazon Simple Storage Service (S3)	"Re: Uploads to S3 fail with OptionsRequestDenied error
Same issue here.  If I switch to the ""old"" console I don't have this issue."

Amazon Simple Storage Service (S3)	"Re: Uploads to S3 fail with OptionsRequestDenied error
If you have an ad blocker, try disabling it for console.aws.amazon.com. I am using Privacy Badger, and disabling it fixed uploads."

Amazon Simple Storage Service (S3)	"Re: Uploads to S3 fail with OptionsRequestDenied error
Thank you! This worked for me too."

Amazon Simple Storage Service (S3)	"Re: Uploads to S3 fail with OptionsRequestDenied error
+1 . . . this was it."

Amazon Simple Storage Service (S3)	"Re: Uploads to S3 fail with OptionsRequestDenied error
I'm getting this same error. Never had it before. Been using the folder for quite some time. It's not a big file. Nothing special or weird about it. A previous post mentioned Privacy badger and Adblocking extensions. Those are turned off, so that's not my issue. What should I think that AWS produces an error that has zero documentation around it? ""Options Request""? I'm uploading a file. What does that have to do with options? What options? There's no metadata on the folder. There's no settings or security that could've been messed up. I keep running into these non-documented bugs with AWS... Things take 10x longer than if I had just been using linux. I'm starting to think I'm wasting my time with this company.

Edited by: IanRae on Dec 5, 2017 7:18 PM"

Amazon Simple Storage Service (S3)	"Re: Uploads to S3 fail with OptionsRequestDenied error
Thank you for the tip! I was having trouble uploading a zip file, and as others mention, a less than usable error message.

But I turned off my uBlock and worked perfectly again. 

Don't know why people complain so much about the service in this thread. This was the FIRST time I've ever had trouble and it was because of an ad blocker, not a AWS issue.

Been using them for some time, love the service, so easy to use and the price can't be beat. Alot cheaper than more disk space from my host, or any host for that matter, they all seem to charge a premium for storage."

Amazon Simple Storage Service (S3)	"Re: Uploads to S3 fail with OptionsRequestDenied error
I was getting this same error.

In my case the problem was the file I was attempting to upload to S3 was a Symlink (Linux), and the symlinked file had been deleted."

Amazon Simple Storage Service (S3)	"Re: Uploads to S3 fail with OptionsRequestDenied error
This worked for me. Thanks. After disabling AdBlocker extension (BraveBrowser), error was gone."

Amazon Simple Storage Service (S3)	"How CONTENT-MD5 header value is calculated for MultiPart Completion POST?
How is the CONTENT-MD5 header value computed in case of multipart upload completion?
As per the following article:
https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadComplete.html
most common request headers are supported. 
But there is no content in the POST request apart from the 
<CompleteMultipartUpload>
  <Part>
    <PartNumber>PartNumber</PartNumber>
    <ETag>ETag</ETag>
  </Part>
  ...
</CompleteMultipartUpload>

I tried calculating the CONTENT-MD5 by using md5sum of request body and performing base64 encoding, but that did not work.

For example, below is capture of S3browser request, the CONTENT-MD5 in the header does not match the encoded md5sum of the body:
POST /bucketxxxxx/filename?uploadId=OTc2NzgwNTItMzZjNS00MWJlLTk3Y2YtOWNlNDU5MjI0MGFl HTTP/1.1
User-Agent: S3 Browser 8-1-5 https://s3browser.com
Authorization: AWS xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Content-MD5: z8hR0GST3Hm6f518pQ7XUQ==
x-amz-date: Thu, 20 Dec 2018 04:22:54 GMT
Host: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Content-Length: 1173
<CompleteMultipartUpload><Part><PartNumber>1</PartNumber><ETag>""3e27d45b10b2d069a00eb079a62b90ea""</ETag></Part><Part><PartNumber>2</PartNumber><ETag>""eea2223793a13e29bbef110f92b29e40""</ETag></Part><Part><PartNumber>3</PartNumber><ETag>""b9bf9ef5a37f63c72eb108767e64d56f""</ETag></Part><Part><PartNumber>4</PartNumber><ETag>""0667c6da478617af1cf86c3f4f0e86ac""</ETag></Part><Part><PartNumber>5</PartNumber><ETag>""5433c390b9fc98fd90b9c0334f56beba""</ETag></Part><Part><PartNumber>6</PartNumber><ETag>""4539dcbbac5f028637d17180a674881e""</ETag></Part><Part><PartNumber>7</PartNumber><ETag>""b079a02747e444b95bec8d3374e1b6df""</ETag></Part><Part><PartNumber>8</PartNumber><ETag>""9f22ca90596f4ff455e1ab79ba12fdc0""</ETag></Part><Part><PartNumber>9</PartNumber><ETag>""c573cc702aa3f17cea3ca4248e6c20e9""</ETag></Part><Part><PartNumber>10</PartNumber><ETag>""b6d53f23fd0fa4bb5b8e702f0cdc2661""</ETag></Part><Part><PartNumber>11</PartNumber><ETag>""e780dd0053d08366b062deb640d61e4d""</ETag></Part><Part><PartNumber>12</PartNumber><ETag>""a212f6c80c8511c6a26405f909dacc70""</ETag></Part><Part><PartNumber>13</PartNumber><ETag>""5530ed86f6423ad8ab1c0cb2a703e9ef""</ETag></Part></CompleteMultipartUpload>

root@VM# echo -n ""<CompleteMultipartUpload><Part><PartNumber>1</PartNumber><ETag>""3e27d45b10b2d069a00eb079a62b90ea""</ETag></Part><Part><PartNumber>2</PartNumber><ETag>""eea2223793a13e29bbef110f92b29e40""</ETag></Part><Part><PartNumber>3</PartNumber><ETag>""b9bf9ef5a37f63c72eb108767e64d56f""</ETag></Part><Part><PartNumber>4</PartNumber><ETag>""0667c6da478617af1cf86c3f4f0e86ac""</ETag></Part><Part><PartNumber>5</PartNumber><ETag>""5433c390b9fc98fd90b9c0334f56beba""</ETag></Part><Part><PartNumber>6</PartNumber><ETag>""4539dcbbac5f028637d17180a674881e""</ETag></Part><Part><PartNumber>7</PartNumber><ETag>""b079a02747e444b95bec8d3374e1b6df""</ETag></Part><Part><PartNumber>8</PartNumber><ETag>""9f22ca90596f4ff455e1ab79ba12fdc0""</ETag></Part><Part><PartNumber>9</PartNumber><ETag>""c573cc702aa3f17cea3ca4248e6c20e9""</ETag></Part><Part><PartNumber>10</PartNumber><ETag>""b6d53f23fd0fa4bb5b8e702f0cdc2661""</ETag></Part><Part><PartNumber>11</PartNumber><ETag>""e780dd0053d08366b062deb640d61e4d""</ETag></Part><Part><PartNumber>12</PartNumber><ETag>""a212f6c80c8511c6a26405f909dacc70""</ETag></Part><Part><PartNumber>13</PartNumber><ETag>""5530ed86f6423ad8ab1c0cb2a703e9ef""</ETag></Part></CompleteMultipartUpload>"" | openssl md5 -binary | openssl enc -base64
cLC/vE8pS43jD2ZSgUB5iQ==

Any pointers?

Regards,
Bhaskar"

Amazon Simple Storage Service (S3)	"Re: How CONTENT-MD5 header value is calculated for MultiPart Completion POST?
The Content-MD5 is indeed base64 encoded md5 of the body/content (also confirmed by s3browser support team).

The mistake above was not escaping the double quotes

root@VM:~$ echo -n ""<CompleteMultipartUpload><Part><PartNumber>1</PartNumber><ETag>\""""3e27d45b10b2d069a00eb079a62b90ea\""""</ETag></Part><Part><PartNumber>2</PartNumber><ETag>\""""eea2223793a13e29bbef110f92b29e40\""""</ETag></Part><Part><PartNumber>3</PartNumber><ETag>\""""b9bf9ef5a37f63c72eb108767e64d56f\""""</ETag></Part><Part><PartNumber>4</PartNumber><ETag>\""""0667c6da478617af1cf86c3f4f0e86ac\""""</ETag></Part><Part><PartNumber>5</PartNumber><ETag>\""""5433c390b9fc98fd90b9c0334f56beba\""""</ETag></Part><Part><PartNumber>6</PartNumber><ETag>\""""4539dcbbac5f028637d17180a674881e\""""</ETag></Part><Part><PartNumber>7</PartNumber><ETag>\""""b079a02747e444b95bec8d3374e1b6df\""""</ETag></Part><Part><PartNumber>8</PartNumber><ETag>\""""9f22ca90596f4ff455e1ab79ba12fdc0\""""</ETag></Part><Part><PartNumber>9</PartNumber><ETag>\""""c573cc702aa3f17cea3ca4248e6c20e9\""""</ETag></Part><Part><PartNumber>10</PartNumber><ETag>\""""b6d53f23fd0fa4bb5b8e702f0cdc2661\""""</ETag></Part><Part><PartNumber>11</PartNumber><ETag>\""""e780dd0053d08366b062deb640d61e4d\""""</ETag></Part><Part><PartNumber>12</PartNumber><ETag>\""""a212f6c80c8511c6a26405f909dacc70\""""</ETag></Part><Part><PartNumber>13</PartNumber><ETag>\""""5530ed86f6423ad8ab1c0cb2a703e9ef\""""</ETag></Part></CompleteMultipartUpload>"" | openssl md5 -binary | openssl enc -base64
z8hR0GST3Hm6f518pQ7XUQ=="

Amazon Simple Storage Service (S3)	"Help with Redirect
Hello. I have a lot of images in an bucket that I have copied to a new bucket into a folder of the same name.
I would like to redirect all requests to the new bucket. (301 redirect)
Does anyone know if this is possible? If so, what method can I use to do this?

Example: 
Bucket: norcal-s3
New Bucket: bucket-123 with folder ""norcal-s3/""

Much thanks ahead of time.

Warren"

Amazon Simple Storage Service (S3)	"Error on s3.getObject call to read file
Hello everyone! First post to AWS forums and new to AWS, so please go easy on me!

Using the AWS JavaScript SDK, I am trying to read a specific S3 bucket as an authenticated Cognito user signed into a protected area of my website. I have verified that I am getting in as an authenticated user, by virtue of the valid authToken that I get after confirming my Cognito Identity. In fact, I have tested and proven out the entire Cognito flow of registering, multi-factor verifying, signing in and signing out.

For some reason, when I issue my first s3.getObject request I get the following TypeError even though I'm CERTAIN I am passing a string (the Key) as the first argument. Here's the error:
TypeError: First argument must be a string, Buffer, ArrayBuffer, Array, or array-like object.

Now, first things first, I include the JavaScript SDK via a script tag on the HTML page as follows:
<script src=""https://sdk.amazonaws.com/js/aws-sdk-2.154.0.min.js""></script>

I have the following IAM role created by Cognito:
Cognito_MyAppNameAuth_Role

To which I have added the following S3 policy:
{
            ""Sid"": ""VisualEditor1"",
            ""Effect"": ""Allow"",
            ""Action"": ""s3:ListBucket"",
            ""Resource"": ""arn:aws:s3:::my-bucket-name""
        },
        {
            ""Sid"": ""VisualEditor2"",
            ""Effect"": ""Allow"",
            ""Action"": ""s3:GetObject"",
            ""Resource"": ""arn:aws:s3:::my-bucket-name/*""
        }

the Visual Editor inserted those Sids automatically after I edited it

Now, the code that I execute to read my S3 bucket resides in a .js file that I include via a <script> tag. It runs after the <body> loads...
<body onLoad=""bodyOnLoad();"">

The first thing the code does is verify the user credentials. I get the User Pool; then I get the current user. Then I get the session. And then I call for the user credentials using AWS.CognitoIdentityCredentials() as follows:
AWS.config.update({ //testing... 12/29
  region: 'us-east-1',
  accessKeyId: 'anything',
  secretAccessKey: 'anything',
  credentials: new AWS.CognitoIdentityCredentials({
     IdentityPoolId: 'us-east-1:<identity-pool-id>',
     Logins: {
        'cognito-idp.us-east-1.amazonaws.com/us-east-1-<user-pool-id>': session.getIdToken().getJwtToken()
        }
     })
});

I literally specify ""anything"" for the accessKeyId and secretAccessKey, as I do not believe those are required. When this runs, there are No errors. Everything appears in order with verifying my authenticated user belonging to the required Cognito Identity Pool.

Here is the s3.getObject attempt that happens next and results in the TypeError given above:
  const s3 = new AWS.S3({
    apiVersion: '2006-03-01',
    params: {
      Bucket: 'my-bucket-name'
    }
  });
 
  var params = {
    Bucket: 'my-bucket-name',
    Key: 'index.json'
  };
 
  s3.getObject(params, (err, data) => {
    if (err) {
      // EXECUTION LANDS HERE!
      callback(err);
      return;
    }
 
    const content = data.Body.toString('utf-8');
    callback(null, content);
  });
}

I have tried many permutations on the way I'm passing params to the getObject method. Including..
s3.getObject({Key: 'index.json'}, (err, data) => { ... }

I get the same error every time.
What in the world am I doing wrong???
Thank you in advance!!!

ANSWER
First I had to remove the bogus settings of accessKeyId and secretAccessKey; that revealed the true error:

Error: Invalid login token. Issuer doesn't match providerName


Then I found I was missing an underscore in my UserPoolId in the Logins section above! Correction here:

Logins: {
   'cognito-idp.us-east-1.amazonaws.com/us-east-1_<user-pool-id>': session.getIdToken().getJwtToken() //underscore after us-east-1_


Edited by: motivus on Dec 31, 2018 4:56 PM -- Inserted paragraph returns

Edited by: motivus on Jan 1, 2019 9:08 AM - Made clear use of IdentityPoolId and UserPoolId in credentials code.

Edited by: motivus on Jan 1, 2019 12:45 PM - Answered question after getting help outside this forum."

Amazon Simple Storage Service (S3)	"Re: Error on s3.getObject call to read file
See inline answer to original question."

Amazon Simple Storage Service (S3)	"S3 Lifecycle rule with unversioned buckets
How does a lifecycle rule on an S3 bucket apply to a bucket with no versioning?  When you create a lifecycle rule, the rule appears to say ""Transition to <storage class> X days after object becomes non-current"".  But if there's no versioning, then no objects become non-concurrent, so the lifecycle rule just never takes effect?  or is there additional configuration I need to do to enable it?"

Amazon Simple Storage Service (S3)	"Re: S3 Lifecycle rule with unversioned buckets
Hello

Transition and Expiration in a non-version bucket happen as soon as date is reached.
the noncurrent version transition and noncurrent version expiration have no effect on non-version bucket.
there is a detail description in table format at this link
https://docs.aws.amazon.com/AmazonS3/latest/dev/intro-lifecycle-rules.html

hope this helps,
RT"

Amazon Simple Storage Service (S3)	"Re: S3 Lifecycle rule with unversioned buckets
This unfortunately seems to be incorrect.  I have a bucket which has objects of varying size ( 1.1KB up to 162 MB ), all of which are older than 3 months.  I recently ( several days ago ) applied a lifecycle rule to the bucket that should have transitioned all object older than 30 days to ONE_IA.  The lifecycle policy has now been in effect for 2 days, and yet all my objects are still in Standard class according to my console.  

As I understand the documentation, the minimum size my objects must be in order to transition is 128KB.  What I expect to see is some objects migrate to ONE_IA ( the ones larger than 128KB), and the smaller object stay at STANDARD, unless I'm mistaken about the documentation."

Amazon Simple Storage Service (S3)	"Re: S3 Lifecycle rule with unversioned buckets
So it seems to be the case, and maybe it's documented somewhere, that S3 Lifecycle Policy now requires you to have Versioning on the bucket for Storage Class transitioning, but not for expiration.  I enabled Versioning on the bucket in question after waiting 4 days for the policy to take effect.  1 day after enabling Versioning the lifecycle policy transitioned my items to One-IA as expected. 

I would like it if someone from AWS could directly confirm this and point me to the point in the documentation that states this, because I could not find it."

Amazon Simple Storage Service (S3)	"How to map AWS S3 Bucket as a Network Drive in Windows 10?
We have an external 2TB hard drive. All its content is already copied to a bucket in AWS. As it is right now we need to always carry that external drive with us. Now when all the files are copied to a bucket in AWS we want simply skip tht external drive and continue working directly with the bucket.   

Our question is ""How to map Amazon Web Services S3 Bucket as a Network Drive in Windows 10?""

We know already that there are several software on the market that you can purchase in order to map AWS S3 Bucket as a Network Drive in Windows! So please stop recommend these! We know them all already!

At other discussion forums, there are people that that say ""this is impossible due to Amazons intentionally restrictions as they don't want that users do that"".

But we can't accept those thoughts as there are so many applications on the market that can map AWS S3 Bucket as a Network Drive in Windows. So some how this is possible!

We have an Access key and Secret Key.

Can we write any program or code in Amazon Lambda, Jason or AWS Tool for Windows,  or Command Line Interface? How about Visual Studio or Java?

Regards / F"

Amazon Simple Storage Service (S3)	"S3 acceleration
Hi Guys...have a query.. Transfer acceleration is enabled still not getting much speed in syncing file to S3 bucket from efs.. . Using any tools helps in acceleration?
OS: Linux

Thanks"

Amazon Simple Storage Service (S3)	"Amazon S3 upload errors
I've been using S3 for about a week now and haven't had any errors until yesterday. Now everytime I upload a video, it will Error. Most of the time, after spending an hour to upload, it will Error at 98% or somewhere in the 90%'s.I've rebooted, and tried various things but it's still not working.I downloaded a FTP client called CrossFTP. I uploaded the video into my bucket, made it public, but the video wouldnt play at all. It only plays when I upload it through the web console, but like I said the recent uploads have been all erroring.Any ideas?"

Amazon Simple Storage Service (S3)	"Re: Amazon S3 upload errors
Hard to say where the failure is: could be in your browser or OS or Internet connection.  

Is your browser up to date?   Have you tried it with a different browser (Firefox, IE, Chrome)?

If this doesn't work, you need to isolate the problem by excluding some variables.  For instance, try the Amazon command line tools to upload so you know it isn't the browser.

Also...S3 uses HTTP to GET/PUT files.  I don't believe it supports FTP so an FTP client might not be your best bet.

Hope this helps."

Amazon Simple Storage Service (S3)	"Re: Amazon S3 upload errors
What is the command line tools?This is getting really fraustrating. I spent all day trying to upload a movie. It always gets to about  95% and then restarts or errors all together."

Amazon Simple Storage Service (S3)	"Re: Amazon S3 upload errors
Check out CloudBerry Explorer freeware to see if it works any better. But our experience suggests that if you have an issue with one of the tool most of the other tools fail too. The issue is in your internet connection. ThanksAndy"

Amazon Simple Storage Service (S3)	"Re: Amazon S3 upload errors
My apologies.  I thought Amazon offered a command line tool to upload/download from S3 but I'm not finding it now.  I'll post back if I find it.  

In the meantime, you could try http://spaceblock.codeplex.com/.  It works well for us."

Amazon Simple Storage Service (S3)	"Re: Amazon S3 upload errors
I keep getting this all the time and it is crippling my productivity!  I use the s3 server like an FTP to host movie files for our website and to deliver large files to clients.  It happens much more frequently on large files (400MB-1.5GB).  My upload speeds are pretty slow so I will let a file upload all day, or leave it to run overnight, only to see it randomly restart when it's almost complete and then usually fail at around 90% on the 2nd or 3rd attempt.  I can't remember the last time I've successfully uploaded a large file to my s3 bucket.  

Mac OSX 10.6.7
Safari 5.05

Our company did recently install a new firewall and I'm worried that might be causing this.  I haven't yet contacted our ISP to ask them but will be doing so shortly.  

None of those other pieces of software seem to be available for Mac, so I can't try any of them.  I haven't tried uploading with Firefox yet to see if it's browser-related.  I will test that now.  

If any other solutions to this have been found please let me know!
Thanks.

--
Tyler"

Amazon Simple Storage Service (S3)	"Re: Amazon S3 upload errors
Just had a successful 821MB file upload using Firefox.... Interesting...  Trying another one now."

Amazon Simple Storage Service (S3)	"Re: Amazon S3 upload errors
I'm not sure I'd use Firefox s3fox for files that large due to the risk of a dropped upload and lengthy resume. 

Various desktop client programs exist which allow for multiple concurrent uploads, as well as multipart resumable uploads. Examples might be S3Browser or Cloudberry Explorer and I think Transmit (Mac)

HTH, Cheers - Neil"

Amazon Simple Storage Service (S3)	"Re: Amazon S3 upload errors
Thanks, streamdart.  

Dropped uploads and lengthy resumes is exactly what was happening to me in Safari.  So far switching to Firefox and just using the plain old web interface seems to have improved things.  I've uploaded two large files successfully and I'm starting on a third.  

Transmit seems to be the only one of those programs available for Mac.  It sounds good, I'm going to check it out!"

Amazon Simple Storage Service (S3)	"Re: Amazon S3 upload errors
This discussion was very helpful.  I was having problems where all of a sudden I could no longer upload a file to a bucket.  I was using Safari 12.0.2 on a MacBook Pro.  I tried using Chrome, and that solved the problem."

Amazon Simple Storage Service (S3)	"ListBucket Action URI for use in API Gateway Integration Request
This documentation https://docs.aws.amazon.com/apigateway/api-reference/resource/integration/  (under the heading uri) appears to suggest there are two ways to interface with an AWS service API, an action query string or path. The examples in the documentation demonstrate the using the GetObject query string and its equivalent path. Both of these working in my environment:

arn:aws:apigateway:us-west-2:s3:action/GetObject&Bucket=my-bucket.com&Key=index.html
arn:aws:apigateway:us-west-2:s3:path/my-bucket.com/index.html

However I couldn't work out how to call the ListBucket action  https://docs.aws.amazon.com/AmazonS3/latest/API/v2-RESTBucketGET.html as a query string to list the objects in a bucket. I tried the flowing but it didn't appear to work:

arn:aws:apigateway:us-west-2:s3:action/ListBucket&Bucket=my-bucket.com

Help is appreciated"

Amazon Simple Storage Service (S3)	"Access Denied when viewing some files with a versionid in the querystring
I currently have a version controlled bucket with the following policy:

{
	""Version"": ""2008-10-17"",
	""Statement"": [
		{
			""Sid"": ""PublicReadGetObject"",
			""Effect"": ""Allow"",
			""Principal"": {
				""AWS"": ""*""
			},
			""Action"": ""s3:GetObject"",
			""Resource"": ""arn:aws:s3:::mysubdomain.mydomain.com/*""
		}
	]
}

If i browse to a file in the bucket, then click on the properties, the interface provices a https url to the document along with a versionid in the querystring.  When i click to view the file i get a access denied error.  If i remove the ?versionid=1234 from the querystring the document displays properly.  It seems to do this on some files but not others and I can't put my finger on it.  

does not work:
http://mysubdomain.mydomain.com/portals/1179/projects/proj_70902/annotations/1136058/myfile.xml?versionId=T4PJNMVOb95F9EjeekdbZwgFNNsX0kfU

works:
http://mysubdomain.mydomain.com/portals/1179/projects/proj_70902/annotations/1136058/myfile.xml"

Amazon Simple Storage Service (S3)	"Re: Access Denied when viewing some files with a versionid in the querystring
Hello.

I'm sorry for the inconvenience you are experiencing.

I'm not exactly sure of the cause of your issue but if you provide the specific bucket name and object name of the files in question, I'll be happy to take a look.

At first glance, I don't see anything wrong with your bucket policy but there could be an object permission issue within the bucket properties itself. 

FYI: Here is an example of how S3 object versioning should work: http://aws.typepad.com/aws/2010/03/amazon-s3-versioning-now-ready.html

Thanks.
Brandon"

Amazon Simple Storage Service (S3)	"Re: Access Denied when viewing some files with a versionid in the querystring
Thanks Brandon!  try this:

https://s3.amazonaws.com/files.devplans.com/portals/1096/projects/proj_61826/0001.+G-001+-+General+OPTIONAL.pdf?versionId=CfIkJCzoW4FX98sval7Bor3Td6qdaTh1

vs

https://s3.amazonaws.com/files.devplans.com/portals/1096/projects/proj_61826/0001.+G-001+-+General+OPTIONAL.pdf

policy is:
{
	""Version"": ""2008-10-17"",
	""Statement"": [
		{
			""Sid"": ""PublicReadGetObject"",
			""Effect"": ""Allow"",
			""Principal"": {
				""AWS"": ""*""
			},
			""Action"": ""s3:GetObject"",
			""Resource"": ""arn:aws:s3:::files.devplans.com/*""
		}
	]
}"

Amazon Simple Storage Service (S3)	"Re: Access Denied when viewing some files with a versionid in the querystring
idtdev01 - Accessing an object by specifying a versionId is a different operation than accessing without it. If you need to access using versionId, please also include ""s3:GetObjectVersion"" action in the Bucket policy in the Action section.

Thanks
Praveen"

Amazon Simple Storage Service (S3)	"Re: Access Denied when viewing some files with a versionid in the querystring
And.... fixed.  Thanks Praveen!"

Amazon Simple Storage Service (S3)	"Re: Access Denied when viewing some files with a versionid in the querystring
versionId is tricky. You enable versioning in a bucket, you need to add ""s3:GetObjectVersion"" to the ""bucket policy"" under ""Actions"". Then your lambda or browser will let you access the object with versionId. 
I faced this issue while making a s3.getObject(new GetObjectRequest(bucket, key, versionId)) call.  
It worked without versionId but failed with it."

Amazon Simple Storage Service (S3)	"S3 event won't work when filter name contains whitespace
I created a S3 event notification to trigger a lambda function. The event trigger should only work on a specific ""folder"", therefore, I set the filter. When the filter name contains a space char, like the original folder name does, then the event notification does not work. 
I checked this and created a parallel folder with underscores instead of spaces and adapted the event config, and this works out-of-the-box.

In the documentation I found nothing that either forbids spaces in s3 folder names or in the event filter setting. Also I assume that every valid key name in S3 can be a valid filter for the event."

Amazon Simple Storage Service (S3)	"Re: S3 event won't work when filter name contains whitespace
Hey there DebugKing2,

If i understood your question correctly, than everything is working as intended. According to AWS Documentation spaces could be lost sometimes, especially in the beginning of the name judging from my experience, here is the small screenshot of the doc where it is explained:

http://prntscr.com/lklnut

I wouldn't suggest using spaces in any way basically."

Amazon Simple Storage Service (S3)	"Re: S3 event won't work when filter name contains whitespace
Hi cloudberryserge.

As spaces should be avoided, we don't have everything under control. 

Unfortunately nothing works as expected. I can create a key in S3 with spaces in it. And I can create a S3 event with spaces in it. Either when I set the event in the console or via cli I don't get any error messages nor warnings. 
Just imagine a key for ""s3://bucket/travel docs/"" and the filter for the event is set to ""travel docs/"". This is the situation where the event is not firing. I think that this is not correct, because when I'm able to create a key with space, it should somehow be visible or warning. Else it would be very inconsistent when I can create sth and without a warning can't use it."

Amazon Simple Storage Service (S3)	"Re: S3 event won't work when filter name contains whitespace
Now I played a little bit around and assumed what may be needed. I have put a + sign for the space, then it works. Looks like AWS for the event filtering compares the uriencoded key with the unsanitized input from the user, which is very bad. I would consider this a bug and for now I have a workaround."

Amazon Simple Storage Service (S3)	"Re: S3 event won't work when filter name contains whitespace
For proper event filtering you have to set uriencoded strings as filter."

Amazon Simple Storage Service (S3)	"Re: S3 event won't work when filter name contains whitespace
Oh interesting investigation, i have also searched a bit of documentation on AWS Doc and found nothing really regarding such behaviour, maybe AWS guys would be able to comment on this ?"

Amazon Simple Storage Service (S3)	"Re: S3 event won't work when filter name contains whitespace
I'm a little bit hoping for this, but after some weeks I have given up that they take a look here."

Amazon Simple Storage Service (S3)	"Re: S3 event won't work when filter name contains whitespace
Hi,

When you create objects/folders in a bucket, the special characters in a prefix name might require additional code handling and likely will need to be URL encoded or referenced as HEX as mentioned in our public doc below.

https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#object-key-guidelines-special-handling

For example,

Special characters                   URL Encoded form

space character                                          + 
+                                                             %2B

In your case,  when a new folder is added to bucket with the key(containing a space character)  ""travel docs"", then you need to the following in the s3 event notification:
""prefix"":""travel+docs""

Also, I have forwarded your request to consider the special character in the prefix name while setting the Event Notification to the concerned team for review. Having said that, I don’t have a specific timeline to share at this point. 

Thanks!"

Amazon Simple Storage Service (S3)	"Bug - Incorrect field name when adding lifecycle rule via console
Steps to reproduce this bug:

1. Open the S3 console (https://s3.console.aws.amazon.com/)
2. Open a bucket
3. Click on the Management tab
4. Click on ""Add lifecycle rule""
5. Enter a rule name
6. Chick Next
7. Under Storage class transition, select ""Current Version""
8. Under For current versions of objects, click ""+ Add transition""
9. The label above the field where you enter the number of days says ""Days after objects become noncurrent"" instead of ""Days after creation"" as shown in the documentation (https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html)

Please let me know if I'm missing something."

Amazon Simple Storage Service (S3)	"Re: Bug - Incorrect field name when adding lifecycle rule via console
Hi,

I tried to replicate the steps, but it's working fine on my end. For current versions of objects, when I clicked ""+ Add transition"" the label above the field where we enter the number of days says ""Days after creation"". I'm attaching the screenshot ""lifecycle.png"" of the same.

Could you please provide us the screenshot of the issue that you're facing?

Thanks.

Edited by: shubham-aws on Dec 26, 2018 9:09 PM"

Amazon Simple Storage Service (S3)	"S3 ""Make Public"" function failing
Hi Guys,

The Make Public function in S3 is perfectly working these past few months and years I should say.
Just last week, we are experiencing failure in the ""Make Public"" function. Once I successfully upload a folder (with multiple files) then Make Public the uploaded files, the notification below shows Failure  

Please see screenshot below.

Please help"

Amazon Simple Storage Service (S3)	"Re: S3 ""Make Public"" function failing
Hi,

I have a similar problem. When I use ""Make Public"" it does go to 100% and says successful but remains in ""In progress"" section and increments number in ""Successful"" row as you can see in screenshot. This never happened before when I was using ""Make Public"" on the earlier versions of the same folder.."

Amazon Simple Storage Service (S3)	"Re: S3 ""Make Public"" function failing
hi, I am also having the same problem, The Make Public function does not seem to be working ... any ideas  ... ????"

Amazon Simple Storage Service (S3)	"Re: S3 ""Make Public"" function failing
I am also having this same issue. Has anyone figured out a fix??"

Amazon Simple Storage Service (S3)	"Re: S3 ""Make Public"" function failing
I fixed my problem by setting permissions to ""Grant public read access to this object(s)"" in step 2 of Upload process in section Manage public permissions."

Amazon Simple Storage Service (S3)	"Re: S3 ""Make Public"" function failing
This worked perfectly for me. Thank you so much!!"

Amazon Simple Storage Service (S3)	"Re: S3 ""Make Public"" function failing
I get an ""Access Denied"" error when I attempt this step."

Amazon Simple Storage Service (S3)	"Re: S3 ""Make Public"" function failing
Hey guys, in case you are still interested or maybe want to pass this knowledge to your colleagues here is a nice article regarding this process:

https://www.cloudberrylab.com/resources/blog/how-to-create-website-using-aws-free-tier/

We have a nice free tool which will help you in managing those files you are uploading called CloudBerry Explorer S3: https://forums.aws.amazon.com/https://www.cloudberrylab.com/explorer.aspx[/URL]. Hope it helps."

Amazon Simple Storage Service (S3)	"Re: S3 ""Make Public"" function failing
Hi,

Reading through the thread, it looks like the issue you're facing because in a recent update Amazon S3 block public access prevents the application of any settings that allow public access to data within S3 buckets. This section describes how to edit block public access settings for one or more S3 buckets. For information about blocking public access using the AWS CLI, AWS SDKs, and the Amazon S3 REST APIs, see Using Amazon S3 Block Public Access (https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html) in the Amazon Simple Storage Service Developer Guide.

Reference -> https://docs.aws.amazon.com/AmazonS3/latest/user-guide/block-public-access-bucket.html

In order to resolve it, You need to assign public Access to the bucket, following these steps:

1. In the permissions tab click on public Access settings
2. Click on edit
3. Deselect the Block new public bucket policies option
4. Save your changes
5. Go back to bucket and try the operation again.

Hope this solves the issue. Let us know if you have any other query/concerns.

Thanks!"

Amazon Simple Storage Service (S3)	"Make Public to file give error ""Access Denied""
I am fairly new to AWS, and I am going through courses online to learn, however in the past I have been able to upload a file to a bucket and then make it public, now I am getting and error saying ""Access Denied"". This is all done with root with an AWS account that is new. I have tried different browsers and files but still getting same result and have found no information on why this may be happening."

Amazon Simple Storage Service (S3)	"Re: Make Public to file give error ""Access Denied""
Hey there, it could be tricky doing it for the first time.

The error you are getting usually means that you either haven't set the bucket permissions correctly or generally got confused in IAM. We have a nice article regarding hosting a web-site from AWS S3 Bucket, you can find it here - https://www.cloudberrylab.com/resources/blog/how-to-create-website-using-aws-free-tier/ you can use either our product or AWS Console doesn't really matter in that case it will give you a nice idea on how to do it in total. Cheers !"

Amazon Simple Storage Service (S3)	"Re: Make Public to file give error ""Access Denied""
I found a work around (if not a solution).
1. Go to the ""Permissions"" tab of your bucket.
2. Click on the ""Edit"" option (on the far right).
3. Remove the checkmarks from all 4 options.
4. Save (and confirm).
5. Go back to the overview tab.
6. Click on the checkmark for the file you want to make Public.
7. Under ""Actions"", select ""Make Public"".
IF it works for you as it worked for me, your file should be accessible for everyone.

Edited by: PauloAbreu on Nov 23, 2018 9:13 AM"

Amazon Simple Storage Service (S3)	"Re: Make Public to file give error ""Access Denied""
I found a work around (if not a solution).
1. Go to the ""Permissions"" tab of your bucket.
2. Click on the ""Edit"" option (on the far right).
3. Remove the checkmarks from all 4 options.
4. Save (and confirm).
5. Go back to the overview tab.
6. Click on the checkmark for the file you want to make Public.
7. Under ""Actions"", select ""Make Public"".
IF it works for you as it worked for me, your file should be accessible for everyone."

Amazon Simple Storage Service (S3)	"Re: Make Public to file give error ""Access Denied""
Yep seems to be that help resolve the issue. Thanks"

Amazon Simple Storage Service (S3)	"Re: Make Public to file give error ""Access Denied""
Hi,

Reading through the complete thread, it looks like the issue has been resolved now.

FYI, please know that Amazon S3 block public access prevents the application of any settings that allow public access to data within S3 buckets. This section describes how to edit block public access settings for one or more S3 buckets. For information about blocking public access using the AWS CLI, AWS SDKs, and the Amazon S3 REST APIs, see Using Amazon S3 Block Public Access (https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html) in the Amazon Simple Storage Service Developer Guide.

Reference - https://docs.aws.amazon.com/AmazonS3/latest/user-guide/block-public-access-bucket.html

Thanks"

Amazon Simple Storage Service (S3)	"S3 Bucket Policy - access denied
I added a bad policy to my S3 Bucket and now I cannot access the bucket, I keep getting permission denied. Is there anyway to change the policy or delete the bucket? I have full permissions and still I am unable to do anything."

Amazon Simple Storage Service (S3)	"Re: S3 Bucket Policy - access denied
Hi Michael,

I've locked myself out of a bucket before so you're not alone! The root user of your account will be able to remove the bucket policies that are causing problems. This can be done by using the AWS CLI tool (with root AWS account credentials) and the following command:

aws s3api delete-bucket-policy --bucket yourbucketname

More information on the CLI tool and the delete-bucket-policy command can be found here:

http://docs.aws.amazon.com/cli/latest/userguide/installing.html
http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html
http://docs.aws.amazon.com/cli/latest/reference/s3api/delete-bucket-policy.html

You might also be able to sign in to the AWS web console as the root user and edit or delete the bucket policy by clicking the ""delete"" button within the Bucket Policy Editor.

I hope this helps!

Kind regards,
Chris"

Amazon Simple Storage Service (S3)	"Re: S3 Bucket Policy - access denied
Hello, I know this thread has been inactive but I was hoping you might be able to help me with an issue I have had now for weeks.  I am attempting to paste the following script into a bucket I created but each time I attempt to save it I am presented with a red circle with an exclamation inside saying 'Error: Access Denied':

{
  ""Version"":""2012-10-17"",
  ""Statement"":[{
    ""Sid"":""PublicReadGetObject"",
        ""Effect"":""Allow"",
      ""Principal"": ""*"",
      ""Action"":,
      ""Resource"":[""arn:aws:s3:::example-bucket/*""
      ]
    }
  ]
}

If you know of how to bypass this issue I would truly appreciate your help."

Amazon Simple Storage Service (S3)	"Re: S3 Bucket Policy - access denied
Same problem, I can't save backet policy or make any file public, getting ""Access Denied"""

Amazon Simple Storage Service (S3)	"Re: S3 Bucket Policy - access denied
I am having the same problem, this means I cannot set up any s3 static websites, can't make the bucket public"

Amazon Simple Storage Service (S3)	"Re: S3 Bucket Policy - access denied
You need to assign public Access to the bucket, following these steps:

1. in the permissions tab click on public Access settings

2. click on edit

3. deselect the Block new public bucket policies option 
4. save your changes

5. go back to bucket policy and try again."

Amazon Simple Storage Service (S3)	"Re: S3 Bucket Policy - access denied
You need to assign public Access to the bucket, following these steps:

1. in the permissions tab click on public Access settings
2. click on edit
3. deselect the Block new public bucket policies option 
4. save your changes
5. go back to bucket policy and try again."

Amazon Simple Storage Service (S3)	"Re: S3 Bucket Policy - access denied
You need to assign public Access to the bucket, following these steps:

1. in the permissions tab click on public Access settings
2. click on edit
3. deselect the Block new public bucket policies option 
4. save your changes
5. go back to bucket policy and try again."

Amazon Simple Storage Service (S3)	"Re: S3 Bucket Policy - access denied
Thank you very much minoyy, your response has enabled me to bypass the Error - Access Denied!"

Amazon Simple Storage Service (S3)	"Re: S3 Bucket Policy - access denied
Hi,

Reading through the complete thread, it looks like the issue has been resolved now.

FYI, please know that Amazon S3 block public access prevents the application of any settings that allow public access to data within S3 buckets. This section describes how to edit block public access settings for one or more S3 buckets. For information about blocking public access using the AWS CLI, AWS SDKs, and the Amazon S3 REST APIs, see Using Amazon S3 Block Public Access (https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html) in the Amazon Simple Storage Service Developer Guide.

Reference - https://docs.aws.amazon.com/AmazonS3/latest/user-guide/block-public-access-bucket.html

Thanks"

Amazon Simple Storage Service (S3)	"AWS deleting files and not moving/copying to other buckets
AWS is now not allowing me to move fiels from one foler to another, if i do, it says it transferred them but they never show up. In the meantime, since they were cut, they are now COMPLETELY REMOVED from my S3 site, which means i have no backup. Anyone want to shed some light on there being a ""hole in the boat""? 

They said my data was safe, but it is destroyed by their own functions/data transfer protocols? I need that data back. 

I sat on the phone and got bounced around 3 times by AWS CS, they did not even know where to send me."

Amazon Simple Storage Service (S3)	"Re: AWS deleting files and not moving/copying to other buckets
Hello
since moving is showing that it completes successfully, could it be that somewhere in the command being use the destination is not what it seems?

aws s3 mv s3://bucket/ /bucket/foo/ --recursive \    --exclude """" --include "".jpg"" --include ""*.txt""
this example moves jpg and txt files from the bucket to a local folder 

https://docs.aws.amazon.com/cli/latest/reference/s3/index.html

hope this helps
RT"

Amazon Simple Storage Service (S3)	"Signature problem when uploading accented files to S3
Hello,

We recently upgraded aws-java-sdk from 1.3.27 to 1.11.348 version (5 years+ versions gap), and we are now having error messages about the signature of the data we send to S3.

The error message is as follows:
The request signature we calculated does not match the signature you provided. Check your key and signing method. (Service: Amazon S3; Status Code: 403; Error Code: SignatureDoesNotMatch; Request ID: 6DC175C59AFED59A; S3 Extended Request ID: W3gKgvRolIkymMf8IigLnt+dVbs7xpNhyeAPfQWJHkt1uAP/e2G/o1J3+f66a1+XSfVxnI9EkjM=)

The problem only occurs when we upload files with accents in the filename. When we normalize the filename, the upload works correctly.

Also, when uploading from the S3 console inside the bucket, the accented filename is accepted as well.

As we understand from the page https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html, the signature is done automatically in the sdk.

For now we have to normalize the filename to allow users to upload, and we also have to provide some backward-compatibility code to allow previously uploaded (accented) files to be downloadable.

The code gets therefore complicated, and as we would like to keep the file name as it is on the users computers, we need to find a solution to this signature problem.

Do you have an idea about how to fix this issue ?
Thank you."

Amazon Simple Storage Service (S3)	"Re: Signature problem when uploading accented files to S3
Problem has been solved. Didn't take the time to post followup, so I'm closing it since we don't remember exactly what was done to fix it."

Amazon Simple Storage Service (S3)	"Access Denied Issue
Hi!
I recently made a bucket to test out my website. Unfortunately, after uploading all my files, I can no longer make them public. I removed my only bucket policy, however that did not make a difference. I tried implementing a new bucket policy as well, however I get the same error ""Access Denied"".

Thanks!"

Amazon Simple Storage Service (S3)	"Re: Access Denied Issue
Same issue here.
Even as root-user, or with newly created buckets, I'm getting Access Denied.
My configuration:

region eu-west-1,
all 4 ""block public access-properties"" in s3-bucket-admin console are set to false
accessed via Web-console as root-user
accessed via Web-console as IAM-User with s3-fullaccess and even with root-user
accessed via aws s3api as IAM-User with s3-fullaccess
 aws s3api put-bucket-policy etc.. 
 -> ""An error occurred (AccessDenied) when calling the PutBucketPolicy operation: Access Denied""


all with no luck."

Amazon Simple Storage Service (S3)	"Re: Access Denied Issue
I'm having this exact same issue!

My region is us-west-2 and if I try to change the permissions for anything in my bucket I receive an ""Access Denied"" error.

I did all the same checks that you did and I can't find a way around it..."

Amazon Simple Storage Service (S3)	"Re: Access Denied Issue
it's weird - but finally I found the missing Options :-))
In the S3 Management Console, there is at the left side a global menu with two navigation-targets:
1) Buckets => that's the default selected
2) ""Public access settings for this account"" => that was the missing place, where I was able to change the global, at account-level applying four public permission-change policies.

After I market all four options to false, I was able to define fine-graned permissions at bucket-level, even by setting custom bucket-policies.
""AccessDenied"" is gone!"

Amazon Simple Storage Service (S3)	"Synology and S3 -  Exception occurred while backing up data
Hi,
i've been using S3 with my Synology for over 2 years, using Hyper Backup.
For some reason since 2 days this doesn't work anymore.
I get these errors:
Error,2018/12/21 21:14:14,SYSTEM,https://forums.aws.amazon.com/https://forums.aws.amazon.com/ Failed to backup data.
Error,2018/12/21 21:14:14,SYSTEM,https://forums.aws.amazon.com/https://forums.aws.amazon.com/ Exception occurred while backing up data. (Network connection timed out. Please check your network settings or the firewall settings.)

This error comes after at least 30 minutes of which the Synology device was uploading several MB's (checked this trough my TPLink network console)
Any idea where to check or what to check?
I can't seem to see any issues on my TPLink setup.
My internet provider checked the backup times and said it doesn't block any outgoing traffic.
S3 credentials were re-entered so should be fine, otherwise I would get specific errors i would assume.

Thanks in advance for any help or ideas.

Edited by: Ganzenman on Dec 23, 2018 10:33 PM"

Amazon Simple Storage Service (S3)	"Beginner AWS S3/Glacier questions
Hi all,

Just starting out with AWS and had a few questions..

1) What are requests? And what constitutes a request?  How can I figure out how many requests my backup product (Veeam, Rapid Recovery) will be making? 

2) Our backup products do not support directly backing up to glacier, and I'd like to save costs as we will only restore from AWS in a disaster recovery situation so my plan was to backup to S3 first then move things over to Glacier. My question what is the best way to go about this? And how do I calculate costs since data will only be stored on S3 momentarily."

Amazon Simple Storage Service (S3)	"Re: Beginner AWS S3/Glacier questions
Heya ITAdmins,

1) What are requests? And what constitutes a request? How can I figure out how many requests my backup product (Veeam, Rapid Recovery) will be making?

In your case you will be using PUT requests on the upload. Put primarily means the number of uploads/updates that are being done on an object.

Requests will be charged and the amount you are charged depends on the storage type and the region. For an example of US-Standard and Standard or Reduced Redundancy Storage for 10,000 requests, you will be charged 49 cents. If you go for infrequent access, you will be charged 1 USD at max, prices may vary from your region ofc.

2) Our backup products do not support directly backing up to glacier, and I'd like to save costs as we will only restore from AWS in a disaster recovery situation so my plan was to backup to S3 first then move things over to Glacier. My question what is the best way to go about this? And how do I calculate costs since data will only be stored on S3 momentarily.

In this case i can suggest trying out CloudBerry Backup solution, we support Pure Glacier and Glacier storage class, the first one allows you to upload directly into the Glacier vault nullifying any extra costs. And we are reasonably priced also. You can check-out a 15 day Trial and see everything for yourself.

Link: https://www.cloudberrylab.com/backup/server/windows.aspx

Edited by: cloudberryserge on Dec 22, 2018 1:47 AM"

Amazon Simple Storage Service (S3)	"Unable to browse bucket content with web interface
Hi,

So, I am experiencing what appears to be a technical issue on the AWS side, but without a way of contacting support directly, here goes.

I am trying to browse the content of a S3 bucket using the web interface.

There are several files/folders in there, therefore there are several pages of results.  By default, page 1 displays results 1-300. Also by default, ""Versions"" parameter is set to ""Hide"" in the UI.

With this default view, If I click the right arrow (>) to jump to the next page, everything works as expected.

However, when I choose to display the versions (Versions = Show), I am now unable to browse the folder. I click the right arrow (>) to load the next page; nothing happens and I am stuck on page 1 forever (results 1-300).  There are no error messages or warnings.

I have verified this behavior using a different bucket under the same account, as well as other buckets on a different account - the result is the same.

Yes, I have already: 1) cleared my browser cache/cookies  2) tried it on a different browser  3) tried it on a different computer.

This seems to me like a bug/glitch on the AWS Console side.  How do I report this issue if I'm not ""allowed"" to contact AWS support?  And most importantly, how do we get this fixed so I can access my resources? (obviously, this is the bigger issue right now - I can't access my stuff from the web console!)

Any help is appreciated - thanks much"

Amazon Simple Storage Service (S3)	"Re: Unable to browse bucket content with web interface
Hey there Joncay,

In my opinion it looks like some kind of local issue, i have just now reproduced you scenario on my side and few machines in our office, also tried using VPN just in case, however we also might be from different countries thus accessing the AWS from different endpoints. Anyway just so you know, you can use our tool called CloudBerry Explorer until this issue is fixed, i is free of charge for personal usage and in fact got some really useful you would like and surely versioning support. You can find it here:

Link: https://www.cloudberrylab.com/explorer/amazon-s3.aspx

Let me know if you would have any questions. Cheers"

Amazon Simple Storage Service (S3)	"Query params using golang sdk
Hi,
    I am trying to get the list of objects beyond a certain date. Its very simple using the aws cli like
aws s3api list-objects --bucket foo --query 'Contents[?LastModified>=`2018-12-10`][].{Key: Key}'


However, i dont see any way of doing this using the golang sdk. I am using this api https://docs.aws.amazon.com/sdk-for-go/api/service/s3/#S3.ListObjectsV2, however, i dont see anyway of passing a query to the ListObjectsInputV2 structure
https://docs.aws.amazon.com/sdk-for-go/api/service/s3/#ListObjectsV2Input"

Amazon Simple Storage Service (S3)	"Unable to log into AWS (Authentication failed...)
The last time I used AWS was probably 5 or 6 years ago when I experimented around with it and stopped using it.

Now I want to get an S3 bucket set up for storing photos and videos, however when I log into AWS I'm immediately greeted with an ""Account suspended"" error stating that ""Authentication failed because your account has been suspended.""

When I first experimented with AWS, it was with S3 and I just tested out hosting my website images on it, nothing super exciting -- however when the free period was done I decided to just stop using it (and save the few pennies it was costing).

I was able to go into my payments page and dashboard and it shows $0.00 for amount due... My credit card was expired so I updated that on Saturday -- however today I'm still unable to get in.

I opened a ticket on Saturday (5608131241), but I received no response so I opened another ticket on Sunday to try to get a phone call (5609887441) but received a message stating that a call back wasn't available.

Does anyone have any other ideas on how I can get everything reactivated?"

Amazon Simple Storage Service (S3)	"Re: Unable to log into AWS (Authentication failed...)
Amazon finally replied to my ticket yesterday and got my account reactivated!"

Amazon Simple Storage Service (S3)	"S3: Previous version Glacier transitioning + restore
So I've got an S3 bucket with versioning enabled. On this bucket I also have a lifecycle rule that transitions the non-current versions of object to Glacier after 7 days. Now, I'd like to access an older version of this object. So as per S3's instructions, I initiated a restore of this object from Glacier that took several hours. Much to my surprise, it looks like the Glacier restore didn't restore all the previous versions of the object to Standard storage that can let me read previous versions of the object. What needs to be done to achieve restoring all versions of the object to standard storage? The screenshot is attached:"

Amazon Simple Storage Service (S3)	"Cost Explorer shows duplicated cost for a day in S3
Hello, 

When using the tool ""Cost Explorer"", S3 Storage costs reported for yesterday in my account shown a 100% increase for all storage classes. It's a HUGE LOT of data i should have duplicated in order to get those numbers, and I can't find any evidence of more data stored in my account. What looks suspicious is that the increase is almost exactly 100%, it is like someone is double-counting the space I'm using in S3

 Is anyone having the same problem?"

Amazon Simple Storage Service (S3)	"Re: Cost Explorer shows duplicated cost for a day in S3
We have also encountered this issue that only appears to have impacted the bill for Dec 9 (so far). Costs are exactly 2x what we would expect for several buckets and for a few of those buckets we can confirm with absolute certainty that there has been no additional usage which would have resulted in those increases."

Amazon Simple Storage Service (S3)	"Re: Cost Explorer shows duplicated cost for a day in S3
Please, can we have an official answer from AWS?"

Amazon Simple Storage Service (S3)	"Re: Cost Explorer shows duplicated cost for a day in S3
We submitted a billing support case and the initial response from our rep was less than satisfying. We will be following up and I will continue to post here to keep the thread updated until it is resolved.

Yesterday's bill returned to normal. The real kicker is that the metrics for everything on Dec 9 besides byte-hours and a few strange outbound transfers was very normal, if not low. The scale of some of our buckets would make that kind of scenario extremely far-fetched, for there to be a doubling of storage usage without the corresponding increase in API activity or other activity just doesn't make sense.

For any AWS mod reading this: It should immediately set off alarms and be cause for concern when certain usage metrics are precisely twice what they were the day before and then immediately return to their previous steady state the day after. This is the definition of an anomaly, in this case due to no action on the customer's part."

Amazon Simple Storage Service (S3)	"Re: Cost Explorer shows duplicated cost for a day in S3
Having the same s3 billing issue. Jump on Dec 9th. Back to normal on the 10th. Messaged AWS billing support and they redirected me to post here"

Amazon Simple Storage Service (S3)	"Re: Cost Explorer shows duplicated cost for a day in S3
Same issue here.  Double on the 9th and then back to normal.

Gave us a little scare because that would be a lot of data and indicative of something terribly wrong in our infrastructure."

Amazon Simple Storage Service (S3)	"Re: Cost Explorer shows duplicated cost for a day in S3
We wanted to provide an update on resolving this issue. We have completed re-calculating the proper storage usage amounts for December 9, and are now updating impacted customers’ accounts with the correct amounts. Only storage that was actually used will be billed. We will confirm here once impacted accounts are updated."

Amazon Simple Storage Service (S3)	"Re: Cost Explorer shows duplicated cost for a day in S3
Some customers observed incorrect storage usage amounts for Amazon S3 in the Northern Virginia (US-EAST-1) Region on December 9, 2018. On December 15, 2018, we completed updating accounts with the correct amounts. Bills for December usage, issued in early January, will reflect the correct amounts. This issue has been resolved."

Amazon Simple Storage Service (S3)	"GetBucketLocation The authorization header is malformed the region is wrong
The latest .Net SDK 
AmazonS3Client.GetBucketLocation
If I get a bucket location via 'us-east-1' client getting the same bucket location via another region client after that fails with ""The authorization header is malformed; the region 'us-west-2' is wrong; expecting 'ca-central-1'."" The ""us-west-2"" is the bucket region. 'ca-central-1' is the second client region.
Looks like the call via 'us-east-1' caches something that make any next call via a different region fail while subsequent calls via different clients with the same 'us-east-1' work.
I can mix different region client calls as many times as I would like to unless the 'us-east-1' is used. After that I can query the bucket location only via 'us-east-1' clients.
I have seen a lot of discussions about 'us-east-1' peculiarity that mentioned some 'fixes' in attempt to stabilize this mess. But obviously the scenario I describe is still not fixed.
I read something about 'global bucket access' but did not figure out how to apply it to my code. I create AmazonS3Client via just the same constructor calls as we did in the V1 but it requires now to specify a region in the AmazonS3Config what I do of course."

Amazon Simple Storage Service (S3)	"Static Hosting Redirects
I am extremely frustrated.  I moved my DNS registration and server from JustHost to AWS just so that I could host a very simple website.  Followed all the instructions, created the buckets example.com and www.example.com (with my domain name replacing example.com of course...), uploaded all the files and set up example.com for hosting and www.example.com for redirects. I can access the site if I type in the complete endpoint address (example.com+alltheAWSjunk).  
But despite creating the records as instructed for each of example.com and www.example.com, I cannot access anything if I just type in http://example.com or http://www.example.com.  Tried clearing the cache, tried a different browser - the same ""site can't be reached"" message.  What am I doing wrong???"

Amazon Simple Storage Service (S3)	"S3 Data Migration options
We need to migrate a set of data (source: Oracle) into the AWS S3 in a manual way, this is maybe archive data and batch data. Data size is around 20 TB and if possible we need to apply any anonymization process to all the data to be compliant with GDPR.

Please advise some good process, tool, and method to migrate data. Thanks in advance."

Amazon Simple Storage Service (S3)	"403 Forbidden error when using S3 Java SDK
Hi,

I'm using hadoop (2.7.0), which uses aws-java-sdk-bundle (1.11.199), to access S3 bucket. When I issue a request to a S3 bucket all works fine. But when the client application (which uses hadoop internally to access s3) is left open overnight and is used again on next morning (like after 12+ hours after last request) with the same query as the last successful the day before I get following error:
Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: D5123B1784190D81; S3 Extended Request ID: ubnn+CeTigwIUYWjGGKpJgOVH8X0heJfvWt2gHuqVmD5TrN/t26nBEHAs/6Z/TJga1ZQUql8BPI=) (Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: D5123B1784190D81; S3 Extended Request ID: ubnn+CeTigwIUYWjGGKpJgOVH8X0heJfvWt2gHuqVmD5TrN/t26nBEHAs/6Z/TJga1ZQUql8BPI=))

with corresponding stacktrace
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: D5123B1784190D81; S3 Extended Request ID: ubnn+CeTigwIUYWjGGKpJgOVH8X0heJfvWt2gHuqVmD5TrN/t26nBEHAs/6Z/TJga1ZQUql8BPI=)
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1638) ~[aws-java-sdk-bundle-1.11.199.jar:na]
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1303) ~[aws-java-sdk-bundle-1.11.199.jar:na]
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1055) ~[aws-java-sdk-bundle-1.11.199.jar:na]
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743) ~[aws-java-sdk-bundle-1.11.199.jar:na]
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717) ~[aws-java-sdk-bundle-1.11.199.jar:na]
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699) ~[aws-java-sdk-bundle-1.11.199.jar:na]
	at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667) ~[aws-java-sdk-bundle-1.11.199.jar:na]
	at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649) ~[aws-java-sdk-bundle-1.11.199.jar:na]
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513) ~[aws-java-sdk-bundle-1.11.199.jar:na]
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4229) ~[aws-java-sdk-bundle-1.11.199.jar:na]
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4176) ~[aws-java-sdk-bundle-1.11.199.jar:na]
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1253) ~[aws-java-sdk-bundle-1.11.199.jar:na]
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1228) ~[aws-java-sdk-bundle-1.11.199.jar:na]
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:892) ~[hadoop-aws-2.7.0-mapr-1808.jar:na]
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:77) ~[hadoop-aws-2.7.0-mapr-1808.jar:na]
	at org.apache.hadoop.fs.FileSystem.access(FileSystem.java:2207) ~[hadoop-common-2.7.0-mapr-1808.jar:na]


Such behavior appears one one machine while on another it works fine. Worth mentioning that the machine with failure has time set to past date: yesterday was 10th of December and last successful request was issued on 11/19/2018 12:22:41 (the time set on the machine). And the time of failed request (issued today, on 11th of December) is 11/20/2018 03:53:52. 

What could be the reason for this behavior? Could it be something to do with NTP (if it's the case it is unclear why the initial request works)?"

Amazon Simple Storage Service (S3)	"Re: 403 Forbidden error when using S3 Java SDK
After actualizing the time on machine with the issue the problem has gone away."

Amazon Simple Storage Service (S3)	"S3 bucket
I have created asp.net core api using aws server less application and i tried to upload image to s3 bucket through it was working fine when i cheked on localhost but when i hosted it to AWS than it uploaded blank or corrupted file
I have enabled binary support in aws console for this api

Any help would be appreciated

Edited by: developercsharp on Dec 17, 2018 11:48 PM"

Amazon Simple Storage Service (S3)	"V4 signatures and host field problems when uploading through Reverse Proxy
We're looking at putting an F5 (or other) as a fixed IP in front of an S3 bucket.  There are a variety of branding, firewall and other reasons for this.  The bucket is KMS encrypted and requires V4 signatures.

We've been unable to upload files to upload files into a KMS encrypted bucket when going through a reverse proxy when using the Java SDK. Note: We prefer path based routing so that we don't have to create a DNS entry for every bucket we use.

Us --> f5.myproxy.foo --> S3-external-1-.amazonaws.com 


Our code sets the endpoint to be the proxy server , f5.myproxy.foo.  The Java SDK sets the host header to be the f5.myproxy.foo when it sends the message. This value then gets put into the V4 signature.  The transparent reverse proxy server then forwards the POST onto S3.   Serverside S3 code assumes that the value in the host field is the bucket name as documented in http://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html

With V2 (or none?) you could hack the host field on the proxy server to be s3-amazonaws.com and the CLI starts working.  We demonstrated that by using the AWS CLI after modifying the Proxy change the host field to and S3 host. That isn't an option with V4 signatures.

There is no way to override the host header in the SDK. It is possible to set an HTTP header called host with the SDK. That results with two host headers, mine and the one generated by the SDK which results in a a signature violation

Edited by: J. Freeman on Apr 18, 2016 10:20 AM - added CLI comment.  added link to example of this discussion related to this Route 53 Latencyh Based Rourting + S3 URL Signatures v4"

Amazon Simple Storage Service (S3)	"Re: V4 signatures and host field problems when uploading through Reverse Proxy
Hi,

I think we are facing the exact same problem that you are, regarding the V4 Signature behind Reverse-Proxy.
The logic goes in the same way, the signature of the client side (via AWS SDK Java) use the Reverse-Proxy Host Header for the signature, meanwhile, at the other side, the AWS S3 use the value of the real URL's Bucket as the Host Header, to compute the signature. Obviously, it will not go to work, as is the case.

 Client --> Reverse-Proxy --> AWS S3 


The only way to get this architecture working were: modifying the AWS SDK Java, creating a ""special"" version to work with our Reverse-Proxy in front of the AWS S3.  This way we could modify the way the algorithm is using the Host Header value, the point is that both sides (Client and Amazon) must use the same to come out with the signature's value, and then match! 

I do not believe that this is the way that should work, this must be managed to work with the original AWS SDK Java, we have made this modification just to make sure what the problem is, that is, just to confirm that this is the only lacking piece of the puzzle. So, indeed, it worked after the modification.

We are not considering going forward with this solution as an alternative, in our vision we should use the original AWS SDK Java, as we could have lots of clients connecting with us. 

I know it is a while already that you post the question, but you (or anybody else) have found some other way to solve this, please let us know.

Regards,

Edited by: UalterJR on Dec 16, 2018 12:33 PM"

