label	description
AWS Import Export Snowball	"EC2 File Gateway with ephemeral storage after stop/start cycle
Hello,

I'm trying to use a File Gateway for our latest project.  I'm launching an i3.4xlarge and using the NVME SSD ephemeral storage for the cache.  It sets up okay and we can create and mount the share.  If I stop and start the File Gateway instance, this is no longer the case -- I can access the instance, but the File Gateway is marked offline and I cannot mount the share.  This is not the case when using an EBS volume for the cache, instead of the ephemeral storage.

I'm aware of the requirements around preventing data loss when using ephemeral storage on a gateway, but in this instance I don't care -- I specifically care about what appears to be a loss of configuration across stop/start cycles.  Is there a solution to this please?

Thanks!"
AWS Import Export Snowball	"Failed to set local disks when trying to setup a new disk for upload buffer
I have a storage gw with 2TB cache and 1Tb upload buffer (working fine)
I turned off vm and increased the size to 3tb cache and 2.7TB upload disks on the VM. 
Went back to AWS console and saw both disks: cache disk is showing the updated disk (3TB) -- good there, but I won't be able to save the upload buffer disk. I keep getting ""Failed to set local disks"" when I hit Save.

I deleted both disk and re-add them to VM (just to test it out). same issue as above.

Any suggestion?

Thanks

Edited by: risabeast on Feb 25, 2019 11:19 AM"
AWS Import Export Snowball	"Issues connecting to AWS Storage Gateway Tape Library
Hello,

We use Veeam to backup our HyperV setup.  We would like to store a copy on AWS via the AWS storage gateway.  Our gateway is running on an EC2 instance.  

We are unable to connect to the AWS storage gateway via ISCSI.  We have set up port forwarding correctly in our router, and tested by firing up another EC2 Windows 2012 r2 server with StarWind.  We are able to connect to the ISCSI on the starwind EC2 instance, but not the AWS storage gateway instance.  

We have gone through the documentation and can't seem to figure out why we cant connect to the AWS storage gateway via ISCSI to our HyperV server.  veeam tech support was not much help either, they referenced the video of how to do it, but it simply does not work.  Do we need additional ports besides 3260 between our network and AWS?

Sorry if I am restating a previous issue, as this is my first attempt with AWS storage gateway.
Thanks - Steve"
AWS Import Export Snowball	"Remote shut down (not stop) gateway appliance VM?
Hi,
Is there a way to remotely shut down the appliance virtual machine? I know I can stop the gateway but I have a peculiar VMware situation in which the host is inaccessible but the VM is running. SSH appears to be disabled on the appliance (unless there's a way to use a key file?). I'd prefer to shut down the VM rather than abruptly cycling the host.
Thanks"
AWS Import Export Snowball	"Storage Gateway Requires 3MB more ram than provided on a T3 Medium
Hi there. 

Recently noticed our storage gateway had gone offline, after further investigation the gateway will not start on the T3.Medium it is running on as it requires 3900MB of RAM and the T3 only reports 3887MB. 

Can this requirement please be adjusted 3MB down so that it can start on a T3 Medium and we can continue to use our gateway. 
	#######################################################################
	##  SYSTEM RESOURCE CHECK FAILURE: 1 ERROR FOUND!
	##  https://forums.aws.amazon.com/ Memory Check: System memory 3887 MiB (3900 MiB required)
	#######################################################################"
AWS Import Export Snowball	"Prevent deletion of files through shared folder
Hi! I'm new to AWS and Storage Gateway, and need some help here.

So, we need to set up a File Gateway. Files are going to be kept on S3 IA 2 months and then they will be transfered to Glacier.

1. We want to make sure that no user can delete files from cache disk, so they're not deleted from S3. The shared folder which Gateway presents might be write-only.

2. And if, for example, a user moves a file to a wrong folder, we want to grant permission to that user to delete or move that file to another folder inside the Gateway. If it's not possible, we can do this by AWS console.

Which is the best way can we address first point?
We can use Versioning if it's the only way.

Thanks so much for the help.

Edited by: mborb on Feb 15, 2019 7:51 AM"
AWS Import Export Snowball	"Re: Prevent deletion of files through shared folder
In order to meet your use case #1, you need extended ACL support that allows a user ""Write"" but no ""Delete"" access. Currently File Gateway only supports basic POSIX ""rwx"" permissions. 

Until File Gateway adds support for extended ACL, your only option is to use versioning on your S3 bucket."
AWS Import Export Snowball	"DNS Problems with File Storage Gateway (Join Domain)
Hi There,

I recently created an file gateway in AWS, and now trying to add SMB shares.  Instructions explain adding to the domain, but I get an error ""The gateway cannot connect to the specified domain"".  This is in a VPC that has a site to site VPN so all ports are open.  A forum post indicated that this was because my VPC uses the default DNS servers in DHCP.  I tried changing the DHCP option to include my domain name and DNS servers.  

By doing that, I lose connectivity to the gateway itself.  When SSH to the gateway instance it can't resolve the 3 endpoints anymore (due to the early DNS server change).

 client-cp.storagegateway.ca-central-1.amazonaws.com:443
https://forums.aws.amazon.com/
        proxy-app.storagegateway.ca-central-1.amazonaws.com:443
https://forums.aws.amazon.com/
        dp-1.storagegateway.ca-central-1.amazonaws.com:443

Is there any easy way around this?"
AWS Import Export Snowball	"Re: DNS Problems with File Storage Gateway (Join Domain)
It appears like you DNS server can either resolve the hosts in the internet (client-cp.storagegateway.ca-central-1.amazonaws.com) or your internal domain controller. In order for this configuration to work, your DNS server should be able to resolve both set of hostnames.

Alternately, you can use the AWS CLI to invoke JoinDomain operation by specifying the IP address of your DomainController using the parameter ""--domain-controllers"". 

https://docs.aws.amazon.com/cli/latest/reference/storagegateway/join-domain.html

Please let us know if this addresses your scenario."
AWS Import Export Snowball	"Storage Gateway - why are virtual tapes so big?
I work for a very small cost sensitive organization with only about 5 TB of data to be backed up. Currently I am using Backup Exec 20, which only natively provides access to S3 Infrequent Access. Apparently for Glacier access I need to use the Storage Gateway.

And yet it does not appear that the Storage Gateway is going to significantly reduce our costs, and may in fact cost more, depending on how many restores need to be done. I am looking very specifically at the virtual tape size, which seems to be locked at a minimum size of 100 gigabytes per tape?

So if I am reading this correctly, if we need to do a restore of a single 100 kilobyte Word document from a virtual tape in Glacier, the gateway needs to download a 100 gig tape so that Backup Exec can restore from that tape. Looking at the cost calculator, this costs US$0.25 for a slow bulk retrieval plus US$9.00 for the data transfer out.

So this one tiny restore job will whack us for $9.25 for the job. Having to do multiple occasional small restores like this can become expensive very quickly.

It would be much less expensive for us to do restores if the virtual tapes could be 1 gigabyte or 500 megabytes each. The tape software is not going to care that I need 5000 or 10000 of these small virtual tapes to do a full data backup.

With a virtual tape size of 500 meg, it now fits within the monthly free tier for transfer out, and so a few restores here and there would not cost us much at all.

So why is there minimum virtual tape size of 100 gig?  Is there some way to muck around in the Storage Gateway virtual machine configuration to allow for much smaller virtual tapes?

Edited by: Jibby on Feb 15, 2019 9:10 AM"
AWS Import Export Snowball	"Re: Storage Gateway - why are virtual tapes so big?
Jibby:

Storage Gateway pricing is based on the amount of data you store not the provisioned capacity of the tapes. So while the minimum tape size is 100 GB, you are not charged based on the size of the tapes you provision.

More specifically, we meter storage by the byte and prorate the per-GB charge. That is if you only store 1 GB of data on a tape with 100 GB of capacity you will only be charged for 1 GB of storage (or less if the data is compressible). If you archive this tape to Glacier, and then need to retrieve it you will be charged for (at most) 1 GB of retrieval.

Regards
Paul"
AWS Import Export Snowball	"Re: Storage Gateway - why are virtual tapes so big?
Assume that I am using the normal 100 GB virtual tapes. I do a 5 TB backup to tape, which produces 50 virtual tapes that are 100% used.

Later I need to restore a 100 KB file from one of these 100% used tapes. How much of the tape is downloaded to access that 100 KB to do a restore?

In the unfortunate situation that a small file happens to span two 100% used 100 GB virtual tapes, how much of those tapes needs to be downloaded to access that file?

The virtual tape software has no idea how a given backup program has stored data on the media so it seems it would have to download the entire tape regardless of the size of the data to be restored."
AWS Import Export Snowball	"Re: Storage Gateway - why are virtual tapes so big?
In the case where you need to restore 100 KB from a full 100 GB tape that you have archived, you'd to retrieve the 100 GB from the archive (Glacier) into the library (S3), and then the gateway would download whatever data your backup application requests from the tape (to the local cache). If the 100 KB file spanned multiple tapes that had been archived then you'd need to retrieve Nx 100 GB to the library (S3) and the gateway would download whatever data the backup application requested.

Archival of tapes is driven by the backup application through the policies you configure to eject or export a tape from the library. In this regard to backup application is aware if the tape can be immediately accessed or if it needs to request the tape to be put back into the library (in the real world this often equates to bringing the tape back from an offsite vault, which takes time and often has a cost). The backup application allow you to configure what remains in the library, and for most customers this is the data that they are most likely to want to restore, such as the most recent nightly or weekly backup. In the end you need to balance your business needs for ongoing cost saving in storage with the cost to restore based on the likelihood you will need to restore the data, and then configure your backup jobs to support this business objective.

In practical terms, the price difference between archived tapes and library tapes in our N.Virginia region is currently $0.019/GB-month (library: $0.023, archive: $0.004/GB-month -- https://aws.amazon.com/storagegateway/pricing/#Tape_Gateway_pricing). The cost to retrieve a tape from archive is $0.01/GB so if you needed to do this restore more than once per month it would be more cost effective to leave the tapes in the library rather than archiving them. 

Thanks
Paul"
AWS Import Export Snowball	"Active Directory access control
Happy Valentine's Day, Lovers.
I am confused about how the access control with s3 works using Active Directory and a Storage Gateway VM.
1. Select my Gateway from https://console.aws.amazon.com/storagegateway/ 
2. Select Actions > Edit SMB Settings
	-a. Click Join Domain
	-b. Domain name: blah.local
	-c. fill in Domain user/password
	-d. back on the gateway webpage receive an acknowledgement that my domain join was successful
3. Select File share > Actions > Edit share access settings
	-a. Allowed groups: Authenticated Users
4. From Windows 10 File Explorer, Navigate to \\192.999.999.999\s3bucketname
	-a. Enter domain login info into the Windows Security popup, get Access is denied. Error.
5. Back on storagegateway console website, remove Authenticated Users from Allowed groups
	-a. Navigate to \\192.999.999.999\s3bucketname and can access everything just fine.
What am I doing wrong?

EDIT: So, I'm an idiot.  Apparently, Authenticated Users isn't a true security group.  I used Domain Users instead.

Edited by: matthaws on Feb 14, 2019 1:28 PM"
AWS Import Export Snowball	"AWS File Storage Gateway offline
Hello,
we are testing AWS File storage Gateway and we've faced same problem that was already mentioned here : AWS SG is a software appliance on VMware ESX, completely accessible from LAN (by ping), with connectivity tests to the cloud completed successfully, but Amazon web console recently started to show it as offline.
It is not possible to connect with http from local machine either, while there is absolutely no firewall rules preventing it.
I saw that AWS specialists were providing support is such situation, but for us the most important is to prevent such situation in the future, when we will use storage gateway in production. 
Please advise what steps could be taken to either prevent this ""offline"" or to resolve this situation if already happened.
Thank you in advance!"
AWS Import Export Snowball	"Re: AWS File Storage Gateway offline
It seems that AWS does not have clear solution to these ""gateway offline"" problems, that re-appear throughout the forum. Can this solution to be implemented in production or it is still a ""proof of concept"" product. 
Of course all data will still be in S3 once gateway is not available and it will be possible to re-create a gateway from the scratch, but that does not look like reliable solution for production. 

as of now - I still have a gateway as offline and while I have created another storage gateway , that runs (so far) but inability to resolve this problem is very disappointing."
AWS Import Export Snowball	"Re: AWS File Storage Gateway offline
VasilyG, 

Are you able to access the files over the File shares associated with Offline gateway? Can you please PM me your Storage Gateway ID, Region?

Once you provide the Storage Gateway ID we will be able to provide you further guidance.

Shashi"
AWS Import Export Snowball	"Re: AWS File Storage Gateway offline
Hello Shashi,

I have sent PM a week ago. But just in case - here is the brief answer:  - no, gateway is still offline, and its share is not accessible in local network.
If I try to access the server locally via http (as if to re-activate) - it refuses the connection.
While from the gateway local console connectivity tests went through ok."
AWS Import Export Snowball	"Re: AWS File Storage Gateway offline
VasilyG,

Can you please check your PM and respond to my request?

Thanks,
Shashi"
AWS Import Export Snowball	"Gateway VTL v/s Native integration
Which option is better to use while using NetBackup Appliance backup: AWS Gateway VTL or Integration of NetBackup with Cloud Storage using NetBackup API?

What is the upload request size recommendations and can they be optimised for better cost & performance?"
AWS Import Export Snowball	"List of IP addresses the AWS Storage gateway communicates with at Amazon?
Hi, 

I want to route the traffic from our AWS Storage Gateway across a secondary circuit.   

Where can I find a complete list of IP's that the SG will use to transmit data?

Thanks!"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
Hi vantagedev2,

Thanks for your question.

AWS storage gateway communicates with the  storage gateway service using specific endpoint DNS names.  These endpoint DNS names are built into the Gateway and do not need to be configured.

Can you clarify your use case/scenario that needs this configuration?

Thanks,

Jun"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
Sure, 

I have two internet circuits.   

One is used as a primary, the other sits idle as a backup.

Right now, my AWS Storage Gateway traffic is going out via my primary circuit.   

If I knew the foreign endpoint IP's (at Amazon) I would be able to tell my router to direct any traffic destined for Amazon to go out via my backup circuit.   

Allowing the traffic to go out via the primary circuit is not desirable because the volume we have to send out combined with the amount of traffic we experience this time of year will oversubscribe the circuit and interfere with operations.  

The ability to take advantage of a second, dormant circuit for back-end operations like this is probably not uncommon.  

Thanks, 

--Freddy"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
vantagedev2,

I've sent you a PM with some more specific questions about your use case.

Regards,

Ian"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
I too am looking for a block of IP address from which I can whitelist. We are evaluating storage gateway as a secondary offsite backup option. The network which the virtual appliance runs on does not allow access ingress or egress to the public internet. I am only able to access amazon services if I can whitelist a block of IPs. 

Let me know what the best course of action is.

Thanks,
Nick"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
Hello nickethier,

I have sent you a PM that will be of help to you.

Thanks.
Santhosh"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
Hello, would it be possible for you to PM me the same information? We're going to be testing this same scenario very shortly."
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
Hi jweblinc,

You can now find the list of Storage Gateway endpoints in our documentation:

http://docs.aws.amazon.com/storagegateway/latest/userguide/allow-firewall-gateway-access.html

Regards,

Ian"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
Updated url:
http://docs.aws.amazon.com/storagegateway/latest/userguide/Requirements.html#networks"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
Can you also send me the info you have sent as a pm to other folks in this forum?  I am working with a client that had direct connect and the storage gateway running on premise.  They setup a file gateway.  We want to configure the setup to allow the traffic for the storage gateway to go out over the direct connect. I also need to know a way to verify that the traffic is indeed going out over that direct connect.  They keep asking me for the ip address or addresses to communicate with S3.  Their thought was to open that ip or ip range on their routers.   Also white list them."
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
rkcloud:

The following pages in our documentation should help you:
https://docs.aws.amazon.com/storagegateway/latest/userguide/using-dx.html
https://docs.aws.amazon.com/storagegateway/latest/userguide/Resource_Ports.html

Thanks
Paul"
AWS Import Export Snowball	"Storage Gateway is either currently unreachable
After rebooting my virtual appliance using vCenter I have lost connectivity to my storage gateway. I have confirmed that the VM has internet access via the console sguser. When I select the gateway, the following message is displayed. 

""Your Storage Gateway is either currently unreachable or is in the process of shutting down or restarting. Re-click on your gateway in the left pane to retry your connection. If you continue to experience connectivity issues, please verify that your local gateway host has Internet access. If you have initiated a shut down or restart of your gateway, it may take a few minutes before your gateway is reachable.

Alternatively, if you no longer wish to use this gateway, you can click below to delete it.""

thanks,
Jim"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hi Jim,
Can you provide me with your AWS Account ID and which AWS Region you're using (you can private message me)?

Thanks,
Arun"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Arun, I sent you a PM with my account number. Gateway is located in US East."
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hi Jim,
Based on what we're seeing, your gateway appears to be reachable as of a few hours ago.  Can you confirm?  Did you make any changes to your hardware in the last few hours?

Thanks,
Arun"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
We are experiencing the same issue - what steps should we take to resolve connectivity?"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hi  brentg73,
Can you provide me with your AWS Account ID and which AWS Region you're using (you can private message me)?

Thanks,
Ankur"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Ankur,

I sent you a PM with the requested information"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
We are experiencing the same issue - what steps should we take to resolve connectivity?"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hi stu-yc,

Can you please PM me with your AWS Account ID and the region in which you've activated your gateway?

Also, have you tried running the Network Connectivity test from the local console?

Test Your AWS Storage Gateway Connection to the Internet

Have you configured a SOCKS proxy or static IP for your environment?

Thanks,
Ian"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
We have the same issue.  SG has internet connectivity, no SOCKS configuration."
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hi vmsitteam
We are looking into your issue and will PM you if we need further information.

thanks"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
I've PMed you for more details regarding your gateway."
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Same here"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
I'm having this same issue.  Is someone available to assist?

https://forums.aws.amazon.com/thread.jspa?messageID=606955

What is the process for getting support from Amazon when this happens?  Do I have to pay?"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
whittakerj:

Did you run the network connectivity test in the local console? See http://docs.aws.amazon.com/storagegateway/latest/userguide/EC2_MaintenanceTestGatewayConnectivity.html. What were the results?

Basic support is included with all accounts, and paid plans are also available. See https://aws.amazon.com/premiumsupport/ for the options and process for obtaining support.

Regards
Paul"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Yes I ran the test results attached."
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
whittakerj:

That looks okay. Please PM me your AWS Account ID and Gateway ID (if known) and we will take a look from the AWS side.

Thanks,
Paul"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
PaulR@AWS I sent you my information I'm still showing offline. How can I expedite this?"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
I'm having the same issue.  I'm trying to set up a storage gateway VTL.  Activation completes successfully, network connectivity seems fine (device responds to ICMP on the LAN), network interface is configured correctly, but always get stuck with ""Storage Gateway Not Connected"".

I just got a connectivity test to complete and, as expected it says ""Connectivity test failed.""

One interesting note (or not) is that if I go into the console and issue the open-support-channel command I end up with ""Successfully connect sic to Storage Gateway Support Server"" and ""Press q to end support session"".

When I ""press q"" I get the ""Connection to 54.201.223.107 closed."" message.  So this at least makes it seem like the Storage Gateway VM is able to establish a connection to a host on the internet.

The other thing that's weird is I've been checking the firewall's traffic logs and I see almost no traffic at all from the Storage Gateway.  Even when I tell the SG to run a connectivity test it doesn't seem to send any packets.

Just a few TCP packets here and there to port 443 on an amazon public IP."
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Not very useful from a technical standpoint, but my issue was resolved after I discovered a cancelled credit card was set as default in my payment methods.  Payment methods updated, outstanding invoice paid, connectivity established about 45 minutes later."
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hello,

I am experiencing the same issue trying to set up my first Storage Gateway.  When I try to activate it I receive ""Your storage gateway is activated but is currently unreachable"".  I verified network connectivity through the console and each test passed.  Anyone know the resolution to this?

Thank you,

Greg"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
gregspryjb:

Can you give me a few more details? Which region are you activating in? Where are you hosting your gateway (VMware, HyperV, EC2)? If on-premises, is there a firewall or socsk server between the VM and AWS? Did you encounter any problems with the activation process itself? Does your new gateway show up in the AWS web Console? Has this web Console ever shown the gateway as available? When you run Network Connectivity in the local console what results do you get?

Thanks
Paul"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hello Paul,

Thank you for the reply.  I was able to activate the gateway this morning after I deleted it from the AWS Storage Gateway page.

Thanks!"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hello, 

I have exactly the same problem here. The connection test is successful, but my gateways still appears offline in the console and I cannot mount any drive. I've tried to reboot the gateway VM but the gateway still appears offline. The gateway is running in Ireland region. 

Thanks

Edited by: rphilipsen on Mar 29, 2017 12:52 AM"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
rphilipsen,

Please PM me your AWS account ID and the Gateway ID for the gateway in question.  Thanks."
AWS Import Export Snowball	"Upload files to S3 but delete them on-premise
Hi!

We need to upload files to an S3 bucket and delete them on-premise after a few days. We need the files to stay on S3, but get rid of them on-premises.

We clearly aren't much familiar with S3.

Can we acomplishe that with Storage Gateway?
Should we upload the files using AWS CLI and then delete them on-premise instead?

Any help will be very appreciated."
AWS Import Export Snowball	"Re: Upload files to S3 but delete them on-premise
mborb:

Using File Gateway will implicitly delete your files when local storage is needed for new files.  There's no explicit delete needed in this cached model. That is, if you write files to File Gateway through NFS or SMB the gateway will cache on the local disk and upload them to S3 as objects. Data will be deleted from the cache when space is needed to store more recently read/written files, so the amount of data that remains on-premises is determined by the size of the cache disk attached to the gateway.

Thanks
Paul"
AWS Import Export Snowball	"Basic up/down monitoring of SG?
Our Volume SG had an update applied during it's maintenance window over this past weekend, however, when I came in this morning, the AWS console showed that it wasn't running.  I logged into vSphere, and saw some messages about it being out of memory, so I allocated more RAM to it, and restarted it.  After doing that, it finally came back in the console as Running, and everything was working again.  We do basic ping checks on the SG appliance (which was fine), however, the AWS console showed it not running.  How do we get alerts if the SG isn't running?  I see I can monitor all sorts of metrics, but I just want to start out with basic up/down alerts if it's not running."
AWS Import Export Snowball	"Re: Basic up/down monitoring of SG?
Hi brucegarlock,

Thanks for your feedback. We don't currently provide an alert on gateway status. I have noted your suggestion and we will consider it as we plan future functionality and features. If you can please also PM me your account and gateway information along with the day/time you had the problem, it will be great.

Regards,
Bhavin
AWS Storage Gateway"
AWS Import Export Snowball	"aws backup product / storage gateway stored volume?
I'm excited about the life cycle management of the AWS backup product.

I'm curious about how it integrates with the Storage Gateway Stored volume,
as the stored volume is already doing a forced snapshot every day (or period you set),  if you use aws backup on
a stored volume, do you then have 2 backup streams ***  that you'd have to pay for? 

Steve"
AWS Import Export Snowball	"Re: aws backup product / storage gateway stored volume?
Hi Steve,

Thanks for the feedback. AWS Backup can backup both cached and stored volumes of Volume Gateway. The default stored volume snapshot scheme remains in place. So if you create an AWS backup policy that backs up stored volumes, that will operate independently of Volume Gateway schedule. You will be able to see the snapshots created by AWS Backup in the EBS console and the snapshots created by AWS Backup of volume gateway volumes can only be managed from AWS Backup (and not from EBS console). 

Hope this helps. If you have any other questions please let me know.

Regards,
Bhavin"
AWS Import Export Snowball	"How to manage cache and buffer cool-down?
I'd like to know what the default settings are for the upload buffer and cache on the SG, and how to wipe them once a job is complete. I was doing some testing today with sequential backup-to-tape operations and the buffer/cache both continued to climb after each job was complete. I noticed rebooting cleared the buffer/cache but that seems excessive.

Left alone, how soon will the cache empty itself and where are the knobs to turn on this?"
AWS Import Export Snowball	"Do files cache locally before uploading to S3
Hi,
I'm just looking to see if anyone can confirm / point me to some documentation relating the way files are uploaded to S3 via Storage Gateway. Essentially I am trying to workout if I have a network problem or misunderstood the way storage gateway worked.

I have an AWS Storage Gateway running via ESXi on a HP microserver. I have a physical Windows 10 machine mapping to a file share that points to an S3 bucket.

What I was expecting when I copy a large file from the Win10 to the mapped drive, it would copy across at LAN speeds to the VM, and then StorageGateway would do its magic in slow time the S3 bucket. What I am seeing is it taking approx 20mins for a 300MB file to copy across. I originally thought it was a LAN issue, but I am not starting to think that it is uploading straight to S3, and only reads are via the cache...

Can anyone confirm that this is whats happening and that's the way AWS has designed storage gateway to work. If so I may need to rethink my use case and solution.

Thanks"
AWS Import Export Snowball	"Re: Do files cache locally before uploading to S3
Bradc,

Your understanding of the gateway behavior is correct. The files that are written to the gateway are saved locally in the cache disk and they are uploaded asynchronously to S3. 

Since you are seeing poor write performance, we suspect the issue to be with the network throughput from the Windows client and the Storage Gateway or the IO throughput of the Cache disk. Can you please look at the the network & IO throughput metrics in ESXi?

Please PM me your gateway ID and the AWS Region, we will look at your gateway metrics and share additional feedback.

Thanks,
Shashi"
AWS Import Export Snowball	"Multi-Facility Usage of Storage Gateway?
I am sure this question has been asked.  I promise I searched for it first!

Is it possible and practical to leverage Storage Gateway services to implement a multi-facility file share?  We maintain large amounts of critical project data at local facilities in various states.  It is becoming more important for the various facilities to have access to this data as a global resource.  Secure storage (Amazon) is also important.  Having a cached storage gateway at Amazon and then deploying the local cached storage at multiple locations such that the data could be accessed and updated from anywhere would be fabulous.  However, this may be too much to hope for.

What usage scenarios are possible that are close to this desire?

Thanks!

Keith"
AWS Import Export Snowball	"Re: Multi-Facility Usage of Storage Gateway?
Hi Keith,
Currently, any data shared between multiple gateways must be done via snapshots.  You can't have two gateways concurrently access the same storage volume.

But would be great to learn more about your use case.  I'll PM you.  

Thanks,
Arun"
AWS Import Export Snowball	"Re: Multi-Facility Usage of Storage Gateway?
I don't know where this PM happens...  but our situation is as follows (and doesn't seem particularly unique):

We have a local ""projects folder hierarchy"" that is organized by project managers.  Projects are organized by ID number, and need to be retained forever.  However, only the most recent year or so requires  low latency access.  Since the data includes large CAD files and such, though, ""local"" access is needed so that the files can be opened/examined/modified using various software tools.  We have developed S3-Based facilities to automatically move suitable projects (those that database information indicates are not likely to be needed anymore) to S3 and to delete them from the local store.  The project managers can restore these archived projects easily if they are needed.

The above sounds like an excellent problem for a Cached Storage Gateway solution.

In addition to this, company expansion introduces the need to establish similar large project folder hierarchies at other locations, some in other states and a few in other countries.  The nature of the data still suggests the need for there to be a local cache so that these remote project managers need quick access, too.  Furthermore, there are cases where sales happens in one region but project management happens in another region - thus, the project folder hierarchies really need to be available in multiple places (obviously there is some ACL aspect of this, but...).

Summary:  The ideal solution would be an Amazon-primary storage with multiple cached instances in multiple locations that would all inter-sync.  But, we know that this isn't actually what the Storage Gateway is designed for.  And that is understandable.

Perhaps a cached storage gateway for each geographic location combined with some project-targeted to synchronize selected data between regions would work.  In this case, I have probably wandered outside this forum's scope (with apologies).  But if anyone has solved this problem or has some enlightened idea (particularly using storage volumes) I would love to hear about it!

Keith"
AWS Import Export Snowball	"Re: Multi-Facility Usage of Storage Gateway?
Did you ever get a reply? I would love to know the solution. I have a similar use case."
AWS Import Export Snowball	"Re: Multi-Facility Usage of Storage Gateway?
The problem isn't really an AWS one. Given they are presenting iSCSI LUNs rather than an actual file share (NFS / CIFS / etc), the problem is how would you even have two systems writing to the same NTFS (or whatever) volume at the same time?

There are solutions to this like HP PolyServe, Redhat GFS2 or something like that to get to the point where you're not going to corrupt the file system by having two servers access the same disk, but even then you haven't really solved the problem of file locking, etc.

The only thing I can think of is if you have several completely separate AWS Cloud Gateways, and then link them at the front end (on the server) using DFS/FRS, GlusterFS or some other sort of Distributed File System. You'd have a lot of duplicated data in AWS is the downside.

It may be worth a look at Nasuni. I know they have the concept of Read-Only replicas of shares which can be presented in multiple locations. I think it's expensive however (even though they still use AWS S3 at the back-end)."
AWS Import Export Snowball	"Re: Multi-Facility Usage of Storage Gateway?
Multi-master would also be a massive enhancement for our usecase as well."
AWS Import Export Snowball	"AWS Volume Gateway High Availability Clustering and Local Logs
We love the concept/idea of this product. We have a scenario where our local NAS/Archived Storage is becoming stressed on space even with a local Archicing/Stubbing solution in place, about 120TB of data. We have tested out the Volume Gateway in our environment and it works well from our testing. The ability to send files to an ever increasing amount of space in S3 while caching highly used/recent files locally is great.

The problem is for an Enterprise Grade solution, we see a couple flaws that are preventing us from pushing forward with this solution:

High Availability - With the SG acting as the frontend for clients/servers to deliver files to, what happens if the SG encounters any service disrupting issue? This could be crippling to an environment relying on those files for their production apps to function. I know with VMware HA this can help a little but doesn't cover everything. Having a second Volume SG in sync/replicating/load-balancing in a cluster is the ideal solution to this.
Local Logs on the SG - We think it would be very helpful to be able to see login to the console as ""sguser"" and see read-only log files related to SG server/activity. If/when there is an issue, at least you would be able to login locally and parse the logs to try and diagnose what may possibly be at fault vs waiting for Amazon support in a critical production down scenario. This way, it doesn't just appear as a magical black box sitting out there working wonderfully to utilize S3 storage.

To my points above, does Amazon have any plans to roll out High Availability (HA) clustering of the Volume Storage Gateways? Also, will we ever be able to see read-only logs related to the SG server/activity when logged in to console as ""sguser""?

Thanks,
Matt"
AWS Import Export Snowball	"Re: AWS Volume Gateway High Availability Clustering and Local Logs
Hello Matt,

Thank you for your input regarding these two features.  At this time we do not have a timeline on these items, but your feedback will help us to prioritize our future roadmap. I have PM'd you to discuss in more detail.

Thanks,
Peter"
AWS Import Export Snowball	"Re: AWS Volume Gateway High Availability Clustering and Local Logs
Hello we need also High Availability (HA) clustering of the Volume Storage Gateways.
Any solutions?

regards
Richie"
AWS Import Export Snowball	"Re: AWS Volume Gateway High Availability Clustering and Local Logs
Adpressi wrote:
Hello we need also High Availability (HA) clustering of the Volume Storage Gateways.
Any solutions?

regards
Richie

Bump.  Same interest."
AWS Import Export Snowball	"Re: AWS Volume Gateway High Availability Clustering and Local Logs
We are currently using SG for our noncritical workloads, but would also require this as part of our critical rollouts.

Would love to see this come out soon."
AWS Import Export Snowball	"Adding Storage Gateway to AD Domain via CLI
Hi

We have a problem with Storage Gateway joining the domain via CLI (version: aws-cli/1.16.93 and aws-cli/1.16.90).
When we run the following command ""$ aws storagegateway join-domain...."", it returns:

An error occurred (InvalidGatewayRequestException) when calling the JoinDomain operation: The gateway cannot connect to the specified domain.

This is the command we used:
aws storagegateway join-domain --gateway-arn arn:aws:storagegateway:<region>:<account-id>:gateway/<gateway-id> --domain-name <our-domainname>  --organizational-unit ""OU=<our-ou-name>,DC=<our-domain>,DC=COM --domain-controllers <our-dc-ip> --user-name <username> --password <password>

Could someone help us debug why we are unable to join the Domain?

We already checked the following:
-Specified DC is reachable and necessary ports are opened
-All traffic inbound/outbound allowed between Storage Gateway and specified DC
-Storage gateway can resolve Domain Name
-DHCP Options Sets specify correct DC and domainname in search list
-The user and/or OU has right to join the domain
-The user and password is correct
-Other windows instance which is in same subnet and same security group can join the domain

Added additional reachable DCs to the domain-controllers list, and the problem remains


Thank you,"
AWS Import Export Snowball	"Re: Adding Storage Gateway to AD Domain via CLI
Please check the logs on your Domain Controller/AD for any errors? Most probably the error is being returned by your DC/AD. You can also capture the network packets while you are executing the ""join-domain"" operation to confirm that the error is returned by the DC/AD.

Can you please PM me your Storage Gateway ID & the Region?"
AWS Import Export Snowball	"Re: Adding Storage Gateway to AD Domain via CLI
Hi shashi-AWS,

Thank you for your advise.
After discussion with our DC/AD admins, we found error log in event viewer.
 -> Event Id:16642, Directory-Service-SAM, The account-identifier allocator was unable to assign a new identifier.

It was because DC in AWS does not have connectivity with FSMO role holder DC. After we switched site2site VPN to other site which has DC with FSMO role, successfully storage gateway could join the domain with same command I posted initially.

Again, thank you for your help."
AWS Import Export Snowball	"Join Storage Gateway to Domain by IP Address?
I've searched all over, but can't find mention of this... Can I specify the IPs of my Active Directory Domain Controllers in the 'domain joining' action when creating a Storage Gateway? This is what we do when domain-joining Windows instances (we do this with some Powershell UserData).

As it stands, the instance has to be able to lookup the AD domain name (so if your AD is called ""ad.example.com"", the instance tries to look up ""_ldap._tcp.dc._msdcs.ad.example.com""). If the DNS lookup fails, then it can't join the domain. To do the lookup, it has to use the correct nameservers.

Is there any way (or any plans to add the feature?) to allow me to say ""my AD servers are at IPs X and Y""? This would greatly simplify the infrastructure required around the Gateway to get it to join the domain.

As it stands, the only solution I have found is to set a DHCP Options set on the VPC that specifies the two DCs as the the nameservers, configure the VPC to use this set, then create the Gateway and then optionally put the DHCP Options back the way they were.

Since I don't want to have to make VPC changes just to instantiate an instance, I'm now looking at ways we can keep the DHCP Options pointing to the DCs - but this requires lots of Route53 associations with the remote account that hosts the Domain. Again, not an attractive option.

Any ways around these problems would be most welcome!"
AWS Import Export Snowball	"Re: Join Storage Gateway to Domain by IP Address?
As you pointed out, currently the only solution is to control it through the ""DHCP Options"". 

We are making enhancements to the Storage Gateway JoinDomain API (https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_JoinDomain.html) to take the AD domain name/IP address as an optional parameter. This enhancement will be available mid Jan, 2019."
AWS Import Export Snowball	"Re: Join Storage Gateway to Domain by IP Address?
Hi,

Is there the date that enhancements to the Storage Gateway JoinDomain API will be available?
I have problem when joining the domain to create smb share on Storage Gateway. The joinig domain widzard returns time out error. 
	> The specified request timed out. (Request ID: xxxxxx)
I assume this is because we have lots of DCs for our remote offices, and only few are accessible from VPC because of design of site2site VPN, and gets time out....
If enhancements includes feature allowing us to select DC for joining domain widzard, my problem may be solved.

Thank you,"
AWS Import Export Snowball	"Re: Join Storage Gateway to Domain by IP Address?
The updated version of the JoinDomain API (https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_JoinDomain.html) has two new optional parameters ""DomainControllers"", ""OrganizationalUnit"". Currently these two new parameters are only available through the AWS CLI (https://docs.aws.amazon.com/cli/latest/reference/storagegateway/join-domain.html). 

In your case, you can specify the IP address or Domain name of the DomainController using the parameter ""--domain-controllers""."
AWS Import Export Snowball	"Re: Join Storage Gateway to Domain by IP Address?
Thank you for your quick reply, shashi-AWS.
After updating awscli, I can see new optional parameters you mentioned.
However, using new parameter we still get error. As our issue is different from this thread topic, I'll post new thread. Thank you for your support!"
AWS Import Export Snowball	"chown not working on NFS file share
Hi,

I have successfully created a NFS file share with a storage gateway using a S3 bucket.
However, when creating files chown says: Operation not permitted. 
Everything is in default configuration (no permission policy added).
On the other side, chmod is working perfectly fine.

Thanks in advance!
Diego"
AWS Import Export Snowball	"Re: chown not working on NFS file share
Diego:

By default file shares are created with ""root squash"" enabled which prevents changing the ownership of files by the root user. You will need to turn off root squash in the file share settings to allow this.

https://docs.aws.amazon.com/storagegateway/latest/userguide/managing-gateway-file.html#edit-nfs-client

Thanks
Paul"
AWS Import Export Snowball	"Migrate Storage Gateway File Share to SMB
Hello,
For some time I've been running at NFS file share off my Storage Gateway, however I would like to use SMB instead as my environment is mostly Windows.

Is there a good way to migrate between NFS to SMB?
I did some testing changing an existing NFS file share to SMB, however I encounter some permissions issues, I believe this is because it stores permissions on file  as metadata in S3.

Thanks"
AWS Import Export Snowball	"Re: Migrate Storage Gateway File Share to SMB
Daryl, 

In order to understand your use case, can you please answer the following questions?

1. Are all your NFS & SMB clients running Windows OS? If yes, what is the Windows OS version? 
2. Are you using SMB guest or Active Directory(AD) for SMB authentication? 
3. Are you using the same Windows user id while accessing the files over NFS & SMB file shares? 
4. On your NFS file share on the SGW, what did you use for ""Squash level""?
5. Can you please describe in detail what permissions issues you encountered?

Thanks,
Shashi"
AWS Import Export Snowball	"Re: Migrate Storage Gateway File Share to SMB
Apologies for taking so long to reply.

1. Are all your NFS & SMB clients running Windows OS? If yes, what is the Windows OS version?
Most our Windows 10, we also have a few Macbooks. 
2. Are you using SMB guest or Active Directory(AD) for SMB authentication?
AD 
3. Are you using the same Windows user id while accessing the files over NFS & SMB file shares? 
Yes I believe we are usng the same Windows id
4. On your NFS file share on the SGW, what did you use for ""Squash level""?
All squash
5. Can you please describe in detail what permissions issues you encountered?
If I try to create new files I'll get a ""You need permissios to perform this action"" in Windows

Thanks"
AWS Import Export Snowball	"Adding Storage Gateway to AD Domain
Trying to add Storage Gateway to AD using CLI and the command is failing with the following error:"" An error occurred (InvalidGatewayRequestException) when calling the JoinDomain operation: Authentication failed.""

I suspect that the problem is with referencing the OU in the cli command, since the system user that is being used in the command has only permission to add machines to that specific OU.

The command executed is below:
aws storagegateway join-domain --gateway-arn some_storage_gateway_arn --domain-name prod.company.com --organizational-unit ""OU=Servers,OU=Production,DC=prod,DC=company,DC=com"" --domain-controllers some_domain_controller --user-name some_user --password some_password --region us-west-2

Is that the correct syntax for the --organizational-unit option?

Thanks!"
AWS Import Export Snowball	"Re: Adding Storage Gateway to AD Domain
The syntax of the value for OU parameter is fine. If you haven't tried already, can you please try without the double quote ("") for the ""organizational-unit"" parameter?

Please check the logs on your Domain Controller/AD for any errors? Most probably the error is being returned by your DC/AD. You can also capture the network packets while you are executing the ""join-domain"" operation to confirm that the error is returned by the DC/AD.

Can you please PM me your Storage Gateway ID & the Region?"
AWS Import Export Snowball	"Re: Adding Storage Gateway to AD Domain
Thanks, Shashi,

Just PM'ed you with the details of the Storage Gateway.
I also, as you mentioned, tried to remove the quotation marks but getting exactly the same error message...
Will be checking the AD logs..."
AWS Import Export Snowball	"Re: Adding Storage Gateway to AD Domain
Alex,

I responded to your PM with the details of the error message we observed in the gateway logs. Please check your AD/DC logs for additional clues. 

Here are few possibilities.

Time synchronization between the Storage Gateway and Domain Controller. 
Domain Name Resolution (DNS) failure resolving the Domain Name.
The user and/or OU does not have right to join the domain
The user name and or password is incorrect

Please keep us posted with your findings."
AWS Import Export Snowball	"Issue with Joining Domain in Storage Gateway
Hi,

I have active directory services configured in a ec2 instance. I want to join the domain in AWS Storage Gateway but it is giving me an error ""The specified endpoint was not found. (Request ID: 0c0f2d65-130b-11e9-b50a-791fcd2811c7)"". DHCP option set has also been changed to my active directory server. Can someone help me?"
AWS Import Export Snowball	"Re: Issue with Joining Domain in Storage Gateway
The error message ""The specified endpoint was not found"" is displayed when Storage Gateway is not able to resolve/identify the Domain associated with the specified DomainName. 

If you have configured DHCP option set with your AD IP address, it should have worked. 

Can you please PM me your GatewayID and AWS region name?

Thanks,
Shashi"
AWS Import Export Snowball	"Re: Issue with Joining Domain in Storage Gateway
Dear Shashi,

I was able to resolve that issue. I am not able to login to SMB path using the AD credentials. Username password prompt is showing and when i enter the correct username and password it gets prompted again. Need you help on it."
AWS Import Export Snowball	"Re: Issue with Joining Domain in Storage Gateway
It is good to know that you resolved the original issue. How did you resolve it? 

Can you please share the command you are using to mount the drive? Are you including the ""DomainName"" as part of the user name (e.g: DomainName\UserName)?"
AWS Import Export Snowball	"Re: Issue with Joining Domain in Storage Gateway
After searching through a lot of AWS blogs and forums. All were suggesting to change the DHCP options set which helped me out. 

Now i am having trouble accessing the smb path. 

yes i am entering the domain login with domainname\username and password still no luck. Can you help me out on this?"
AWS Import Export Snowball	"How to validate gateway is listening on port 445
Ive set up a couple of new SMB file gateways, but one of them is not responding on port 445.  Cant telnet to that port and network access looks fine.  Is there a way to confirm that the gateway is in fact listening and responding on 445?"
AWS Import Export Snowball	"Re: How to validate gateway is listening on port 445
File Gateway will start listening on the port 445 only after you created the first SMB file share on the Gateway. Did you create the SMB file share on the gateway where the port 445 is not open?

By the way, why are checking whether the port 445 is open on the File Gateway? Are you troubleshooting a SMB issue?"
AWS Import Export Snowball	"Re: How to validate gateway is listening on port 445
Yes, I created an SMB share on the gateway and cant access it.  I cant telnet to 445 and the network access should be fine.  I just want to validate that the gateway is truly listening on 445.  There is no option for that when logging into the gateway via SSH"
AWS Import Export Snowball	"Re: How to validate gateway is listening on port 445
What is the problem you are troubleshooting? Are you not able to mount the SMB file share from your windows host?

You mentioned that you have multiple SMB gateways. What is unique about the gateway where you noticed this behavior?

Can you please PM me your Storage Gateway ID and the region where you are experiencing this issue?"
AWS Import Export Snowball	"boto3 refresh_cache not accepting FolderList or Recursive kwargs
I am trying to provide a FolderList to refresh_cache on a file share. When I don't provide either FolderList or Recursive the call goes through just fine. But if I provide either kwarg I get:

botocore.exceptions.ParamValidationError: Parameter validation failed:
Unknown parameter in input: ""Recursive"", must be one of: FileShareARN
Unknown parameter in input: ""FolderList"", must be one of: FileShareARN

Example call is:
client.refresh_cache(FileShareARN=filesharearn, FolderList=[""/""], Recursive=False)


Where filesharearn is the ARN of the file share.

Is there an example for adding these kwargs so that I can see what I am doing wrong? 

Any help would be appreciated. Thank you

https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/storagegateway.html#StorageGateway.Client.refresh_cache"
AWS Import Export Snowball	"Re: boto3 refresh_cache not accepting FolderList or Recursive kwargs
The support for the last two parameters are recently added to the RefreshCache API. The errors you are seeing indicates that the version of the SDK you are using does not understand these parameters. Please update your SDK version to the latest."
AWS Import Export Snowball	"AWS gateway, can it transfer local NFS volumes to AWS cloud on block level?
AWS can transfer NFS files, or iSCSI volume to AWS S3/EBS, as I know of. Can it transfer local NFS volumes block by block to AWS S3/EBS? If no, what products/services can do that?

Is GW free of charge?

Thanks!"
AWS Import Export Snowball	"Re: AWS gateway, can it transfer local NFS volumes to AWS cloud on block level?
awsmagic8,

Here are the answers to your questions:

1) AWS can transfer NFS files, or iSCSI volume to AWS S3/EBS, as I know of. Can it transfer local NFS volumes block by block to AWS S3/EBS? If no, what products/services can do that?

If you're trying to transfer files hosted in an NFS server to AWS, the best service offering we have is AWS DataSync which can copy your data from your NFS server to S3 or EFS.  Details can be found at the link below.  When you're copying files via NFS you're using a file level protocol (rather than a block level protocol like iSCSI) and can't do a block by block copy regardless of what product/service you use.  AWS DataSync does perform an integrity verification after the sync operation to ensure that your data was copied correctly, if that is your concern.

https://aws.amazon.com/datasync/

If your goal is to get your data into EBS then your best choice is a Volume gateway where you copy your data to a volume of 16TB or less, take a snapshot and use that snapshot to create an EBS volume.  I can provide more details on this if you want.

2) Is GW free of charge?

I've included a link below with details on DataSync pricing.  Please let us know if you need more information.  Thanks.

https://aws.amazon.com/datasync/pricing/

Thanks!"
AWS Import Export Snowball	"SMB File Share password Incorrect
Hello,

I am trying to mount SMB file share in my windows Machine. It is setup as Guest access.

When I am trying to mount using net use command, it gives the following error: 

System error 86 has occurred.
 
The specified network password is not correct.


I know the password is correct, but its still showing the error

Edited by: USPLAdmin on Oct 8, 2018 9:55 AM"
AWS Import Export Snowball	"Re: SMB File Share password Incorrect
USPLAdmin,

Can you please try setting your Guest password again using ""Edit SMB Settings"" menu? Please refer to the following instruction. After saving your Guest password again, please let us know if you are able to mount the SMB file share again.

https://docs.aws.amazon.com/storagegateway/latest/userguide/managing-gateway-file.html#enable-ad-settings"
AWS Import Export Snowball	"Re: SMB File Share password Incorrect
Hello Shashi,

I tried resetting the password. But still its giving the same error."
AWS Import Export Snowball	"Re: SMB File Share password Incorrect
USPLAdmin,

Are you using the ""net use"" command as specified in the AWS console. Are you specifying the Gateway ID before ""smbguest"" ? 

Here is snippet from the documentation https://docs.aws.amazon.com/storagegateway/latest/userguide/using-smb-fileshare.html

net use <WindowsDriveLetter>: <Gateway IP Address>\<path> /user:<Gateway ID>\smbguest

Here is an example:

net use Z: \\10.0.0.19\smb-share /user:sgw-F2BE5A9B\smbguest

Please let us know whether this helps."
AWS Import Export Snowball	"Re: SMB File Share password Incorrect
Yes. Exactly the same command"
AWS Import Export Snowball	"Re: SMB File Share password Incorrect
USPLAdmin,

Please PM me your AWS account ID, region, Gateway ID and Support Channel ID.

Please refer to the instructions below on how to setup the support channel on your gateway.

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html#enable-support-access-on-premises
https://docs.aws.amazon.com/storagegateway/latest/userguide/EC2GatewayTroubleshooting.html#EC2-EnableAWSSupportAccess

Thanks,
Shashi"
AWS Import Export Snowball	"Re: SMB File Share password Incorrect
Hello, 

I found the solution. 
Posting it just in-case anyone else faces the same problem.

The below solution is from AWS Support
 Need to change the lmCompatibilityLevel. This specifies what mode of authentication and session security is used for network logons. To do so, there are 2 ways: 
1. Registry Edit: 

Run regedit 
HKEY_LOCAL_MACHINE 
SYSTEM 
CurrentControlSet 
Control 
Lsa 
Select LmCompatibilityLevel Set Value to either 3 OR 4 OR 5

2. Security Policy
 Open your Windows Local Security Policy 
Local Policies 
Security Options 
Network security: LAN Manager authentication level 
Set to - Send NTLMv2 responses only. 
OR - Send NTLMv2 responses only. Refuse LM 
OR - Send NTLMv2 responses only. Refuse LM & NTLM"
AWS Import Export Snowball	"Re: SMB File Share password Incorrect
Thank you for this!  I ran into the same issue with the Guest SMB account, and the regedit fixes it."
AWS Import Export Snowball	"Storage Gateway S3 Multipart Chunk Size
Hi All,

Apologies if I have not spotted this in documentation but is it possible to determine the chunk-size the Storage Gateway uses when it copies multipart files up to S3?

Im trying to re-engineer the etag of stored objects but have no idea what Storage Gateway used. Maybe this is the wrong approach...

Many thanks,
J"
AWS Import Export Snowball	"Re: Storage Gateway S3 Multipart Chunk Size
mbj:

Thanks for your question. The chunk-sizes used by File Gateway for multi-part PUTs will vary depending upon the size of the file and a number of other factors, so it's there's no rubric which can be commonly applied. For small files we use a single-part PUT so the eTag should be easy to determine. 

What are you wanting to do with the eTags in this case?

Thanks
Paul"
AWS Import Export Snowball	"Re: Storage Gateway S3 Multipart Chunk Size
Hi Paul,

Many thanks for getting back to me, this is very useful to know.

I am trying to double check that the files we have transferred into AWS storage have actually copied correctly (currently using rsync). In this particular case, the copy was being managed between an S3 gateway and an nfs mount across a ssh-tunnel so there was some worry about the integrity of the transfer (I know there are other S3 transfer mechanisms, but this is the situation we were in). The ability to verify the checksum stored in S3 without having to re-read the object would have been useful. 

Its likely that be that I have gone down the wrong path with this approach...

Either way its a shame that Gateway does not store the chunk size as it is currently hidden information 

Best,
J"
AWS Import Export Snowball	"Re: Storage Gateway S3 Multipart Chunk Size
mbj:

Thanks for your reply. Your request makes sense given the number of tools involved in the transfer (this use case is one of the reasons we introduced AWS DataSync, https://aws.amazon.com/datasync/, which does all the checksums for you). For File Gateway we'll take your feedback into consideration as we plan updates on our roadmap.

Regards
Paul"
AWS Import Export Snowball	"Re: Storage Gateway S3 Multipart Chunk Size
Hi Paul,

Thanks agin, I will go ahead and verify the checksums the old fashioned way using md5sum. I will certainly look at the other options in the future.

Kind Regards,
J

Edited by: mbj on Jan 3, 2019 1:45 AM"
AWS Import Export Snowball	"Using a EC2 Volume Gateway
Hello All:
I wanted to know how can I use a EC2 Volume Gateway, I used AMI image but seems bit tricky to activate, it simply says :
ww.xxx.yy.zz./?gatewayType=CACHED&activationRegion=us-west-2 
can't be reached

It has open ports as suggested by lauch wizard

What can I do? Please help me"
AWS Import Export Snowball	"Re: Using a EC2 Volume Gateway
Hi mnino,

I was wondering if you have checked out all the networking and firewall requirements available at the Storage Gateway user guide link below.

https://docs.aws.amazon.com/storagegateway/latest/userguide/Requirements.html

Specifically, I would recommend to check out the ""Configuring Security Groups for your Amazon EC2 Gateway Instance"" section.

Please let us know how it goes. If you have followed all the steps and it still doesn't work, I would recommend to file a ticket with the support team so we can have your issue looked at by our experts.

Regards,
Bhavin"
AWS Import Export Snowball	"What happened if the network between storage and AWS is broken?
If I use file storage gateway to transfer local application to AWS over Direct Connect. If the DX network is broken, I have several questions to ask?
1. Will the application be stopped when storage gateway can not connect to AWS?
2. I think the application will continue to write data to cache disk until the cache disk is full. Then what will happen?
3. When the network connects again, will storage gateway automatically sync the un-transferred cache data to S3?"
AWS Import Export Snowball	"Re: What happened if the network between storage and AWS is broken?
xiaweidong:

Thanks for your questions.

1. The application won't be aware than the network between the gateway and AWS is not connected. 

2. Yes, the application can continue to write data until the cache disks are full (indicated by the CachePercentDirty metric hitting 100%) at which point the application will get an error. Conversely, if your application is reading data and the data is in cache it will be served to your application. If your application attempts to read data that is not in the cache, the gateway will attempt to fetch this data from AWS which will eventually fail and return a read error to your application.

3. When connectivity is established any any data written to the gateway will be uploaded to AWS and the gateway will resume normal operation.

Overall you should consider the gateway resilient to temporary loss of network connectivity, but it is not designed as an offline storage appliance.

Regards
Paul"
AWS Import Export Snowball	"Re: What happened if the network between storage and AWS is broken?
Paul,

Thank you very much!"
AWS Import Export Snowball	"AWS Storage Gateway on-premise VM ""read-only filesystem""
Hi,

We are using a cached volume gateway with on-premise VM appliance, and it stopped working a while ago, with the ""Your gateway is currently unreachable"" message on the AWS console. I connected to the VM and it has lose its local ip, I ""reset all to DHCP"" which lead to a restart. After the VM came up, the local ip is back and I was able to ping it. The time is synced and System Resource check reported no error. However, when I tried to ""Test Network Connectivity"", every option would fail with a message about ""Read-only filesystem"":
""/usr/local/aws-storage-gateway/console/bin/testconn: line 31: cannot create temp file for here document: Read-only filesystem""

Please advise on how to recover the VM from this state."
AWS Import Export Snowball	"Re: AWS Storage Gateway on-premise VM ""read-only filesystem""
Hi arastra 

There appears to be some issues with your local disks associated with the gateway VM. Can you verify / validate the underlying disks hosting the Storage Gateway VM are healthy?. If you still cant get it to work, PM me the gateway ID, account Id, and the region this is activated in and I will be able to take a look the logs for more information. Also, if you could open the support channel for the gateway and post in the PM the ID along with the other information, it would be helpful to diagnose the problem.

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Gateway unresponsive after doing a refresh cache!!! HELP!
Hello,

We just force a cache refresh to update files that has been uploaded via S3 API directly and the Gateway stop functioning...

We do a reboot but ALL shares hangs, not only the one that we refresh the cache!

SGW ID: sgw-009F7369

Please we need help urgently.

Thank in advance.

Regards"
AWS Import Export Snowball	"Re: Gateway unresponsive after doing a refresh cache!!! HELP!
After 1 hour SGW start working again.

Thanks!"
AWS Import Export Snowball	"Re: Gateway unresponsive after doing a refresh cache!!! HELP!
Hi, 

How many objects were in the bucket? How long was it between cache refresh calls? Did you call cache refresh during the S3 upload?

Cheers,
John

Edited by: AWS-JK on Dec 14, 2018 6:53 AM"
AWS Import Export Snowball	"Moving 120TB DFS file system to AWS
We are currently in a position where we need to move our entire 120Tb DFS shared drive structure, including existing NTFS permissions off of the existing 220 geographically dispersed servers. We are intending to move everything to AWS in the short term while we prune and filter out the dross from the folders. We have in excess of 3.2 Million folders in the structure. We cannot prune in situ for a number of logistical and political reasons so we want to move everything to one pot where we can work on it.

What is the best way to accomplish this in AWS for this amount of data? I need to retain the NTFS permissions (some folders are secure) so wherever it sites needs to support NTFS natively.

Purchasing hardware to host this on premise would set us back £300k+ and we really do not have the space. Moving to AWS will be expensive, yes, but still cheaper than buying hardware. We intend to keep the files on AWS (~30Tb) once it has been pruned and the unwanted folders removed."
AWS Import Export Snowball	"Re: Moving 120TB DFS file system to AWS
Hi,

@skinnyb Do you have any progress with that? I have similar issue right now, only it's 10 countries and 40TB of data fo me... Can't find a solution with AWS as the file gateway doesn't really support NTFS permissions (posix style only - and i haven't found any way to adapt that automatically while transferring files FROM on-promises file share TO S3 and back) and cached gateway is basically iSCSI volume presented remotely which also will not work for me, as people want to access that remotely within AWS... (i.e all offices access S3 bucket with files through nextcloud on AWS and HQ uploads\syncs files through AWS file\whatever gateway...)
Also I found out that AWS gateways DOES NOT support S3 acceleration which is really a big bummer for me, as HQ is located in UAE and their upload speed without S3 acceleration is about 1.5-2.5MBps only, while with acceleration it's 50+ MBPS... 

Regards,
Vladimir."
AWS Import Export Snowball	"""Unable to load local disks"" after activating Storage Gateway with EC2
Hello community,

I did the following steps in the us-east-a region:

created a new Storage Gateway from type ""File gateway""
chosen ""Amazon EC2"" as host platform
created a new EC2 with 150GiB of type x4.large by clicking the ""launch instance"" button in the wizzard
add ports 80, 22 and 2049 to the security group
In the step ""Connect to gateway"" I put the public IP address and the Gateway gets activated.


In the ""Configure local disks"" step I get a ""Unable to load local disks"" error. The status of this created gateway is ""Offline""

Does someone found out how to handle it?"
AWS Import Export Snowball	"Re: ""Unable to load local disks"" after activating Storage Gateway with EC2
Hi,

Do you have an EBS Volume mounted to your EC2 instance?
You may find this useful:
https://docs.aws.amazon.com/storagegateway/latest/userguide/ManagingLocalStorage-common.html

When you said x4.large, did you mean m4.large? The minimum specification for a gateway is m4.xlarge:
https://docs.aws.amazon.com/storagegateway/latest/userguide/Requirements.html#requirements-hardware-storage 

Hope this helps."
AWS Import Export Snowball	"Syncing Files to Storage Gateway In a Timely Manner
I have about 92 million files making up about 10TB of data that I need to move to ASG from a local nfs share. I do not have access to the underlying hardware just the nfs share itself where the data currently resides. I want to move these files to s3 though the local ASG, the initial sync can take weeks and that would be fine but I need to know of a way to do the final cut over sync in a timely way.

I think rsync would take to long to index files on the source nfs share and the ASG presented nfs share.

I'm currently investigating using netapp XCP to sync the data since it builds its own indexes and speeds up indexing a lot. 

What are some strategies customers use to do the final sync/cut over quickly ? 

I would imagine there is a process to do this as I would think this is a common need with snowball since both systems would have to do a final sync before switching to s3 storage."
AWS Import Export Snowball	"Re: Syncing Files to Storage Gateway In a Timely Manner
Hello -
Did you end up using XCP?  If so, how long did it take to copy the 10TB of data?

Best Regards
Mike"
AWS Import Export Snowball	"Re: Syncing Files to Storage Gateway In a Timely Manner
Hi there,

I researched a bit as this is an interesting question! 

Here’s what I found: When moving large amounts of data from on-premise to S3 it can take a significant amount of time for indexing when using 'sync' command. This can even cause the uploading process to appear as frozen. 

As a solution, instead of using 'sync' command, you can use the 'cp' command which copies the files without indexing overhead to S3 with improved speed. Although this works for newly created files, if we add files after the 'cp' command execution completes, we need to use the 'sync' command for the later added files. This works faster rather using the 'sync' command at the beginning.

If you need to create a Storage Gateway using an S3 bucket with a large number of files it can again take a long time to populate the cache in Storage Gateway.  This could be avoided by creating the Storage Gateway pointing to an empty bucket, and after that executing the 'sync' command to sync the files from the source bucket to the newly created bucket in S3. However, when accessing files through Storage Gateway, you’ll need to fetch them from S3 which potentially slows down the initial file retrieval. You can work around this by manually doing a query of files to populate the cache initially.

Since you have mentioned NetApp, another option is to use NetApp Cloud Sync, it’s a replication and synchronization service (SaaS) used to transfer and synchronize NAS data to and from cloud object storage. They support many formats, including NFS, SMB/CIFS (see https://cloud.netapp.com/cloud-sync-service). If you are moving to a fully managed cloud volumes by NetApp for aws storage, you can leverage SnapMirror replication technology to replicate on-premises data to AWS. This also makes it easier to have secondary copies available for multiple use cases with high availability."
AWS Import Export Snowball	"Re: Syncing Files to Storage Gateway In a Timely Manner
This is the perfect use-case for our new service DataSync:
https://aws.amazon.com/about-aws/whats-new/2018/11/introducing-aws-datasync-for-accelerated-online-data-transfer/

DataSync and Storage Gateway are meta-data compatible, so transferring via DataSync and then maintaining access via Storage Gateway should give you the best outcome."
AWS Import Export Snowball	"Trouble Joining Storage Gateway to Windows Domain
I'm having trouble getting my storage gateway joined to a Windows domain.  I'm getting the error message, ""The gateway cannot connect to the specified domain"".  I'd appreciate any direction on where to find relevant detailed logs and also any domain specific requirements like DNS records that need to exist."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
Hi, 
Have you opened up the ports necessary for the gateway to join your AD domain? This page  https://docs.aws.amazon.com/storagegateway/latest/userguide/Resource_Ports.html has a list of ports that need to be opened up incoming to the gateway for that operation to succeed.

If you have already done that, please private message me your gateway ID and account ID and we will look into it.

Thanks,
Smitha"
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
We are also not able to get the file gateway to join our Windows domain. 

We ""edit SMB settings"" and on the form which appears, tick ""join domain"" and provide the username / password of a domain admin account which is able to add new machines to the domain. 

When we submit the form, after a delay of about 5 seconds, we see a red banner message at the top of the ""gateways"" view: ""The specified request timed out"". 

I've seen the link in the thread above re: ports which need to be open for the domain join to work. However our domain controller is local to our storage gateway with no intervening firewall, so all ports are open."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
I've had time to do a little detective work, but I don't have an answer yet.

On the storage appliance, in""/usr/local/aws-storage-gateway/var/output/logs/access_log"", I found:
Jun 21 19:24:33 localhost sudo: sgserver : TTY=unknown ; PWD=/usr/local/aws-storage-gateway ; USER=root ; COMMAND=/usr/local/aws-storage-gateway/join-domain my.domain.com 123.123.123.123

The script it's calling is using ""net ads join"" to join the domain.

Edited by: PaulR@AWS on Jun 29, 2018 8:14 PM"
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
That's interesting - how do you get a command prompt such that you can inspect the logs? I built a storage appliance from the ""new"" image date 18th June 18, it seems pretty locked down. 

I've opened a support case on this with AWS, thus far they've asked me to ""open the support channel"" which I've done so they can look at the gateway themselves. 

I'll post here if we find the solution."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
I mounted the filesystem to an existing VM.  The appliance is running Amazon Linux 2017.09 and Samba version 4.6.2.

Edited by: randomUserMO on Jun 27, 2018 10:39 AM

Edited by: randomUserMO on Jun 27, 2018 12:01 PM"
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
I found this error in the AwsAppliance.log.2018-06-27-16 log file:

DNS Update for sgw-XXXXXXXX.XX.XXXX.com failed: ERROR_DNS_GSS_ERROR

Which indicated it's having trouble registering the host name to DNS.  So I manually created a DNS entry in that zone (which is managed by Windows), gave EVERYONE full control over the record for testing, and I'm now able to join the appliance to my domain and create shares.

I'll be redeploying from scratch to make sure none of my other fiddling was what actually fixed it."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
Sounds like a clever workaround. 

AWS support got back to me having looked at the logs on our gateway etc. via the ""support channel"". 

They said the problems we're having will be addressed by fixes which should be released in an updated image of the file gateway next week. 

It's good to know it wasn't misconfiguration on our part. I guess the SMB functionality for the file gateway is quite new and it still has a few teething problems. 

We are evaluating the file gateway against similar offerings e.g. Azure Storsimple but it seems best to wait for the fixes before we proceed with that."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
Just reporting back on the actual fix.

As long as I create a DNS record of the format sgw-xxxxxxxx.my.WindowsDomain.name before trying to join the appliance to domain, it's successful."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
Hi randomUserMO,

Would you be able to elaborate on the fix that you did to be able to join your storage gateway to the domain.

i.e, I am adding a DNS entry as sgw-1234789.test-us.local where test-us.local is my domain. Am I doing this right?"
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
For those of you following this thread, despite whatever updates AWS was intent on making a couple of months ago, the problem persists.

All traffic is allowed on my dev' network. Everything is connected and I've joined a Windows Server instance to my dev' domain. However, I am unable to add my existing Storage Gateway to a Simple AD, for SMB usage.

The error that I receive: The specified endpoint was not found. (Request ID: http://...)

As per the previous poster, please advise how, specifically, you overcame the issue with your workaround. I have manually added an A record for sgw-....my.domain, to no avail.

Thank you."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
Merlin,

There are multiple reasons for Storage Gateway failing to join a domain.

Can you please private message me your gateway ID and account ID and we will look into it.

Thanks,
Shashi"
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
Thanks Shashi.

I have PMed you.

Much appreciated."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
I am also facing trouble while joining & activating gateway.

During activation, many times UI hangs at Configure Local Disks, and then suddenly ends at Unable to Load Local Disks.
I am using self-hosted Hyper-V instance of AWS Storage Gateway & assigned it to appropriate virtual switch with 10GB RAM & 2 Seperate SCSI hard disks of 50 GB.

On certain days it just activates without any issue, but after that I face the issue related to joining the domain.

any pointers would be really helpful."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
After a lot of head-scratching, I found that the Gateway needs to be able to lookup DNS names within the AD domain name. So if your AD is called ""ad.example.com"", the instance tries to look up ""_ldap._tcp.dc._msdcs.ad.example.com"". If the DNS lookup fails, then it can't join the domain. To do the lookup, it has to use the correct nameservers.

Instances (probably - unless you've changed thing) use the AWS built-in nameservers. You can change this by setting DHCP Options on your VPC. You need to create a new DHCP Options set, specify the AD domain name and the IPs of the Domain Controllers as the nameservers. Then configure you VPC to use this new Options set.

Once all that's done, any new instances will use the DCs as their nameservers. Storage Gateway will then be able to do the DNS lookup and locate the Domain Controllers to join the domain. After it's joined, you can potentially put the DHCP Options back the way they were."
AWS Import Export Snowball	"AWS Storage Gateway conectivity ok but offline
Hi,
I've an Storage Gateway offline. I've already rebooted the local vm, made all network connectivity tests, checked ntp time, and opened a support connection. All of the tests runs ok but the gateway in the console is offline.

Are there any posibility of an Amazon side check?

Thanks.

****
Some more info:
We have 3 gateways. One of them failed last week. It recovered one day, without any action.

Today we have the two others offline. 

I see in the one which is still online  that the last software update date is 11/21/2018, 3:57:18 AM. 
Maybe some update makes the gw not to get online.

Edited by: DavidMA on Nov 27, 2018 5:37 AM"
AWS Import Export Snowball	"Re: AWS Storage Gateway conectivity ok but offline
Hi DavidMA,

PM me the gateway id(s), account id, region they are activated in along with the support channel for the affected gateways. I will be happy to take a look at them,

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Re: AWS Storage Gateway conectivity ok but offline
Info already sent. Thanks for your help."
AWS Import Export Snowball	"Re: AWS Storage Gateway conectivity ok but offline
The gateways are up and running againg.
The problem was a change in an outbound traffic rule in our firewalls. It make to fail the ssl handsake between the onpremise storage gateways and the Amazon site.

When we added the excetion to the local gateways they reconnected and all the services are back online."
AWS Import Export Snowball	"Doesn't work if underpowered?
I'm trying to get a practical feel for SG, so I've created one using T2 Micro free instances.  Everything seems to be ok, tests pass and I can ssh to the ec2 image, but when I add the gateway instance to the SG by IP it always come back offline.  I've destroyed and recreated 4-5 times and I keep getting the same result.  Is there any way I can play with this tech without paying out for a large server?  Or is that even the problem?"
AWS Import Export Snowball	"Input/output error listing some folders and files via nfs
Hello!

We are trying to move some data from EBS to S3 bucket mounted via SG Service. We can copy all data using s3cmd sync without problems, but when we mount the share via nfs and list some folders and files we get Input/output error... We check that files on the bucket and they are ok.

We try deleting the share and creating again but error still the same.

Some ideas?

Thanks in advance.

Regards

Javier Aszerman"
AWS Import Export Snowball	"Re: Input/output error listing some folders and files via nfs
We notice that the destination bucket is from other aws account. We have full permissions set for the source account that has the SG service with the share...

When we access the bucket from the other account, the same files that we get Input/output error, the properties of Encryption, Metadata and tags are ""access denied"".

The fact is, how can I fix permissions to be all the same as the files that has the correct ones?

Thanks in advance

Regards

Javier"
AWS Import Export Snowball	"Adding new disks to the Storage file gateway HOW??
Hi all,

I've created a storage file gateway and the cache disk is only 200GB currently so I wanted to add one or even two 1 TB EBS drives.

I've successfully created and attached two new EBS volumes.  The problem is that when I go to the storage gateway console --> Actions --> edit Local Disks, I do not see the new drives.  It only shows the old 200GB disk.

Am I missing a command to refresh or something like that?

Any help would be appreciated.

Thanks"
AWS Import Export Snowball	"Seeding Storage Gateway Virtual Tape Library with Snowball
Is it possible to populate a Storage Gateway VTL with tape images via Snowball?  We have a large number of tapes that would need to be moved to AWS as part of a conversion to using VTL, but our connection speed makes this impractical.  Is there a way to move tape images to a Snowball device locally and have them imported as tapes in the VTL?"
AWS Import Export Snowball	"Re: Seeding Storage Gateway Virtual Tape Library with Snowball
Hi,

Thanks for reaching out. I have sent you a PM regarding this post.

Thanks,
Bhavin"
AWS Import Export Snowball	"500 Internal Server Error
I get the following error while trying to log in. Can you help with this? 

The server encountered an internal error or misconfiguration and was unable to complete your request.

Please contact the server administrator, root@localhost and inform them of the time the error occurred, and anything you might have done that may have caused the error.

More information about this error may be available in the server error log.

Apache/2.2.29 (Amazon) Server at eureka.nextility.net Port 80"
AWS Import Export Snowball	"Re: 500 Internal Server Error
Hi UrNzWy 

This could be coming from a blocked firewall within your local network and / or the ISP. If you are still having issues after checking these, please PM me your gateway Id, account Id and the region your gateway is activated in and I would be glad to take a look at this for you.

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Powershell script to pull volume statistics
We are using the Volume Storage Gateway.
I have a script that pulls all the volumes and as much information as the get-sgvolume commandlet will get me which is what I have shown below. In addition to this I would like to be able to show volume space ""used."" I don't see this in any other commandlet but I know AWS has this information because they show it from the AWS web console. Thank you in advance for any help!

   TypeName: Amazon.StorageGateway.Model.VolumeInfo

Name              MemberType Definition
----
----------
Equals            Method     bool Equals(System.Object obj)
GetHashCode       Method     int GetHashCode()
GetType           Method     type GetType()
ToString          Method     string ToString()
GatewayARN        Property   string GatewayARN {get;set;}
GatewayId         Property   string GatewayId {get;set;}
VolumeARN         Property   string VolumeARN {get;set;}
VolumeId          Property   string VolumeId {get;set;}
VolumeSizeInBytes Property   long VolumeSizeInBytes {get;set;}
VolumeType        Property   string VolumeType {get;set;}"
AWS Import Export Snowball	"Re: Powershell script to pull volume statistics
Hi doubleoh7 

Yes, we have a cmdlet that shows the VolumeUsedInBytes (what you are looking for) and is available in Get-SGCachediSCSIVolume or Get-SGStorediSCSIVolume depending on the volume gateway of interest.

https://docs.aws.amazon.com/powershell/latest/reference/items/Get-SGCachediSCSIVolume.html
 or
https://docs.aws.amazon.com/powershell/latest/reference/items/Get-SGStorediSCSIVolume.html

The corresponding API from the SDK doc showing this is:

https://docs.aws.amazon.com/sdkfornet/v3/apidocs/index.html?page=StorageGateway/TStorageGatewayCachediSCSIVolume.html&tocid=Amazon_StorageGateway_Model_CachediSCSIVolume

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Not reaching the Storage Gateway EC2 for activation
Hello!

I'm working on a very simple lab for file gateway, and i'm having issues on the first step. I'm trying to set an EC2 as the storage gw for file gw following the docs instructions. 
Seems to be pretty simple, i create the ec2 instance from de AMI and start the instance, login as admin and thats all.
When I try to active the gw from the storage gateway console, I enter de public ip address (yes, I reach the ec2, at least ssh), and i never reach the next step, i get the following error message when trying to connect to the gw for activation:

""This page isn’t working 18.205.217.181 is currently unable to handle this request.
HTTP ERROR 500""

I tried several things.... enabling ports on the security group following the docs, tried different instance family types, ssh the appliance and tried to set something, what seems that there's no need to set anything there.

So can anyone give me clue about it? Do I miss a step? Do I have to do something on the Appliance, like set an specific hostname, port? Network configuration? Any specific configuration outside the ec2?

thanks in advanced!"
AWS Import Export Snowball	"Re: Not reaching the Storage Gateway EC2 for activation
That error in general is a firewall rule issue.  If you can PM me your instance ID we can help you to check it out.  Thanks.

John"
AWS Import Export Snowball	"AWS Storage Gateway offline
My storage gateway has been offline (in the console) for nearly 12 hours now - all the tests pass (i.e. network connectivity), but still no luck. Have tried rebooting but also no luck.

It did work at one time, so I know it was setup properly - can this be checked from the AWS end?

Thanks."
AWS Import Export Snowball	"Re: AWS Storage Gateway offline
Can you please PM me the gateway id, region its activated in and your account Id?. Also please open a support channel for me to remotely login to the gateway and take a look at this (take a look at the link below):

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Re: AWS Storage Gateway offline
I have sent you the information you requested via private message - thanks. 

Gateway is in us-east-1"
AWS Import Export Snowball	"Issues with SMB File Gateway
We are attempting to use the semi-new SMB version of the Storage Gateway to present 7 different shares across our Windows environment. We are having quite a few issues in trying to do so.

1. We are writing SQL Backups to the share. Some backups work just fine, others will write for 20 minutes and then fail with an access denied message, even though they've been writing to the same folder the whole time.

2. Our application uploads files to the share. These files seem to disappear after a day or so. The file still exists on the share, but there is no matching file in the corresponding S3 bucket.

3. Somehow, we've got folders being created with a ""\"" in them. Obviously this is causing some issues because windows interprets that character as a path separator. So we end up with a folder called ""~\foo\bar"", but in the same root dir there is a folder called ""foo"" with a subfolder called ""bar"". Windows has no idea what to do about this and it's causing all kinds of weird issues.I'm not sure if we are using the gateway for it's intended purposes. There are a few million files spread across the 7 shares, should the gateway function at this scale?Any help or advice would be appreciated!"
AWS Import Export Snowball	"Re: Issues with SMB File Gateway
Figured out issue number 3.  Somehow we created a folder inside the bucket with a \ in the name.  The File Gateway shared this file out, and windows had no idea what to do with it.

I was able to move the contents of that folder to another folder to resolve this issue."
AWS Import Export Snowball	"Re: Issues with SMB File Gateway
Do you have other applications that periodically scan files in the file share (like a virus scanner) that could be preventing the SQL process from writing to the share?"
AWS Import Export Snowball	"Re: Issues with SMB File Gateway
We are running a virus scanning software, but it should be scanning on every write.  The backup fails at the same point (percentage wise on the sql backup) each time.  I'm not sure if it's a file size/number of files it's hitting.  

How large would you recommend the cache volume to be?"
AWS Import Export Snowball	"Re: Issues with SMB File Gateway
For #1, please try increasing the client session timeout for SMB connection.

https://blogs.msdn.microsoft.com/openspecification/2013/03/19/cifs-and-smb-timeouts-in-windows/

Add a new DWORD, with name ""SessTimeout"" under HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\LanmanWorkstation\Parameters\ in the Windows Registry on your SMB client host. Populate it with the value '0xe10' (3600 seconds).

Please let us know if this helps.

Shashi

Edited by: shashi-AWS on Nov 8, 2018 2:22 PM"
AWS Import Export Snowball	"Re: Issues with SMB File Gateway
An hour seems like an absurd amount of time to wait for a timeout...."
AWS Import Export Snowball	"Re: Issues with SMB File Gateway
I suggested a large value to confirm whether it addresses the issue you are experiencing. Once you confirm that it resolves the issue, you can change it to a shorter value that works for your environment.

Few of our customers reported success after making similar change while backing up large SQL server backups.

Please let us know whether it resolves the issue."
AWS Import Export Snowball	"Re: Issues with SMB File Gateway
Ok, I'll give it a shot.  Seeing this error on some of our SQL backups now """"The operating system returned the error '59(An unexpected network error occurred.)' """
AWS Import Export Snowball	"File Share Stuck in ""Creating"" Status
Hi All

I have an issue where a newly created file share is stuck in creating status. I think I know why. I changed the instance type on the VM and it broke the gateway, so I deleted it, deleted the VM and started again. I have tried to recreate it 5 times, same issue. When I delete it then it also gets stuck in deleting status and I have to force delete the gateway. I think my issue exists as in the backend these aren't actually deleted and they are still there, therefore the s3 bucket is not allowing multiple connections.

Does anyone know how I can expedite this to Amazon support please?

Thanks

Dave"
AWS Import Export Snowball	"Re: File Share Stuck in ""Creating"" Status
Please PM me your AWS account ID, Gateway ID, and region.  Thanks."
AWS Import Export Snowball	"Re: File Share Stuck in ""Creating"" Status
Thanks John I have messaged you"
AWS Import Export Snowball	"Re: File Share Stuck in ""Creating"" Status
Anyone able to help please?

I am confused how AWS support works, or the fct you have to pay for it. To me this is an issue I cannot fix and they need to do something their side"
AWS Import Export Snowball	"Re: File Share Stuck in ""Creating"" Status
I've run into the same issue, and I've tried several attempts to a couple different buckets. I've noticed that in order to delete a share, I have to execute the ""Delete file share"" action twice to get the share to truly delete.

My configuration is a Storage Gateway running in a VMWare Cloud on AWS cluster attempting to use an S3 bucket in the same region. Anyone have any suggestions?

Thanks,
Jason"
AWS Import Export Snowball	"Storage Gateway - File Gateway inside VPC with no internet gateway?
I have been searching for an answer to this and I am hoping that someone with more experience with storage gateway may be able to help please?

I have activated a storage gateway using an internet gateway. Ideally, I would like the storage gateway and file shares to be private, but it is my understanding that the storage gateway needs outbound access to the internet? Is this correct? I tried attaching a NAT gateway instead of an internet gateway, but reading the documentation I don't this will work and the Storage Gateway status was shown as ""Offline"" from the AWS management console when I did this.

I have seen some recommendations about adding the IP addresses associated with the storage gateway and file gateway endpoints to the security group for the EC2 instance running Storage Gateway. This would then limit the storage gateway to those endpoints. However, the link ""AWS IP Address Ranges"" on  https://docs.aws.amazon.com/storagegateway/latest/userguide/Requirements.html#networks does not work. Has anyone achieved this?

I'm already using the Amazon S3 Gateway Endpoint as part of my routing for the storage gateway. So I'm happy that calls to S3 are not going over the public internet. It would be great to achieve this with the remaining connections that storage gateway makes, if possible?

Does AWS have any update on when the storage gateway endpoints will be made available as private VPC endpoints?  https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html 

Any recommendations would be really appreciated?"
AWS Import Export Snowball	"Re: Storage Gateway - File Gateway inside VPC with no internet gateway?
If your gateway is running inside a private subnet in your VPC, a NAT gateway will allow you to connect and route traffic to the Storage Gateway service. Your gateway requires access to endpoints documented in https://docs.aws.amazon.com/storagegateway/latest/userguide/Requirements.html#allow-firewall-gateway-access. An alternative to this would be to run your gateway in a public subnet in your VPC to avoid need for a NAT gateway. We do not currently support private VPC endpoints for the Storage Gateway service, but are interested in learning more. Feel free to PM me your requirements if you are looking for this feature and the use case driving the need for it."
AWS Import Export Snowball	"NotifyWhenUploaded
hi,

i am not receiving any file upload notification , ex: ""NotifyWhenUploaded""  fron VM Storage gateway... with NFS clients , is something am i missing in gateway setup...
Appreciate your help..

SS"
AWS Import Export Snowball	"Re: NotifyWhenUploaded
Hi sirigiri,

Assuming you called the API (https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_NotifyWhenUploaded.html) to request the CloudWatch event, could you please Private Message me your gateway ID, region and time of request of the event through the API call so we can look into it?

Thanks,
Smitha"
AWS Import Export Snowball	"How to decrypt a tape encrypted with KMS key on AWS storage gateway
Dear Experts,

Could you please help to show me show to decrypt a tape encrypted with KMS key on AWS storage gateway.
I created the tape with below command :
 aws storagegateway create-tapes --gateway-arn arn:aws:storagegateway:ap-southeast-1:1111111111:gateway/sgw-222222222--kms-encrypted --kms-key arn:aws:kms:ap-southeast-1:333333333:key/4444444444444444 --tape-size-in-bytes 107374182400  --num-tapes-to-create 1 --tape-barcode-prefix TEST --client-token 77777
thank you so much."
AWS Import Export Snowball	"Re: How to decrypt a tape encrypted with KMS key on AWS storage gateway
Hi PhuongNguyen

Are you looking to create a decrypted tape from this encrypted tape? I am afraid that's not allowed as in we do not support creating a plain text resource from an encrypted resource. However you can directly read and write to this encrypted tape using the normal workflow. You can delete and then re-create a new tape with no KMS encryption though. Let me know if you have any more questions on this.

https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_CreateTapes.html

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Re: How to decrypt a tape encrypted with KMS key on AWS storage gateway
thank you so much Sanjaya"
AWS Import Export Snowball	"File Gateway currently unreachable. It may be shutting down or restarting
Hi Community, 

Currently, our on-prem file gateway is unable to connect to AWS. 

Your gateway is currently unreachable. It may be shutting down or restarting which can take a few minutes. (Over 12 hours now)

Network test 
all 4 endpoints pass 
System Resources all OK

I have restarted the VM with no luck. 

Any ideas? Is there a way to manually start the gateway via console?"
AWS Import Export Snowball	"Re: File Gateway currently unreachable. It may be shutting down or restarting
JBeans,

Please PM me your AWS account ID, region, and Gateway ID.  Thanks."
AWS Import Export Snowball	"Re: File Gateway currently unreachable. It may be shutting down or restarting
Hi,
I am having the same issue, is this a very common issue with AWS Storage Gateway?

Scenario 1: Creating a SG for the first time in AWS Console, 
below steps works fine

**Select gateway type**
**Select host platform (Download and Deploy OVF Template)**
**Connect to gateway**
**Activate Gateway**
However, problem starts at step ""Configure local disk"" after allocating Cache and Upload buffer (170GB each). ""Gateway is Active"" but ""Saving failed, please refresh local disks to see the current status"", eventually ""Unable to load local disks."". 

Refreshing doesn't make any difference. 

Scenario 2: Delete activated Gateway and re-create using the existing deployed OVF gateway in vSphere:
It fails at **Connect to gateway** regardless I use the same mac and ip address or a different IP address. Solution is to Delete the earlier deployed gateway in vSphere, re-deploy OVF template and reconnect in AWS cloud but then again I am stuck at ""Configure local disk"". 

I have created and re-created this SG more than 20 times so far and I have managed to get it working only once last night. However, this morning SG status showing offline and obviously lost the volume. 

I have checked the network connection in vSphere gateway console, test connection passes all 4, connection to different region works fine, tried setting static ip and preferred DNS or without static and dns, either way it's the same issue.

I have no SOCKS configuration and from Sydney Region.

Any help would be much appreciated.
Thanks"
AWS Import Export Snowball	"Re: File Gateway currently unreachable. It may be shutting down or restarting
I also have the same issue Your gateway is currently unreachable. It may be shutting down or restarting which can take a few minutes."
AWS Import Export Snowball	"Re: File Gateway currently unreachable. It may be shutting down or restarting
skimani,

Did you create & activate the gateway recently? 

If the root of the S3 bucket has large number of files it takes time to initialize for the first time. While the Gateway is initializing the files in the root of the file share, the status shows up as Offline. The status will change to Online, once the initialization is complete. We are working on improving the user experience in this scenario.

Can you please PM me your AccountID, GatewayID & Region?

Thanks,
Shashi"
AWS Import Export Snowball	"storage gateway snapshot management
Hi - 
I'm wondering if there is any tool, or whether anyone has developed any tool for
snapshot management for storage gateway snapshots.

For example, many backup products have the concept of 
keep daily backups for X days
weekly for x weeks
monthly for X mnths

So, this would be a program that would some how tag a snapshot
as a daily/weekly/monthly and then just keep a certain amount of them
deleting others.   

I Understand that we aren't paying for any extra storage with the extra snapshots,
but having a few hundred, or manually deleting them every few months, seems backwards.

Thanks for any comments.

Steve"
AWS Import Export Snowball	"Re: storage gateway snapshot management
There are some example scripts in the User Guide of using the SDK to delete unwanted snapshots: http://docs.aws.amazon.com/storagegateway/latest/userguide/managing-volumes.html#DeletingASnapshot. Could you use or adapt these to suit your needs?
Thanks
Paul"
AWS Import Export Snowball	"Re: storage gateway snapshot management
Hi Steve

I have the same problem. Did you managed to get a proper solution?
Thank you

Regards,
Chalitha"
AWS Import Export Snowball	"Re: storage gateway snapshot management
@Paul:
As you can see here, and elsewhere in this forum, snapshot management for Storage Gateway is lacking.  Aside from not being able to define retention rules (e.g. like RDS), it doesn't appear that Storage Gateway allows for being automated via Ops Automator, SNS/Lambda, etc.  Nor can you tag snapshots, e.g. with the volume/gateway name, except via manually/API/CLI.

Yes, I can code my own CLI scripts or API app to do this, but the whole snapshot scheduling/management end of things needs work.  I'm trying to use Storage Gateway as a way of handling offsite backups - it's mostly working, except this piece."
AWS Import Export Snowball	"Re: storage gateway snapshot management
Hi,

I will PM you to understand more about your use case.

Regards,
Bhavin"
AWS Import Export Snowball	"Voumes in pass through, inaccessible,  upload buffer was filled. Action?
Hi,

Over the weekend it looks like we ran out of upload buffer space and now have 5 volumes (e.g. vol-c63b4fe8) in Pass through mode. I added another upload buffer disk and cache disk to our VM but the CachePercentDirty remains at 100%.

Is there any way to tell when the volumes will come back online or if they are even trying to recover? What other actions can I take or statuses can I monitor?

Thanks

Edited by: awsipgd on Oct 22, 2018 10:26 AM"
AWS Import Export Snowball	"Re: Voumes in pass through, inaccessible,  upload buffer was filled. Action?
Hi awsipgd,

Can you please PM me your account Id, gateway id and the region its active in? I will be able to take a look at it for you.

Thanks
Sanjaya-AWS"
AWS Import Export Snowball	"Re: Voumes in pass through, inaccessible,  upload buffer was filled. Action?
PM sent, thanks.

FYI I added an additional cache volume and the the percentage has gone down from 100,"
AWS Import Export Snowball	"Re: Voumes in pass through, inaccessible,  upload buffer was filled. Action?
Hi,

24 hrs later there are still 5 volumes in pass through mode. How can I tell when/if they will recover?

Thanks"
AWS Import Export Snowball	"Re: Voumes in pass through, inaccessible,  upload buffer was filled. Action?
Hi

Still not seeing any status changes. Also notice that there have been no successful snapshots since 10/17 and two are ""pending"".
How do I resync these volumes?

Thanks"
AWS Import Export Snowball	"Re: Voumes in pass through, inaccessible,  upload buffer was filled. Action?
awsipgd,

I responded to you via PM a possible scenario where this could occur. Please confirm our theory and if its correct, I will post the details in this thread to close the loop.

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Re: Voumes in pass through, inaccessible,  upload buffer was filled. Action?
Hi,

Any luck seeing what is happening and how to fix? The buffers are slowly filling again, we have degraded performance and one of the volumes is now 100% full.

Thanks"
AWS Import Export Snowball	"Re: Voumes in pass through, inaccessible,  upload buffer was filled. Action?
Yesterday I stopped the gateway through the AWS dashboard, then rebooted the local VM, then restarted the gateway through the AWS dashboard. The upload buffer and cache utilization that had been stuck started to decrease and the volumes began changing from pass through to bootstrapping to available. Today they are all back to available with near zero cache and upload usage."
AWS Import Export Snowball	"Should we see reduced performance while copying files directly to S3 bucket
We are in the midsts of copying from a Snowball into S3 bucket which is the storage behind our AWS Storage Gateway (File Gateway Mode). We're finding that as the copy to the S3 bucket occurs, we're seeing very reduced performance reading existing data in the bucket.

Is this expected?"
AWS Import Export Snowball	"Re: Should we see reduced performance while copying files directly to S3 bucket
The act of writing data to S3 bucket directly (in this case Snowball), should not impact the performance of the File Gateway. How are you trying to read the data written through Snowball from the File Gateway? Are you invoking RefreshCache API? If yes, how often are you invoking this API?"
AWS Import Export Snowball	"Re: Should we see reduced performance while copying files directly to S3 bucket
We were not trying to access the data that's coming off the snowball, but other data that was already in the bucket. During the transfer we ran the Refresh Cache only a couple of times via the UI over a period of several days.

The way we are set up is that the storage gateway is mounted via NFS onto another Linux box.

The Snowball just finished transferring, we ran a manual sync afterwards. 

We don't seem to be having those issues now, but we're concerned that when we transfer additional data there, we will run into issues."
AWS Import Export Snowball	"Storage Gateway File Gateway and Encryption
Is the local buffer area on the on-premise storage gateway file gateway encrypted?
Can we utilize encryption on the NFS 4.1 connection to the storage gateway?"
AWS Import Export Snowball	"Re: Storage Gateway File Gateway and Encryption
The local buffer of File Gateway is not encrypted. Currently we do not support NFS4 Kerberos-based encryption. 

Thanks for your input as it is valuable for us to put out our product roadmap. We'd appreciate it if you could share more about your use case so we can better understand your needs. Please feel free to PM me."
AWS Import Export Snowball	"Re: Storage Gateway File Gateway and Encryption
Is there any plan to implement NFS encryption in the future?

I prefer to take a ""better safe than sorry"" approach to configuring my systems, which includes choosing an encrypted option whenever one is available. I can say that my VPC is reasonably secure from compromise, but I like to guard against that eventuality anyway by doing my due diligence to harden configurations whenever possible.

Also, if I wanted to make a storage gateway available to a host outside of the VPC (a local box), without NFS encryption, I have to resort to a VPN to ensure the data remains private. I'd most likely still use a VPN anyway, but I'd prefer the option was available in case it were needed."
AWS Import Export Snowball	"Cannot Delete Virtual Tape Gateway
Hello,

I have trouble deleting Virtual Tape Gateway. Whenever I try to delete, it gives an error.
Cannot be deleted."
AWS Import Export Snowball	"Re: Cannot Delete Virtual Tape Gateway
Hello

Have you checked to see if this VTL doesnt have any tapes in RETRIEVED or RETRIEVING state? If so, you will need to eject those tapes from your backup application and then first delete them before deleting the VTL gateway. If you dont have the original gateway available, then you will need to recover the tape (see below link) and then delete the VTL gateway,

https://docs.aws.amazon.com/storagegateway/latest/userguide/Main_TapesIssues-vtl.html#creating-recovery-tape-vtl 

Sanjaya-AWS"
AWS Import Export Snowball	"Re: Cannot Delete Virtual Tape Gateway
There are 2 tapes, status of one is Availabel  and second is Retrieved.
None of the tapes can be deleted. When trying to delete, it gives an error :  Tapes that failed deletion: 


The tapes are already ejected from the backup application."
AWS Import Export Snowball	"Can't Delete SG Stored Volume
Hi -
I'm trying to delete a stored volume and am getting an error.

I noticed that my snapshot charges went up quite a bit in the last month,
and this appears to be on the 1 TB stored volume I have.  Stored Volumes
require snapshots, Even though my stored volume was virtual empty,
my snaphot costs shot up to covering 1 TB. 
As there isn't anyway to tell how much storage a snapshot is actually using,
its kind of hard to debug where the culprit is, so I'm just bailing from
using stored volumes.  ** I don't want to pay for 1TB of snapshot
data when I am not storing 1 TB. 


Anyway, since I can't ""NOT HAVE"" snapshots, I'm wanting to delete the
volume, but am unable to do that.  I get an error of -

Volumes that failed deletion:  ......

Any Ideas on how to delete the volume?   
I've marked the disk as offline on my win machine.

steve"
AWS Import Export Snowball	"Re: Can't Delete SG Stored Volume
Check to make sure the underlying disks used for the stored volumes are still accessible. If not, make sure to add them back and retry the delete action.

https://docs.aws.amazon.com/storagegateway/latest/userguide/troubleshoot-volume-issues.html#troubleshoot-volume-issues.VolumeIrrecoverable"
AWS Import Export Snowball	"Re: Can't Delete SG Stored Volume
Hi - I did have the volume ""offline"" on my windows host, as I thought that 
would be the proper procedure for deleting a volume.

I could not delete it when offline and i couldn't delete when it was online also.
I've deleted all snapshots, but can't delete the snapshot schedule as this is
required (reason why i have to get rid of this stored volume). 

I've deleted all my cloudwatch items, but it still says its monitored by
cloudwatch, not sure if I can enable or disable that.

So, I still can't delete it."
AWS Import Export Snowball	"Re: Can't Delete SG Stored Volume
Hi Steve

I was talking about the disks configured for your volumes in the Storage Gateway, are they online and active? PM me your gateway-id, regions its activated in and the account number. I will take a look at it for you.

Thanks
Sanjaya-AWS"
AWS Import Export Snowball	"Re: Can't Delete SG Stored Volume
I was able to get help from the billing contact, who gave me a tip 
on ensuring the initiator was disconnected.

I'd done everything but that.
So, going into the iSCSI initiator,  making that volume inactive after
marking the volume as offline in disk manager, made me able to delete
the volume.

Things I'd tried,  volume offline, delete cloudwatch metrics,  taking the
gateway offline.  It makes sense now, you'd have remove the initiator from
the volume."
AWS Import Export Snowball	"AWS Gateway Virtual Appliance Pricing
A few years back we looked at the AWS gateway but its fixed $125/mo cost made it unjustifiable for a small nonprofit.  I am revisiting this for a different use case and I don't see any reference to pricing for the Virtual Appliance.  Is there no longer a charge for each active virtual appliance or am I just missing the pricing somewhere?"
AWS Import Export Snowball	"Storage Gateway Installation - EC2?
I am attempting to test the Storage Gateway for our company; however, we have a policy that we must use standard AMIs.  Is it possible to install the Storage Gateway on a normal EC2 instance such as Amazon Linux 2, RedHat, etc?"
AWS Import Export Snowball	"Re: Storage Gateway Installation - EC2?
Hi, The Storage gateway is a managed appliance we provide for your applications to connect and access cloud storage. You cannot install the gateway separately as an application on an OS."
AWS Import Export Snowball	"Is it feasible to use a File Storage Gateway for network file services
I am setting up a .NET based application on Windows servers and require a file share (SMB) for the web and application servers to access for shared files.  My initial thought was EFS, sounded like a perfect match to what we need until I read that it only supports NFS and not EC2 Windows servers (plus the NFS client on Windows doesn't persist).  

So now thinking of using a File Storage Gateway to use as a possible solution.  
First concern, latency and file availability. I don't know what the performance of a file gateway would be. I know it has cache volumes but how long could first file request be? 
Second concern high availability. Storage Gateway docs recommend only one writer per bucket, so I would not be able to setup a Storage Gateway VM in each AZ.  So no redundancy. Is it possible to put in an auto-scaling group if an AZ becomes unavailable? Regardless we'll be hit with cross AZ charges with a single point in one AZ (still not sure how much this is in reality).  

So, is this a good idea/approach?  If not, what are some better approaches to provide and SMB file share to servers in AWS? 

Thank you for your time"
AWS Import Export Snowball	"Re: Is it feasible to use a File Storage Gateway for network file services
Hi brettski,

If you are provisioning a Storage Gateway on EC2, the performance of the gateway will depend on EC2 instance and a number of other factors. These include the network bandwidth between the client and gateway, the speed and configuration of your underlying local disks the amount of local storage allocated to your gateway, and the bandwidth between your gateway and Amazon storage. 

In general we don't recommend HA solutions such as the one you suggest, but I will check if we have any information regarding the use of Auto-Scaling groups with gateways.

PM me for more details on your requirements for high availability, so we get an understanding of what is driving the need."
AWS Import Export Snowball	"AWS Stored Volume Gateway - Data being uploaded to S3 or not?
Hi

I have recently changed our AWS storage gateway from cached to stored volume as we've acquired more local disks. We use this for backup purposes only so we have a faster restore locally but have backups in the cloud for Disaster Recovery if needed. 

It has an upload buffer of 2.5TB but I haven't yet seen any data being used on here or on the AWS S3 volumes yet which is worrying. The backups ran at the weekend.

The gateway ID is:
sgw-C841A5A1

Is there anything else I need to do or should this start to upload automatically at some point?

Thanks
Sarah

*I'll also add that I did try and remove and recreate the gateway yesterday but there is still the original volume data to replicate

Edited by: sCBarbon on Oct 2, 2018 12:19 AM"
AWS Import Export Snowball	"Re: AWS Stored Volume Gateway - Data being uploaded to S3 or not?
Hi sCBarbon,

Please PM me the region your storage gateway was activated in and the account Id, I will take a look at it for you. Also if you can please open the support channel for the gateway and include that output in the PM, that would help with the troubleshooting.

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html

regards
Sanjaya-AWS"
AWS Import Export Snowball	"Re: AWS Stored Volume Gateway - Data being uploaded to S3 or not?
Hi Sanjaya, 

Thanks for your response. I have sent a PM with all the details requested.

Thanks
Sarah"
AWS Import Export Snowball	"Volume bootstrapping
Hi,

I have few new volumes that have been set up few weeks ago.  The problem is it doesn't seem they're syncing correctly as Restore/Bootstrap progress has been sitting at 0% since they were created. How can I fix this? Any ideas? The connectivity between storage gateway and AWS is fine from what I can see.

Thanks

Tomas"
AWS Import Export Snowball	"Re: Volume bootstrapping
Hi sceanz,

    We see that uploads from this gateway to AWS are timing out. This is the reason why the Bootstrap progress for the volume is at 0%. See: http://docs.aws.amazon.com/storagegateway/latest/userguide/PerfGatewayAWS.html. Can you increase your upload bandwidth rate limit (http://docs.aws.amazon.com/storagegateway/latest/userguide/MaintenanceUpdateBandwidth.html) and check the upload throughput from the gateway to AWS.

Thanks,
Nishanth"
AWS Import Export Snowball	"Re: Volume bootstrapping
Thank you for your response Nishanth.
The affected storage gateway was set to upload rate of 200KiB/sec. I've increased it to 1000KiB/sec one hour ago and I still don't see any traffic uploading. CloudWatch is showing readtime and writetime readings just not uploading is happening. What else would be causing this?

Regards,

Tomas"
AWS Import Export Snowball	"Re: Volume bootstrapping
The issue is now fixed. Turns out AWS upload traffic was routed incorrectly."
AWS Import Export Snowball	"Re: Volume bootstrapping
Hi

This is happening on the volumes on my gateway too. I've taken the limit off of our gateway.

Can you help?

Thanks
Sarah"
AWS Import Export Snowball	"OOM Killer on Storage Gateway
Hi
We have a storage gateway on a new account that just houses our backups (we previously had  a storage gateway on our main account) that keeps getting OOM killer messages:
""72484.369672 Out of memory: Kill process 3499 (java) score 825 or sacrifice child
72484.371564 killed process 3499 (java) total-vm:15147824kb, anon-rss:13543352kbm file-rss:0kb, shmem-rss:0kb"" which kills our backups.
This occurs when we do our backups to the onsite storage gateway every night. The jobs are just some simple robocopy scripts. The storage gateway was deployed to our vmware host by downloading the latest ovf from aws (updated to the latest version). The ovf deployed originally with 8GB of ram, I have since increased it to 16GB of ram but even with that it still OOM kills Java every night. Exactly the same scripts used to run on our old storage gateway (with 8GB ram) without an issue every night. I cannot access any logs on the gateway or see how much memory is allocated to Java, as the console is very locked down. 
Has anyone got any ideas on how to troubleshoot this?
ps we cannot justifiably give it any more ram as it is stretching the hosts resources as it is
Thanks,
Dave

Edited by: mortality101 on Sep 18, 2018 3:29 AM

Edited by: mortality101 on Sep 18, 2018 3:31 AM"
AWS Import Export Snowball	"Re: OOM Killer on Storage Gateway
Hi mortality101,

Is this a file gateway, volume gateway or a tape gateway? Please if you can PM me the account id, gateway Id, and the region its activated in that would be helpful. If possible please also open the support channel for the gateway and include the id in the PM.

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html

regards
Sanjaya-AWS"
AWS Import Export Snowball	"Re: OOM Killer on Storage Gateway
Sanjaya-AWS correctly diagnosed it was the oom_kill_process() being called from vmballoon_work(). 
He advised to disable the VMWare balloon driver and allocating dedicated memory to the Storage Gateway VM.
This fixed it"
AWS Import Export Snowball	"Managing ESXI File Gateway
I've created a file gateway and successfully have an SMB share bound to my domain. I'd like to bind it to a different domain and manage permissions, but I don't see a way to do this in the console (since it is on prem) or via the command line tools.

How do I manage the gateway now that it's been deployed?"
AWS Import Export Snowball	"Re: Managing ESXI File Gateway
Is this page useful?
https://docs.aws.amazon.com/storagegateway/latest/userguide/managing-gateway-file.html#enable-ad-settings"
AWS Import Export Snowball	"Re: Managing ESXI File Gateway
When you use ""Edit SMB Settings"" from the AWS Storage Gateway Console and enter new Domain, SGW will disconnect/leave from the currently connected Domain and joins the new Domain."
AWS Import Export Snowball	"Switched share from NFS to SMB, all existing folders have no write access
We are having issues with our NFS shares, so have switched one of them to SMB to see if that is any better.  However, after removing the NFS and reactivating as an SMB with guest access only, and export as ""Read-Write"", we can only create files in the top folder.  Inspecting the Security tab on a folder shows that the gateway user/smbguest account is not present on any of the existing folders.

Why doesn't the storage gateway automate the process of recursively adding the smbguest user as part of the share setup?

I only see the smbguest set on new folders/files I create at the root folder.  I tried using the icacls powershell command to recursively add it, but I keep getting the following error: ""\smbguest: No mapping between account names and security IDs was done.""

How do we recursively add this now after the fact?

Edited by: billkg on Sep 17, 2018 3:26 PM"
AWS Import Export Snowball	"Re: Switched share from NFS to SMB, all existing folders have no write access
billkg,

The behavior you observed is as expected as the permissions between NFS and SMB are different. If a customer has a use case where both NFS and SMB file shares are connected to the same S3 bucket, we don't want all the files to be available to all the users unless the customer explicitly configures the file ownership/permissions as appropriate.

In your case, if you would like to provide access to all the files written from NFS share you need to make it available as a NFS share again. After you make it available as a NFS share, update the read/write permissions appropriately so that ""other"" users have access. Then you will be able to access the files/folders as ""smbguest"" from the SMB share. 

Note that you cannot export both NFS and SMB file shares simultaneously from the single gateway pointing to the same S3 bucket. So you either create two Gateways or do the following.

1. Delete the SMB file share
2. Create NFS file share & Update the file/folder permissions.
3. Delete the NFS file share
4. Create SMB file share

Please PM me if you would like to discuss further, also please check your messages."
AWS Import Export Snowball	"Re: Switched share from NFS to SMB, all existing folders have no write access
I may be mistaken as it's been a while since I looked, but I believe the permissions stored as object tags. Depending on the scope of what you need to do, you might be able to create a temporary SMB share, create some files, figure out the tags, and just use a short script to overwrite the tags directly to the files in the original S3 bucket."
AWS Import Export Snowball	"Re: Switched share from NFS to SMB, all existing folders have no write access
Thanks for the suggestions.

Due to the ongoing stability issues with NFS we wanted to switch to SMB.  The SMB shares continue to stay up for days so seems like the better route to go.  

I will take a look at the tags route to see if I can fix it easily, as we had a bunch of challenges with NFS permissions to start with as well.

I will give it a shot and report back if/how I solved it.

Thank you!"
AWS Import Export Snowball	"Re: Switched share from NFS to SMB, all existing folders have no write access
billkg,

If you take the route of updating the tags in S3 directly, you need to invoke RefreshCache against your File Gateway to make it read the updated tags."
AWS Import Export Snowball	"EC2 gateway activation fails at ""connect to gateway"" step
I can't create an EC2 file gateway through the AWS console.  At the page where I enter the IP address and click the ""connect to gateway"" button, the browser loads http://xx.xxx.xx.xx/?gatewayType=FILE_S3&activationRegion=us-east-2 and displays a blank page: ""This site can’t be reached"".

I enabled port 80 for my computer.  I can ping the EC2 instance and access port 80. The EC2 instance meets minimum requirements (m4.xlarge with 80 GB and 150 GB disks).

2/20 UPDATE:
This activation error didn't occur when I used a Storage Gateway AMI listed on the user guide below.
 However, with these AMIs the key pair doesn't work (""server refused our key"") and I can't SSH to the EC2 instance.
https://docs.aws.amazon.com/storagegateway/latest/userguide/ec2-gateway-file.html

Edited by: craws on Feb 20, 2018 9:54 AM"
AWS Import Export Snowball	"Re: EC2 gateway activation fails at ""connect to gateway"" step
craws,

In response to your 2/20 update that you have an EC2 gateway running but the key pair is being refused.  Did you use an existing key pair or create a new one when prompted as part of the setup process?"
AWS Import Export Snowball	"Re: EC2 gateway activation fails at ""connect to gateway"" step
Hi, I am having the same issue but I have created a new key pair when prompted. It appears my web browser cannot connect to the IPv4 address (either for m4.xlarge or t2.micro instances) in the EC2 console.

Hi, I have attached the screen I have, I then type in the IPv4 address from the deployed sync agent in the EC2 management console. I then get the following error message:

This site can’t be reached
13.236.60.225 took too long to respond.
Try:

Checking the connection
Checking the proxy and the firewall
Running Windows Network Diagnostics
ERR_CONNECTION_TIMED_OUT

Edited by: Cassius on Sep 16, 2018 11:12 PM"
AWS Import Export Snowball	"Gateway keeps going offline while running tape backups from Veeam.
We are using Veeam to do tape backups to our Storage Gateway. We are using 2 network adapters and one is configured as the adapter for Internet traffic, the other is used for iSCSI with Veeam. We have a gigabit Internet connection which over which only AWS traffic is routed. The main problem we need resolved is that out Gateway keeps going offline and the only way we can get it to come back online is to restart it (power reset on the VMWare VM). When we do this sometimes the job continues, but more often the backup job fails. We are also experiencing hugely varied speed anywhere from 70MBps to 5MBps. Does anyone know what could be causing this.

Edited by: aseo on Sep 13, 2018 11:14 AM"
AWS Import Export Snowball	"Re: Gateway keeps going offline while running tape backups from Veeam.
Hi Aseo

Please PM me the gateway Id, region and if possible open a support channel [https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html] and I will have someone from our team look at this for you.

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"VPN access from client desktops to our EC2 instance
Hi,
We have a Windows based EC2 instance that amongst other things generates reports and does word merges for our clients.  We would like to be able to share a folder on the EC2 instance with our client's workstations so that they can retrieve and save files from Windows applications as if they were simply retrieving and saving to and from a folder attached to a windows drive letter.  Outside of AWS this can be done by means of a VPN between a VPN server and the client's workstations.  Can it be done in some way to our EC2 instance?  The important factor here is ease of setup and use for our clients - some of them have no sophisticated IT help.

Thanks"
AWS Import Export Snowball	"Tape backup to AWS - Advisable to do full backup ?
Hello people ,
  At our company we are looking at using Tape gateway/VTL to replace our on-prem Tape library .
  Backup application is Netbackup and we do full backup of our Teradata database once a week worth 50 TB ( completes in about 14 hrs) .
  Now with a 500 mbps dedicate line to AWS this upload will take days to complete . Storage gateway upload limit maxes out at 120 MBPS or about 1 Gbps . It wont help a lot even if we manage to get a 1 Gbps connection .
  So my question is how do you guys use Storage Gateway to backup tens of TB s worth of data ? Or is it fit for incremental backups only ( in range of GB s and not TB s) ? Any pointer is greatly appreciated ."
AWS Import Export Snowball	"Re: Tape backup to AWS - Advisable to do full backup ?
Anybody having any pointers ? Maybe the AWS Support team ?
To circumvent the bandwidth problem we are looking to spawn multiple Storage Gateways to upload in parallel .
So with 4 Storage Gateway VM we can theoretically have 2 Gbits/s (on 4 500Mbits/s lines )
Have someone connected Netbackup application to multiple Storage Gateways for parallel upload ? Are there any downsides ?
Also Upload Buffer max capacity is 2 TB , but we are looking to push through about 5 TB per Storage Gateway , so upload buffer will get full from time to time , but with sufficient Cache storage Netbackup should still be able to write to Storage Gateway without failing i believe ?"
AWS Import Export Snowball	"Re: Tape backup to AWS - Advisable to do full backup ?
Hi dipayan80 

The performance numbers provided here https://docs.aws.amazon.com/storagegateway/latest/userguide/resource-gateway-limits.html#performance-limits  are on a per gateway basis and provided for general guidelines based on certain test conditions. These numbers are not enforced in AWS Storage Gateway software and are driven by initiator to gateway bandwidth, ram/cache/disk/cpu on the gateway and gateway to cloud bandwidth. In practice we’ve seen customers get higher numbers, based on the specific platform they use.

We have customers using Storage Gateway VTL solution to do both full as well as incremental backups and it’s also possible to achieve a higher upload throughput by parallelizing through multiple gateways.

With upload buffer max capacity of 2TiB, you could choose to use higher cache up to 16TiB to ensure successful backup.

Hope this helps
Sanjaya-AWS"
AWS Import Export Snowball	"Re: Tape backup to AWS - Advisable to do full backup ?
Thanks a lot Sanjaya , your inputs are very helpful !
We will push about 6 TB data through each VM with input speed of 1 GBytes/s and output of 500 Mbits/s (compression enabled ) .
For each VM we are planning to use 24 CPU cores , 2 TB upload buffer , 8 TB Cache and 32 GB RAM . 
I think the above VM configuration will enable us to upload the data in about 26-28 hrs , do you think the configuration is optimum , or would you suggest beefing it up somewhat , perhaps the RAM ?"
AWS Import Export Snowball	"My File backup storage Gateway on S3 status is offline
Hi

My backup storage on S3 stopped backing up since August 1 and is now showing ""offline"". How do I fix that please?"
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
Please PM me your AWS account ID, region, Gateway ID and Support Channel ID.

Please open support channel from the Storage Gateway Local console and PM the support channel ID. Please refer to the instructions below on how to setup the support channel.

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html#enable-support-access-on-premises

Thanks."
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
My Gateway ID is : sgw-9805E0F1

Region is: N Vigirnia

My Account ID is: 110766076107 

Support Channel ID: 22225"
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
Can anyone help me please?"
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
I really need this backup to start again."
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
We are investigating, we will post an update."
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
The Storage Gateway process is in a restart loop as the Cache disk is not available hence it appears OFFLINE. You need to reattach the Cache disk and restart the Storage Gateway.

Another thing we noticed is, the available memory of 8GB is less than the recommended minimum of 16 GB. Please increase the memory as well for optimal performance.

Please keep us posted with your findings."
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
Not sure how to reset the gateway and re-attach the cache. Do we have documentation(s) on this?"
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
Any hint on to attach the cache disk? 
thanks."
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
It appears like your Gateway is hosted on Hyper-V. When you have setup the Storage Gateway, you have created and attached a local disk through Hyper-V to the Storage Gateway. You must have followed the instructions available at the links below. The Cache disk that you attached to Storage Gateway originally seems to be no longer available. Please investigate in your Hypervisor logs for relevant information. Once you resolve the issue with the disk and confirm that it is assigned to the Storage Gateway VM, please restart the VM. Alternately, you can create a new Gateway VM and delete the old VM.

https://docs.aws.amazon.com/storagegateway/latest/userguide/create-gateway-file.html#hosting-options-file
https://docs.aws.amazon.com/storagegateway/latest/userguide/create-gateway-file.html#configure-local-storage-alarms-file"
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
I saw the cache file  on the server and it's healthy, but don't see where I can 'reconnect' it to the AWS gateway.
I tried to create a new cache disk and it's asking me for an IP address which says offline every time  I enter it using the same local IP (192.168.1.242)  that I saw on the AWS storage configuration.
When I enter my public IP it goes to my local router.

Please help."
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
Any ideas on what next step(s) I should take?"
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
Please PM me your AWS account ID, region, Gateway ID and Support Channel ID. The support channel ID that you have provided previously is closed now, so please share the new support channel ID."
AWS Import Export Snowball	"Storage Gateway File share becoming unresponsive
Hello,
We have been having a number of issues this week with our on-premise Storage Gateway NFS file share becoming unresponsive. Resolving the issue usually requires a reboot of the Gateway.

I've checked a number of the troubleshooting guides and can't seem to nail down the issue, I've also checked the performance optimization.

I have done some things this week to see if it would resolve, this includes adding another cache drive and bumping the vCPU.
Any suggestions?
Thanks,
Daryl"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
Daryl,

Please PM me your AWS account ID, region, and Gateway ID. We'll take a look at why your file share is becoming unresponsive. Thanks."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
We are still experiencing issues."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
Rollemad, 

Could you please run the ""mount"" command at a command prompt and PM me the output?

Thanks,
Smitha"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
I'm able to successfully mount the NFS share now, I don't have an example of a failed mount.

Thanks,
Daryl

Edited by: Rollemad on Aug 15, 2018 9:46 AM

Edited by: Rollemad on Aug 15, 2018 9:48 AM"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
Hi smithaAWS,
Are you able to connect to some back-end support channel, this device is a bit of a black box to me.

Thanks"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
It stopped responding again. I ran a mount command and it appears to mount successfully but hangs."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
Did you see any output from the mount command? If so, please private message me the text. You could also check to make sure you are using ""-o nolock"" and ""-o hard"" (especially for Windows)

https://docs.aws.amazon.com/storagegateway/latest/userguide/GettingStartedAccessFileShare.html"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
The mount command basically timed out, and yes were are using the mount commands documented."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
What OS (Linux, Windows 10/2012/2016 etc) are you using on your NFS clients? If you have any Windows NFS clients, please specify the options ""-o nolock"" and ""-o hard"" for the mount command.

Can you please open support channel from the Storage Gateway Local console and PM the support channel ID? Please refer to the instructions below on how to setup the support channel.

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html#enable-support-access-on-premises"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
Support channel ID sent."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
We are having the same problem since last week. It has gotten progressively worse and now we are able to traverse directory structure but cannot read or write any files."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
Carley,

Please PM me your AWS account ID, region, and Gateway ID.

Here is the recommendation I shared with Daryl and it seem to help their situation.

Please unmount and remount their filesystem using the ""-o nolock"" option from all your Windows NFS client hosts. Note that for windows hosts, we recommend a pause of 60 seconds between unmount and subsequent mount, to prevent reuse of previous mount. 

Please let us know how it goes. Please make sure you repeat these steps on all your Windows NFS client hosts."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
We recently had an the issue again. Reboot seemed to fix it.
A couple things I noticed:
All my NFS File Shares were inaccessible. I currently only have two, one should only be accessible from my workstation. I wasn't able to mount either with the Windows mount command with ""nolock""
However, my SMB file share was still accessible.

Is there something that's causing the nfs service to fail, and are there any fixes in the Storage Gateway update available August 30th?

Thanks,
Daryl"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
I am having the exact same problem.  I have two gateways, one that is a volume gateway running for months with never any issues, mounting volumes via ISCSI to veeam backup software.  

I wanted to replace a locally racked NAS server with a file storage gateway and wanted to use nfs mounts.  I almost need to reboot my gateway VMware VM daily.  That vm has 4 vcpu and 16 GB RAM allocated to it.  I played around switching the network adapter type (currently e1000) and that didn't solve the problem either.

On reboot it works correctly, but even after the latest 8/30 update it still requires at least a daily reboot.  The host itself isn't even able to be reached.  When they are not available it is confirmed down on linux and windows 7/10/2008/2012/2016 machines, so it is a global outage.

I didn't try SMB mounts but I will.  NFS was easier as we have a mix of windows and linux machines sharing files.

Does NFS just not work?"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
Daryl,

In the Aug 30 release, we added improvements to deal with NFS clients that mounted the shares without ""nolock"" option. 

In order to understand the new behavior you reported, we request support channel access to your gateway. 

Please PM me your AWS account ID, region, Gateway ID and Support Channel ID. The Support channel you previously opened is no longer active."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
billkg ,

We are sorry for your bad experience.

Please PM me your AWS account ID, region, Gateway ID and Support Channel ID. We will investigate the reason for the behavior you are experiencing."
AWS Import Export Snowball	"AWS Tape Gateway with Backup Exec wiping tapes
we are using Backup Exec 15 with AWS Tape Gateway. It is set to run a full backup on a set of tapes each day and upload. For the most part this backup runs successfully each day. However I am getting it where now and then tapes previously used are showing as empty when they should obviously have data and only be overwritten when a new back up is run. 
Had this with one or two tapes before but now I have just had this with 8/10 tapes. 

Has anyone else experienced similar problems with Tape Gateway? Though I do suspect that really this issue would lie within Backup Exec"
AWS Import Export Snowball	"Re: AWS Tape Gateway with Backup Exec wiping tapes
On further inspection of the Backup Exec logs, our backup job was erasing all tapes so will investigate here"
AWS Import Export Snowball	"Veeam Tape jobs to AWS VTL
I am currently running a backup copy/copy to tape job in Veeam B&R 9.53a to an Amazon VTL Gateway running on Premise using the Amazon VTL image in Hyper-V. 

I have 550GB of Upload Buffer
I have 500GB of Cache

My backup copy is running at about 418Kbps, we have 100M fiber. For the first 30 minutes it romps along at 84Mbps (I guess it would, it's loading the caches and buffers) but then dies a horrible death. I have 3TB which I would like to GFS via Veeam once a week.

My cloud watch monitoring shows the following currently:

2018-08-21 08:35 UTC
1. UploadBufferUsed: 318G
2. QueuedWrites: 294G
3. UploadBufferFree: 227G
4. CacheUsed: 222G
5. TotalCacheSize: 222G
6. CacheHitPercent: 100
7. CachePercentDirty: 100
8. CachePercentUsed: 100
9. UploadBufferPercentUsed: 58.0
10. CacheFree: 0
11. CloudBytesDownloaded: -
12. CloudBytesUploaded: -
13. CloudDownloadLatency: -
14. TimeSinceLastRecoveryPoint: -

This is my first foray in to the world of AWS and I'm guessing something is wrong with my config. Any help is greatly appreciated.

Edited by: CDL on Aug 21, 2018 2:19 AM"
AWS Import Export Snowball	"Re: Veeam Tape jobs to AWS VTL
Any help out there?"
AWS Import Export Snowball	"Re: Veeam Tape jobs to AWS VTL
Hi CDL,

I have sent you a private message regarding this post.

Regards,
Bhavin"
AWS Import Export Snowball	"File Gateway - Change in behavior for files in Glacier Storage Class
Has anyone else noticed a change when using the File Gateway/NFS Mount?  If a user tried to access a file in the Glacier storage class (which still appear within the file system) it used to throw an I/O error.   Now, trying to access one of these files is crashing the application and bringing down the whole NFS mount on the client server."
AWS Import Export Snowball	"Re: File Gateway - Change in behavior for files in Glacier Storage Class
JeremyKane,

Please private message me your account id, gateway id and region and we will look into it right away. 

Thanks,
Smitha"
AWS Import Export Snowball	"Environmental Testing Instruments
Procure your test instruments from reliable suppliers that deal in varied equipment like multifunctional Harmonics Analyzer, Electrosmog Meter, Hygro Anemometers, Insulation Testers, etc. Such suppliers won't just deal in quality  https://www.mecoinst.com/meco-category-details/environmental-testing-instruments.aspx  but also offer consultation services as well."
AWS Import Export Snowball	"File Storage Gateway in EC2 slows down over time as files are copied in
So, we are transferring in millions... 10s of millions files into File Storage Gateway (NFS). There are lot of directories and a lot of small files ( < 128kb).

We have NFS share mounted on a ec2 instance in the same az, and just rsync files over to it. As time goes the rate of transfers becomes very slow. To start we can move in 1000s of files every minute, but after few hours the rate drops to 100s every minutes, and sometimes even stalls to 1-2 files per minute.

I have no idea what is causing this and can not pin point to one issue. 

The storage gateway has 1TB GP2 root volume, and 2x2TB gp2 volumes as cache. I checked IOP usage when things stall and they are very low ( less than 50 iops per volume or even less ). The cpu usage is also < 20%. Bandwidth usage is low. There is no metric that is out of ordinary, but the performance is poor.

Tried with c5.4xlarge, and r5.4xlarge instance types and both have the same issue.

One thing I noticed is if we let the SG ""cool"" off ( as in stop the rsync ) for a few hours, and resume the rsync we get good performance again for a bit. Are we reaching any limits? Api Limits? Some kind of capacity?

Any guidance on how to improve the performance would be great. Right now I can not pinpoint the issue."
AWS Import Export Snowball	"Re: File Storage Gateway in EC2 slows down over time as files are copied in
Please check the IOPS configured for your Root & Cache volumes. You must be exceeding the IOPS configured for your EBS volumes and hence they might be getting throttled resulting in slow IO performance."
AWS Import Export Snowball	"Re: File Storage Gateway in EC2 slows down over time as files are copied in
Hi Shashi,

This is not the case,
For the root we have 3000 iops ( 1 TB gp2), and for cache we have  6000 IOPS per volume and we have 2 cache volumes( 2 x 2TB gp2). Since the EBS volumes are over 1TB, there should not be any burst buckets.

While the Storage Gateway is at high performance level, we do see high IOPS on the volumes  https://imgur.com/a/IwQ6jiT . You can see the root volume is at almost 1k IOPS and the cache are humming along at about 600iops.

So we are definitely within our limits. At about 7:00 we noticed HUGE drop in performance, which is not caused by going over ebs iops. On the graph you can see our IOPS drop from 1k to less than 100. 

Again, i checked all the metrics and nothing explains the 10x drop in storage gateway performance.

Thanks for your help!"
AWS Import Export Snowball	"Re: File Storage Gateway in EC2 slows down over time as files are copied in
Please PM me your AWS account ID, region, Gateway ID and Support Channel ID.

Please refer to the instructions below on how to setup the support channel on your EC2 gateway.

https://docs.aws.amazon.com/storagegateway/latest/userguide/EC2GatewayTroubleshooting.html#EC2-EnableAWSSupportAccess"
AWS Import Export Snowball	"Amanda backup and Storage Gateway
Can the Amanda backup software use the Virtual Tape Library? I'm not sure how I would set up tapes and changer"
AWS Import Export Snowball	"Re: Amanda backup and Storage Gateway
Hi sobrik,

The Gateway-VTL has not been qualified with Amanda backup, but we are in the process of qualifying new backup applications and this application is on our list.

Thanks for your feedback and interest in AWS Storage Gateway, and please let us know if we can help with anything else.

Regards,
Ian"
AWS Import Export Snowball	"Re: Amanda backup and Storage Gateway
I dont know about Amanda backup, If you want other backup software then use  Ahsay Software . It is amazing backup software which allows businesses and managed backup service providers to backup. I am using this software from last 10 months. It is running smoothly and up-to-date. Thanks"
AWS Import Export Snowball	"Re: Amanda backup and Storage Gateway
Any update on Amanda support for Storage Gteway?"
AWS Import Export Snowball	"Re: Amanda backup and Storage Gateway
Hi ooboyle,

Thanks for your post. We don't currently support Amanda backup with the Storage Gateway. I have noted your interest and we'll consider it as we plan future functionality and features.

Regards,
Bhavin"
AWS Import Export Snowball	"Re: Amanda backup and Storage Gateway
Thanks, Bhavin.

We're also looking into Bareos. Any chance you are too?

Oliver"
AWS Import Export Snowball	"Using AWS Storage Gateway under Linux
What is simplest setup (if possible) to use AWS Storage Gateway under KVM (i.e., from native Linux environment)?

Judging by description, only VMWare ESXi and Hyper-V virtual machine images are supplied to make use of the service. I can't understand what prevented you from providing Xen or KVM image apart from intention to make using service under Linux as hard as possible.

If there are known steps to set up the service under KVM, I'd be very grateful to know that. VM images often convert nicely to KVM, but I'd prefer to know whether this works, or whether I should abandon the service and choose more Linux-friendly product.

Thanks in advance for any piece of advice.

Edited by: Konstantin Boyandin on Jan 26, 2017 3:52 AM"
AWS Import Export Snowball	"Re: Using AWS Storage Gateway under Linux
Hello Konstantin,

Thank you for your feedback regarding support for KVM. At this point, the current release supports Hyper-V and VMWare. We plan on supporting additional hypervisors in the future, although we do not have a specific timeframe at the moment for KVM support. Your feedback is appreciated and certainly helps us to prioritize our future roadmap.

Thanks,
Peter"
AWS Import Export Snowball	"Re: Using AWS Storage Gateway under Linux
So currently I have to use something like s3fs and similar FUSE-using tools, to mount storage resources.

The other part of my question remained unanswered - can converting Hyper-V/ESXi images to KVM work (are there known successful cases), or not.

Thanks."
AWS Import Export Snowball	"Re: Using AWS Storage Gateway under Linux
The Storage Gateway ""ESXi Image"" is actually an OVA (at least for the File and Volume gateways). I haven't tried Xen or KVM but I was able to import it in to VirtualBox and got it working without too many issues. Worse comes to worst you can just untar it and use the vmdk directly.

For FUSE drivers I'd recommend RioFS or Goofys over S3FS, we had nothing but problems with that thing."
AWS Import Export Snowball	"Re: Using AWS Storage Gateway under Linux
Are we any closer to an iso?"
AWS Import Export Snowball	"Re: Using AWS Storage Gateway under Linux
+1 for an iso we can use on any hardware or hypervisor"
AWS Import Export Snowball	"Re: Using AWS Storage Gateway under Linux
I have also managed to import and run the File Gateway appliance on Ubuntu 16.04 Virtualbox. It functions OK but I cannot see the console output in the Virtualbox window. (Looks like black characters on black screen... Only the cursor is jumping around...)

I changed the appliance network interface type from NAT to bridge. Then I set my router to forward ports 80 and 443 to the appliance IP. That way AWS was happy to connect to it.

Volume Gateway also runs fine on Ubuntu Virtualbox, using the same port forwards. Additionally the above mentioned black characters on black background problem is not experienced, console functions well.

Edited by: triesz on Aug 20, 2018 4:57 AM"
AWS Import Export Snowball	"Virtual Tape Ejected, unable to find it
Dear forum members,

I've configured HP Data Protector 9 with AWS.

In AWS I have created two tapes.

When I finished to configured HP Data Protector, I could see both tapes in my slots.

But I clicked on one tape, and right button -> Eject.

The tape was gone, and althougt I select Instert, It did not find the tape.


I receive this message from Data Protector:

Warning] From: UMA@bckcentos01 ""bckcentos01"" Time: 15/12/2016 13:06:31

Mailslot(s) is (are) empty. Waiting for mailslot(s) to get filled.

But I do not know how to find and move the tape.

Do not kow where it goes.....

Could you help me?

Many thanks in advance,

Edited by: NetworkingCIMD on Dec 15, 2016 4:25 AM

Edited by: NetworkingCIMD on Dec 15, 2016 4:25 AM"
AWS Import Export Snowball	"Re: Virtual Tape Ejected, unable to find it
Dear forum members,

I've found the solution.

I did the eject yesterday, and I did not see any change in my AWS Gateway.
It shows two tapes.

I've entered my AWS Gateway just before to post this message, and I could see that I have only one tape.
I went to tape tab, and I could see my two tapes.

I retreive it, and I have this message:


You have successfully initiated retrieval of the tape AMNZ617DC4.

It takes about 24 hours for the retrieval to complete.

So, I think that all will be fine tomorrow.
Sorry and many thanks.

Edited by: NetworkingCIMD on Dec 15, 2016 4:32 AM"
AWS Import Export Snowball	"Re: Virtual Tape Ejected, unable to find it
You are correct. When you eject a tape it is archived and will no longer be available in your VTL:

http://docs.aws.amazon.com/storagegateway/latest/userguide/backup-hpdataprotector.html#hpdataprotector-archive-tape

If you want to access the tape again you can retrieve it:

http://docs.aws.amazon.com/storagegateway/latest/userguide/backup-hpdataprotector.html#hpdataprotector-retrieve-archived-tape"
AWS Import Export Snowball	"Re: Virtual Tape Ejected, unable to find it
One question regarding this issue...

Although I can see the tape in my HP Data Protector, I could not do anything with it.
In AWS Storage Gateway it says:
Barcode: AMNZ617DC4
Status: Retrieved

But my other tape has the State: Available.
I can not change the State from retreived to available. 
It is posible?

And in the Data Protector it says:

Major] From: MMA@bckcentos01 ""AWS-DRIVE-8""  Time: 20/12/2016 9:48:16
90:54]  	/dev/nst1
	Cannot open device (The medium is Write Protected.)

But the tape is blank. I only eject it and retreive it but without formating it or writing data on it.

Do you know how can I use this tape again?

Many thanks in advance,

Edited by: NetworkingCIMD on Dec 20, 2016 12:55 AM"
AWS Import Export Snowball	"Re: Virtual Tape Ejected, unable to find it
Ok.
I could read in the documentation that:
Note
Retrieved virtual tapes are read-only.

As this tape was send to Glacier when I eject it, I can understand that is read only.
But I can not delete it.

AWS Storage Gateway says:

Cannot delete resources due to one or more resources' status such as archiving or retrieving
Check the box to confirm deletion of the following resource(s):
•AMNZ617DC4 - RETRIEVED

It is no posible to delete tapes that has been send to Glacier?

Edited by: NetworkingCIMD on Dec 20, 2016 3:06 AM"
AWS Import Export Snowball	"Re: Virtual Tape Ejected, unable to find it
To delete a tape that's in RETRIEVED status you must eject it back to Glacier. See http://docs.aws.amazon.com/storagegateway/latest/userguide/managing-gateway-vtl.html#deleting-tapes-vtl

Sorry for the confusion here, we're looking into what we can do to simplify this.

Thanks
Paul"
AWS Import Export Snowball	"Re: Virtual Tape Ejected, unable to find it
Find here tape library rental in bangalore service provider in overall India.. http://www.racwg.com/backup-devices/"
AWS Import Export Snowball	"Re: Virtual Tape Ejected, unable to find it
Laptop Rental Providers in Mumbai, Maharashtra. Get contact details and address of Laptop Rental firms and companies in Mumbai. View a list of the available laptop on rent. Connect directly with owners to get immediately laptop on rent schedule by any time. It will beneficial for the industrial uses and also start companies. Now Hire laptop on rent, servers, computers on rent at affordable prices in India. We provide IT equipment to companies and individuals in India.
http://www.racwg.com/"
AWS Import Export Snowball	"Retrieving uncached data from AWS
When restoring from a tape that is in the VTL, but not cached locally, does the storage gateway have to retrieve the entire tape to local cache, or does it only call back the byte ranges needed for the restore?"
AWS Import Export Snowball	"Re: Retrieving uncached data from AWS
Hello,

In this case, Storage Gateway retrieves the requested bytes of the tape into the local cache not the entire tape. This reduces the amount of data that needs to be transferred to perform a restore.

Thank you,
Bhavin"
AWS Import Export Snowball	"File Storage Gateway - Using Robocopy
Hello,

I am doing some rudimentary testing with File Storage Gateway that runs in my VMware environment.  I am using SMB and had no issues setting it up, joining to my AD ..etc.  I wanted to copy some data from local drive (IE:  C:\temp\Folder1) to an SMB drive on the gateway mounted as letter Z:\  so i figured i would use robocopy for some testing.  Here is my command:

robocopy C:\Temp Z:\  /COPY:DT /DCOPY:T /E

Robocopy by default will only copy new and changed files but everytime i run robocopy where my target is the gateway, it always performs a full copy.  I checked the time on the gateway and it's identical to my Windows workstation.  When i look at Folder1 on local drive and Z: drive, time stamps for ""modified"" look identical.  Any thoughts ?

Thank you"
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
Hi dynamox 
Thanks for bringing this to our attention. We've taken note of this issue.
Regards,
Smitha"
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
Smitha,

Do you have a test environment where you can validate what i am seeing ?

Thank you"
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
Yes we do. 

Thanks,
Smitha"
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
The root cause is that the Gateway store time at MILLISECOND granularity. Windows keeps time at NANOSECOND granularity. So when Robocopy compares the time it thinks that the Windows version of the file is newer and re-copies it.

Here is a workaround: 

Use the /FFT option with robocopy.  This option tells robocopy to use a 2-second time granularity.

robocopy C:\Temp Z:\ /COPY:DT /DCOPY:T /E /FFT

Please let us know if it helps."
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
Shashi,

That helped where it longer performs full copy. Another interesting question is why the security parameter is not supported? For example:

robocopy C:\Temp Z:\ /COPY:DTS /DCOPY:T /E /FFT

The error message that I get is :

************************************

C:\Temp>robocopy C:\Temp z:\ /mir  /copy:dts /dcopy:t /fft


   ROBOCOPY     ::     Robust File Copy for Windows

  Started : Friday, August 10, 2018 3:31:36 PM
   Source : C:\Temp\
     Dest : z:\

    Files : .

  Options : . /FFT /S /E /DCOPY:T /COPY:DTS /PURGE /MIR /R:1000000 /W:30


                           0    C:\Temp\
2018/08/10 15:31:36 ERROR 5 (0x00000005) Copying NTFS Security to Destination Directory z:\
Access is denied.

************************************

I then decided to go to mapped network drive , created a folder and try to set ACLs using Windows Explorer and also received ""Access Denied"".  So the question is, can we not apply NTFS ACLs to files/folders that reside on SMB Share presented from the gateway ?  (My gateway is joined to AD).

Thank you"
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
dynamox,
As documented in our FAQS (https://aws.amazon.com/storagegateway/faqs/#file), file gateway supports POSIX permissions that is a compatible subset of NTFS -- and it appears you're not being allowed to copy if you are trying to carryover all of the NTFS permissions and/or set NTFS ACLs on files in the SMB share.

Please PM me directly if you want to discuss characteristics of your particular workload and what you are trying to do.

Thanks,
Smitha

Edited by: smithaAWS on Aug 12, 2018 11:00 AM"
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
I PM'ed you.   I mean simple stuff like being able to go to mapped drive (from File Gateway) and either adjusting folder owner or adding an additional AD account to ACL is not working.  Every time i try to do these operations i get  Windows Explorer error: ""Failed to enumerate objects in the container. Access is denied""

Kind of defeats the purpose of the gateway joining to my AD if i can't modify ACLs."
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
I'm having exactly the same issue... File Storage Gateway + SMB share and inability to change permissions post-upload to S3.

What is the actual method of mapping Windows ACLs to POSIX permissions in the S3 metadata?

It seems that during creation of an object, the following metadata are set per object:

x-amz-meta-file-group (posix-style gid)
x-amz-meta-file-owner (posix-style uid)
x-amz-meta-file-permissions (posix-style permissions correlating to uid, gid, and Everybody)

Is there no facility to change any of this information after upload, even with object copy, by manipulating Windows ACLs in the Storage Gateway file share?

Can we inspect the mapping x-amz-meta-file-owner or x-amz-meta-file-group to our Active Directo"
AWS Import Export Snowball	"SMB file share configure allowed/denied users and groups.
Hello,
I've deployed the latest GW on 2018-08-07 and configured SMB file share. I joined our AD without issue. I can access the SMB share without issue using ANY domain account.

I cannot figure out the syntax to specify users and groups in the ""Edit allowed/denied users and groups"" panel. I can't find any documentation on this...

Specifically, I want to restrict access to the SMB file share to specified users and groups, everyone else in AD is denied access.
Thank you,
Max"
AWS Import Export Snowball	"Re: SMB file share configure allowed/denied users and groups.
The following page describes how to ""Edit allowed/denied users and groups""
https://docs.aws.amazon.com/storagegateway/latest/userguide/managing-gateway-file.html#enable-ad-settings

As far as your question, you can simply enter the ""user name"" or ""group name"". You don't need to specify the Domain name as it is implied by the AD the gateway joined. We will update the documentation to clarify this."
AWS Import Export Snowball	"Activating Gateway: 500 Internal Error
Hi,

I have setup the Storage gateway on our Internal VMWare ESXi. While activating the Gateway, I get the following error: 

is currently unable to handle this request.
HTTP ERROR 500


This error is only visible in Chrome, in other browsers: Firefox, Edge. There is a white screen of death.

I tested Network Connectivity, it passes every time. The Network time is also synced. There is no difference in NTP Time and Host Time."
AWS Import Export Snowball	"Re: Activating Gateway: 500 Internal Error
USPLIT,

Can you please the name of the OVA image you downloaded and the type of the Gateway (File, Volume, Tape) you are trying to activate? If they don't match, you will encounter Activation error.

Once you confirm that you are using the correct version and still having the Activation issue, please open support channel through the Local Console and PM me the support channel ID.

Thanks"
AWS Import Export Snowball	"Files and Folders in Linux NFS mount are not using uid/gid of share
I have newly created a storage gateway with NFS mounts set using all of the defaults.

When I mount the share on CentOS 7, everything shows up and can be read.  However, nothing can be written into any of the folders due to permission issues.  The share root properly is using uid/gid 65534 (nfsnobody), but all of the other folders and files within the mount and below are owned by 4294967294:4294967294, which prevents writing into any folder.

The same NFS mount on windows allows reading and writing into any folder or file.

If I open the file metadata in my S3 bucket, the x-amz-meta-file-owner and x-amz-meta-file-group metadata fields have 4294967294 set in both of them.  

If I look at the mount, instead of it being 0777/0666 as noted in the SG Console mount properties, both folders and files are 0755.

I tried mounting with both NFS version 3 and version 4 and also do not see a difference.

What am I missing?"
AWS Import Export Snowball	"Re: Files and Folders in Linux NFS mount are not using uid/gid of share
billkg,

Based on your description below it looks like when you mounted the file share from Windows the Windows OS wrote it's permissions to the files and your Linux client lacks the appropriate permissions to access those files.  Most likely this is an issue with your squash settings on the NFS file share through the File Gateway.  I've included a link to the relevant documentation below.  Please let me know if you have any other questions.  Thanks.

John

https://docs.aws.amazon.com/storagegateway/latest/userguide/managing-gateway-file.html#edit-storage-class"
AWS Import Export Snowball	"Re: Files and Folders in Linux NFS mount are not using uid/gid of share
I tried to use ""all squash"", mounted as root in CentOS7 and see this:
drwxrwxrwx  1 nfsnobody  nfsnobody          0 Jul 18 09:41 .
drwxr-xr-x. 4 root       root              34 Aug  1 12:53 ..
-rwxr-xr-x  1 4294967294 4294967294    240955 Apr  9  2010 0000402902_out.mov

I then updated to ""no root squash"", mounted again and see:
drwxrwxrwx  1 nfsnobody  nfsnobody          0 Jul 18 09:41 .
drwxr-xr-x. 4 root       root              34 Aug  1 12:53 ..
-rwxr-xr-x  1 4294967294 4294967294    240955 Apr  9  2010 0000402902_out.mov

I then updated to ""Root squash"", mounted again and see:
drwxrwxrwx  1 nfsnobody  nfsnobody          0 Jul 18 09:41 .
drwxr-xr-x. 4 root       root              34 Aug  1 12:53 ..
-rwxr-xr-x  1 4294967294 4294967294    240955 Apr  9  2010 0000402902_out.mov

So, there doesn't appear to be a difference when I choose any of the three options.

After mounting, this is what comes back from the ""mount"" command:
nas:/temp on /mnt/temp type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=XXX.XXX.XXX.XXX,local_lock=none,addr=XXX.XXX.XXX.XXX)

Let me know if you have any other ideas."
AWS Import Export Snowball	"Re: Files and Folders in Linux NFS mount are not using uid/gid of share
Once you change the permissions to ""no root squash"" you next need to do a chmod as root to allow anyone to write to it.  If you just change the squash level and don't do a chmod the permissions remain the same.  Thanks."
AWS Import Export Snowball	"Re: Files and Folders in Linux NFS mount are not using uid/gid of share
I changed to ""no root squash"" and did a chmod -R +w . on the mount, but afterwards when I do a ls -la I still see the exact same thing:
drwxrwxrwx  1 nfsnobody  nfsnobody          0 Jul 18 09:41 .
drwxr-xr-x. 4 root       root              34 Aug  1 12:53 ..
-rwxr-xr-x  1 4294967294 4294967294    240955 Apr  9  2010 0000402902_out.mov

However, I did cd into a few directories and was able to actually write now as my settings for the share are 777 as defined in the console.  When I view the properties in windows they are shown correctly, so must be something odd happening in Linux to cause these to display incorrectly.

I had this same issue on another mount, didn't chmod anything and can write after changing to ""no root squash"", so perhaps that was it, just odd that it is still showing as 755 in the ls output via ssh."
AWS Import Export Snowball	"Storage Gateway Load
Hi All

I have been getting to grips with the AWS Storage Gateway. Everything is working and I am using it to send Veeam backups to S3. It all works OK, except Veeam reports a bottleneck at ""target"" and if I set multiple backups going they consistently drop connection. I have a 500GB cache with 500 iops and using m5.2xlarge. I can scale these up, but I am not sure if this is the issue.

Can anyone offer any advice please?

Thanks

Dave"
AWS Import Export Snowball	"Re: Storage Gateway Load
Dave,

Which gateway type are you using?  Also how many backups are you trying to run simultaneously? Thanks."
AWS Import Export Snowball	"Queries and issues regarding File Gateway
Mounted a File Gateway on a windows instance. Used a file generation script written in python to pump data straight into the mount point. The file gateway had 3x100GB Cache SSDs

Problems encountered:

1. The data produced by the script and the space consumed on cache did not seem to be in sync.
Before running the script the cache usage was 30MB while after the script had generated 100MB data(as seen by exploring Windows Properties and size in s3 console), the cache usage shot up to 300GB.

2. One of the processes generating the data failed even though the cache free metric in CloudWatch never dropped below 15GB.
Error Snippet: 
File ""E:\makefileset.py"", line 215, in create_fileset
    f.close()
IOError: [Errno 22] Invalid argument

3. The directories being created were specified as UNC type paths(\\?\E:\xxx) to the os.makedirs(python) api. The calls were failing till the path was changed to start directly at E:\
Error Snippet: ""WindowsError: [Error 1006] The volume for a file has been externally altered so that the opened file is no longer valid: '\\\\?\\E:'""

4. Also because of the key length limit of s3(1k) file paths length will be limited, which might cause issues when using long file paths in windows(32k) or linux(4k). Is there any way around this limitation?

Edited by: ishangupta on Jun 27, 2018 5:38 AM"
AWS Import Export Snowball	"Re: Queries and issues regarding File Gateway
ishangupta wrote:
Mounted a File Gateway on a windows instance. Used a file generation script written in python to pump data straight into the mount point. The file gateway had 3x100GB Cache SSDs

Problems encountered:

1. The data produced by the script and the space consumed on cache did not seem to be in sync.
Before running the script the cache usage was 30MB while after the script had generated 100MB data(as seen by exploring Windows Properties and size in s3 console), the cache usage shot up to 300GB.

2. One of the processes generating the data failed even though the cache free metric in CloudWatch never dropped below 15GB.
Error Snippet: 
File ""E:\makefileset.py"", line 215, in create_fileset
    f.close()
IOError: [Errno 22] Invalid argument

3. The directories being created were specified as UNC type paths(\\?\E:\xxx) to the os.makedirs(python) api. The calls were failing till the path was changed to start directly at E:\
Error Snippet: ""WindowsError: [Error 1006] The volume for a file has been externally altered so that the opened file is no longer valid: '\\\\?\\E:'""

4. Also because of the key length limit of s3(1k) file paths length will be limited, which might cause issues when using long file paths in windows(32k) or linux(4k). Is there any way around this limitation?

Edited by: ishangupta on Jun 27, 2018 5:38 AM
Answers below
#1 - How many files are you storing in your 100MB dataset? While we do not evict the cache unless there is newer data, depending on the file size you are trying to write you may see sparse usage of the cache. The gateway is trying to mediate between NFS which favors small reads and writes (KB size) and S3 which prefers data in larger tranches (MB size).

#2 As I mentioned above, the CacheFree metric isn't an indicator of any issue as we do not evict from cache unless there is more recent data that needs to be stored for local access. 

#3 UNC paths should work, so we'd be happy to take a look at what's going on. 

#4 The gateway enforces a maximum path length of 1024 bytes; clients are not allowed to create a path exceeding this length and will result in an error if they try to do so. There is no workaround this limitation as it's is driven as you noticed by the limit on the S3 object key length.

Please PM me your account ID and gateway ID and we will look into the failures you are seeing during uploads and with the UNC path

Thanks,
Smitha"
AWS Import Export Snowball	"Re: Queries and issues regarding File Gateway
Hi Smitha,

Thanks for your response. The answers are as below:

1. ""How many files are you storing in your 100MB"": The file number is fluctuating, we randomly created files between 1KB and 1GB.

2. I mentioned the CacheFree metric because of the documentation which stated that it may cause backups to fail. But as we discussed previously, the CacheDirty percent never went above 5%.

3. UNC paths not working might be an OS issue. We would look into it at our end also.

aws account id: 356683483434
gateway id: sgw-E8AF4A81

We are also planning to run the tests again in a more segregated way(small files vs large files) with better logging at our end too. Will send across the results and exact timings for them.

Edited by: ishangupta on Jul 19, 2018 11:04 AM"
AWS Import Export Snowball	"Time drifting way off after AWS GW reboot
Every time we reboot or an update is applied the time on the gateway is off by 6000 ish seconds. The GW is on HyperV and the system time and hardware time are set properly. The time zone is set properly in AWS GS settings. Can anyone tell we where it is getting its time when it reboots? We have a few AWS gateways and this only happens to one. It causes the GW to crash after a few hours if the time is way off."
AWS Import Export Snowball	"Re: Time drifting way off after AWS GW reboot
Hi

To resolve the clock drift issue, we recommend: 

1) Ensure that the guest time is configured to be correctly synchronized (see https://docs.aws.amazon.com/storagegateway/latest/userguide/manage-on-premises-Hyperv.html#MaintenanceTimeSync-hyperv ) 

2) If this was already appropriately addressed, check the hosts's time, to make sure that it is accurate. 

3) If you still have issues please open a support channel and PM me with that info along with gateway ID and the region its activated in.

Thanks
Sanjaya-AWS"
AWS Import Export Snowball	"Gateway failed deletion
Hi

We had issues with one of our on premise datastores some time ago (where the gateway appliance resides), and our VTL gateway is showing as offline in the AWS console. I want to remove the gateway from view but everytime I select delete gateway I get the error:

Gateways that failed deletion:
sgw-1628CC7F

I can't get to the gateway appliance as it's long gone now after the corruption. What can I do to remove this?

Thanks
Sarah

Edited by: sCBarbon on Jul 16, 2018 4:56 AM"
AWS Import Export Snowball	"Re: Gateway failed deletion
Hi

It appears the VTL is associated with tape(s) that is / are in 'RETRIEVED' status (VTL tapes which were ARCHIVED at some point in time but later retrieved). When VTL tapes are in RETRIEVED status, they are write protected and will prevent the VTL gateway from being deleted until the RETRIEVED tape is back in ARCHIVED status. One option would be to spin up a new temporary gateway, recover the 'RETRIEVED' tape to this gateway and then eject the tape using the backup application. 

https://docs.aws.amazon.com/storagegateway/latest/userguide/Main_TapesIssues-vtl.html#creating-recovery-tape-vtl

http://docs.aws.amazon.com/storagegateway/latest/userguide/managing-gateway-vtl.html#understand-tapes-status

http://docs.aws.amazon.com/storagegateway/latest/userguide/backup_netbackup-vtl.html#GettingStarted-archiving-tapes-vtl

Hope this helps. If you need further help please reach out to our AWS Dev support team and they would be able to help.

regards
Sanjaya-AWS"
AWS Import Export Snowball	"Restore to AWS using Veeam
Hi Community, 

Is it possible to restore veeam backups to AWS ec2 instances?"
AWS Import Export Snowball	"Re: Restore to AWS using Veeam
Hello,

You can use Veeam with AWS Storage Gateway VTLs to backup your on-premises enviroment to AWS. This is one solution. 
You can export the backup vmdk and use the AWS VM Import Service. Following formats are supported for the VM Import:
For Image Import: OVA, VHD, VHDX, VMDK, raw
For Instance Import: VHD, VMDK, raw

Best regards."
AWS Import Export Snowball	"Re: Restore to AWS using Veeam
Unfortunately this is not possible right now. The best place for these answers is probably the Veeam forum, especially a thread like this one

https://forums.veeam.com/post275006.html

Edited by: kylegordon on Jul 10, 2018 3:18 AM"
AWS Import Export Snowball	"Is SOCKS proxy needed to use SG with VEEAM.
Hi

I'm having a play with SG but just need to know if you have to setup SOCKS or if you can run without it.

John"
AWS Import Export Snowball	"Re: Is SOCKS proxy needed to use SG with VEEAM.
JohnMcG:

You don't need to use a SOCKS proxy if your gateway can connect to the service endpoints directly.

The network requirements for the VM are documented:
https://docs.aws.amazon.com/storagegateway/latest/userguide/Requirements.html#networks

Thanks
Paul"
AWS Import Export Snowball	"Need Explanation for Archiving Tapes
Hi,

I am trying to understand the need to archive tapes in the AWS Storage Gateway program.  I built a number of tapes, and now after about a month of usage a handful of tapes are full.  My question is do I need to archive the full tapes?  What is the issue if I just let them sit there?  My understanding is that we do not pay for tapes, only for data throughput.  If the tape is sitting in this virtual library or ""archived "" and moved to an alternate location somewhere at AWS, its still ""in the cloud"" as far as my needs are concerned, and I dont have to wait for the tape to be made available again if I need to restore.  I am not understanding some part of the process, and would appreciate clarification.

Thanks

Joe"
AWS Import Export Snowball	"Re: Need Explanation for Archiving Tapes
Hello Joe,

The need to archive the full tapes you have in your library depends on your recovery needs. e.g. Do you expect to need this data in the short-term for recovery purposes or don’t anticipate needing it anymore and would like to put it away for your regulatory or compliance needs?

It’s true that you do not pay for the tapes, but you will be billed for the amount of virtual tape storage i.e. data stored in the cloud by your tapes. I have included the link to Storage Gateway pricing below so you can see tape related pricing elements.  

https://aws.amazon.com/storagegateway/pricing/

If you don’t have an immediate restore need, by archiving the tapes, you will store data in Glacier at archival storage price. Some more information regarding archiving and retrieving tapes that you can also find at the link below:

https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html

Archiving tapes: When your backup software ejects a tape, your gateway moves the tape to the archive for long-term storage. The archive is located in the AWS Region in which you activated the gateway. Tapes in the archive are stored in the Virtual Tape Shelf, which is backed by Amazon Glacier, a low-cost storage service for data archiving and backup. For more information, see Amazon Glacier.

Retrieving tapes: You can't read archived tapes directly. To read an archived tape, you must first retrieve it to your tape gateway either by using the Storage Gateway console or by using the Storage Gateway API. A retrieved tape is available in your VTL in about three to five hours after you start retrieval.

Please let us know if you have any further questions.

Thank you,
Bhavin"
AWS Import Export Snowball	"Edit bandwidth rate limit - greyed out in AWS console
I need to edit the upload bandwidth rate limit for my storage gateway as it is causing my business cable modem to crash for some reason (working on this from another direction, our 15 public IP address assignment setup is such that I cannot just swap it out with one from another mfg). For some reason ""Edit bandwidth rate limit"" is greyed out in the AWS storage gateway console and I cannot click/select it. Any idea why this is and how to enable it? I can limit the bandwidth through my firewall but would rather put the burden for this on the AWS storage gateway itself."
AWS Import Export Snowball	"Re: Edit bandwidth rate limit - greyed out in AWS console
AnthonyG:
Edit the bandwidth limits will be greyed out in the console if your gateway is offline, you select more that one gateway, or if you have selected a file gateway. The latter gateway type doesn't currently support bandwidth limiting. It's on our roadmap for 2017.
Thanks
Paul"
AWS Import Export Snowball	"Re: Edit bandwidth rate limit - greyed out in AWS console
Is there any update? The option is still unavailable for files.

Thanks,
     -Steve"
AWS Import Export Snowball	"Setup error at Connect to Gateway
I am attempting to setup a file gateway. I have the VM loaded in ESX and the network and time configured. I can successfully ping and connect to the VM over port 80 from my computer using telnet. The Test Network Connectivity and System Resource Checks all pass.

However, when running through the setup wizard in the AWS console, I hit a block. I am on the 3rd step of the wizard, ""connect to gateway"". I put in the IP address, and it tries to send me to the local VM. However, I am getting a HTTP 500 error. I have tried this on Chrome and IE from 3 different computers. Any suggestions?"
AWS Import Export Snowball	"Re: Setup error at Connect to Gateway
BenTempe,

Can you please PM me your AWS account ID, open a support channel and PM me the number.  I've included instructions below.  Thanks.

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html#enable-support-access-on-premises"
AWS Import Export Snowball	"Restores from NDMP Source to NDMP destination  no errors, but no data
Hi,
Pretty new to this.  I am running HyperV on Windows 2008 with Symantec Netbackup 7.5.  

Backup works perfectly.  Attempting a restore appears to run without error, completes normally, but no data is in the destination.  Both the source and destination are NetApp filers, and both are using NDMP.  

I have the source filer and the HyperV Client listed as NFS Exports on the destination filer.  

Anyone have any experience with a similar setup?  I am really under the gun here.  Any help would be much appreciated.

Joe"
AWS Import Export Snowball	"Re: Restores from NDMP Source to NDMP destination  no errors, but no data
Hi joe1871

Just to make sure, I see that you mentioned the source and destination are both NetApp filers where is the AWS storage gateway used in your setup?

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Re: Restores from NDMP Source to NDMP destination  no errors, but no data
THis was user error.  I was not formatting the path for he restore correctly.  I was able to get the restore to work beautifully once I solved this issue.  Thanks.

JB"
AWS Import Export Snowball	"Backup performance and pricing of storage (tape) gateway
I am considering using AWS storage gateway for backups to Amazon S3 but I am concerned about the backup performance and the pricing.  I signed up for an AWS ""free tier"" account so I can do some basic testing and see how it would perform.  I use Veritas backup and they have a ""cloud connector"" to Amazon S3.  I know this is different than storage gateway but hear me out.

As a test, I ran a backup job of 2 GB over a 5 Mbps cable modem connection (upload speed to Internet).  It took a long time but then it eventually failed, haven’t had a chance to look into the reasons why.  Anyway, even with this small backup job, it put me at 2,000 PUT requests and 18,826 GET requests which is almost at the limit of the free tier.  That sure seems like a lot of activity for such a small job.

Then I was doing some more research and it looks like the Amazon Storage Gateway may be a good option for running backups to AWS.  It looks like there are several ways to use the Storage Gateway, the one that I’d be interested in is the Tape Gateway.  I have some questions I am hoping someone can help me answer.

With the Amazon Storage Gateway option, I read somewhere that backups are compressed and only the data that changes is sent to AWS.  I know the time it takes to backup to AWS can depend on a lot of factors but let’s just say that I was to get an Internet connection with a 50 Mbps upload speed and I was trying to do a full backup of 1 TB of data on a daily basis.  Out of that 1 TB, only 20 GB would change on a daily basis.  Let’s also say the data being backed up is mostly file server data (i.e. documents, pictures, videos, etc.) but there's a MS SQL database and also a Microsoft Exchange server database.

1.	How long should I expect for the backup to take?  
2.     How good is the compression on these backups?
3.	If the backup job is configured as a “full backup”, will the storage gateway only upload the 20 GB of changes to AWS after the initial backup?
4.	If I run full backups 3 days in a row, will I be taking up 3 TB of storage on AWS or will I only be taking up the initial 1 TB + 20 GB for Day 2 + 20 GB for Day 3 ?
5.     Would I get charged for the PUT/GET requests when using the Storage Gateway or is that included in the pricing model?

I've been working with an AWS sales rep but he's not really able to give me ANY information on what performance I can expect.  Of course this makes it very difficult to make a decision without knowing how long these backups are going to take and if this is even feasible.  I mean, what's everyone else using for an Internet connection speed and how much data are you guys backing up?"
AWS Import Export Snowball	"Re: Backup performance and pricing of storage (tape) gateway
Gilbert:

Thanks for your detailed question.

1. Tape Gateway throughput is up to 120 MB/s (see https://docs.aws.amazon.com/storagegateway/latest/userguide/resource-gateway-limits.html#performance-limits) which is more than your available upload bandwidth so that would be the rate limiting factor. Assuming you can utilize 100% of your 50 Mbps bandwidth, rough calculations are that the 1 TB full backup will take a little under 2 days, and your 20 GB incremental will take a little under 1 hour.

2. We use industry standard compression algorithms, and the amount of compression will vary depending upon the data being stored, so if you compress the data locally with a common tool then you should see similar levels of compression from SGW. For example, documents typically see high compression ratios whereas videos see much lower ratios.

3. No. If the backup job is configured as a ""full"" and the backup application writes 1 TB then SGW will upload and store 1 TB of data. SGW has no insight into whether you are running a ""full"" or ""incremental"", it is simply providing storage for the backup application.

4. If you run 3 days of full backups at 1 TB per day you will have 3 TB of data stored (less any reduction from compression).

5. No. With SGW you pay $0.01/GB written (with a $125/month cap) and per-GB for storage. See https://aws.amazon.com/storagegateway/pricing/

Thanks
Paul"
AWS Import Export Snowball	"File Share - High Availability
We have a legacy application installed on several Windows EC2 instances . This application works on shared data that is loaded to NFS.  

Since EFS does not work well with Windows EC2 instance, I have planned to use Storage Gateway ( File Share ) which gives the NFS feature by default. This simulated file share seems to work as expected.

However, it appears that the storage gateway is a single point of failure ( in case the Storage gateway EC2 instance goes down or the Region/ Availability zone is offline ). What options do I have in order to mitigate this issue ? 
        1 . Can I assign an Elastic IP to the active instance and re-assign it to the secondary instance when the primary goes offline?
        2 .  Any option of using Elastic load balancer in tandem with this?"
AWS Import Export Snowball	"Configuring Windows Server OS Volume in Storage, Volume Gateway
I have currently configured Storage volume gateway in Mumbai region. I tried creating a local disks, upload buffer and volumes all working fine. Even I tried creating volume with data and its working fine now. By Question is instead of adding D Drive or other drives in volume storage gateway how can I add  OS Drive to Volume gateway and take snapshots. So that incase of OS Failure, I will restore the whole server in the EC2 with my snapshots. Is it possible ? If yes what will be the best practice."
AWS Import Export Snowball	"EC2 File Gateway with ephemeral storage after stop/start cycle
Hello,

I'm trying to use a File Gateway for our latest project.  I'm launching an i3.4xlarge and using the NVME SSD ephemeral storage for the cache.  It sets up okay and we can create and mount the share.  If I stop and start the File Gateway instance, this is no longer the case -- I can access the instance, but the File Gateway is marked offline and I cannot mount the share.  This is not the case when using an EBS volume for the cache, instead of the ephemeral storage.

I'm aware of the requirements around preventing data loss when using ephemeral storage on a gateway, but in this instance I don't care -- I specifically care about what appears to be a loss of configuration across stop/start cycles.  Is there a solution to this please?

Thanks!"
AWS Import Export Snowball	"EC2 File Gateway with ephemeral storage after stop/start cycle
Hello,

I'm trying to use a File Gateway for our latest project.  I'm launching an i3.4xlarge and using the NVME SSD ephemeral storage for the cache.  It sets up okay and we can create and mount the share.  If I stop and start the File Gateway instance, this is no longer the case -- I can access the instance, but the File Gateway is marked offline and I cannot mount the share.  This is not the case when using an EBS volume for the cache, instead of the ephemeral storage.

I'm aware of the requirements around preventing data loss when using ephemeral storage on a gateway, but in this instance I don't care -- I specifically care about what appears to be a loss of configuration across stop/start cycles.  Is there a solution to this please?

Thanks!"
AWS Import Export Snowball	"Failed to set local disks when trying to setup a new disk for upload buffer
I have a storage gw with 2TB cache and 1Tb upload buffer (working fine)
I turned off vm and increased the size to 3tb cache and 2.7TB upload disks on the VM. 
Went back to AWS console and saw both disks: cache disk is showing the updated disk (3TB) -- good there, but I won't be able to save the upload buffer disk. I keep getting ""Failed to set local disks"" when I hit Save.

I deleted both disk and re-add them to VM (just to test it out). same issue as above.

Any suggestion?

Thanks

Edited by: risabeast on Feb 25, 2019 11:19 AM"
AWS Import Export Snowball	"Issues connecting to AWS Storage Gateway Tape Library
Hello,

We use Veeam to backup our HyperV setup.  We would like to store a copy on AWS via the AWS storage gateway.  Our gateway is running on an EC2 instance.  

We are unable to connect to the AWS storage gateway via ISCSI.  We have set up port forwarding correctly in our router, and tested by firing up another EC2 Windows 2012 r2 server with StarWind.  We are able to connect to the ISCSI on the starwind EC2 instance, but not the AWS storage gateway instance.  

We have gone through the documentation and can't seem to figure out why we cant connect to the AWS storage gateway via ISCSI to our HyperV server.  veeam tech support was not much help either, they referenced the video of how to do it, but it simply does not work.  Do we need additional ports besides 3260 between our network and AWS?

Sorry if I am restating a previous issue, as this is my first attempt with AWS storage gateway.
Thanks - Steve"
AWS Import Export Snowball	"Remote shut down (not stop) gateway appliance VM?
Hi,
Is there a way to remotely shut down the appliance virtual machine? I know I can stop the gateway but I have a peculiar VMware situation in which the host is inaccessible but the VM is running. SSH appears to be disabled on the appliance (unless there's a way to use a key file?). I'd prefer to shut down the VM rather than abruptly cycling the host.
Thanks"
AWS Import Export Snowball	"Storage Gateway Requires 3MB more ram than provided on a T3 Medium
Hi there. 

Recently noticed our storage gateway had gone offline, after further investigation the gateway will not start on the T3.Medium it is running on as it requires 3900MB of RAM and the T3 only reports 3887MB. 

Can this requirement please be adjusted 3MB down so that it can start on a T3 Medium and we can continue to use our gateway. 
	#######################################################################
	##  SYSTEM RESOURCE CHECK FAILURE: 1 ERROR FOUND!
	##  https://forums.aws.amazon.com/ Memory Check: System memory 3887 MiB (3900 MiB required)
	#######################################################################"
AWS Import Export Snowball	"Prevent deletion of files through shared folder
Hi! I'm new to AWS and Storage Gateway, and need some help here.

So, we need to set up a File Gateway. Files are going to be kept on S3 IA 2 months and then they will be transfered to Glacier.

1. We want to make sure that no user can delete files from cache disk, so they're not deleted from S3. The shared folder which Gateway presents might be write-only.

2. And if, for example, a user moves a file to a wrong folder, we want to grant permission to that user to delete or move that file to another folder inside the Gateway. If it's not possible, we can do this by AWS console.

Which is the best way can we address first point?
We can use Versioning if it's the only way.

Thanks so much for the help.

Edited by: mborb on Feb 15, 2019 7:51 AM"
AWS Import Export Snowball	"Re: Prevent deletion of files through shared folder
In order to meet your use case #1, you need extended ACL support that allows a user ""Write"" but no ""Delete"" access. Currently File Gateway only supports basic POSIX ""rwx"" permissions. 

Until File Gateway adds support for extended ACL, your only option is to use versioning on your S3 bucket."
AWS Import Export Snowball	"DNS Problems with File Storage Gateway (Join Domain)
Hi There,

I recently created an file gateway in AWS, and now trying to add SMB shares.  Instructions explain adding to the domain, but I get an error ""The gateway cannot connect to the specified domain"".  This is in a VPC that has a site to site VPN so all ports are open.  A forum post indicated that this was because my VPC uses the default DNS servers in DHCP.  I tried changing the DHCP option to include my domain name and DNS servers.  

By doing that, I lose connectivity to the gateway itself.  When SSH to the gateway instance it can't resolve the 3 endpoints anymore (due to the early DNS server change).

 client-cp.storagegateway.ca-central-1.amazonaws.com:443
https://forums.aws.amazon.com/
        proxy-app.storagegateway.ca-central-1.amazonaws.com:443
https://forums.aws.amazon.com/
        dp-1.storagegateway.ca-central-1.amazonaws.com:443

Is there any easy way around this?"
AWS Import Export Snowball	"Re: DNS Problems with File Storage Gateway (Join Domain)
It appears like you DNS server can either resolve the hosts in the internet (client-cp.storagegateway.ca-central-1.amazonaws.com) or your internal domain controller. In order for this configuration to work, your DNS server should be able to resolve both set of hostnames.

Alternately, you can use the AWS CLI to invoke JoinDomain operation by specifying the IP address of your DomainController using the parameter ""--domain-controllers"". 

https://docs.aws.amazon.com/cli/latest/reference/storagegateway/join-domain.html

Please let us know if this addresses your scenario."
AWS Import Export Snowball	"Storage Gateway - why are virtual tapes so big?
I work for a very small cost sensitive organization with only about 5 TB of data to be backed up. Currently I am using Backup Exec 20, which only natively provides access to S3 Infrequent Access. Apparently for Glacier access I need to use the Storage Gateway.

And yet it does not appear that the Storage Gateway is going to significantly reduce our costs, and may in fact cost more, depending on how many restores need to be done. I am looking very specifically at the virtual tape size, which seems to be locked at a minimum size of 100 gigabytes per tape?

So if I am reading this correctly, if we need to do a restore of a single 100 kilobyte Word document from a virtual tape in Glacier, the gateway needs to download a 100 gig tape so that Backup Exec can restore from that tape. Looking at the cost calculator, this costs US$0.25 for a slow bulk retrieval plus US$9.00 for the data transfer out.

So this one tiny restore job will whack us for $9.25 for the job. Having to do multiple occasional small restores like this can become expensive very quickly.

It would be much less expensive for us to do restores if the virtual tapes could be 1 gigabyte or 500 megabytes each. The tape software is not going to care that I need 5000 or 10000 of these small virtual tapes to do a full data backup.

With a virtual tape size of 500 meg, it now fits within the monthly free tier for transfer out, and so a few restores here and there would not cost us much at all.

So why is there minimum virtual tape size of 100 gig?  Is there some way to muck around in the Storage Gateway virtual machine configuration to allow for much smaller virtual tapes?

Edited by: Jibby on Feb 15, 2019 9:10 AM"
AWS Import Export Snowball	"Re: Storage Gateway - why are virtual tapes so big?
Jibby:

Storage Gateway pricing is based on the amount of data you store not the provisioned capacity of the tapes. So while the minimum tape size is 100 GB, you are not charged based on the size of the tapes you provision.

More specifically, we meter storage by the byte and prorate the per-GB charge. That is if you only store 1 GB of data on a tape with 100 GB of capacity you will only be charged for 1 GB of storage (or less if the data is compressible). If you archive this tape to Glacier, and then need to retrieve it you will be charged for (at most) 1 GB of retrieval.

Regards
Paul"
AWS Import Export Snowball	"Re: Storage Gateway - why are virtual tapes so big?
Assume that I am using the normal 100 GB virtual tapes. I do a 5 TB backup to tape, which produces 50 virtual tapes that are 100% used.

Later I need to restore a 100 KB file from one of these 100% used tapes. How much of the tape is downloaded to access that 100 KB to do a restore?

In the unfortunate situation that a small file happens to span two 100% used 100 GB virtual tapes, how much of those tapes needs to be downloaded to access that file?

The virtual tape software has no idea how a given backup program has stored data on the media so it seems it would have to download the entire tape regardless of the size of the data to be restored."
AWS Import Export Snowball	"Re: Storage Gateway - why are virtual tapes so big?
In the case where you need to restore 100 KB from a full 100 GB tape that you have archived, you'd to retrieve the 100 GB from the archive (Glacier) into the library (S3), and then the gateway would download whatever data your backup application requests from the tape (to the local cache). If the 100 KB file spanned multiple tapes that had been archived then you'd need to retrieve Nx 100 GB to the library (S3) and the gateway would download whatever data the backup application requested.

Archival of tapes is driven by the backup application through the policies you configure to eject or export a tape from the library. In this regard to backup application is aware if the tape can be immediately accessed or if it needs to request the tape to be put back into the library (in the real world this often equates to bringing the tape back from an offsite vault, which takes time and often has a cost). The backup application allow you to configure what remains in the library, and for most customers this is the data that they are most likely to want to restore, such as the most recent nightly or weekly backup. In the end you need to balance your business needs for ongoing cost saving in storage with the cost to restore based on the likelihood you will need to restore the data, and then configure your backup jobs to support this business objective.

In practical terms, the price difference between archived tapes and library tapes in our N.Virginia region is currently $0.019/GB-month (library: $0.023, archive: $0.004/GB-month -- https://aws.amazon.com/storagegateway/pricing/#Tape_Gateway_pricing). The cost to retrieve a tape from archive is $0.01/GB so if you needed to do this restore more than once per month it would be more cost effective to leave the tapes in the library rather than archiving them. 

Thanks
Paul"
AWS Import Export Snowball	"Active Directory access control
Happy Valentine's Day, Lovers.
I am confused about how the access control with s3 works using Active Directory and a Storage Gateway VM.
1. Select my Gateway from https://console.aws.amazon.com/storagegateway/ 
2. Select Actions > Edit SMB Settings
	-a. Click Join Domain
	-b. Domain name: blah.local
	-c. fill in Domain user/password
	-d. back on the gateway webpage receive an acknowledgement that my domain join was successful
3. Select File share > Actions > Edit share access settings
	-a. Allowed groups: Authenticated Users
4. From Windows 10 File Explorer, Navigate to \\192.999.999.999\s3bucketname
	-a. Enter domain login info into the Windows Security popup, get Access is denied. Error.
5. Back on storagegateway console website, remove Authenticated Users from Allowed groups
	-a. Navigate to \\192.999.999.999\s3bucketname and can access everything just fine.
What am I doing wrong?

EDIT: So, I'm an idiot.  Apparently, Authenticated Users isn't a true security group.  I used Domain Users instead.

Edited by: matthaws on Feb 14, 2019 1:28 PM"
AWS Import Export Snowball	"AWS File Storage Gateway offline
Hello,
we are testing AWS File storage Gateway and we've faced same problem that was already mentioned here : AWS SG is a software appliance on VMware ESX, completely accessible from LAN (by ping), with connectivity tests to the cloud completed successfully, but Amazon web console recently started to show it as offline.
It is not possible to connect with http from local machine either, while there is absolutely no firewall rules preventing it.
I saw that AWS specialists were providing support is such situation, but for us the most important is to prevent such situation in the future, when we will use storage gateway in production. 
Please advise what steps could be taken to either prevent this ""offline"" or to resolve this situation if already happened.
Thank you in advance!"
AWS Import Export Snowball	"Re: AWS File Storage Gateway offline
It seems that AWS does not have clear solution to these ""gateway offline"" problems, that re-appear throughout the forum. Can this solution to be implemented in production or it is still a ""proof of concept"" product. 
Of course all data will still be in S3 once gateway is not available and it will be possible to re-create a gateway from the scratch, but that does not look like reliable solution for production. 

as of now - I still have a gateway as offline and while I have created another storage gateway , that runs (so far) but inability to resolve this problem is very disappointing."
AWS Import Export Snowball	"Re: AWS File Storage Gateway offline
VasilyG, 

Are you able to access the files over the File shares associated with Offline gateway? Can you please PM me your Storage Gateway ID, Region?

Once you provide the Storage Gateway ID we will be able to provide you further guidance.

Shashi"
AWS Import Export Snowball	"Re: AWS File Storage Gateway offline
Hello Shashi,

I have sent PM a week ago. But just in case - here is the brief answer:  - no, gateway is still offline, and its share is not accessible in local network.
If I try to access the server locally via http (as if to re-activate) - it refuses the connection.
While from the gateway local console connectivity tests went through ok."
AWS Import Export Snowball	"Re: AWS File Storage Gateway offline
VasilyG,

Can you please check your PM and respond to my request?

Thanks,
Shashi"
AWS Import Export Snowball	"Gateway VTL v/s Native integration
Which option is better to use while using NetBackup Appliance backup: AWS Gateway VTL or Integration of NetBackup with Cloud Storage using NetBackup API?

What is the upload request size recommendations and can they be optimised for better cost & performance?"
AWS Import Export Snowball	"List of IP addresses the AWS Storage gateway communicates with at Amazon?
Hi, 

I want to route the traffic from our AWS Storage Gateway across a secondary circuit.   

Where can I find a complete list of IP's that the SG will use to transmit data?

Thanks!"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
Hi vantagedev2,

Thanks for your question.

AWS storage gateway communicates with the  storage gateway service using specific endpoint DNS names.  These endpoint DNS names are built into the Gateway and do not need to be configured.

Can you clarify your use case/scenario that needs this configuration?

Thanks,

Jun"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
Sure, 

I have two internet circuits.   

One is used as a primary, the other sits idle as a backup.

Right now, my AWS Storage Gateway traffic is going out via my primary circuit.   

If I knew the foreign endpoint IP's (at Amazon) I would be able to tell my router to direct any traffic destined for Amazon to go out via my backup circuit.   

Allowing the traffic to go out via the primary circuit is not desirable because the volume we have to send out combined with the amount of traffic we experience this time of year will oversubscribe the circuit and interfere with operations.  

The ability to take advantage of a second, dormant circuit for back-end operations like this is probably not uncommon.  

Thanks, 

--Freddy"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
vantagedev2,

I've sent you a PM with some more specific questions about your use case.

Regards,

Ian"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
I too am looking for a block of IP address from which I can whitelist. We are evaluating storage gateway as a secondary offsite backup option. The network which the virtual appliance runs on does not allow access ingress or egress to the public internet. I am only able to access amazon services if I can whitelist a block of IPs. 

Let me know what the best course of action is.

Thanks,
Nick"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
Hello nickethier,

I have sent you a PM that will be of help to you.

Thanks.
Santhosh"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
Hello, would it be possible for you to PM me the same information? We're going to be testing this same scenario very shortly."
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
Hi jweblinc,

You can now find the list of Storage Gateway endpoints in our documentation:

http://docs.aws.amazon.com/storagegateway/latest/userguide/allow-firewall-gateway-access.html

Regards,

Ian"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
Updated url:
http://docs.aws.amazon.com/storagegateway/latest/userguide/Requirements.html#networks"
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
Can you also send me the info you have sent as a pm to other folks in this forum?  I am working with a client that had direct connect and the storage gateway running on premise.  They setup a file gateway.  We want to configure the setup to allow the traffic for the storage gateway to go out over the direct connect. I also need to know a way to verify that the traffic is indeed going out over that direct connect.  They keep asking me for the ip address or addresses to communicate with S3.  Their thought was to open that ip or ip range on their routers.   Also white list them."
AWS Import Export Snowball	"Re: List of IP addresses the AWS Storage gateway communicates with at Amazon?
rkcloud:

The following pages in our documentation should help you:
https://docs.aws.amazon.com/storagegateway/latest/userguide/using-dx.html
https://docs.aws.amazon.com/storagegateway/latest/userguide/Resource_Ports.html

Thanks
Paul"
AWS Import Export Snowball	"Storage Gateway is either currently unreachable
After rebooting my virtual appliance using vCenter I have lost connectivity to my storage gateway. I have confirmed that the VM has internet access via the console sguser. When I select the gateway, the following message is displayed. 

""Your Storage Gateway is either currently unreachable or is in the process of shutting down or restarting. Re-click on your gateway in the left pane to retry your connection. If you continue to experience connectivity issues, please verify that your local gateway host has Internet access. If you have initiated a shut down or restart of your gateway, it may take a few minutes before your gateway is reachable.

Alternatively, if you no longer wish to use this gateway, you can click below to delete it.""

thanks,
Jim"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hi Jim,
Can you provide me with your AWS Account ID and which AWS Region you're using (you can private message me)?

Thanks,
Arun"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Arun, I sent you a PM with my account number. Gateway is located in US East."
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hi Jim,
Based on what we're seeing, your gateway appears to be reachable as of a few hours ago.  Can you confirm?  Did you make any changes to your hardware in the last few hours?

Thanks,
Arun"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
We are experiencing the same issue - what steps should we take to resolve connectivity?"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hi  brentg73,
Can you provide me with your AWS Account ID and which AWS Region you're using (you can private message me)?

Thanks,
Ankur"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Ankur,

I sent you a PM with the requested information"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
We are experiencing the same issue - what steps should we take to resolve connectivity?"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hi stu-yc,

Can you please PM me with your AWS Account ID and the region in which you've activated your gateway?

Also, have you tried running the Network Connectivity test from the local console?

Test Your AWS Storage Gateway Connection to the Internet

Have you configured a SOCKS proxy or static IP for your environment?

Thanks,
Ian"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
We have the same issue.  SG has internet connectivity, no SOCKS configuration."
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hi vmsitteam
We are looking into your issue and will PM you if we need further information.

thanks"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
I've PMed you for more details regarding your gateway."
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Same here"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
I'm having this same issue.  Is someone available to assist?

https://forums.aws.amazon.com/thread.jspa?messageID=606955

What is the process for getting support from Amazon when this happens?  Do I have to pay?"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
whittakerj:

Did you run the network connectivity test in the local console? See http://docs.aws.amazon.com/storagegateway/latest/userguide/EC2_MaintenanceTestGatewayConnectivity.html. What were the results?

Basic support is included with all accounts, and paid plans are also available. See https://aws.amazon.com/premiumsupport/ for the options and process for obtaining support.

Regards
Paul"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Yes I ran the test results attached."
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
whittakerj:

That looks okay. Please PM me your AWS Account ID and Gateway ID (if known) and we will take a look from the AWS side.

Thanks,
Paul"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
PaulR@AWS I sent you my information I'm still showing offline. How can I expedite this?"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
I'm having the same issue.  I'm trying to set up a storage gateway VTL.  Activation completes successfully, network connectivity seems fine (device responds to ICMP on the LAN), network interface is configured correctly, but always get stuck with ""Storage Gateway Not Connected"".

I just got a connectivity test to complete and, as expected it says ""Connectivity test failed.""

One interesting note (or not) is that if I go into the console and issue the open-support-channel command I end up with ""Successfully connect sic to Storage Gateway Support Server"" and ""Press q to end support session"".

When I ""press q"" I get the ""Connection to 54.201.223.107 closed."" message.  So this at least makes it seem like the Storage Gateway VM is able to establish a connection to a host on the internet.

The other thing that's weird is I've been checking the firewall's traffic logs and I see almost no traffic at all from the Storage Gateway.  Even when I tell the SG to run a connectivity test it doesn't seem to send any packets.

Just a few TCP packets here and there to port 443 on an amazon public IP."
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Not very useful from a technical standpoint, but my issue was resolved after I discovered a cancelled credit card was set as default in my payment methods.  Payment methods updated, outstanding invoice paid, connectivity established about 45 minutes later."
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hello,

I am experiencing the same issue trying to set up my first Storage Gateway.  When I try to activate it I receive ""Your storage gateway is activated but is currently unreachable"".  I verified network connectivity through the console and each test passed.  Anyone know the resolution to this?

Thank you,

Greg"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
gregspryjb:

Can you give me a few more details? Which region are you activating in? Where are you hosting your gateway (VMware, HyperV, EC2)? If on-premises, is there a firewall or socsk server between the VM and AWS? Did you encounter any problems with the activation process itself? Does your new gateway show up in the AWS web Console? Has this web Console ever shown the gateway as available? When you run Network Connectivity in the local console what results do you get?

Thanks
Paul"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hello Paul,

Thank you for the reply.  I was able to activate the gateway this morning after I deleted it from the AWS Storage Gateway page.

Thanks!"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
Hello, 

I have exactly the same problem here. The connection test is successful, but my gateways still appears offline in the console and I cannot mount any drive. I've tried to reboot the gateway VM but the gateway still appears offline. The gateway is running in Ireland region. 

Thanks

Edited by: rphilipsen on Mar 29, 2017 12:52 AM"
AWS Import Export Snowball	"Re: Storage Gateway is either currently unreachable
rphilipsen,

Please PM me your AWS account ID and the Gateway ID for the gateway in question.  Thanks."
AWS Import Export Snowball	"Upload files to S3 but delete them on-premise
Hi!

We need to upload files to an S3 bucket and delete them on-premise after a few days. We need the files to stay on S3, but get rid of them on-premises.

We clearly aren't much familiar with S3.

Can we acomplishe that with Storage Gateway?
Should we upload the files using AWS CLI and then delete them on-premise instead?

Any help will be very appreciated."
AWS Import Export Snowball	"Re: Upload files to S3 but delete them on-premise
mborb:

Using File Gateway will implicitly delete your files when local storage is needed for new files.  There's no explicit delete needed in this cached model. That is, if you write files to File Gateway through NFS or SMB the gateway will cache on the local disk and upload them to S3 as objects. Data will be deleted from the cache when space is needed to store more recently read/written files, so the amount of data that remains on-premises is determined by the size of the cache disk attached to the gateway.

Thanks
Paul"
AWS Import Export Snowball	"Basic up/down monitoring of SG?
Our Volume SG had an update applied during it's maintenance window over this past weekend, however, when I came in this morning, the AWS console showed that it wasn't running.  I logged into vSphere, and saw some messages about it being out of memory, so I allocated more RAM to it, and restarted it.  After doing that, it finally came back in the console as Running, and everything was working again.  We do basic ping checks on the SG appliance (which was fine), however, the AWS console showed it not running.  How do we get alerts if the SG isn't running?  I see I can monitor all sorts of metrics, but I just want to start out with basic up/down alerts if it's not running."
AWS Import Export Snowball	"Re: Basic up/down monitoring of SG?
Hi brucegarlock,

Thanks for your feedback. We don't currently provide an alert on gateway status. I have noted your suggestion and we will consider it as we plan future functionality and features. If you can please also PM me your account and gateway information along with the day/time you had the problem, it will be great.

Regards,
Bhavin
AWS Storage Gateway"
AWS Import Export Snowball	"aws backup product / storage gateway stored volume?
I'm excited about the life cycle management of the AWS backup product.

I'm curious about how it integrates with the Storage Gateway Stored volume,
as the stored volume is already doing a forced snapshot every day (or period you set),  if you use aws backup on
a stored volume, do you then have 2 backup streams ***  that you'd have to pay for? 

Steve"
AWS Import Export Snowball	"Re: aws backup product / storage gateway stored volume?
Hi Steve,

Thanks for the feedback. AWS Backup can backup both cached and stored volumes of Volume Gateway. The default stored volume snapshot scheme remains in place. So if you create an AWS backup policy that backs up stored volumes, that will operate independently of Volume Gateway schedule. You will be able to see the snapshots created by AWS Backup in the EBS console and the snapshots created by AWS Backup of volume gateway volumes can only be managed from AWS Backup (and not from EBS console). 

Hope this helps. If you have any other questions please let me know.

Regards,
Bhavin"
AWS Import Export Snowball	"How to manage cache and buffer cool-down?
I'd like to know what the default settings are for the upload buffer and cache on the SG, and how to wipe them once a job is complete. I was doing some testing today with sequential backup-to-tape operations and the buffer/cache both continued to climb after each job was complete. I noticed rebooting cleared the buffer/cache but that seems excessive.

Left alone, how soon will the cache empty itself and where are the knobs to turn on this?"
AWS Import Export Snowball	"Do files cache locally before uploading to S3
Hi,
I'm just looking to see if anyone can confirm / point me to some documentation relating the way files are uploaded to S3 via Storage Gateway. Essentially I am trying to workout if I have a network problem or misunderstood the way storage gateway worked.

I have an AWS Storage Gateway running via ESXi on a HP microserver. I have a physical Windows 10 machine mapping to a file share that points to an S3 bucket.

What I was expecting when I copy a large file from the Win10 to the mapped drive, it would copy across at LAN speeds to the VM, and then StorageGateway would do its magic in slow time the S3 bucket. What I am seeing is it taking approx 20mins for a 300MB file to copy across. I originally thought it was a LAN issue, but I am not starting to think that it is uploading straight to S3, and only reads are via the cache...

Can anyone confirm that this is whats happening and that's the way AWS has designed storage gateway to work. If so I may need to rethink my use case and solution.

Thanks"
AWS Import Export Snowball	"Re: Do files cache locally before uploading to S3
Bradc,

Your understanding of the gateway behavior is correct. The files that are written to the gateway are saved locally in the cache disk and they are uploaded asynchronously to S3. 

Since you are seeing poor write performance, we suspect the issue to be with the network throughput from the Windows client and the Storage Gateway or the IO throughput of the Cache disk. Can you please look at the the network & IO throughput metrics in ESXi?

Please PM me your gateway ID and the AWS Region, we will look at your gateway metrics and share additional feedback.

Thanks,
Shashi"
AWS Import Export Snowball	"Multi-Facility Usage of Storage Gateway?
I am sure this question has been asked.  I promise I searched for it first!

Is it possible and practical to leverage Storage Gateway services to implement a multi-facility file share?  We maintain large amounts of critical project data at local facilities in various states.  It is becoming more important for the various facilities to have access to this data as a global resource.  Secure storage (Amazon) is also important.  Having a cached storage gateway at Amazon and then deploying the local cached storage at multiple locations such that the data could be accessed and updated from anywhere would be fabulous.  However, this may be too much to hope for.

What usage scenarios are possible that are close to this desire?

Thanks!

Keith"
AWS Import Export Snowball	"Re: Multi-Facility Usage of Storage Gateway?
Hi Keith,
Currently, any data shared between multiple gateways must be done via snapshots.  You can't have two gateways concurrently access the same storage volume.

But would be great to learn more about your use case.  I'll PM you.  

Thanks,
Arun"
AWS Import Export Snowball	"Re: Multi-Facility Usage of Storage Gateway?
I don't know where this PM happens...  but our situation is as follows (and doesn't seem particularly unique):

We have a local ""projects folder hierarchy"" that is organized by project managers.  Projects are organized by ID number, and need to be retained forever.  However, only the most recent year or so requires  low latency access.  Since the data includes large CAD files and such, though, ""local"" access is needed so that the files can be opened/examined/modified using various software tools.  We have developed S3-Based facilities to automatically move suitable projects (those that database information indicates are not likely to be needed anymore) to S3 and to delete them from the local store.  The project managers can restore these archived projects easily if they are needed.

The above sounds like an excellent problem for a Cached Storage Gateway solution.

In addition to this, company expansion introduces the need to establish similar large project folder hierarchies at other locations, some in other states and a few in other countries.  The nature of the data still suggests the need for there to be a local cache so that these remote project managers need quick access, too.  Furthermore, there are cases where sales happens in one region but project management happens in another region - thus, the project folder hierarchies really need to be available in multiple places (obviously there is some ACL aspect of this, but...).

Summary:  The ideal solution would be an Amazon-primary storage with multiple cached instances in multiple locations that would all inter-sync.  But, we know that this isn't actually what the Storage Gateway is designed for.  And that is understandable.

Perhaps a cached storage gateway for each geographic location combined with some project-targeted to synchronize selected data between regions would work.  In this case, I have probably wandered outside this forum's scope (with apologies).  But if anyone has solved this problem or has some enlightened idea (particularly using storage volumes) I would love to hear about it!

Keith"
AWS Import Export Snowball	"Re: Multi-Facility Usage of Storage Gateway?
Did you ever get a reply? I would love to know the solution. I have a similar use case."
AWS Import Export Snowball	"Re: Multi-Facility Usage of Storage Gateway?
The problem isn't really an AWS one. Given they are presenting iSCSI LUNs rather than an actual file share (NFS / CIFS / etc), the problem is how would you even have two systems writing to the same NTFS (or whatever) volume at the same time?

There are solutions to this like HP PolyServe, Redhat GFS2 or something like that to get to the point where you're not going to corrupt the file system by having two servers access the same disk, but even then you haven't really solved the problem of file locking, etc.

The only thing I can think of is if you have several completely separate AWS Cloud Gateways, and then link them at the front end (on the server) using DFS/FRS, GlusterFS or some other sort of Distributed File System. You'd have a lot of duplicated data in AWS is the downside.

It may be worth a look at Nasuni. I know they have the concept of Read-Only replicas of shares which can be presented in multiple locations. I think it's expensive however (even though they still use AWS S3 at the back-end)."
AWS Import Export Snowball	"Re: Multi-Facility Usage of Storage Gateway?
Multi-master would also be a massive enhancement for our usecase as well."
AWS Import Export Snowball	"AWS Volume Gateway High Availability Clustering and Local Logs
We love the concept/idea of this product. We have a scenario where our local NAS/Archived Storage is becoming stressed on space even with a local Archicing/Stubbing solution in place, about 120TB of data. We have tested out the Volume Gateway in our environment and it works well from our testing. The ability to send files to an ever increasing amount of space in S3 while caching highly used/recent files locally is great.

The problem is for an Enterprise Grade solution, we see a couple flaws that are preventing us from pushing forward with this solution:

High Availability - With the SG acting as the frontend for clients/servers to deliver files to, what happens if the SG encounters any service disrupting issue? This could be crippling to an environment relying on those files for their production apps to function. I know with VMware HA this can help a little but doesn't cover everything. Having a second Volume SG in sync/replicating/load-balancing in a cluster is the ideal solution to this.
Local Logs on the SG - We think it would be very helpful to be able to see login to the console as ""sguser"" and see read-only log files related to SG server/activity. If/when there is an issue, at least you would be able to login locally and parse the logs to try and diagnose what may possibly be at fault vs waiting for Amazon support in a critical production down scenario. This way, it doesn't just appear as a magical black box sitting out there working wonderfully to utilize S3 storage.

To my points above, does Amazon have any plans to roll out High Availability (HA) clustering of the Volume Storage Gateways? Also, will we ever be able to see read-only logs related to the SG server/activity when logged in to console as ""sguser""?

Thanks,
Matt"
AWS Import Export Snowball	"Re: AWS Volume Gateway High Availability Clustering and Local Logs
Hello Matt,

Thank you for your input regarding these two features.  At this time we do not have a timeline on these items, but your feedback will help us to prioritize our future roadmap. I have PM'd you to discuss in more detail.

Thanks,
Peter"
AWS Import Export Snowball	"Re: AWS Volume Gateway High Availability Clustering and Local Logs
Hello we need also High Availability (HA) clustering of the Volume Storage Gateways.
Any solutions?

regards
Richie"
AWS Import Export Snowball	"Re: AWS Volume Gateway High Availability Clustering and Local Logs
Adpressi wrote:
Hello we need also High Availability (HA) clustering of the Volume Storage Gateways.
Any solutions?

regards
Richie

Bump.  Same interest."
AWS Import Export Snowball	"Re: AWS Volume Gateway High Availability Clustering and Local Logs
We are currently using SG for our noncritical workloads, but would also require this as part of our critical rollouts.

Would love to see this come out soon."
AWS Import Export Snowball	"Adding Storage Gateway to AD Domain via CLI
Hi

We have a problem with Storage Gateway joining the domain via CLI (version: aws-cli/1.16.93 and aws-cli/1.16.90).
When we run the following command ""$ aws storagegateway join-domain...."", it returns:

An error occurred (InvalidGatewayRequestException) when calling the JoinDomain operation: The gateway cannot connect to the specified domain.

This is the command we used:
aws storagegateway join-domain --gateway-arn arn:aws:storagegateway:<region>:<account-id>:gateway/<gateway-id> --domain-name <our-domainname>  --organizational-unit ""OU=<our-ou-name>,DC=<our-domain>,DC=COM --domain-controllers <our-dc-ip> --user-name <username> --password <password>

Could someone help us debug why we are unable to join the Domain?

We already checked the following:
-Specified DC is reachable and necessary ports are opened
-All traffic inbound/outbound allowed between Storage Gateway and specified DC
-Storage gateway can resolve Domain Name
-DHCP Options Sets specify correct DC and domainname in search list
-The user and/or OU has right to join the domain
-The user and password is correct
-Other windows instance which is in same subnet and same security group can join the domain

Added additional reachable DCs to the domain-controllers list, and the problem remains


Thank you,"
AWS Import Export Snowball	"Re: Adding Storage Gateway to AD Domain via CLI
Please check the logs on your Domain Controller/AD for any errors? Most probably the error is being returned by your DC/AD. You can also capture the network packets while you are executing the ""join-domain"" operation to confirm that the error is returned by the DC/AD.

Can you please PM me your Storage Gateway ID & the Region?"
AWS Import Export Snowball	"Re: Adding Storage Gateway to AD Domain via CLI
Hi shashi-AWS,

Thank you for your advise.
After discussion with our DC/AD admins, we found error log in event viewer.
 -> Event Id:16642, Directory-Service-SAM, The account-identifier allocator was unable to assign a new identifier.

It was because DC in AWS does not have connectivity with FSMO role holder DC. After we switched site2site VPN to other site which has DC with FSMO role, successfully storage gateway could join the domain with same command I posted initially.

Again, thank you for your help."
AWS Import Export Snowball	"Join Storage Gateway to Domain by IP Address?
I've searched all over, but can't find mention of this... Can I specify the IPs of my Active Directory Domain Controllers in the 'domain joining' action when creating a Storage Gateway? This is what we do when domain-joining Windows instances (we do this with some Powershell UserData).

As it stands, the instance has to be able to lookup the AD domain name (so if your AD is called ""ad.example.com"", the instance tries to look up ""_ldap._tcp.dc._msdcs.ad.example.com""). If the DNS lookup fails, then it can't join the domain. To do the lookup, it has to use the correct nameservers.

Is there any way (or any plans to add the feature?) to allow me to say ""my AD servers are at IPs X and Y""? This would greatly simplify the infrastructure required around the Gateway to get it to join the domain.

As it stands, the only solution I have found is to set a DHCP Options set on the VPC that specifies the two DCs as the the nameservers, configure the VPC to use this set, then create the Gateway and then optionally put the DHCP Options back the way they were.

Since I don't want to have to make VPC changes just to instantiate an instance, I'm now looking at ways we can keep the DHCP Options pointing to the DCs - but this requires lots of Route53 associations with the remote account that hosts the Domain. Again, not an attractive option.

Any ways around these problems would be most welcome!"
AWS Import Export Snowball	"Re: Join Storage Gateway to Domain by IP Address?
As you pointed out, currently the only solution is to control it through the ""DHCP Options"". 

We are making enhancements to the Storage Gateway JoinDomain API (https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_JoinDomain.html) to take the AD domain name/IP address as an optional parameter. This enhancement will be available mid Jan, 2019."
AWS Import Export Snowball	"Re: Join Storage Gateway to Domain by IP Address?
Hi,

Is there the date that enhancements to the Storage Gateway JoinDomain API will be available?
I have problem when joining the domain to create smb share on Storage Gateway. The joinig domain widzard returns time out error. 
	> The specified request timed out. (Request ID: xxxxxx)
I assume this is because we have lots of DCs for our remote offices, and only few are accessible from VPC because of design of site2site VPN, and gets time out....
If enhancements includes feature allowing us to select DC for joining domain widzard, my problem may be solved.

Thank you,"
AWS Import Export Snowball	"Re: Join Storage Gateway to Domain by IP Address?
The updated version of the JoinDomain API (https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_JoinDomain.html) has two new optional parameters ""DomainControllers"", ""OrganizationalUnit"". Currently these two new parameters are only available through the AWS CLI (https://docs.aws.amazon.com/cli/latest/reference/storagegateway/join-domain.html). 

In your case, you can specify the IP address or Domain name of the DomainController using the parameter ""--domain-controllers""."
AWS Import Export Snowball	"Re: Join Storage Gateway to Domain by IP Address?
Thank you for your quick reply, shashi-AWS.
After updating awscli, I can see new optional parameters you mentioned.
However, using new parameter we still get error. As our issue is different from this thread topic, I'll post new thread. Thank you for your support!"
AWS Import Export Snowball	"chown not working on NFS file share
Hi,

I have successfully created a NFS file share with a storage gateway using a S3 bucket.
However, when creating files chown says: Operation not permitted. 
Everything is in default configuration (no permission policy added).
On the other side, chmod is working perfectly fine.

Thanks in advance!
Diego"
AWS Import Export Snowball	"Re: chown not working on NFS file share
Diego:

By default file shares are created with ""root squash"" enabled which prevents changing the ownership of files by the root user. You will need to turn off root squash in the file share settings to allow this.

https://docs.aws.amazon.com/storagegateway/latest/userguide/managing-gateway-file.html#edit-nfs-client

Thanks
Paul"
AWS Import Export Snowball	"Migrate Storage Gateway File Share to SMB
Hello,
For some time I've been running at NFS file share off my Storage Gateway, however I would like to use SMB instead as my environment is mostly Windows.

Is there a good way to migrate between NFS to SMB?
I did some testing changing an existing NFS file share to SMB, however I encounter some permissions issues, I believe this is because it stores permissions on file  as metadata in S3.

Thanks"
AWS Import Export Snowball	"Re: Migrate Storage Gateway File Share to SMB
Daryl, 

In order to understand your use case, can you please answer the following questions?

1. Are all your NFS & SMB clients running Windows OS? If yes, what is the Windows OS version? 
2. Are you using SMB guest or Active Directory(AD) for SMB authentication? 
3. Are you using the same Windows user id while accessing the files over NFS & SMB file shares? 
4. On your NFS file share on the SGW, what did you use for ""Squash level""?
5. Can you please describe in detail what permissions issues you encountered?

Thanks,
Shashi"
AWS Import Export Snowball	"Re: Migrate Storage Gateway File Share to SMB
Apologies for taking so long to reply.

1. Are all your NFS & SMB clients running Windows OS? If yes, what is the Windows OS version?
Most our Windows 10, we also have a few Macbooks. 
2. Are you using SMB guest or Active Directory(AD) for SMB authentication?
AD 
3. Are you using the same Windows user id while accessing the files over NFS & SMB file shares? 
Yes I believe we are usng the same Windows id
4. On your NFS file share on the SGW, what did you use for ""Squash level""?
All squash
5. Can you please describe in detail what permissions issues you encountered?
If I try to create new files I'll get a ""You need permissios to perform this action"" in Windows

Thanks"
AWS Import Export Snowball	"Adding Storage Gateway to AD Domain
Trying to add Storage Gateway to AD using CLI and the command is failing with the following error:"" An error occurred (InvalidGatewayRequestException) when calling the JoinDomain operation: Authentication failed.""

I suspect that the problem is with referencing the OU in the cli command, since the system user that is being used in the command has only permission to add machines to that specific OU.

The command executed is below:
aws storagegateway join-domain --gateway-arn some_storage_gateway_arn --domain-name prod.company.com --organizational-unit ""OU=Servers,OU=Production,DC=prod,DC=company,DC=com"" --domain-controllers some_domain_controller --user-name some_user --password some_password --region us-west-2

Is that the correct syntax for the --organizational-unit option?

Thanks!"
AWS Import Export Snowball	"Re: Adding Storage Gateway to AD Domain
The syntax of the value for OU parameter is fine. If you haven't tried already, can you please try without the double quote ("") for the ""organizational-unit"" parameter?

Please check the logs on your Domain Controller/AD for any errors? Most probably the error is being returned by your DC/AD. You can also capture the network packets while you are executing the ""join-domain"" operation to confirm that the error is returned by the DC/AD.

Can you please PM me your Storage Gateway ID & the Region?"
AWS Import Export Snowball	"Re: Adding Storage Gateway to AD Domain
Thanks, Shashi,

Just PM'ed you with the details of the Storage Gateway.
I also, as you mentioned, tried to remove the quotation marks but getting exactly the same error message...
Will be checking the AD logs..."
AWS Import Export Snowball	"Re: Adding Storage Gateway to AD Domain
Alex,

I responded to your PM with the details of the error message we observed in the gateway logs. Please check your AD/DC logs for additional clues. 

Here are few possibilities.

Time synchronization between the Storage Gateway and Domain Controller. 
Domain Name Resolution (DNS) failure resolving the Domain Name.
The user and/or OU does not have right to join the domain
The user name and or password is incorrect

Please keep us posted with your findings."
AWS Import Export Snowball	"Issue with Joining Domain in Storage Gateway
Hi,

I have active directory services configured in a ec2 instance. I want to join the domain in AWS Storage Gateway but it is giving me an error ""The specified endpoint was not found. (Request ID: 0c0f2d65-130b-11e9-b50a-791fcd2811c7)"". DHCP option set has also been changed to my active directory server. Can someone help me?"
AWS Import Export Snowball	"Re: Issue with Joining Domain in Storage Gateway
The error message ""The specified endpoint was not found"" is displayed when Storage Gateway is not able to resolve/identify the Domain associated with the specified DomainName. 

If you have configured DHCP option set with your AD IP address, it should have worked. 

Can you please PM me your GatewayID and AWS region name?

Thanks,
Shashi"
AWS Import Export Snowball	"Re: Issue with Joining Domain in Storage Gateway
Dear Shashi,

I was able to resolve that issue. I am not able to login to SMB path using the AD credentials. Username password prompt is showing and when i enter the correct username and password it gets prompted again. Need you help on it."
AWS Import Export Snowball	"Re: Issue with Joining Domain in Storage Gateway
It is good to know that you resolved the original issue. How did you resolve it? 

Can you please share the command you are using to mount the drive? Are you including the ""DomainName"" as part of the user name (e.g: DomainName\UserName)?"
AWS Import Export Snowball	"Re: Issue with Joining Domain in Storage Gateway
After searching through a lot of AWS blogs and forums. All were suggesting to change the DHCP options set which helped me out. 

Now i am having trouble accessing the smb path. 

yes i am entering the domain login with domainname\username and password still no luck. Can you help me out on this?"
AWS Import Export Snowball	"How to validate gateway is listening on port 445
Ive set up a couple of new SMB file gateways, but one of them is not responding on port 445.  Cant telnet to that port and network access looks fine.  Is there a way to confirm that the gateway is in fact listening and responding on 445?"
AWS Import Export Snowball	"Re: How to validate gateway is listening on port 445
File Gateway will start listening on the port 445 only after you created the first SMB file share on the Gateway. Did you create the SMB file share on the gateway where the port 445 is not open?

By the way, why are checking whether the port 445 is open on the File Gateway? Are you troubleshooting a SMB issue?"
AWS Import Export Snowball	"Re: How to validate gateway is listening on port 445
Yes, I created an SMB share on the gateway and cant access it.  I cant telnet to 445 and the network access should be fine.  I just want to validate that the gateway is truly listening on 445.  There is no option for that when logging into the gateway via SSH"
AWS Import Export Snowball	"Re: How to validate gateway is listening on port 445
What is the problem you are troubleshooting? Are you not able to mount the SMB file share from your windows host?

You mentioned that you have multiple SMB gateways. What is unique about the gateway where you noticed this behavior?

Can you please PM me your Storage Gateway ID and the region where you are experiencing this issue?"
AWS Import Export Snowball	"boto3 refresh_cache not accepting FolderList or Recursive kwargs
I am trying to provide a FolderList to refresh_cache on a file share. When I don't provide either FolderList or Recursive the call goes through just fine. But if I provide either kwarg I get:

botocore.exceptions.ParamValidationError: Parameter validation failed:
Unknown parameter in input: ""Recursive"", must be one of: FileShareARN
Unknown parameter in input: ""FolderList"", must be one of: FileShareARN

Example call is:
client.refresh_cache(FileShareARN=filesharearn, FolderList=[""/""], Recursive=False)


Where filesharearn is the ARN of the file share.

Is there an example for adding these kwargs so that I can see what I am doing wrong? 

Any help would be appreciated. Thank you

https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/storagegateway.html#StorageGateway.Client.refresh_cache"
AWS Import Export Snowball	"Re: boto3 refresh_cache not accepting FolderList or Recursive kwargs
The support for the last two parameters are recently added to the RefreshCache API. The errors you are seeing indicates that the version of the SDK you are using does not understand these parameters. Please update your SDK version to the latest."
AWS Import Export Snowball	"AWS gateway, can it transfer local NFS volumes to AWS cloud on block level?
AWS can transfer NFS files, or iSCSI volume to AWS S3/EBS, as I know of. Can it transfer local NFS volumes block by block to AWS S3/EBS? If no, what products/services can do that?

Is GW free of charge?

Thanks!"
AWS Import Export Snowball	"Re: AWS gateway, can it transfer local NFS volumes to AWS cloud on block level?
awsmagic8,

Here are the answers to your questions:

1) AWS can transfer NFS files, or iSCSI volume to AWS S3/EBS, as I know of. Can it transfer local NFS volumes block by block to AWS S3/EBS? If no, what products/services can do that?

If you're trying to transfer files hosted in an NFS server to AWS, the best service offering we have is AWS DataSync which can copy your data from your NFS server to S3 or EFS.  Details can be found at the link below.  When you're copying files via NFS you're using a file level protocol (rather than a block level protocol like iSCSI) and can't do a block by block copy regardless of what product/service you use.  AWS DataSync does perform an integrity verification after the sync operation to ensure that your data was copied correctly, if that is your concern.

https://aws.amazon.com/datasync/

If your goal is to get your data into EBS then your best choice is a Volume gateway where you copy your data to a volume of 16TB or less, take a snapshot and use that snapshot to create an EBS volume.  I can provide more details on this if you want.

2) Is GW free of charge?

I've included a link below with details on DataSync pricing.  Please let us know if you need more information.  Thanks.

https://aws.amazon.com/datasync/pricing/

Thanks!"
AWS Import Export Snowball	"SMB File Share password Incorrect
Hello,

I am trying to mount SMB file share in my windows Machine. It is setup as Guest access.

When I am trying to mount using net use command, it gives the following error: 

System error 86 has occurred.
 
The specified network password is not correct.


I know the password is correct, but its still showing the error

Edited by: USPLAdmin on Oct 8, 2018 9:55 AM"
AWS Import Export Snowball	"Re: SMB File Share password Incorrect
USPLAdmin,

Can you please try setting your Guest password again using ""Edit SMB Settings"" menu? Please refer to the following instruction. After saving your Guest password again, please let us know if you are able to mount the SMB file share again.

https://docs.aws.amazon.com/storagegateway/latest/userguide/managing-gateway-file.html#enable-ad-settings"
AWS Import Export Snowball	"Re: SMB File Share password Incorrect
Hello Shashi,

I tried resetting the password. But still its giving the same error."
AWS Import Export Snowball	"Re: SMB File Share password Incorrect
USPLAdmin,

Are you using the ""net use"" command as specified in the AWS console. Are you specifying the Gateway ID before ""smbguest"" ? 

Here is snippet from the documentation https://docs.aws.amazon.com/storagegateway/latest/userguide/using-smb-fileshare.html

net use <WindowsDriveLetter>: <Gateway IP Address>\<path> /user:<Gateway ID>\smbguest

Here is an example:

net use Z: \\10.0.0.19\smb-share /user:sgw-F2BE5A9B\smbguest

Please let us know whether this helps."
AWS Import Export Snowball	"Re: SMB File Share password Incorrect
Yes. Exactly the same command"
AWS Import Export Snowball	"Re: SMB File Share password Incorrect
USPLAdmin,

Please PM me your AWS account ID, region, Gateway ID and Support Channel ID.

Please refer to the instructions below on how to setup the support channel on your gateway.

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html#enable-support-access-on-premises
https://docs.aws.amazon.com/storagegateway/latest/userguide/EC2GatewayTroubleshooting.html#EC2-EnableAWSSupportAccess

Thanks,
Shashi"
AWS Import Export Snowball	"Re: SMB File Share password Incorrect
Hello, 

I found the solution. 
Posting it just in-case anyone else faces the same problem.

The below solution is from AWS Support
 Need to change the lmCompatibilityLevel. This specifies what mode of authentication and session security is used for network logons. To do so, there are 2 ways: 
1. Registry Edit: 

Run regedit 
HKEY_LOCAL_MACHINE 
SYSTEM 
CurrentControlSet 
Control 
Lsa 
Select LmCompatibilityLevel Set Value to either 3 OR 4 OR 5

2. Security Policy
 Open your Windows Local Security Policy 
Local Policies 
Security Options 
Network security: LAN Manager authentication level 
Set to - Send NTLMv2 responses only. 
OR - Send NTLMv2 responses only. Refuse LM 
OR - Send NTLMv2 responses only. Refuse LM & NTLM"
AWS Import Export Snowball	"Re: SMB File Share password Incorrect
Thank you for this!  I ran into the same issue with the Guest SMB account, and the regedit fixes it."
AWS Import Export Snowball	"Storage Gateway S3 Multipart Chunk Size
Hi All,

Apologies if I have not spotted this in documentation but is it possible to determine the chunk-size the Storage Gateway uses when it copies multipart files up to S3?

Im trying to re-engineer the etag of stored objects but have no idea what Storage Gateway used. Maybe this is the wrong approach...

Many thanks,
J"
AWS Import Export Snowball	"Re: Storage Gateway S3 Multipart Chunk Size
mbj:

Thanks for your question. The chunk-sizes used by File Gateway for multi-part PUTs will vary depending upon the size of the file and a number of other factors, so it's there's no rubric which can be commonly applied. For small files we use a single-part PUT so the eTag should be easy to determine. 

What are you wanting to do with the eTags in this case?

Thanks
Paul"
AWS Import Export Snowball	"Re: Storage Gateway S3 Multipart Chunk Size
Hi Paul,

Many thanks for getting back to me, this is very useful to know.

I am trying to double check that the files we have transferred into AWS storage have actually copied correctly (currently using rsync). In this particular case, the copy was being managed between an S3 gateway and an nfs mount across a ssh-tunnel so there was some worry about the integrity of the transfer (I know there are other S3 transfer mechanisms, but this is the situation we were in). The ability to verify the checksum stored in S3 without having to re-read the object would have been useful. 

Its likely that be that I have gone down the wrong path with this approach...

Either way its a shame that Gateway does not store the chunk size as it is currently hidden information 

Best,
J"
AWS Import Export Snowball	"Re: Storage Gateway S3 Multipart Chunk Size
mbj:

Thanks for your reply. Your request makes sense given the number of tools involved in the transfer (this use case is one of the reasons we introduced AWS DataSync, https://aws.amazon.com/datasync/, which does all the checksums for you). For File Gateway we'll take your feedback into consideration as we plan updates on our roadmap.

Regards
Paul"
AWS Import Export Snowball	"Re: Storage Gateway S3 Multipart Chunk Size
Hi Paul,

Thanks agin, I will go ahead and verify the checksums the old fashioned way using md5sum. I will certainly look at the other options in the future.

Kind Regards,
J

Edited by: mbj on Jan 3, 2019 1:45 AM"
AWS Import Export Snowball	"Using a EC2 Volume Gateway
Hello All:
I wanted to know how can I use a EC2 Volume Gateway, I used AMI image but seems bit tricky to activate, it simply says :
ww.xxx.yy.zz./?gatewayType=CACHED&activationRegion=us-west-2 
can't be reached

It has open ports as suggested by lauch wizard

What can I do? Please help me"
AWS Import Export Snowball	"Re: Using a EC2 Volume Gateway
Hi mnino,

I was wondering if you have checked out all the networking and firewall requirements available at the Storage Gateway user guide link below.

https://docs.aws.amazon.com/storagegateway/latest/userguide/Requirements.html

Specifically, I would recommend to check out the ""Configuring Security Groups for your Amazon EC2 Gateway Instance"" section.

Please let us know how it goes. If you have followed all the steps and it still doesn't work, I would recommend to file a ticket with the support team so we can have your issue looked at by our experts.

Regards,
Bhavin"
AWS Import Export Snowball	"What happened if the network between storage and AWS is broken?
If I use file storage gateway to transfer local application to AWS over Direct Connect. If the DX network is broken, I have several questions to ask?
1. Will the application be stopped when storage gateway can not connect to AWS?
2. I think the application will continue to write data to cache disk until the cache disk is full. Then what will happen?
3. When the network connects again, will storage gateway automatically sync the un-transferred cache data to S3?"
AWS Import Export Snowball	"Re: What happened if the network between storage and AWS is broken?
xiaweidong:

Thanks for your questions.

1. The application won't be aware than the network between the gateway and AWS is not connected. 

2. Yes, the application can continue to write data until the cache disks are full (indicated by the CachePercentDirty metric hitting 100%) at which point the application will get an error. Conversely, if your application is reading data and the data is in cache it will be served to your application. If your application attempts to read data that is not in the cache, the gateway will attempt to fetch this data from AWS which will eventually fail and return a read error to your application.

3. When connectivity is established any any data written to the gateway will be uploaded to AWS and the gateway will resume normal operation.

Overall you should consider the gateway resilient to temporary loss of network connectivity, but it is not designed as an offline storage appliance.

Regards
Paul"
AWS Import Export Snowball	"Re: What happened if the network between storage and AWS is broken?
Paul,

Thank you very much!"
AWS Import Export Snowball	"AWS Storage Gateway on-premise VM ""read-only filesystem""
Hi,

We are using a cached volume gateway with on-premise VM appliance, and it stopped working a while ago, with the ""Your gateway is currently unreachable"" message on the AWS console. I connected to the VM and it has lose its local ip, I ""reset all to DHCP"" which lead to a restart. After the VM came up, the local ip is back and I was able to ping it. The time is synced and System Resource check reported no error. However, when I tried to ""Test Network Connectivity"", every option would fail with a message about ""Read-only filesystem"":
""/usr/local/aws-storage-gateway/console/bin/testconn: line 31: cannot create temp file for here document: Read-only filesystem""

Please advise on how to recover the VM from this state."
AWS Import Export Snowball	"Re: AWS Storage Gateway on-premise VM ""read-only filesystem""
Hi arastra 

There appears to be some issues with your local disks associated with the gateway VM. Can you verify / validate the underlying disks hosting the Storage Gateway VM are healthy?. If you still cant get it to work, PM me the gateway ID, account Id, and the region this is activated in and I will be able to take a look the logs for more information. Also, if you could open the support channel for the gateway and post in the PM the ID along with the other information, it would be helpful to diagnose the problem.

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Gateway unresponsive after doing a refresh cache!!! HELP!
Hello,

We just force a cache refresh to update files that has been uploaded via S3 API directly and the Gateway stop functioning...

We do a reboot but ALL shares hangs, not only the one that we refresh the cache!

SGW ID: sgw-009F7369

Please we need help urgently.

Thank in advance.

Regards"
AWS Import Export Snowball	"Re: Gateway unresponsive after doing a refresh cache!!! HELP!
After 1 hour SGW start working again.

Thanks!"
AWS Import Export Snowball	"Re: Gateway unresponsive after doing a refresh cache!!! HELP!
Hi, 

How many objects were in the bucket? How long was it between cache refresh calls? Did you call cache refresh during the S3 upload?

Cheers,
John

Edited by: AWS-JK on Dec 14, 2018 6:53 AM"
AWS Import Export Snowball	"Moving 120TB DFS file system to AWS
We are currently in a position where we need to move our entire 120Tb DFS shared drive structure, including existing NTFS permissions off of the existing 220 geographically dispersed servers. We are intending to move everything to AWS in the short term while we prune and filter out the dross from the folders. We have in excess of 3.2 Million folders in the structure. We cannot prune in situ for a number of logistical and political reasons so we want to move everything to one pot where we can work on it.

What is the best way to accomplish this in AWS for this amount of data? I need to retain the NTFS permissions (some folders are secure) so wherever it sites needs to support NTFS natively.

Purchasing hardware to host this on premise would set us back £300k+ and we really do not have the space. Moving to AWS will be expensive, yes, but still cheaper than buying hardware. We intend to keep the files on AWS (~30Tb) once it has been pruned and the unwanted folders removed."
AWS Import Export Snowball	"Re: Moving 120TB DFS file system to AWS
Hi,

@skinnyb Do you have any progress with that? I have similar issue right now, only it's 10 countries and 40TB of data fo me... Can't find a solution with AWS as the file gateway doesn't really support NTFS permissions (posix style only - and i haven't found any way to adapt that automatically while transferring files FROM on-promises file share TO S3 and back) and cached gateway is basically iSCSI volume presented remotely which also will not work for me, as people want to access that remotely within AWS... (i.e all offices access S3 bucket with files through nextcloud on AWS and HQ uploads\syncs files through AWS file\whatever gateway...)
Also I found out that AWS gateways DOES NOT support S3 acceleration which is really a big bummer for me, as HQ is located in UAE and their upload speed without S3 acceleration is about 1.5-2.5MBps only, while with acceleration it's 50+ MBPS... 

Regards,
Vladimir."
AWS Import Export Snowball	"""Unable to load local disks"" after activating Storage Gateway with EC2
Hello community,

I did the following steps in the us-east-a region:

created a new Storage Gateway from type ""File gateway""
chosen ""Amazon EC2"" as host platform
created a new EC2 with 150GiB of type x4.large by clicking the ""launch instance"" button in the wizzard
add ports 80, 22 and 2049 to the security group
In the step ""Connect to gateway"" I put the public IP address and the Gateway gets activated.


In the ""Configure local disks"" step I get a ""Unable to load local disks"" error. The status of this created gateway is ""Offline""

Does someone found out how to handle it?"
AWS Import Export Snowball	"Re: ""Unable to load local disks"" after activating Storage Gateway with EC2
Hi,

Do you have an EBS Volume mounted to your EC2 instance?
You may find this useful:
https://docs.aws.amazon.com/storagegateway/latest/userguide/ManagingLocalStorage-common.html

When you said x4.large, did you mean m4.large? The minimum specification for a gateway is m4.xlarge:
https://docs.aws.amazon.com/storagegateway/latest/userguide/Requirements.html#requirements-hardware-storage 

Hope this helps."
AWS Import Export Snowball	"Syncing Files to Storage Gateway In a Timely Manner
I have about 92 million files making up about 10TB of data that I need to move to ASG from a local nfs share. I do not have access to the underlying hardware just the nfs share itself where the data currently resides. I want to move these files to s3 though the local ASG, the initial sync can take weeks and that would be fine but I need to know of a way to do the final cut over sync in a timely way.

I think rsync would take to long to index files on the source nfs share and the ASG presented nfs share.

I'm currently investigating using netapp XCP to sync the data since it builds its own indexes and speeds up indexing a lot. 

What are some strategies customers use to do the final sync/cut over quickly ? 

I would imagine there is a process to do this as I would think this is a common need with snowball since both systems would have to do a final sync before switching to s3 storage."
AWS Import Export Snowball	"Re: Syncing Files to Storage Gateway In a Timely Manner
Hello -
Did you end up using XCP?  If so, how long did it take to copy the 10TB of data?

Best Regards
Mike"
AWS Import Export Snowball	"Re: Syncing Files to Storage Gateway In a Timely Manner
Hi there,

I researched a bit as this is an interesting question! 

Here’s what I found: When moving large amounts of data from on-premise to S3 it can take a significant amount of time for indexing when using 'sync' command. This can even cause the uploading process to appear as frozen. 

As a solution, instead of using 'sync' command, you can use the 'cp' command which copies the files without indexing overhead to S3 with improved speed. Although this works for newly created files, if we add files after the 'cp' command execution completes, we need to use the 'sync' command for the later added files. This works faster rather using the 'sync' command at the beginning.

If you need to create a Storage Gateway using an S3 bucket with a large number of files it can again take a long time to populate the cache in Storage Gateway.  This could be avoided by creating the Storage Gateway pointing to an empty bucket, and after that executing the 'sync' command to sync the files from the source bucket to the newly created bucket in S3. However, when accessing files through Storage Gateway, you’ll need to fetch them from S3 which potentially slows down the initial file retrieval. You can work around this by manually doing a query of files to populate the cache initially.

Since you have mentioned NetApp, another option is to use NetApp Cloud Sync, it’s a replication and synchronization service (SaaS) used to transfer and synchronize NAS data to and from cloud object storage. They support many formats, including NFS, SMB/CIFS (see https://cloud.netapp.com/cloud-sync-service). If you are moving to a fully managed cloud volumes by NetApp for aws storage, you can leverage SnapMirror replication technology to replicate on-premises data to AWS. This also makes it easier to have secondary copies available for multiple use cases with high availability."
AWS Import Export Snowball	"Re: Syncing Files to Storage Gateway In a Timely Manner
This is the perfect use-case for our new service DataSync:
https://aws.amazon.com/about-aws/whats-new/2018/11/introducing-aws-datasync-for-accelerated-online-data-transfer/

DataSync and Storage Gateway are meta-data compatible, so transferring via DataSync and then maintaining access via Storage Gateway should give you the best outcome."
AWS Import Export Snowball	"Trouble Joining Storage Gateway to Windows Domain
I'm having trouble getting my storage gateway joined to a Windows domain.  I'm getting the error message, ""The gateway cannot connect to the specified domain"".  I'd appreciate any direction on where to find relevant detailed logs and also any domain specific requirements like DNS records that need to exist."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
Hi, 
Have you opened up the ports necessary for the gateway to join your AD domain? This page  https://docs.aws.amazon.com/storagegateway/latest/userguide/Resource_Ports.html has a list of ports that need to be opened up incoming to the gateway for that operation to succeed.

If you have already done that, please private message me your gateway ID and account ID and we will look into it.

Thanks,
Smitha"
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
We are also not able to get the file gateway to join our Windows domain. 

We ""edit SMB settings"" and on the form which appears, tick ""join domain"" and provide the username / password of a domain admin account which is able to add new machines to the domain. 

When we submit the form, after a delay of about 5 seconds, we see a red banner message at the top of the ""gateways"" view: ""The specified request timed out"". 

I've seen the link in the thread above re: ports which need to be open for the domain join to work. However our domain controller is local to our storage gateway with no intervening firewall, so all ports are open."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
I've had time to do a little detective work, but I don't have an answer yet.

On the storage appliance, in""/usr/local/aws-storage-gateway/var/output/logs/access_log"", I found:
Jun 21 19:24:33 localhost sudo: sgserver : TTY=unknown ; PWD=/usr/local/aws-storage-gateway ; USER=root ; COMMAND=/usr/local/aws-storage-gateway/join-domain my.domain.com 123.123.123.123

The script it's calling is using ""net ads join"" to join the domain.

Edited by: PaulR@AWS on Jun 29, 2018 8:14 PM"
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
That's interesting - how do you get a command prompt such that you can inspect the logs? I built a storage appliance from the ""new"" image date 18th June 18, it seems pretty locked down. 

I've opened a support case on this with AWS, thus far they've asked me to ""open the support channel"" which I've done so they can look at the gateway themselves. 

I'll post here if we find the solution."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
I mounted the filesystem to an existing VM.  The appliance is running Amazon Linux 2017.09 and Samba version 4.6.2.

Edited by: randomUserMO on Jun 27, 2018 10:39 AM

Edited by: randomUserMO on Jun 27, 2018 12:01 PM"
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
I found this error in the AwsAppliance.log.2018-06-27-16 log file:

DNS Update for sgw-XXXXXXXX.XX.XXXX.com failed: ERROR_DNS_GSS_ERROR

Which indicated it's having trouble registering the host name to DNS.  So I manually created a DNS entry in that zone (which is managed by Windows), gave EVERYONE full control over the record for testing, and I'm now able to join the appliance to my domain and create shares.

I'll be redeploying from scratch to make sure none of my other fiddling was what actually fixed it."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
Sounds like a clever workaround. 

AWS support got back to me having looked at the logs on our gateway etc. via the ""support channel"". 

They said the problems we're having will be addressed by fixes which should be released in an updated image of the file gateway next week. 

It's good to know it wasn't misconfiguration on our part. I guess the SMB functionality for the file gateway is quite new and it still has a few teething problems. 

We are evaluating the file gateway against similar offerings e.g. Azure Storsimple but it seems best to wait for the fixes before we proceed with that."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
Just reporting back on the actual fix.

As long as I create a DNS record of the format sgw-xxxxxxxx.my.WindowsDomain.name before trying to join the appliance to domain, it's successful."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
Hi randomUserMO,

Would you be able to elaborate on the fix that you did to be able to join your storage gateway to the domain.

i.e, I am adding a DNS entry as sgw-1234789.test-us.local where test-us.local is my domain. Am I doing this right?"
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
For those of you following this thread, despite whatever updates AWS was intent on making a couple of months ago, the problem persists.

All traffic is allowed on my dev' network. Everything is connected and I've joined a Windows Server instance to my dev' domain. However, I am unable to add my existing Storage Gateway to a Simple AD, for SMB usage.

The error that I receive: The specified endpoint was not found. (Request ID: http://...)

As per the previous poster, please advise how, specifically, you overcame the issue with your workaround. I have manually added an A record for sgw-....my.domain, to no avail.

Thank you."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
Merlin,

There are multiple reasons for Storage Gateway failing to join a domain.

Can you please private message me your gateway ID and account ID and we will look into it.

Thanks,
Shashi"
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
Thanks Shashi.

I have PMed you.

Much appreciated."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
I am also facing trouble while joining & activating gateway.

During activation, many times UI hangs at Configure Local Disks, and then suddenly ends at Unable to Load Local Disks.
I am using self-hosted Hyper-V instance of AWS Storage Gateway & assigned it to appropriate virtual switch with 10GB RAM & 2 Seperate SCSI hard disks of 50 GB.

On certain days it just activates without any issue, but after that I face the issue related to joining the domain.

any pointers would be really helpful."
AWS Import Export Snowball	"Re: Trouble Joining Storage Gateway to Windows Domain
After a lot of head-scratching, I found that the Gateway needs to be able to lookup DNS names within the AD domain name. So if your AD is called ""ad.example.com"", the instance tries to look up ""_ldap._tcp.dc._msdcs.ad.example.com"". If the DNS lookup fails, then it can't join the domain. To do the lookup, it has to use the correct nameservers.

Instances (probably - unless you've changed thing) use the AWS built-in nameservers. You can change this by setting DHCP Options on your VPC. You need to create a new DHCP Options set, specify the AD domain name and the IPs of the Domain Controllers as the nameservers. Then configure you VPC to use this new Options set.

Once all that's done, any new instances will use the DCs as their nameservers. Storage Gateway will then be able to do the DNS lookup and locate the Domain Controllers to join the domain. After it's joined, you can potentially put the DHCP Options back the way they were."
AWS Import Export Snowball	"AWS Storage Gateway conectivity ok but offline
Hi,
I've an Storage Gateway offline. I've already rebooted the local vm, made all network connectivity tests, checked ntp time, and opened a support connection. All of the tests runs ok but the gateway in the console is offline.

Are there any posibility of an Amazon side check?

Thanks.

****
Some more info:
We have 3 gateways. One of them failed last week. It recovered one day, without any action.

Today we have the two others offline. 

I see in the one which is still online  that the last software update date is 11/21/2018, 3:57:18 AM. 
Maybe some update makes the gw not to get online.

Edited by: DavidMA on Nov 27, 2018 5:37 AM"
AWS Import Export Snowball	"Re: AWS Storage Gateway conectivity ok but offline
Hi DavidMA,

PM me the gateway id(s), account id, region they are activated in along with the support channel for the affected gateways. I will be happy to take a look at them,

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Re: AWS Storage Gateway conectivity ok but offline
Info already sent. Thanks for your help."
AWS Import Export Snowball	"Re: AWS Storage Gateway conectivity ok but offline
The gateways are up and running againg.
The problem was a change in an outbound traffic rule in our firewalls. It make to fail the ssl handsake between the onpremise storage gateways and the Amazon site.

When we added the excetion to the local gateways they reconnected and all the services are back online."
AWS Import Export Snowball	"Doesn't work if underpowered?
I'm trying to get a practical feel for SG, so I've created one using T2 Micro free instances.  Everything seems to be ok, tests pass and I can ssh to the ec2 image, but when I add the gateway instance to the SG by IP it always come back offline.  I've destroyed and recreated 4-5 times and I keep getting the same result.  Is there any way I can play with this tech without paying out for a large server?  Or is that even the problem?"
AWS Import Export Snowball	"Input/output error listing some folders and files via nfs
Hello!

We are trying to move some data from EBS to S3 bucket mounted via SG Service. We can copy all data using s3cmd sync without problems, but when we mount the share via nfs and list some folders and files we get Input/output error... We check that files on the bucket and they are ok.

We try deleting the share and creating again but error still the same.

Some ideas?

Thanks in advance.

Regards

Javier Aszerman"
AWS Import Export Snowball	"Re: Input/output error listing some folders and files via nfs
We notice that the destination bucket is from other aws account. We have full permissions set for the source account that has the SG service with the share...

When we access the bucket from the other account, the same files that we get Input/output error, the properties of Encryption, Metadata and tags are ""access denied"".

The fact is, how can I fix permissions to be all the same as the files that has the correct ones?

Thanks in advance

Regards

Javier"
AWS Import Export Snowball	"Adding new disks to the Storage file gateway HOW??
Hi all,

I've created a storage file gateway and the cache disk is only 200GB currently so I wanted to add one or even two 1 TB EBS drives.

I've successfully created and attached two new EBS volumes.  The problem is that when I go to the storage gateway console --> Actions --> edit Local Disks, I do not see the new drives.  It only shows the old 200GB disk.

Am I missing a command to refresh or something like that?

Any help would be appreciated.

Thanks"
AWS Import Export Snowball	"Seeding Storage Gateway Virtual Tape Library with Snowball
Is it possible to populate a Storage Gateway VTL with tape images via Snowball?  We have a large number of tapes that would need to be moved to AWS as part of a conversion to using VTL, but our connection speed makes this impractical.  Is there a way to move tape images to a Snowball device locally and have them imported as tapes in the VTL?"
AWS Import Export Snowball	"Re: Seeding Storage Gateway Virtual Tape Library with Snowball
Hi,

Thanks for reaching out. I have sent you a PM regarding this post.

Thanks,
Bhavin"
AWS Import Export Snowball	"500 Internal Server Error
I get the following error while trying to log in. Can you help with this? 

The server encountered an internal error or misconfiguration and was unable to complete your request.

Please contact the server administrator, root@localhost and inform them of the time the error occurred, and anything you might have done that may have caused the error.

More information about this error may be available in the server error log.

Apache/2.2.29 (Amazon) Server at eureka.nextility.net Port 80"
AWS Import Export Snowball	"Re: 500 Internal Server Error
Hi UrNzWy 

This could be coming from a blocked firewall within your local network and / or the ISP. If you are still having issues after checking these, please PM me your gateway Id, account Id and the region your gateway is activated in and I would be glad to take a look at this for you.

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Powershell script to pull volume statistics
We are using the Volume Storage Gateway.
I have a script that pulls all the volumes and as much information as the get-sgvolume commandlet will get me which is what I have shown below. In addition to this I would like to be able to show volume space ""used."" I don't see this in any other commandlet but I know AWS has this information because they show it from the AWS web console. Thank you in advance for any help!

   TypeName: Amazon.StorageGateway.Model.VolumeInfo

Name              MemberType Definition
----
----------
Equals            Method     bool Equals(System.Object obj)
GetHashCode       Method     int GetHashCode()
GetType           Method     type GetType()
ToString          Method     string ToString()
GatewayARN        Property   string GatewayARN {get;set;}
GatewayId         Property   string GatewayId {get;set;}
VolumeARN         Property   string VolumeARN {get;set;}
VolumeId          Property   string VolumeId {get;set;}
VolumeSizeInBytes Property   long VolumeSizeInBytes {get;set;}
VolumeType        Property   string VolumeType {get;set;}"
AWS Import Export Snowball	"Re: Powershell script to pull volume statistics
Hi doubleoh7 

Yes, we have a cmdlet that shows the VolumeUsedInBytes (what you are looking for) and is available in Get-SGCachediSCSIVolume or Get-SGStorediSCSIVolume depending on the volume gateway of interest.

https://docs.aws.amazon.com/powershell/latest/reference/items/Get-SGCachediSCSIVolume.html
 or
https://docs.aws.amazon.com/powershell/latest/reference/items/Get-SGStorediSCSIVolume.html

The corresponding API from the SDK doc showing this is:

https://docs.aws.amazon.com/sdkfornet/v3/apidocs/index.html?page=StorageGateway/TStorageGatewayCachediSCSIVolume.html&tocid=Amazon_StorageGateway_Model_CachediSCSIVolume

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Not reaching the Storage Gateway EC2 for activation
Hello!

I'm working on a very simple lab for file gateway, and i'm having issues on the first step. I'm trying to set an EC2 as the storage gw for file gw following the docs instructions. 
Seems to be pretty simple, i create the ec2 instance from de AMI and start the instance, login as admin and thats all.
When I try to active the gw from the storage gateway console, I enter de public ip address (yes, I reach the ec2, at least ssh), and i never reach the next step, i get the following error message when trying to connect to the gw for activation:

""This page isn’t working 18.205.217.181 is currently unable to handle this request.
HTTP ERROR 500""

I tried several things.... enabling ports on the security group following the docs, tried different instance family types, ssh the appliance and tried to set something, what seems that there's no need to set anything there.

So can anyone give me clue about it? Do I miss a step? Do I have to do something on the Appliance, like set an specific hostname, port? Network configuration? Any specific configuration outside the ec2?

thanks in advanced!"
AWS Import Export Snowball	"Re: Not reaching the Storage Gateway EC2 for activation
That error in general is a firewall rule issue.  If you can PM me your instance ID we can help you to check it out.  Thanks.

John"
AWS Import Export Snowball	"AWS Storage Gateway offline
My storage gateway has been offline (in the console) for nearly 12 hours now - all the tests pass (i.e. network connectivity), but still no luck. Have tried rebooting but also no luck.

It did work at one time, so I know it was setup properly - can this be checked from the AWS end?

Thanks."
AWS Import Export Snowball	"Re: AWS Storage Gateway offline
Can you please PM me the gateway id, region its activated in and your account Id?. Also please open a support channel for me to remotely login to the gateway and take a look at this (take a look at the link below):

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Re: AWS Storage Gateway offline
I have sent you the information you requested via private message - thanks. 

Gateway is in us-east-1"
AWS Import Export Snowball	"Issues with SMB File Gateway
We are attempting to use the semi-new SMB version of the Storage Gateway to present 7 different shares across our Windows environment. We are having quite a few issues in trying to do so.

1. We are writing SQL Backups to the share. Some backups work just fine, others will write for 20 minutes and then fail with an access denied message, even though they've been writing to the same folder the whole time.

2. Our application uploads files to the share. These files seem to disappear after a day or so. The file still exists on the share, but there is no matching file in the corresponding S3 bucket.

3. Somehow, we've got folders being created with a ""\"" in them. Obviously this is causing some issues because windows interprets that character as a path separator. So we end up with a folder called ""~\foo\bar"", but in the same root dir there is a folder called ""foo"" with a subfolder called ""bar"". Windows has no idea what to do about this and it's causing all kinds of weird issues.I'm not sure if we are using the gateway for it's intended purposes. There are a few million files spread across the 7 shares, should the gateway function at this scale?Any help or advice would be appreciated!"
AWS Import Export Snowball	"Re: Issues with SMB File Gateway
Figured out issue number 3.  Somehow we created a folder inside the bucket with a \ in the name.  The File Gateway shared this file out, and windows had no idea what to do with it.

I was able to move the contents of that folder to another folder to resolve this issue."
AWS Import Export Snowball	"Re: Issues with SMB File Gateway
Do you have other applications that periodically scan files in the file share (like a virus scanner) that could be preventing the SQL process from writing to the share?"
AWS Import Export Snowball	"Re: Issues with SMB File Gateway
We are running a virus scanning software, but it should be scanning on every write.  The backup fails at the same point (percentage wise on the sql backup) each time.  I'm not sure if it's a file size/number of files it's hitting.  

How large would you recommend the cache volume to be?"
AWS Import Export Snowball	"Re: Issues with SMB File Gateway
For #1, please try increasing the client session timeout for SMB connection.

https://blogs.msdn.microsoft.com/openspecification/2013/03/19/cifs-and-smb-timeouts-in-windows/

Add a new DWORD, with name ""SessTimeout"" under HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\LanmanWorkstation\Parameters\ in the Windows Registry on your SMB client host. Populate it with the value '0xe10' (3600 seconds).

Please let us know if this helps.

Shashi

Edited by: shashi-AWS on Nov 8, 2018 2:22 PM"
AWS Import Export Snowball	"Re: Issues with SMB File Gateway
An hour seems like an absurd amount of time to wait for a timeout...."
AWS Import Export Snowball	"Re: Issues with SMB File Gateway
I suggested a large value to confirm whether it addresses the issue you are experiencing. Once you confirm that it resolves the issue, you can change it to a shorter value that works for your environment.

Few of our customers reported success after making similar change while backing up large SQL server backups.

Please let us know whether it resolves the issue."
AWS Import Export Snowball	"Re: Issues with SMB File Gateway
Ok, I'll give it a shot.  Seeing this error on some of our SQL backups now """"The operating system returned the error '59(An unexpected network error occurred.)' """
AWS Import Export Snowball	"File Share Stuck in ""Creating"" Status
Hi All

I have an issue where a newly created file share is stuck in creating status. I think I know why. I changed the instance type on the VM and it broke the gateway, so I deleted it, deleted the VM and started again. I have tried to recreate it 5 times, same issue. When I delete it then it also gets stuck in deleting status and I have to force delete the gateway. I think my issue exists as in the backend these aren't actually deleted and they are still there, therefore the s3 bucket is not allowing multiple connections.

Does anyone know how I can expedite this to Amazon support please?

Thanks

Dave"
AWS Import Export Snowball	"Re: File Share Stuck in ""Creating"" Status
Please PM me your AWS account ID, Gateway ID, and region.  Thanks."
AWS Import Export Snowball	"Re: File Share Stuck in ""Creating"" Status
Thanks John I have messaged you"
AWS Import Export Snowball	"Re: File Share Stuck in ""Creating"" Status
Anyone able to help please?

I am confused how AWS support works, or the fct you have to pay for it. To me this is an issue I cannot fix and they need to do something their side"
AWS Import Export Snowball	"Re: File Share Stuck in ""Creating"" Status
I've run into the same issue, and I've tried several attempts to a couple different buckets. I've noticed that in order to delete a share, I have to execute the ""Delete file share"" action twice to get the share to truly delete.

My configuration is a Storage Gateway running in a VMWare Cloud on AWS cluster attempting to use an S3 bucket in the same region. Anyone have any suggestions?

Thanks,
Jason"
AWS Import Export Snowball	"Storage Gateway - File Gateway inside VPC with no internet gateway?
I have been searching for an answer to this and I am hoping that someone with more experience with storage gateway may be able to help please?

I have activated a storage gateway using an internet gateway. Ideally, I would like the storage gateway and file shares to be private, but it is my understanding that the storage gateway needs outbound access to the internet? Is this correct? I tried attaching a NAT gateway instead of an internet gateway, but reading the documentation I don't this will work and the Storage Gateway status was shown as ""Offline"" from the AWS management console when I did this.

I have seen some recommendations about adding the IP addresses associated with the storage gateway and file gateway endpoints to the security group for the EC2 instance running Storage Gateway. This would then limit the storage gateway to those endpoints. However, the link ""AWS IP Address Ranges"" on  https://docs.aws.amazon.com/storagegateway/latest/userguide/Requirements.html#networks does not work. Has anyone achieved this?

I'm already using the Amazon S3 Gateway Endpoint as part of my routing for the storage gateway. So I'm happy that calls to S3 are not going over the public internet. It would be great to achieve this with the remaining connections that storage gateway makes, if possible?

Does AWS have any update on when the storage gateway endpoints will be made available as private VPC endpoints?  https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html 

Any recommendations would be really appreciated?"
AWS Import Export Snowball	"Re: Storage Gateway - File Gateway inside VPC with no internet gateway?
If your gateway is running inside a private subnet in your VPC, a NAT gateway will allow you to connect and route traffic to the Storage Gateway service. Your gateway requires access to endpoints documented in https://docs.aws.amazon.com/storagegateway/latest/userguide/Requirements.html#allow-firewall-gateway-access. An alternative to this would be to run your gateway in a public subnet in your VPC to avoid need for a NAT gateway. We do not currently support private VPC endpoints for the Storage Gateway service, but are interested in learning more. Feel free to PM me your requirements if you are looking for this feature and the use case driving the need for it."
AWS Import Export Snowball	"NotifyWhenUploaded
hi,

i am not receiving any file upload notification , ex: ""NotifyWhenUploaded""  fron VM Storage gateway... with NFS clients , is something am i missing in gateway setup...
Appreciate your help..

SS"
AWS Import Export Snowball	"Re: NotifyWhenUploaded
Hi sirigiri,

Assuming you called the API (https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_NotifyWhenUploaded.html) to request the CloudWatch event, could you please Private Message me your gateway ID, region and time of request of the event through the API call so we can look into it?

Thanks,
Smitha"
AWS Import Export Snowball	"How to decrypt a tape encrypted with KMS key on AWS storage gateway
Dear Experts,

Could you please help to show me show to decrypt a tape encrypted with KMS key on AWS storage gateway.
I created the tape with below command :
 aws storagegateway create-tapes --gateway-arn arn:aws:storagegateway:ap-southeast-1:1111111111:gateway/sgw-222222222--kms-encrypted --kms-key arn:aws:kms:ap-southeast-1:333333333:key/4444444444444444 --tape-size-in-bytes 107374182400  --num-tapes-to-create 1 --tape-barcode-prefix TEST --client-token 77777
thank you so much."
AWS Import Export Snowball	"Re: How to decrypt a tape encrypted with KMS key on AWS storage gateway
Hi PhuongNguyen

Are you looking to create a decrypted tape from this encrypted tape? I am afraid that's not allowed as in we do not support creating a plain text resource from an encrypted resource. However you can directly read and write to this encrypted tape using the normal workflow. You can delete and then re-create a new tape with no KMS encryption though. Let me know if you have any more questions on this.

https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_CreateTapes.html

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Re: How to decrypt a tape encrypted with KMS key on AWS storage gateway
thank you so much Sanjaya"
AWS Import Export Snowball	"File Gateway currently unreachable. It may be shutting down or restarting
Hi Community, 

Currently, our on-prem file gateway is unable to connect to AWS. 

Your gateway is currently unreachable. It may be shutting down or restarting which can take a few minutes. (Over 12 hours now)

Network test 
all 4 endpoints pass 
System Resources all OK

I have restarted the VM with no luck. 

Any ideas? Is there a way to manually start the gateway via console?"
AWS Import Export Snowball	"Re: File Gateway currently unreachable. It may be shutting down or restarting
JBeans,

Please PM me your AWS account ID, region, and Gateway ID.  Thanks."
AWS Import Export Snowball	"Re: File Gateway currently unreachable. It may be shutting down or restarting
Hi,
I am having the same issue, is this a very common issue with AWS Storage Gateway?

Scenario 1: Creating a SG for the first time in AWS Console, 
below steps works fine

**Select gateway type**
**Select host platform (Download and Deploy OVF Template)**
**Connect to gateway**
**Activate Gateway**
However, problem starts at step ""Configure local disk"" after allocating Cache and Upload buffer (170GB each). ""Gateway is Active"" but ""Saving failed, please refresh local disks to see the current status"", eventually ""Unable to load local disks."". 

Refreshing doesn't make any difference. 

Scenario 2: Delete activated Gateway and re-create using the existing deployed OVF gateway in vSphere:
It fails at **Connect to gateway** regardless I use the same mac and ip address or a different IP address. Solution is to Delete the earlier deployed gateway in vSphere, re-deploy OVF template and reconnect in AWS cloud but then again I am stuck at ""Configure local disk"". 

I have created and re-created this SG more than 20 times so far and I have managed to get it working only once last night. However, this morning SG status showing offline and obviously lost the volume. 

I have checked the network connection in vSphere gateway console, test connection passes all 4, connection to different region works fine, tried setting static ip and preferred DNS or without static and dns, either way it's the same issue.

I have no SOCKS configuration and from Sydney Region.

Any help would be much appreciated.
Thanks"
AWS Import Export Snowball	"Re: File Gateway currently unreachable. It may be shutting down or restarting
I also have the same issue Your gateway is currently unreachable. It may be shutting down or restarting which can take a few minutes."
AWS Import Export Snowball	"Re: File Gateway currently unreachable. It may be shutting down or restarting
skimani,

Did you create & activate the gateway recently? 

If the root of the S3 bucket has large number of files it takes time to initialize for the first time. While the Gateway is initializing the files in the root of the file share, the status shows up as Offline. The status will change to Online, once the initialization is complete. We are working on improving the user experience in this scenario.

Can you please PM me your AccountID, GatewayID & Region?

Thanks,
Shashi"
AWS Import Export Snowball	"storage gateway snapshot management
Hi - 
I'm wondering if there is any tool, or whether anyone has developed any tool for
snapshot management for storage gateway snapshots.

For example, many backup products have the concept of 
keep daily backups for X days
weekly for x weeks
monthly for X mnths

So, this would be a program that would some how tag a snapshot
as a daily/weekly/monthly and then just keep a certain amount of them
deleting others.   

I Understand that we aren't paying for any extra storage with the extra snapshots,
but having a few hundred, or manually deleting them every few months, seems backwards.

Thanks for any comments.

Steve"
AWS Import Export Snowball	"Re: storage gateway snapshot management
There are some example scripts in the User Guide of using the SDK to delete unwanted snapshots: http://docs.aws.amazon.com/storagegateway/latest/userguide/managing-volumes.html#DeletingASnapshot. Could you use or adapt these to suit your needs?
Thanks
Paul"
AWS Import Export Snowball	"Re: storage gateway snapshot management
Hi Steve

I have the same problem. Did you managed to get a proper solution?
Thank you

Regards,
Chalitha"
AWS Import Export Snowball	"Re: storage gateway snapshot management
@Paul:
As you can see here, and elsewhere in this forum, snapshot management for Storage Gateway is lacking.  Aside from not being able to define retention rules (e.g. like RDS), it doesn't appear that Storage Gateway allows for being automated via Ops Automator, SNS/Lambda, etc.  Nor can you tag snapshots, e.g. with the volume/gateway name, except via manually/API/CLI.

Yes, I can code my own CLI scripts or API app to do this, but the whole snapshot scheduling/management end of things needs work.  I'm trying to use Storage Gateway as a way of handling offsite backups - it's mostly working, except this piece."
AWS Import Export Snowball	"Re: storage gateway snapshot management
Hi,

I will PM you to understand more about your use case.

Regards,
Bhavin"
AWS Import Export Snowball	"Voumes in pass through, inaccessible,  upload buffer was filled. Action?
Hi,

Over the weekend it looks like we ran out of upload buffer space and now have 5 volumes (e.g. vol-c63b4fe8) in Pass through mode. I added another upload buffer disk and cache disk to our VM but the CachePercentDirty remains at 100%.

Is there any way to tell when the volumes will come back online or if they are even trying to recover? What other actions can I take or statuses can I monitor?

Thanks

Edited by: awsipgd on Oct 22, 2018 10:26 AM"
AWS Import Export Snowball	"Re: Voumes in pass through, inaccessible,  upload buffer was filled. Action?
Hi awsipgd,

Can you please PM me your account Id, gateway id and the region its active in? I will be able to take a look at it for you.

Thanks
Sanjaya-AWS"
AWS Import Export Snowball	"Re: Voumes in pass through, inaccessible,  upload buffer was filled. Action?
PM sent, thanks.

FYI I added an additional cache volume and the the percentage has gone down from 100,"
AWS Import Export Snowball	"Re: Voumes in pass through, inaccessible,  upload buffer was filled. Action?
Hi,

24 hrs later there are still 5 volumes in pass through mode. How can I tell when/if they will recover?

Thanks"
AWS Import Export Snowball	"Re: Voumes in pass through, inaccessible,  upload buffer was filled. Action?
Hi

Still not seeing any status changes. Also notice that there have been no successful snapshots since 10/17 and two are ""pending"".
How do I resync these volumes?

Thanks"
AWS Import Export Snowball	"Re: Voumes in pass through, inaccessible,  upload buffer was filled. Action?
awsipgd,

I responded to you via PM a possible scenario where this could occur. Please confirm our theory and if its correct, I will post the details in this thread to close the loop.

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Re: Voumes in pass through, inaccessible,  upload buffer was filled. Action?
Hi,

Any luck seeing what is happening and how to fix? The buffers are slowly filling again, we have degraded performance and one of the volumes is now 100% full.

Thanks"
AWS Import Export Snowball	"Re: Voumes in pass through, inaccessible,  upload buffer was filled. Action?
Yesterday I stopped the gateway through the AWS dashboard, then rebooted the local VM, then restarted the gateway through the AWS dashboard. The upload buffer and cache utilization that had been stuck started to decrease and the volumes began changing from pass through to bootstrapping to available. Today they are all back to available with near zero cache and upload usage."
AWS Import Export Snowball	"Should we see reduced performance while copying files directly to S3 bucket
We are in the midsts of copying from a Snowball into S3 bucket which is the storage behind our AWS Storage Gateway (File Gateway Mode). We're finding that as the copy to the S3 bucket occurs, we're seeing very reduced performance reading existing data in the bucket.

Is this expected?"
AWS Import Export Snowball	"Re: Should we see reduced performance while copying files directly to S3 bucket
The act of writing data to S3 bucket directly (in this case Snowball), should not impact the performance of the File Gateway. How are you trying to read the data written through Snowball from the File Gateway? Are you invoking RefreshCache API? If yes, how often are you invoking this API?"
AWS Import Export Snowball	"Re: Should we see reduced performance while copying files directly to S3 bucket
We were not trying to access the data that's coming off the snowball, but other data that was already in the bucket. During the transfer we ran the Refresh Cache only a couple of times via the UI over a period of several days.

The way we are set up is that the storage gateway is mounted via NFS onto another Linux box.

The Snowball just finished transferring, we ran a manual sync afterwards. 

We don't seem to be having those issues now, but we're concerned that when we transfer additional data there, we will run into issues."
AWS Import Export Snowball	"Storage Gateway File Gateway and Encryption
Is the local buffer area on the on-premise storage gateway file gateway encrypted?
Can we utilize encryption on the NFS 4.1 connection to the storage gateway?"
AWS Import Export Snowball	"Re: Storage Gateway File Gateway and Encryption
The local buffer of File Gateway is not encrypted. Currently we do not support NFS4 Kerberos-based encryption. 

Thanks for your input as it is valuable for us to put out our product roadmap. We'd appreciate it if you could share more about your use case so we can better understand your needs. Please feel free to PM me."
AWS Import Export Snowball	"Re: Storage Gateway File Gateway and Encryption
Is there any plan to implement NFS encryption in the future?

I prefer to take a ""better safe than sorry"" approach to configuring my systems, which includes choosing an encrypted option whenever one is available. I can say that my VPC is reasonably secure from compromise, but I like to guard against that eventuality anyway by doing my due diligence to harden configurations whenever possible.

Also, if I wanted to make a storage gateway available to a host outside of the VPC (a local box), without NFS encryption, I have to resort to a VPN to ensure the data remains private. I'd most likely still use a VPN anyway, but I'd prefer the option was available in case it were needed."
AWS Import Export Snowball	"Cannot Delete Virtual Tape Gateway
Hello,

I have trouble deleting Virtual Tape Gateway. Whenever I try to delete, it gives an error.
Cannot be deleted."
AWS Import Export Snowball	"Re: Cannot Delete Virtual Tape Gateway
Hello

Have you checked to see if this VTL doesnt have any tapes in RETRIEVED or RETRIEVING state? If so, you will need to eject those tapes from your backup application and then first delete them before deleting the VTL gateway. If you dont have the original gateway available, then you will need to recover the tape (see below link) and then delete the VTL gateway,

https://docs.aws.amazon.com/storagegateway/latest/userguide/Main_TapesIssues-vtl.html#creating-recovery-tape-vtl 

Sanjaya-AWS"
AWS Import Export Snowball	"Re: Cannot Delete Virtual Tape Gateway
There are 2 tapes, status of one is Availabel  and second is Retrieved.
None of the tapes can be deleted. When trying to delete, it gives an error :  Tapes that failed deletion: 


The tapes are already ejected from the backup application."
AWS Import Export Snowball	"Can't Delete SG Stored Volume
Hi -
I'm trying to delete a stored volume and am getting an error.

I noticed that my snapshot charges went up quite a bit in the last month,
and this appears to be on the 1 TB stored volume I have.  Stored Volumes
require snapshots, Even though my stored volume was virtual empty,
my snaphot costs shot up to covering 1 TB. 
As there isn't anyway to tell how much storage a snapshot is actually using,
its kind of hard to debug where the culprit is, so I'm just bailing from
using stored volumes.  ** I don't want to pay for 1TB of snapshot
data when I am not storing 1 TB. 


Anyway, since I can't ""NOT HAVE"" snapshots, I'm wanting to delete the
volume, but am unable to do that.  I get an error of -

Volumes that failed deletion:  ......

Any Ideas on how to delete the volume?   
I've marked the disk as offline on my win machine.

steve"
AWS Import Export Snowball	"Re: Can't Delete SG Stored Volume
Check to make sure the underlying disks used for the stored volumes are still accessible. If not, make sure to add them back and retry the delete action.

https://docs.aws.amazon.com/storagegateway/latest/userguide/troubleshoot-volume-issues.html#troubleshoot-volume-issues.VolumeIrrecoverable"
AWS Import Export Snowball	"Re: Can't Delete SG Stored Volume
Hi - I did have the volume ""offline"" on my windows host, as I thought that 
would be the proper procedure for deleting a volume.

I could not delete it when offline and i couldn't delete when it was online also.
I've deleted all snapshots, but can't delete the snapshot schedule as this is
required (reason why i have to get rid of this stored volume). 

I've deleted all my cloudwatch items, but it still says its monitored by
cloudwatch, not sure if I can enable or disable that.

So, I still can't delete it."
AWS Import Export Snowball	"Re: Can't Delete SG Stored Volume
Hi Steve

I was talking about the disks configured for your volumes in the Storage Gateway, are they online and active? PM me your gateway-id, regions its activated in and the account number. I will take a look at it for you.

Thanks
Sanjaya-AWS"
AWS Import Export Snowball	"Re: Can't Delete SG Stored Volume
I was able to get help from the billing contact, who gave me a tip 
on ensuring the initiator was disconnected.

I'd done everything but that.
So, going into the iSCSI initiator,  making that volume inactive after
marking the volume as offline in disk manager, made me able to delete
the volume.

Things I'd tried,  volume offline, delete cloudwatch metrics,  taking the
gateway offline.  It makes sense now, you'd have remove the initiator from
the volume."
AWS Import Export Snowball	"AWS Gateway Virtual Appliance Pricing
A few years back we looked at the AWS gateway but its fixed $125/mo cost made it unjustifiable for a small nonprofit.  I am revisiting this for a different use case and I don't see any reference to pricing for the Virtual Appliance.  Is there no longer a charge for each active virtual appliance or am I just missing the pricing somewhere?"
AWS Import Export Snowball	"Storage Gateway Installation - EC2?
I am attempting to test the Storage Gateway for our company; however, we have a policy that we must use standard AMIs.  Is it possible to install the Storage Gateway on a normal EC2 instance such as Amazon Linux 2, RedHat, etc?"
AWS Import Export Snowball	"Re: Storage Gateway Installation - EC2?
Hi, The Storage gateway is a managed appliance we provide for your applications to connect and access cloud storage. You cannot install the gateway separately as an application on an OS."
AWS Import Export Snowball	"Is it feasible to use a File Storage Gateway for network file services
I am setting up a .NET based application on Windows servers and require a file share (SMB) for the web and application servers to access for shared files.  My initial thought was EFS, sounded like a perfect match to what we need until I read that it only supports NFS and not EC2 Windows servers (plus the NFS client on Windows doesn't persist).  

So now thinking of using a File Storage Gateway to use as a possible solution.  
First concern, latency and file availability. I don't know what the performance of a file gateway would be. I know it has cache volumes but how long could first file request be? 
Second concern high availability. Storage Gateway docs recommend only one writer per bucket, so I would not be able to setup a Storage Gateway VM in each AZ.  So no redundancy. Is it possible to put in an auto-scaling group if an AZ becomes unavailable? Regardless we'll be hit with cross AZ charges with a single point in one AZ (still not sure how much this is in reality).  

So, is this a good idea/approach?  If not, what are some better approaches to provide and SMB file share to servers in AWS? 

Thank you for your time"
AWS Import Export Snowball	"Re: Is it feasible to use a File Storage Gateway for network file services
Hi brettski,

If you are provisioning a Storage Gateway on EC2, the performance of the gateway will depend on EC2 instance and a number of other factors. These include the network bandwidth between the client and gateway, the speed and configuration of your underlying local disks the amount of local storage allocated to your gateway, and the bandwidth between your gateway and Amazon storage. 

In general we don't recommend HA solutions such as the one you suggest, but I will check if we have any information regarding the use of Auto-Scaling groups with gateways.

PM me for more details on your requirements for high availability, so we get an understanding of what is driving the need."
AWS Import Export Snowball	"AWS Stored Volume Gateway - Data being uploaded to S3 or not?
Hi

I have recently changed our AWS storage gateway from cached to stored volume as we've acquired more local disks. We use this for backup purposes only so we have a faster restore locally but have backups in the cloud for Disaster Recovery if needed. 

It has an upload buffer of 2.5TB but I haven't yet seen any data being used on here or on the AWS S3 volumes yet which is worrying. The backups ran at the weekend.

The gateway ID is:
sgw-C841A5A1

Is there anything else I need to do or should this start to upload automatically at some point?

Thanks
Sarah

*I'll also add that I did try and remove and recreate the gateway yesterday but there is still the original volume data to replicate

Edited by: sCBarbon on Oct 2, 2018 12:19 AM"
AWS Import Export Snowball	"Re: AWS Stored Volume Gateway - Data being uploaded to S3 or not?
Hi sCBarbon,

Please PM me the region your storage gateway was activated in and the account Id, I will take a look at it for you. Also if you can please open the support channel for the gateway and include that output in the PM, that would help with the troubleshooting.

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html

regards
Sanjaya-AWS"
AWS Import Export Snowball	"Re: AWS Stored Volume Gateway - Data being uploaded to S3 or not?
Hi Sanjaya, 

Thanks for your response. I have sent a PM with all the details requested.

Thanks
Sarah"
AWS Import Export Snowball	"Volume bootstrapping
Hi,

I have few new volumes that have been set up few weeks ago.  The problem is it doesn't seem they're syncing correctly as Restore/Bootstrap progress has been sitting at 0% since they were created. How can I fix this? Any ideas? The connectivity between storage gateway and AWS is fine from what I can see.

Thanks

Tomas"
AWS Import Export Snowball	"Re: Volume bootstrapping
Hi sceanz,

    We see that uploads from this gateway to AWS are timing out. This is the reason why the Bootstrap progress for the volume is at 0%. See: http://docs.aws.amazon.com/storagegateway/latest/userguide/PerfGatewayAWS.html. Can you increase your upload bandwidth rate limit (http://docs.aws.amazon.com/storagegateway/latest/userguide/MaintenanceUpdateBandwidth.html) and check the upload throughput from the gateway to AWS.

Thanks,
Nishanth"
AWS Import Export Snowball	"Re: Volume bootstrapping
Thank you for your response Nishanth.
The affected storage gateway was set to upload rate of 200KiB/sec. I've increased it to 1000KiB/sec one hour ago and I still don't see any traffic uploading. CloudWatch is showing readtime and writetime readings just not uploading is happening. What else would be causing this?

Regards,

Tomas"
AWS Import Export Snowball	"Re: Volume bootstrapping
The issue is now fixed. Turns out AWS upload traffic was routed incorrectly."
AWS Import Export Snowball	"Re: Volume bootstrapping
Hi

This is happening on the volumes on my gateway too. I've taken the limit off of our gateway.

Can you help?

Thanks
Sarah"
AWS Import Export Snowball	"OOM Killer on Storage Gateway
Hi
We have a storage gateway on a new account that just houses our backups (we previously had  a storage gateway on our main account) that keeps getting OOM killer messages:
""72484.369672 Out of memory: Kill process 3499 (java) score 825 or sacrifice child
72484.371564 killed process 3499 (java) total-vm:15147824kb, anon-rss:13543352kbm file-rss:0kb, shmem-rss:0kb"" which kills our backups.
This occurs when we do our backups to the onsite storage gateway every night. The jobs are just some simple robocopy scripts. The storage gateway was deployed to our vmware host by downloading the latest ovf from aws (updated to the latest version). The ovf deployed originally with 8GB of ram, I have since increased it to 16GB of ram but even with that it still OOM kills Java every night. Exactly the same scripts used to run on our old storage gateway (with 8GB ram) without an issue every night. I cannot access any logs on the gateway or see how much memory is allocated to Java, as the console is very locked down. 
Has anyone got any ideas on how to troubleshoot this?
ps we cannot justifiably give it any more ram as it is stretching the hosts resources as it is
Thanks,
Dave

Edited by: mortality101 on Sep 18, 2018 3:29 AM

Edited by: mortality101 on Sep 18, 2018 3:31 AM"
AWS Import Export Snowball	"Re: OOM Killer on Storage Gateway
Hi mortality101,

Is this a file gateway, volume gateway or a tape gateway? Please if you can PM me the account id, gateway Id, and the region its activated in that would be helpful. If possible please also open the support channel for the gateway and include the id in the PM.

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html

regards
Sanjaya-AWS"
AWS Import Export Snowball	"Re: OOM Killer on Storage Gateway
Sanjaya-AWS correctly diagnosed it was the oom_kill_process() being called from vmballoon_work(). 
He advised to disable the VMWare balloon driver and allocating dedicated memory to the Storage Gateway VM.
This fixed it"
AWS Import Export Snowball	"Managing ESXI File Gateway
I've created a file gateway and successfully have an SMB share bound to my domain. I'd like to bind it to a different domain and manage permissions, but I don't see a way to do this in the console (since it is on prem) or via the command line tools.

How do I manage the gateway now that it's been deployed?"
AWS Import Export Snowball	"Re: Managing ESXI File Gateway
Is this page useful?
https://docs.aws.amazon.com/storagegateway/latest/userguide/managing-gateway-file.html#enable-ad-settings"
AWS Import Export Snowball	"Re: Managing ESXI File Gateway
When you use ""Edit SMB Settings"" from the AWS Storage Gateway Console and enter new Domain, SGW will disconnect/leave from the currently connected Domain and joins the new Domain."
AWS Import Export Snowball	"Switched share from NFS to SMB, all existing folders have no write access
We are having issues with our NFS shares, so have switched one of them to SMB to see if that is any better.  However, after removing the NFS and reactivating as an SMB with guest access only, and export as ""Read-Write"", we can only create files in the top folder.  Inspecting the Security tab on a folder shows that the gateway user/smbguest account is not present on any of the existing folders.

Why doesn't the storage gateway automate the process of recursively adding the smbguest user as part of the share setup?

I only see the smbguest set on new folders/files I create at the root folder.  I tried using the icacls powershell command to recursively add it, but I keep getting the following error: ""\smbguest: No mapping between account names and security IDs was done.""

How do we recursively add this now after the fact?

Edited by: billkg on Sep 17, 2018 3:26 PM"
AWS Import Export Snowball	"Re: Switched share from NFS to SMB, all existing folders have no write access
billkg,

The behavior you observed is as expected as the permissions between NFS and SMB are different. If a customer has a use case where both NFS and SMB file shares are connected to the same S3 bucket, we don't want all the files to be available to all the users unless the customer explicitly configures the file ownership/permissions as appropriate.

In your case, if you would like to provide access to all the files written from NFS share you need to make it available as a NFS share again. After you make it available as a NFS share, update the read/write permissions appropriately so that ""other"" users have access. Then you will be able to access the files/folders as ""smbguest"" from the SMB share. 

Note that you cannot export both NFS and SMB file shares simultaneously from the single gateway pointing to the same S3 bucket. So you either create two Gateways or do the following.

1. Delete the SMB file share
2. Create NFS file share & Update the file/folder permissions.
3. Delete the NFS file share
4. Create SMB file share

Please PM me if you would like to discuss further, also please check your messages."
AWS Import Export Snowball	"Re: Switched share from NFS to SMB, all existing folders have no write access
I may be mistaken as it's been a while since I looked, but I believe the permissions stored as object tags. Depending on the scope of what you need to do, you might be able to create a temporary SMB share, create some files, figure out the tags, and just use a short script to overwrite the tags directly to the files in the original S3 bucket."
AWS Import Export Snowball	"Re: Switched share from NFS to SMB, all existing folders have no write access
Thanks for the suggestions.

Due to the ongoing stability issues with NFS we wanted to switch to SMB.  The SMB shares continue to stay up for days so seems like the better route to go.  

I will take a look at the tags route to see if I can fix it easily, as we had a bunch of challenges with NFS permissions to start with as well.

I will give it a shot and report back if/how I solved it.

Thank you!"
AWS Import Export Snowball	"Re: Switched share from NFS to SMB, all existing folders have no write access
billkg,

If you take the route of updating the tags in S3 directly, you need to invoke RefreshCache against your File Gateway to make it read the updated tags."
AWS Import Export Snowball	"EC2 gateway activation fails at ""connect to gateway"" step
I can't create an EC2 file gateway through the AWS console.  At the page where I enter the IP address and click the ""connect to gateway"" button, the browser loads http://xx.xxx.xx.xx/?gatewayType=FILE_S3&activationRegion=us-east-2 and displays a blank page: ""This site can’t be reached"".

I enabled port 80 for my computer.  I can ping the EC2 instance and access port 80. The EC2 instance meets minimum requirements (m4.xlarge with 80 GB and 150 GB disks).

2/20 UPDATE:
This activation error didn't occur when I used a Storage Gateway AMI listed on the user guide below.
 However, with these AMIs the key pair doesn't work (""server refused our key"") and I can't SSH to the EC2 instance.
https://docs.aws.amazon.com/storagegateway/latest/userguide/ec2-gateway-file.html

Edited by: craws on Feb 20, 2018 9:54 AM"
AWS Import Export Snowball	"Re: EC2 gateway activation fails at ""connect to gateway"" step
craws,

In response to your 2/20 update that you have an EC2 gateway running but the key pair is being refused.  Did you use an existing key pair or create a new one when prompted as part of the setup process?"
AWS Import Export Snowball	"Re: EC2 gateway activation fails at ""connect to gateway"" step
Hi, I am having the same issue but I have created a new key pair when prompted. It appears my web browser cannot connect to the IPv4 address (either for m4.xlarge or t2.micro instances) in the EC2 console.

Hi, I have attached the screen I have, I then type in the IPv4 address from the deployed sync agent in the EC2 management console. I then get the following error message:

This site can’t be reached
13.236.60.225 took too long to respond.
Try:

Checking the connection
Checking the proxy and the firewall
Running Windows Network Diagnostics
ERR_CONNECTION_TIMED_OUT

Edited by: Cassius on Sep 16, 2018 11:12 PM"
AWS Import Export Snowball	"Gateway keeps going offline while running tape backups from Veeam.
We are using Veeam to do tape backups to our Storage Gateway. We are using 2 network adapters and one is configured as the adapter for Internet traffic, the other is used for iSCSI with Veeam. We have a gigabit Internet connection which over which only AWS traffic is routed. The main problem we need resolved is that out Gateway keeps going offline and the only way we can get it to come back online is to restart it (power reset on the VMWare VM). When we do this sometimes the job continues, but more often the backup job fails. We are also experiencing hugely varied speed anywhere from 70MBps to 5MBps. Does anyone know what could be causing this.

Edited by: aseo on Sep 13, 2018 11:14 AM"
AWS Import Export Snowball	"Re: Gateway keeps going offline while running tape backups from Veeam.
Hi Aseo

Please PM me the gateway Id, region and if possible open a support channel [https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html] and I will have someone from our team look at this for you.

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"VPN access from client desktops to our EC2 instance
Hi,
We have a Windows based EC2 instance that amongst other things generates reports and does word merges for our clients.  We would like to be able to share a folder on the EC2 instance with our client's workstations so that they can retrieve and save files from Windows applications as if they were simply retrieving and saving to and from a folder attached to a windows drive letter.  Outside of AWS this can be done by means of a VPN between a VPN server and the client's workstations.  Can it be done in some way to our EC2 instance?  The important factor here is ease of setup and use for our clients - some of them have no sophisticated IT help.

Thanks"
AWS Import Export Snowball	"Tape backup to AWS - Advisable to do full backup ?
Hello people ,
  At our company we are looking at using Tape gateway/VTL to replace our on-prem Tape library .
  Backup application is Netbackup and we do full backup of our Teradata database once a week worth 50 TB ( completes in about 14 hrs) .
  Now with a 500 mbps dedicate line to AWS this upload will take days to complete . Storage gateway upload limit maxes out at 120 MBPS or about 1 Gbps . It wont help a lot even if we manage to get a 1 Gbps connection .
  So my question is how do you guys use Storage Gateway to backup tens of TB s worth of data ? Or is it fit for incremental backups only ( in range of GB s and not TB s) ? Any pointer is greatly appreciated ."
AWS Import Export Snowball	"Re: Tape backup to AWS - Advisable to do full backup ?
Anybody having any pointers ? Maybe the AWS Support team ?
To circumvent the bandwidth problem we are looking to spawn multiple Storage Gateways to upload in parallel .
So with 4 Storage Gateway VM we can theoretically have 2 Gbits/s (on 4 500Mbits/s lines )
Have someone connected Netbackup application to multiple Storage Gateways for parallel upload ? Are there any downsides ?
Also Upload Buffer max capacity is 2 TB , but we are looking to push through about 5 TB per Storage Gateway , so upload buffer will get full from time to time , but with sufficient Cache storage Netbackup should still be able to write to Storage Gateway without failing i believe ?"
AWS Import Export Snowball	"Re: Tape backup to AWS - Advisable to do full backup ?
Hi dipayan80 

The performance numbers provided here https://docs.aws.amazon.com/storagegateway/latest/userguide/resource-gateway-limits.html#performance-limits  are on a per gateway basis and provided for general guidelines based on certain test conditions. These numbers are not enforced in AWS Storage Gateway software and are driven by initiator to gateway bandwidth, ram/cache/disk/cpu on the gateway and gateway to cloud bandwidth. In practice we’ve seen customers get higher numbers, based on the specific platform they use.

We have customers using Storage Gateway VTL solution to do both full as well as incremental backups and it’s also possible to achieve a higher upload throughput by parallelizing through multiple gateways.

With upload buffer max capacity of 2TiB, you could choose to use higher cache up to 16TiB to ensure successful backup.

Hope this helps
Sanjaya-AWS"
AWS Import Export Snowball	"Re: Tape backup to AWS - Advisable to do full backup ?
Thanks a lot Sanjaya , your inputs are very helpful !
We will push about 6 TB data through each VM with input speed of 1 GBytes/s and output of 500 Mbits/s (compression enabled ) .
For each VM we are planning to use 24 CPU cores , 2 TB upload buffer , 8 TB Cache and 32 GB RAM . 
I think the above VM configuration will enable us to upload the data in about 26-28 hrs , do you think the configuration is optimum , or would you suggest beefing it up somewhat , perhaps the RAM ?"
AWS Import Export Snowball	"My File backup storage Gateway on S3 status is offline
Hi

My backup storage on S3 stopped backing up since August 1 and is now showing ""offline"". How do I fix that please?"
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
Please PM me your AWS account ID, region, Gateway ID and Support Channel ID.

Please open support channel from the Storage Gateway Local console and PM the support channel ID. Please refer to the instructions below on how to setup the support channel.

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html#enable-support-access-on-premises

Thanks."
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
My Gateway ID is : sgw-9805E0F1

Region is: N Vigirnia

My Account ID is: 110766076107 

Support Channel ID: 22225"
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
Can anyone help me please?"
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
I really need this backup to start again."
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
We are investigating, we will post an update."
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
The Storage Gateway process is in a restart loop as the Cache disk is not available hence it appears OFFLINE. You need to reattach the Cache disk and restart the Storage Gateway.

Another thing we noticed is, the available memory of 8GB is less than the recommended minimum of 16 GB. Please increase the memory as well for optimal performance.

Please keep us posted with your findings."
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
Not sure how to reset the gateway and re-attach the cache. Do we have documentation(s) on this?"
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
Any hint on to attach the cache disk? 
thanks."
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
It appears like your Gateway is hosted on Hyper-V. When you have setup the Storage Gateway, you have created and attached a local disk through Hyper-V to the Storage Gateway. You must have followed the instructions available at the links below. The Cache disk that you attached to Storage Gateway originally seems to be no longer available. Please investigate in your Hypervisor logs for relevant information. Once you resolve the issue with the disk and confirm that it is assigned to the Storage Gateway VM, please restart the VM. Alternately, you can create a new Gateway VM and delete the old VM.

https://docs.aws.amazon.com/storagegateway/latest/userguide/create-gateway-file.html#hosting-options-file
https://docs.aws.amazon.com/storagegateway/latest/userguide/create-gateway-file.html#configure-local-storage-alarms-file"
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
I saw the cache file  on the server and it's healthy, but don't see where I can 'reconnect' it to the AWS gateway.
I tried to create a new cache disk and it's asking me for an IP address which says offline every time  I enter it using the same local IP (192.168.1.242)  that I saw on the AWS storage configuration.
When I enter my public IP it goes to my local router.

Please help."
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
Any ideas on what next step(s) I should take?"
AWS Import Export Snowball	"Re: My File backup storage Gateway on S3 status is offline
Please PM me your AWS account ID, region, Gateway ID and Support Channel ID. The support channel ID that you have provided previously is closed now, so please share the new support channel ID."
AWS Import Export Snowball	"Storage Gateway File share becoming unresponsive
Hello,
We have been having a number of issues this week with our on-premise Storage Gateway NFS file share becoming unresponsive. Resolving the issue usually requires a reboot of the Gateway.

I've checked a number of the troubleshooting guides and can't seem to nail down the issue, I've also checked the performance optimization.

I have done some things this week to see if it would resolve, this includes adding another cache drive and bumping the vCPU.
Any suggestions?
Thanks,
Daryl"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
Daryl,

Please PM me your AWS account ID, region, and Gateway ID. We'll take a look at why your file share is becoming unresponsive. Thanks."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
We are still experiencing issues."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
Rollemad, 

Could you please run the ""mount"" command at a command prompt and PM me the output?

Thanks,
Smitha"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
I'm able to successfully mount the NFS share now, I don't have an example of a failed mount.

Thanks,
Daryl

Edited by: Rollemad on Aug 15, 2018 9:46 AM

Edited by: Rollemad on Aug 15, 2018 9:48 AM"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
Hi smithaAWS,
Are you able to connect to some back-end support channel, this device is a bit of a black box to me.

Thanks"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
It stopped responding again. I ran a mount command and it appears to mount successfully but hangs."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
Did you see any output from the mount command? If so, please private message me the text. You could also check to make sure you are using ""-o nolock"" and ""-o hard"" (especially for Windows)

https://docs.aws.amazon.com/storagegateway/latest/userguide/GettingStartedAccessFileShare.html"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
The mount command basically timed out, and yes were are using the mount commands documented."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
What OS (Linux, Windows 10/2012/2016 etc) are you using on your NFS clients? If you have any Windows NFS clients, please specify the options ""-o nolock"" and ""-o hard"" for the mount command.

Can you please open support channel from the Storage Gateway Local console and PM the support channel ID? Please refer to the instructions below on how to setup the support channel.

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html#enable-support-access-on-premises"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
Support channel ID sent."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
We are having the same problem since last week. It has gotten progressively worse and now we are able to traverse directory structure but cannot read or write any files."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
Carley,

Please PM me your AWS account ID, region, and Gateway ID.

Here is the recommendation I shared with Daryl and it seem to help their situation.

Please unmount and remount their filesystem using the ""-o nolock"" option from all your Windows NFS client hosts. Note that for windows hosts, we recommend a pause of 60 seconds between unmount and subsequent mount, to prevent reuse of previous mount. 

Please let us know how it goes. Please make sure you repeat these steps on all your Windows NFS client hosts."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
We recently had an the issue again. Reboot seemed to fix it.
A couple things I noticed:
All my NFS File Shares were inaccessible. I currently only have two, one should only be accessible from my workstation. I wasn't able to mount either with the Windows mount command with ""nolock""
However, my SMB file share was still accessible.

Is there something that's causing the nfs service to fail, and are there any fixes in the Storage Gateway update available August 30th?

Thanks,
Daryl"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
I am having the exact same problem.  I have two gateways, one that is a volume gateway running for months with never any issues, mounting volumes via ISCSI to veeam backup software.  

I wanted to replace a locally racked NAS server with a file storage gateway and wanted to use nfs mounts.  I almost need to reboot my gateway VMware VM daily.  That vm has 4 vcpu and 16 GB RAM allocated to it.  I played around switching the network adapter type (currently e1000) and that didn't solve the problem either.

On reboot it works correctly, but even after the latest 8/30 update it still requires at least a daily reboot.  The host itself isn't even able to be reached.  When they are not available it is confirmed down on linux and windows 7/10/2008/2012/2016 machines, so it is a global outage.

I didn't try SMB mounts but I will.  NFS was easier as we have a mix of windows and linux machines sharing files.

Does NFS just not work?"
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
Daryl,

In the Aug 30 release, we added improvements to deal with NFS clients that mounted the shares without ""nolock"" option. 

In order to understand the new behavior you reported, we request support channel access to your gateway. 

Please PM me your AWS account ID, region, Gateway ID and Support Channel ID. The Support channel you previously opened is no longer active."
AWS Import Export Snowball	"Re: Storage Gateway File share becoming unresponsive
billkg ,

We are sorry for your bad experience.

Please PM me your AWS account ID, region, Gateway ID and Support Channel ID. We will investigate the reason for the behavior you are experiencing."
AWS Import Export Snowball	"AWS Tape Gateway with Backup Exec wiping tapes
we are using Backup Exec 15 with AWS Tape Gateway. It is set to run a full backup on a set of tapes each day and upload. For the most part this backup runs successfully each day. However I am getting it where now and then tapes previously used are showing as empty when they should obviously have data and only be overwritten when a new back up is run. 
Had this with one or two tapes before but now I have just had this with 8/10 tapes. 

Has anyone else experienced similar problems with Tape Gateway? Though I do suspect that really this issue would lie within Backup Exec"
AWS Import Export Snowball	"Re: AWS Tape Gateway with Backup Exec wiping tapes
On further inspection of the Backup Exec logs, our backup job was erasing all tapes so will investigate here"
AWS Import Export Snowball	"Veeam Tape jobs to AWS VTL
I am currently running a backup copy/copy to tape job in Veeam B&R 9.53a to an Amazon VTL Gateway running on Premise using the Amazon VTL image in Hyper-V. 

I have 550GB of Upload Buffer
I have 500GB of Cache

My backup copy is running at about 418Kbps, we have 100M fiber. For the first 30 minutes it romps along at 84Mbps (I guess it would, it's loading the caches and buffers) but then dies a horrible death. I have 3TB which I would like to GFS via Veeam once a week.

My cloud watch monitoring shows the following currently:

2018-08-21 08:35 UTC
1. UploadBufferUsed: 318G
2. QueuedWrites: 294G
3. UploadBufferFree: 227G
4. CacheUsed: 222G
5. TotalCacheSize: 222G
6. CacheHitPercent: 100
7. CachePercentDirty: 100
8. CachePercentUsed: 100
9. UploadBufferPercentUsed: 58.0
10. CacheFree: 0
11. CloudBytesDownloaded: -
12. CloudBytesUploaded: -
13. CloudDownloadLatency: -
14. TimeSinceLastRecoveryPoint: -

This is my first foray in to the world of AWS and I'm guessing something is wrong with my config. Any help is greatly appreciated.

Edited by: CDL on Aug 21, 2018 2:19 AM"
AWS Import Export Snowball	"Re: Veeam Tape jobs to AWS VTL
Any help out there?"
AWS Import Export Snowball	"Re: Veeam Tape jobs to AWS VTL
Hi CDL,

I have sent you a private message regarding this post.

Regards,
Bhavin"
AWS Import Export Snowball	"File Gateway - Change in behavior for files in Glacier Storage Class
Has anyone else noticed a change when using the File Gateway/NFS Mount?  If a user tried to access a file in the Glacier storage class (which still appear within the file system) it used to throw an I/O error.   Now, trying to access one of these files is crashing the application and bringing down the whole NFS mount on the client server."
AWS Import Export Snowball	"Re: File Gateway - Change in behavior for files in Glacier Storage Class
JeremyKane,

Please private message me your account id, gateway id and region and we will look into it right away. 

Thanks,
Smitha"
AWS Import Export Snowball	"Environmental Testing Instruments
Procure your test instruments from reliable suppliers that deal in varied equipment like multifunctional Harmonics Analyzer, Electrosmog Meter, Hygro Anemometers, Insulation Testers, etc. Such suppliers won't just deal in quality  https://www.mecoinst.com/meco-category-details/environmental-testing-instruments.aspx  but also offer consultation services as well."
AWS Import Export Snowball	"File Storage Gateway in EC2 slows down over time as files are copied in
So, we are transferring in millions... 10s of millions files into File Storage Gateway (NFS). There are lot of directories and a lot of small files ( < 128kb).

We have NFS share mounted on a ec2 instance in the same az, and just rsync files over to it. As time goes the rate of transfers becomes very slow. To start we can move in 1000s of files every minute, but after few hours the rate drops to 100s every minutes, and sometimes even stalls to 1-2 files per minute.

I have no idea what is causing this and can not pin point to one issue. 

The storage gateway has 1TB GP2 root volume, and 2x2TB gp2 volumes as cache. I checked IOP usage when things stall and they are very low ( less than 50 iops per volume or even less ). The cpu usage is also < 20%. Bandwidth usage is low. There is no metric that is out of ordinary, but the performance is poor.

Tried with c5.4xlarge, and r5.4xlarge instance types and both have the same issue.

One thing I noticed is if we let the SG ""cool"" off ( as in stop the rsync ) for a few hours, and resume the rsync we get good performance again for a bit. Are we reaching any limits? Api Limits? Some kind of capacity?

Any guidance on how to improve the performance would be great. Right now I can not pinpoint the issue."
AWS Import Export Snowball	"Re: File Storage Gateway in EC2 slows down over time as files are copied in
Please check the IOPS configured for your Root & Cache volumes. You must be exceeding the IOPS configured for your EBS volumes and hence they might be getting throttled resulting in slow IO performance."
AWS Import Export Snowball	"Re: File Storage Gateway in EC2 slows down over time as files are copied in
Hi Shashi,

This is not the case,
For the root we have 3000 iops ( 1 TB gp2), and for cache we have  6000 IOPS per volume and we have 2 cache volumes( 2 x 2TB gp2). Since the EBS volumes are over 1TB, there should not be any burst buckets.

While the Storage Gateway is at high performance level, we do see high IOPS on the volumes  https://imgur.com/a/IwQ6jiT . You can see the root volume is at almost 1k IOPS and the cache are humming along at about 600iops.

So we are definitely within our limits. At about 7:00 we noticed HUGE drop in performance, which is not caused by going over ebs iops. On the graph you can see our IOPS drop from 1k to less than 100. 

Again, i checked all the metrics and nothing explains the 10x drop in storage gateway performance.

Thanks for your help!"
AWS Import Export Snowball	"Re: File Storage Gateway in EC2 slows down over time as files are copied in
Please PM me your AWS account ID, region, Gateway ID and Support Channel ID.

Please refer to the instructions below on how to setup the support channel on your EC2 gateway.

https://docs.aws.amazon.com/storagegateway/latest/userguide/EC2GatewayTroubleshooting.html#EC2-EnableAWSSupportAccess"
AWS Import Export Snowball	"Amanda backup and Storage Gateway
Can the Amanda backup software use the Virtual Tape Library? I'm not sure how I would set up tapes and changer"
AWS Import Export Snowball	"Re: Amanda backup and Storage Gateway
Hi sobrik,

The Gateway-VTL has not been qualified with Amanda backup, but we are in the process of qualifying new backup applications and this application is on our list.

Thanks for your feedback and interest in AWS Storage Gateway, and please let us know if we can help with anything else.

Regards,
Ian"
AWS Import Export Snowball	"Re: Amanda backup and Storage Gateway
I dont know about Amanda backup, If you want other backup software then use  Ahsay Software . It is amazing backup software which allows businesses and managed backup service providers to backup. I am using this software from last 10 months. It is running smoothly and up-to-date. Thanks"
AWS Import Export Snowball	"Re: Amanda backup and Storage Gateway
Any update on Amanda support for Storage Gteway?"
AWS Import Export Snowball	"Re: Amanda backup and Storage Gateway
Hi ooboyle,

Thanks for your post. We don't currently support Amanda backup with the Storage Gateway. I have noted your interest and we'll consider it as we plan future functionality and features.

Regards,
Bhavin"
AWS Import Export Snowball	"Re: Amanda backup and Storage Gateway
Thanks, Bhavin.

We're also looking into Bareos. Any chance you are too?

Oliver"
AWS Import Export Snowball	"Using AWS Storage Gateway under Linux
What is simplest setup (if possible) to use AWS Storage Gateway under KVM (i.e., from native Linux environment)?

Judging by description, only VMWare ESXi and Hyper-V virtual machine images are supplied to make use of the service. I can't understand what prevented you from providing Xen or KVM image apart from intention to make using service under Linux as hard as possible.

If there are known steps to set up the service under KVM, I'd be very grateful to know that. VM images often convert nicely to KVM, but I'd prefer to know whether this works, or whether I should abandon the service and choose more Linux-friendly product.

Thanks in advance for any piece of advice.

Edited by: Konstantin Boyandin on Jan 26, 2017 3:52 AM"
AWS Import Export Snowball	"Re: Using AWS Storage Gateway under Linux
Hello Konstantin,

Thank you for your feedback regarding support for KVM. At this point, the current release supports Hyper-V and VMWare. We plan on supporting additional hypervisors in the future, although we do not have a specific timeframe at the moment for KVM support. Your feedback is appreciated and certainly helps us to prioritize our future roadmap.

Thanks,
Peter"
AWS Import Export Snowball	"Re: Using AWS Storage Gateway under Linux
So currently I have to use something like s3fs and similar FUSE-using tools, to mount storage resources.

The other part of my question remained unanswered - can converting Hyper-V/ESXi images to KVM work (are there known successful cases), or not.

Thanks."
AWS Import Export Snowball	"Re: Using AWS Storage Gateway under Linux
The Storage Gateway ""ESXi Image"" is actually an OVA (at least for the File and Volume gateways). I haven't tried Xen or KVM but I was able to import it in to VirtualBox and got it working without too many issues. Worse comes to worst you can just untar it and use the vmdk directly.

For FUSE drivers I'd recommend RioFS or Goofys over S3FS, we had nothing but problems with that thing."
AWS Import Export Snowball	"Re: Using AWS Storage Gateway under Linux
Are we any closer to an iso?"
AWS Import Export Snowball	"Re: Using AWS Storage Gateway under Linux
+1 for an iso we can use on any hardware or hypervisor"
AWS Import Export Snowball	"Re: Using AWS Storage Gateway under Linux
I have also managed to import and run the File Gateway appliance on Ubuntu 16.04 Virtualbox. It functions OK but I cannot see the console output in the Virtualbox window. (Looks like black characters on black screen... Only the cursor is jumping around...)

I changed the appliance network interface type from NAT to bridge. Then I set my router to forward ports 80 and 443 to the appliance IP. That way AWS was happy to connect to it.

Volume Gateway also runs fine on Ubuntu Virtualbox, using the same port forwards. Additionally the above mentioned black characters on black background problem is not experienced, console functions well.

Edited by: triesz on Aug 20, 2018 4:57 AM"
AWS Import Export Snowball	"Virtual Tape Ejected, unable to find it
Dear forum members,

I've configured HP Data Protector 9 with AWS.

In AWS I have created two tapes.

When I finished to configured HP Data Protector, I could see both tapes in my slots.

But I clicked on one tape, and right button -> Eject.

The tape was gone, and althougt I select Instert, It did not find the tape.


I receive this message from Data Protector:

Warning] From: UMA@bckcentos01 ""bckcentos01"" Time: 15/12/2016 13:06:31

Mailslot(s) is (are) empty. Waiting for mailslot(s) to get filled.

But I do not know how to find and move the tape.

Do not kow where it goes.....

Could you help me?

Many thanks in advance,

Edited by: NetworkingCIMD on Dec 15, 2016 4:25 AM

Edited by: NetworkingCIMD on Dec 15, 2016 4:25 AM"
AWS Import Export Snowball	"Re: Virtual Tape Ejected, unable to find it
Dear forum members,

I've found the solution.

I did the eject yesterday, and I did not see any change in my AWS Gateway.
It shows two tapes.

I've entered my AWS Gateway just before to post this message, and I could see that I have only one tape.
I went to tape tab, and I could see my two tapes.

I retreive it, and I have this message:


You have successfully initiated retrieval of the tape AMNZ617DC4.

It takes about 24 hours for the retrieval to complete.

So, I think that all will be fine tomorrow.
Sorry and many thanks.

Edited by: NetworkingCIMD on Dec 15, 2016 4:32 AM"
AWS Import Export Snowball	"Re: Virtual Tape Ejected, unable to find it
You are correct. When you eject a tape it is archived and will no longer be available in your VTL:

http://docs.aws.amazon.com/storagegateway/latest/userguide/backup-hpdataprotector.html#hpdataprotector-archive-tape

If you want to access the tape again you can retrieve it:

http://docs.aws.amazon.com/storagegateway/latest/userguide/backup-hpdataprotector.html#hpdataprotector-retrieve-archived-tape"
AWS Import Export Snowball	"Re: Virtual Tape Ejected, unable to find it
One question regarding this issue...

Although I can see the tape in my HP Data Protector, I could not do anything with it.
In AWS Storage Gateway it says:
Barcode: AMNZ617DC4
Status: Retrieved

But my other tape has the State: Available.
I can not change the State from retreived to available. 
It is posible?

And in the Data Protector it says:

Major] From: MMA@bckcentos01 ""AWS-DRIVE-8""  Time: 20/12/2016 9:48:16
90:54]  	/dev/nst1
	Cannot open device (The medium is Write Protected.)

But the tape is blank. I only eject it and retreive it but without formating it or writing data on it.

Do you know how can I use this tape again?

Many thanks in advance,

Edited by: NetworkingCIMD on Dec 20, 2016 12:55 AM"
AWS Import Export Snowball	"Re: Virtual Tape Ejected, unable to find it
Ok.
I could read in the documentation that:
Note
Retrieved virtual tapes are read-only.

As this tape was send to Glacier when I eject it, I can understand that is read only.
But I can not delete it.

AWS Storage Gateway says:

Cannot delete resources due to one or more resources' status such as archiving or retrieving
Check the box to confirm deletion of the following resource(s):
•AMNZ617DC4 - RETRIEVED

It is no posible to delete tapes that has been send to Glacier?

Edited by: NetworkingCIMD on Dec 20, 2016 3:06 AM"
AWS Import Export Snowball	"Re: Virtual Tape Ejected, unable to find it
To delete a tape that's in RETRIEVED status you must eject it back to Glacier. See http://docs.aws.amazon.com/storagegateway/latest/userguide/managing-gateway-vtl.html#deleting-tapes-vtl

Sorry for the confusion here, we're looking into what we can do to simplify this.

Thanks
Paul"
AWS Import Export Snowball	"Re: Virtual Tape Ejected, unable to find it
Find here tape library rental in bangalore service provider in overall India.. http://www.racwg.com/backup-devices/"
AWS Import Export Snowball	"Re: Virtual Tape Ejected, unable to find it
Laptop Rental Providers in Mumbai, Maharashtra. Get contact details and address of Laptop Rental firms and companies in Mumbai. View a list of the available laptop on rent. Connect directly with owners to get immediately laptop on rent schedule by any time. It will beneficial for the industrial uses and also start companies. Now Hire laptop on rent, servers, computers on rent at affordable prices in India. We provide IT equipment to companies and individuals in India.
http://www.racwg.com/"
AWS Import Export Snowball	"Retrieving uncached data from AWS
When restoring from a tape that is in the VTL, but not cached locally, does the storage gateway have to retrieve the entire tape to local cache, or does it only call back the byte ranges needed for the restore?"
AWS Import Export Snowball	"Re: Retrieving uncached data from AWS
Hello,

In this case, Storage Gateway retrieves the requested bytes of the tape into the local cache not the entire tape. This reduces the amount of data that needs to be transferred to perform a restore.

Thank you,
Bhavin"
AWS Import Export Snowball	"File Storage Gateway - Using Robocopy
Hello,

I am doing some rudimentary testing with File Storage Gateway that runs in my VMware environment.  I am using SMB and had no issues setting it up, joining to my AD ..etc.  I wanted to copy some data from local drive (IE:  C:\temp\Folder1) to an SMB drive on the gateway mounted as letter Z:\  so i figured i would use robocopy for some testing.  Here is my command:

robocopy C:\Temp Z:\  /COPY:DT /DCOPY:T /E

Robocopy by default will only copy new and changed files but everytime i run robocopy where my target is the gateway, it always performs a full copy.  I checked the time on the gateway and it's identical to my Windows workstation.  When i look at Folder1 on local drive and Z: drive, time stamps for ""modified"" look identical.  Any thoughts ?

Thank you"
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
Hi dynamox 
Thanks for bringing this to our attention. We've taken note of this issue.
Regards,
Smitha"
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
Smitha,

Do you have a test environment where you can validate what i am seeing ?

Thank you"
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
Yes we do. 

Thanks,
Smitha"
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
The root cause is that the Gateway store time at MILLISECOND granularity. Windows keeps time at NANOSECOND granularity. So when Robocopy compares the time it thinks that the Windows version of the file is newer and re-copies it.

Here is a workaround: 

Use the /FFT option with robocopy.  This option tells robocopy to use a 2-second time granularity.

robocopy C:\Temp Z:\ /COPY:DT /DCOPY:T /E /FFT

Please let us know if it helps."
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
Shashi,

That helped where it longer performs full copy. Another interesting question is why the security parameter is not supported? For example:

robocopy C:\Temp Z:\ /COPY:DTS /DCOPY:T /E /FFT

The error message that I get is :

************************************

C:\Temp>robocopy C:\Temp z:\ /mir  /copy:dts /dcopy:t /fft


   ROBOCOPY     ::     Robust File Copy for Windows

  Started : Friday, August 10, 2018 3:31:36 PM
   Source : C:\Temp\
     Dest : z:\

    Files : .

  Options : . /FFT /S /E /DCOPY:T /COPY:DTS /PURGE /MIR /R:1000000 /W:30


                           0    C:\Temp\
2018/08/10 15:31:36 ERROR 5 (0x00000005) Copying NTFS Security to Destination Directory z:\
Access is denied.

************************************

I then decided to go to mapped network drive , created a folder and try to set ACLs using Windows Explorer and also received ""Access Denied"".  So the question is, can we not apply NTFS ACLs to files/folders that reside on SMB Share presented from the gateway ?  (My gateway is joined to AD).

Thank you"
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
dynamox,
As documented in our FAQS (https://aws.amazon.com/storagegateway/faqs/#file), file gateway supports POSIX permissions that is a compatible subset of NTFS -- and it appears you're not being allowed to copy if you are trying to carryover all of the NTFS permissions and/or set NTFS ACLs on files in the SMB share.

Please PM me directly if you want to discuss characteristics of your particular workload and what you are trying to do.

Thanks,
Smitha

Edited by: smithaAWS on Aug 12, 2018 11:00 AM"
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
I PM'ed you.   I mean simple stuff like being able to go to mapped drive (from File Gateway) and either adjusting folder owner or adding an additional AD account to ACL is not working.  Every time i try to do these operations i get  Windows Explorer error: ""Failed to enumerate objects in the container. Access is denied""

Kind of defeats the purpose of the gateway joining to my AD if i can't modify ACLs."
AWS Import Export Snowball	"Re: File Storage Gateway - Using Robocopy
I'm having exactly the same issue... File Storage Gateway + SMB share and inability to change permissions post-upload to S3.

What is the actual method of mapping Windows ACLs to POSIX permissions in the S3 metadata?

It seems that during creation of an object, the following metadata are set per object:

x-amz-meta-file-group (posix-style gid)
x-amz-meta-file-owner (posix-style uid)
x-amz-meta-file-permissions (posix-style permissions correlating to uid, gid, and Everybody)

Is there no facility to change any of this information after upload, even with object copy, by manipulating Windows ACLs in the Storage Gateway file share?

Can we inspect the mapping x-amz-meta-file-owner or x-amz-meta-file-group to our Active Directo"
AWS Import Export Snowball	"SMB file share configure allowed/denied users and groups.
Hello,
I've deployed the latest GW on 2018-08-07 and configured SMB file share. I joined our AD without issue. I can access the SMB share without issue using ANY domain account.

I cannot figure out the syntax to specify users and groups in the ""Edit allowed/denied users and groups"" panel. I can't find any documentation on this...

Specifically, I want to restrict access to the SMB file share to specified users and groups, everyone else in AD is denied access.
Thank you,
Max"
AWS Import Export Snowball	"Re: SMB file share configure allowed/denied users and groups.
The following page describes how to ""Edit allowed/denied users and groups""
https://docs.aws.amazon.com/storagegateway/latest/userguide/managing-gateway-file.html#enable-ad-settings

As far as your question, you can simply enter the ""user name"" or ""group name"". You don't need to specify the Domain name as it is implied by the AD the gateway joined. We will update the documentation to clarify this."
AWS Import Export Snowball	"Activating Gateway: 500 Internal Error
Hi,

I have setup the Storage gateway on our Internal VMWare ESXi. While activating the Gateway, I get the following error: 

is currently unable to handle this request.
HTTP ERROR 500


This error is only visible in Chrome, in other browsers: Firefox, Edge. There is a white screen of death.

I tested Network Connectivity, it passes every time. The Network time is also synced. There is no difference in NTP Time and Host Time."
AWS Import Export Snowball	"Re: Activating Gateway: 500 Internal Error
USPLIT,

Can you please the name of the OVA image you downloaded and the type of the Gateway (File, Volume, Tape) you are trying to activate? If they don't match, you will encounter Activation error.

Once you confirm that you are using the correct version and still having the Activation issue, please open support channel through the Local Console and PM me the support channel ID.

Thanks"
AWS Import Export Snowball	"Files and Folders in Linux NFS mount are not using uid/gid of share
I have newly created a storage gateway with NFS mounts set using all of the defaults.

When I mount the share on CentOS 7, everything shows up and can be read.  However, nothing can be written into any of the folders due to permission issues.  The share root properly is using uid/gid 65534 (nfsnobody), but all of the other folders and files within the mount and below are owned by 4294967294:4294967294, which prevents writing into any folder.

The same NFS mount on windows allows reading and writing into any folder or file.

If I open the file metadata in my S3 bucket, the x-amz-meta-file-owner and x-amz-meta-file-group metadata fields have 4294967294 set in both of them.  

If I look at the mount, instead of it being 0777/0666 as noted in the SG Console mount properties, both folders and files are 0755.

I tried mounting with both NFS version 3 and version 4 and also do not see a difference.

What am I missing?"
AWS Import Export Snowball	"Re: Files and Folders in Linux NFS mount are not using uid/gid of share
billkg,

Based on your description below it looks like when you mounted the file share from Windows the Windows OS wrote it's permissions to the files and your Linux client lacks the appropriate permissions to access those files.  Most likely this is an issue with your squash settings on the NFS file share through the File Gateway.  I've included a link to the relevant documentation below.  Please let me know if you have any other questions.  Thanks.

John

https://docs.aws.amazon.com/storagegateway/latest/userguide/managing-gateway-file.html#edit-storage-class"
AWS Import Export Snowball	"Re: Files and Folders in Linux NFS mount are not using uid/gid of share
I tried to use ""all squash"", mounted as root in CentOS7 and see this:
drwxrwxrwx  1 nfsnobody  nfsnobody          0 Jul 18 09:41 .
drwxr-xr-x. 4 root       root              34 Aug  1 12:53 ..
-rwxr-xr-x  1 4294967294 4294967294    240955 Apr  9  2010 0000402902_out.mov

I then updated to ""no root squash"", mounted again and see:
drwxrwxrwx  1 nfsnobody  nfsnobody          0 Jul 18 09:41 .
drwxr-xr-x. 4 root       root              34 Aug  1 12:53 ..
-rwxr-xr-x  1 4294967294 4294967294    240955 Apr  9  2010 0000402902_out.mov

I then updated to ""Root squash"", mounted again and see:
drwxrwxrwx  1 nfsnobody  nfsnobody          0 Jul 18 09:41 .
drwxr-xr-x. 4 root       root              34 Aug  1 12:53 ..
-rwxr-xr-x  1 4294967294 4294967294    240955 Apr  9  2010 0000402902_out.mov

So, there doesn't appear to be a difference when I choose any of the three options.

After mounting, this is what comes back from the ""mount"" command:
nas:/temp on /mnt/temp type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=XXX.XXX.XXX.XXX,local_lock=none,addr=XXX.XXX.XXX.XXX)

Let me know if you have any other ideas."
AWS Import Export Snowball	"Re: Files and Folders in Linux NFS mount are not using uid/gid of share
Once you change the permissions to ""no root squash"" you next need to do a chmod as root to allow anyone to write to it.  If you just change the squash level and don't do a chmod the permissions remain the same.  Thanks."
AWS Import Export Snowball	"Re: Files and Folders in Linux NFS mount are not using uid/gid of share
I changed to ""no root squash"" and did a chmod -R +w . on the mount, but afterwards when I do a ls -la I still see the exact same thing:
drwxrwxrwx  1 nfsnobody  nfsnobody          0 Jul 18 09:41 .
drwxr-xr-x. 4 root       root              34 Aug  1 12:53 ..
-rwxr-xr-x  1 4294967294 4294967294    240955 Apr  9  2010 0000402902_out.mov

However, I did cd into a few directories and was able to actually write now as my settings for the share are 777 as defined in the console.  When I view the properties in windows they are shown correctly, so must be something odd happening in Linux to cause these to display incorrectly.

I had this same issue on another mount, didn't chmod anything and can write after changing to ""no root squash"", so perhaps that was it, just odd that it is still showing as 755 in the ls output via ssh."
AWS Import Export Snowball	"Storage Gateway Load
Hi All

I have been getting to grips with the AWS Storage Gateway. Everything is working and I am using it to send Veeam backups to S3. It all works OK, except Veeam reports a bottleneck at ""target"" and if I set multiple backups going they consistently drop connection. I have a 500GB cache with 500 iops and using m5.2xlarge. I can scale these up, but I am not sure if this is the issue.

Can anyone offer any advice please?

Thanks

Dave"
AWS Import Export Snowball	"Re: Storage Gateway Load
Dave,

Which gateway type are you using?  Also how many backups are you trying to run simultaneously? Thanks."
AWS Import Export Snowball	"Queries and issues regarding File Gateway
Mounted a File Gateway on a windows instance. Used a file generation script written in python to pump data straight into the mount point. The file gateway had 3x100GB Cache SSDs

Problems encountered:

1. The data produced by the script and the space consumed on cache did not seem to be in sync.
Before running the script the cache usage was 30MB while after the script had generated 100MB data(as seen by exploring Windows Properties and size in s3 console), the cache usage shot up to 300GB.

2. One of the processes generating the data failed even though the cache free metric in CloudWatch never dropped below 15GB.
Error Snippet: 
File ""E:\makefileset.py"", line 215, in create_fileset
    f.close()
IOError: [Errno 22] Invalid argument

3. The directories being created were specified as UNC type paths(\\?\E:\xxx) to the os.makedirs(python) api. The calls were failing till the path was changed to start directly at E:\
Error Snippet: ""WindowsError: [Error 1006] The volume for a file has been externally altered so that the opened file is no longer valid: '\\\\?\\E:'""

4. Also because of the key length limit of s3(1k) file paths length will be limited, which might cause issues when using long file paths in windows(32k) or linux(4k). Is there any way around this limitation?

Edited by: ishangupta on Jun 27, 2018 5:38 AM"
AWS Import Export Snowball	"Re: Queries and issues regarding File Gateway
ishangupta wrote:
Mounted a File Gateway on a windows instance. Used a file generation script written in python to pump data straight into the mount point. The file gateway had 3x100GB Cache SSDs

Problems encountered:

1. The data produced by the script and the space consumed on cache did not seem to be in sync.
Before running the script the cache usage was 30MB while after the script had generated 100MB data(as seen by exploring Windows Properties and size in s3 console), the cache usage shot up to 300GB.

2. One of the processes generating the data failed even though the cache free metric in CloudWatch never dropped below 15GB.
Error Snippet: 
File ""E:\makefileset.py"", line 215, in create_fileset
    f.close()
IOError: [Errno 22] Invalid argument

3. The directories being created were specified as UNC type paths(\\?\E:\xxx) to the os.makedirs(python) api. The calls were failing till the path was changed to start directly at E:\
Error Snippet: ""WindowsError: [Error 1006] The volume for a file has been externally altered so that the opened file is no longer valid: '\\\\?\\E:'""

4. Also because of the key length limit of s3(1k) file paths length will be limited, which might cause issues when using long file paths in windows(32k) or linux(4k). Is there any way around this limitation?

Edited by: ishangupta on Jun 27, 2018 5:38 AM
Answers below
#1 - How many files are you storing in your 100MB dataset? While we do not evict the cache unless there is newer data, depending on the file size you are trying to write you may see sparse usage of the cache. The gateway is trying to mediate between NFS which favors small reads and writes (KB size) and S3 which prefers data in larger tranches (MB size).

#2 As I mentioned above, the CacheFree metric isn't an indicator of any issue as we do not evict from cache unless there is more recent data that needs to be stored for local access. 

#3 UNC paths should work, so we'd be happy to take a look at what's going on. 

#4 The gateway enforces a maximum path length of 1024 bytes; clients are not allowed to create a path exceeding this length and will result in an error if they try to do so. There is no workaround this limitation as it's is driven as you noticed by the limit on the S3 object key length.

Please PM me your account ID and gateway ID and we will look into the failures you are seeing during uploads and with the UNC path

Thanks,
Smitha"
AWS Import Export Snowball	"Re: Queries and issues regarding File Gateway
Hi Smitha,

Thanks for your response. The answers are as below:

1. ""How many files are you storing in your 100MB"": The file number is fluctuating, we randomly created files between 1KB and 1GB.

2. I mentioned the CacheFree metric because of the documentation which stated that it may cause backups to fail. But as we discussed previously, the CacheDirty percent never went above 5%.

3. UNC paths not working might be an OS issue. We would look into it at our end also.

aws account id: 356683483434
gateway id: sgw-E8AF4A81

We are also planning to run the tests again in a more segregated way(small files vs large files) with better logging at our end too. Will send across the results and exact timings for them.

Edited by: ishangupta on Jul 19, 2018 11:04 AM"
AWS Import Export Snowball	"Time drifting way off after AWS GW reboot
Every time we reboot or an update is applied the time on the gateway is off by 6000 ish seconds. The GW is on HyperV and the system time and hardware time are set properly. The time zone is set properly in AWS GS settings. Can anyone tell we where it is getting its time when it reboots? We have a few AWS gateways and this only happens to one. It causes the GW to crash after a few hours if the time is way off."
AWS Import Export Snowball	"Re: Time drifting way off after AWS GW reboot
Hi

To resolve the clock drift issue, we recommend: 

1) Ensure that the guest time is configured to be correctly synchronized (see https://docs.aws.amazon.com/storagegateway/latest/userguide/manage-on-premises-Hyperv.html#MaintenanceTimeSync-hyperv ) 

2) If this was already appropriately addressed, check the hosts's time, to make sure that it is accurate. 

3) If you still have issues please open a support channel and PM me with that info along with gateway ID and the region its activated in.

Thanks
Sanjaya-AWS"
AWS Import Export Snowball	"Gateway failed deletion
Hi

We had issues with one of our on premise datastores some time ago (where the gateway appliance resides), and our VTL gateway is showing as offline in the AWS console. I want to remove the gateway from view but everytime I select delete gateway I get the error:

Gateways that failed deletion:
sgw-1628CC7F

I can't get to the gateway appliance as it's long gone now after the corruption. What can I do to remove this?

Thanks
Sarah

Edited by: sCBarbon on Jul 16, 2018 4:56 AM"
AWS Import Export Snowball	"Re: Gateway failed deletion
Hi

It appears the VTL is associated with tape(s) that is / are in 'RETRIEVED' status (VTL tapes which were ARCHIVED at some point in time but later retrieved). When VTL tapes are in RETRIEVED status, they are write protected and will prevent the VTL gateway from being deleted until the RETRIEVED tape is back in ARCHIVED status. One option would be to spin up a new temporary gateway, recover the 'RETRIEVED' tape to this gateway and then eject the tape using the backup application. 

https://docs.aws.amazon.com/storagegateway/latest/userguide/Main_TapesIssues-vtl.html#creating-recovery-tape-vtl

http://docs.aws.amazon.com/storagegateway/latest/userguide/managing-gateway-vtl.html#understand-tapes-status

http://docs.aws.amazon.com/storagegateway/latest/userguide/backup_netbackup-vtl.html#GettingStarted-archiving-tapes-vtl

Hope this helps. If you need further help please reach out to our AWS Dev support team and they would be able to help.

regards
Sanjaya-AWS"
AWS Import Export Snowball	"Restore to AWS using Veeam
Hi Community, 

Is it possible to restore veeam backups to AWS ec2 instances?"
AWS Import Export Snowball	"Re: Restore to AWS using Veeam
Hello,

You can use Veeam with AWS Storage Gateway VTLs to backup your on-premises enviroment to AWS. This is one solution. 
You can export the backup vmdk and use the AWS VM Import Service. Following formats are supported for the VM Import:
For Image Import: OVA, VHD, VHDX, VMDK, raw
For Instance Import: VHD, VMDK, raw

Best regards."
AWS Import Export Snowball	"Re: Restore to AWS using Veeam
Unfortunately this is not possible right now. The best place for these answers is probably the Veeam forum, especially a thread like this one

https://forums.veeam.com/post275006.html

Edited by: kylegordon on Jul 10, 2018 3:18 AM"
AWS Import Export Snowball	"Is SOCKS proxy needed to use SG with VEEAM.
Hi

I'm having a play with SG but just need to know if you have to setup SOCKS or if you can run without it.

John"
AWS Import Export Snowball	"Re: Is SOCKS proxy needed to use SG with VEEAM.
JohnMcG:

You don't need to use a SOCKS proxy if your gateway can connect to the service endpoints directly.

The network requirements for the VM are documented:
https://docs.aws.amazon.com/storagegateway/latest/userguide/Requirements.html#networks

Thanks
Paul"
AWS Import Export Snowball	"Need Explanation for Archiving Tapes
Hi,

I am trying to understand the need to archive tapes in the AWS Storage Gateway program.  I built a number of tapes, and now after about a month of usage a handful of tapes are full.  My question is do I need to archive the full tapes?  What is the issue if I just let them sit there?  My understanding is that we do not pay for tapes, only for data throughput.  If the tape is sitting in this virtual library or ""archived "" and moved to an alternate location somewhere at AWS, its still ""in the cloud"" as far as my needs are concerned, and I dont have to wait for the tape to be made available again if I need to restore.  I am not understanding some part of the process, and would appreciate clarification.

Thanks

Joe"
AWS Import Export Snowball	"Re: Need Explanation for Archiving Tapes
Hello Joe,

The need to archive the full tapes you have in your library depends on your recovery needs. e.g. Do you expect to need this data in the short-term for recovery purposes or don’t anticipate needing it anymore and would like to put it away for your regulatory or compliance needs?

It’s true that you do not pay for the tapes, but you will be billed for the amount of virtual tape storage i.e. data stored in the cloud by your tapes. I have included the link to Storage Gateway pricing below so you can see tape related pricing elements.  

https://aws.amazon.com/storagegateway/pricing/

If you don’t have an immediate restore need, by archiving the tapes, you will store data in Glacier at archival storage price. Some more information regarding archiving and retrieving tapes that you can also find at the link below:

https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html

Archiving tapes: When your backup software ejects a tape, your gateway moves the tape to the archive for long-term storage. The archive is located in the AWS Region in which you activated the gateway. Tapes in the archive are stored in the Virtual Tape Shelf, which is backed by Amazon Glacier, a low-cost storage service for data archiving and backup. For more information, see Amazon Glacier.

Retrieving tapes: You can't read archived tapes directly. To read an archived tape, you must first retrieve it to your tape gateway either by using the Storage Gateway console or by using the Storage Gateway API. A retrieved tape is available in your VTL in about three to five hours after you start retrieval.

Please let us know if you have any further questions.

Thank you,
Bhavin"
AWS Import Export Snowball	"Edit bandwidth rate limit - greyed out in AWS console
I need to edit the upload bandwidth rate limit for my storage gateway as it is causing my business cable modem to crash for some reason (working on this from another direction, our 15 public IP address assignment setup is such that I cannot just swap it out with one from another mfg). For some reason ""Edit bandwidth rate limit"" is greyed out in the AWS storage gateway console and I cannot click/select it. Any idea why this is and how to enable it? I can limit the bandwidth through my firewall but would rather put the burden for this on the AWS storage gateway itself."
AWS Import Export Snowball	"Re: Edit bandwidth rate limit - greyed out in AWS console
AnthonyG:
Edit the bandwidth limits will be greyed out in the console if your gateway is offline, you select more that one gateway, or if you have selected a file gateway. The latter gateway type doesn't currently support bandwidth limiting. It's on our roadmap for 2017.
Thanks
Paul"
AWS Import Export Snowball	"Re: Edit bandwidth rate limit - greyed out in AWS console
Is there any update? The option is still unavailable for files.

Thanks,
     -Steve"
AWS Import Export Snowball	"Setup error at Connect to Gateway
I am attempting to setup a file gateway. I have the VM loaded in ESX and the network and time configured. I can successfully ping and connect to the VM over port 80 from my computer using telnet. The Test Network Connectivity and System Resource Checks all pass.

However, when running through the setup wizard in the AWS console, I hit a block. I am on the 3rd step of the wizard, ""connect to gateway"". I put in the IP address, and it tries to send me to the local VM. However, I am getting a HTTP 500 error. I have tried this on Chrome and IE from 3 different computers. Any suggestions?"
AWS Import Export Snowball	"Re: Setup error at Connect to Gateway
BenTempe,

Can you please PM me your AWS account ID, open a support channel and PM me the number.  I've included instructions below.  Thanks.

https://docs.aws.amazon.com/storagegateway/latest/userguide/GatewayTroubleshooting.html#enable-support-access-on-premises"
AWS Import Export Snowball	"Restores from NDMP Source to NDMP destination  no errors, but no data
Hi,
Pretty new to this.  I am running HyperV on Windows 2008 with Symantec Netbackup 7.5.  

Backup works perfectly.  Attempting a restore appears to run without error, completes normally, but no data is in the destination.  Both the source and destination are NetApp filers, and both are using NDMP.  

I have the source filer and the HyperV Client listed as NFS Exports on the destination filer.  

Anyone have any experience with a similar setup?  I am really under the gun here.  Any help would be much appreciated.

Joe"
AWS Import Export Snowball	"Re: Restores from NDMP Source to NDMP destination  no errors, but no data
Hi joe1871

Just to make sure, I see that you mentioned the source and destination are both NetApp filers where is the AWS storage gateway used in your setup?

Regards
Sanjaya-AWS"
AWS Import Export Snowball	"Re: Restores from NDMP Source to NDMP destination  no errors, but no data
THis was user error.  I was not formatting the path for he restore correctly.  I was able to get the restore to work beautifully once I solved this issue.  Thanks.

JB"
AWS Import Export Snowball	"Backup performance and pricing of storage (tape) gateway
I am considering using AWS storage gateway for backups to Amazon S3 but I am concerned about the backup performance and the pricing.  I signed up for an AWS ""free tier"" account so I can do some basic testing and see how it would perform.  I use Veritas backup and they have a ""cloud connector"" to Amazon S3.  I know this is different than storage gateway but hear me out.

As a test, I ran a backup job of 2 GB over a 5 Mbps cable modem connection (upload speed to Internet).  It took a long time but then it eventually failed, haven’t had a chance to look into the reasons why.  Anyway, even with this small backup job, it put me at 2,000 PUT requests and 18,826 GET requests which is almost at the limit of the free tier.  That sure seems like a lot of activity for such a small job.

Then I was doing some more research and it looks like the Amazon Storage Gateway may be a good option for running backups to AWS.  It looks like there are several ways to use the Storage Gateway, the one that I’d be interested in is the Tape Gateway.  I have some questions I am hoping someone can help me answer.

With the Amazon Storage Gateway option, I read somewhere that backups are compressed and only the data that changes is sent to AWS.  I know the time it takes to backup to AWS can depend on a lot of factors but let’s just say that I was to get an Internet connection with a 50 Mbps upload speed and I was trying to do a full backup of 1 TB of data on a daily basis.  Out of that 1 TB, only 20 GB would change on a daily basis.  Let’s also say the data being backed up is mostly file server data (i.e. documents, pictures, videos, etc.) but there's a MS SQL database and also a Microsoft Exchange server database.

1.	How long should I expect for the backup to take?  
2.     How good is the compression on these backups?
3.	If the backup job is configured as a “full backup”, will the storage gateway only upload the 20 GB of changes to AWS after the initial backup?
4.	If I run full backups 3 days in a row, will I be taking up 3 TB of storage on AWS or will I only be taking up the initial 1 TB + 20 GB for Day 2 + 20 GB for Day 3 ?
5.     Would I get charged for the PUT/GET requests when using the Storage Gateway or is that included in the pricing model?

I've been working with an AWS sales rep but he's not really able to give me ANY information on what performance I can expect.  Of course this makes it very difficult to make a decision without knowing how long these backups are going to take and if this is even feasible.  I mean, what's everyone else using for an Internet connection speed and how much data are you guys backing up?"
AWS Import Export Snowball	"Re: Backup performance and pricing of storage (tape) gateway
Gilbert:

Thanks for your detailed question.

1. Tape Gateway throughput is up to 120 MB/s (see https://docs.aws.amazon.com/storagegateway/latest/userguide/resource-gateway-limits.html#performance-limits) which is more than your available upload bandwidth so that would be the rate limiting factor. Assuming you can utilize 100% of your 50 Mbps bandwidth, rough calculations are that the 1 TB full backup will take a little under 2 days, and your 20 GB incremental will take a little under 1 hour.

2. We use industry standard compression algorithms, and the amount of compression will vary depending upon the data being stored, so if you compress the data locally with a common tool then you should see similar levels of compression from SGW. For example, documents typically see high compression ratios whereas videos see much lower ratios.

3. No. If the backup job is configured as a ""full"" and the backup application writes 1 TB then SGW will upload and store 1 TB of data. SGW has no insight into whether you are running a ""full"" or ""incremental"", it is simply providing storage for the backup application.

4. If you run 3 days of full backups at 1 TB per day you will have 3 TB of data stored (less any reduction from compression).

5. No. With SGW you pay $0.01/GB written (with a $125/month cap) and per-GB for storage. See https://aws.amazon.com/storagegateway/pricing/

Thanks
Paul"
AWS Import Export Snowball	"File Share - High Availability
We have a legacy application installed on several Windows EC2 instances . This application works on shared data that is loaded to NFS.  

Since EFS does not work well with Windows EC2 instance, I have planned to use Storage Gateway ( File Share ) which gives the NFS feature by default. This simulated file share seems to work as expected.

However, it appears that the storage gateway is a single point of failure ( in case the Storage gateway EC2 instance goes down or the Region/ Availability zone is offline ). What options do I have in order to mitigate this issue ? 
        1 . Can I assign an Elastic IP to the active instance and re-assign it to the secondary instance when the primary goes offline?
        2 .  Any option of using Elastic load balancer in tandem with this?"
AWS Import Export Snowball	"Configuring Windows Server OS Volume in Storage, Volume Gateway
I have currently configured Storage volume gateway in Mumbai region. I tried creating a local disks, upload buffer and volumes all working fine. Even I tried creating volume with data and its working fine now. By Question is instead of adding D Drive or other drives in volume storage gateway how can I add  OS Drive to Volume gateway and take snapshots. So that incase of OS Failure, I will restore the whole server in the EC2 with my snapshots. Is it possible ? If yes what will be the best practice."
AWS Import Export Snowball	"Snowball questions
Hi. Hopefully these are easy to answer:

1) We have 2 branch offices in the UK that I need to import data from into S3. Can I order a snowball to Office 1, do an import, unplug it, drive it down to Office 2, do an import and then ship it back from Office 2?

2) Our EC2 instances, and S3 buckets are all in the eu-west-1 (Ireland) region. Can I have a snowball from eu-west-1 shipped to the UK, and then ship it back so that the data is in the correct region for S3?
Thanks!"
AWS Import Export Snowball	"Re: Snowball questions
EDITED TO CORRECT #1

Hi there,

1) Yes, minor correction, you can move devices once you receive them, you just can't move the\ devices m outside of the the EU (or just 'country' if for a country not in the EU) where it was received.
2) Yes, you can have a device shipped from eu-west-1 to the UK (currently). From the docs ""For data transfers in the EU regions, we only ship devices to the EU member countries listed following: Austria, Belgium, Bulgaria, Croatia, Republic of Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, Italy, Ireland, Latvia, Lithuania, Luxembourg, Malta, Netherlands, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, Sweden, and the UK.""

Hope that helps!

-Dan

Edited by: Dan@AWS on Jan 21, 2019 10:26 AM"
AWS Import Export Snowball	"Re: Snowball questions
I was wrong on #1, I've edited it with the correct information. Yes, it can be moved. So yes and yes!"
AWS Import Export Snowball	"Re: Snowball questions
Great, thanks for the responds."
AWS Import Export Snowball	"Snowball Export job stuck in Job created for 6 days.
Hi,

I have an export job created on Dec, 28th 2018 (today is Jan, 3rd 2019) and it seems it is stuck in ""Job created"" status. Is this a normal duration? I mean, how long does it take to change from ""Job created"" status to next step (Preparing appliance)? Weeks? Months? 

I searched for more details on aws documentations but looks like no information is available regarding this subject (ETA, Duration). 

Job ID: JID915cca63-b6f9-40a2-9d53-94348ad9b382

Thanks,
Pedro"
AWS Import Export Snowball	"Re: Snowball Export job stuck in Job created for 6 days.
Hi,

For future reference, and in hoping this helps anyone that may face similar situation.

It seems using Forum for this sort of issues, delay in processing snow ball request, may work better instead of opening a ticket in support !?!?!?!?!.

I got response from snowball team and they opened another ticket to address our request (snowball export job).

Best Regards,

Pedro"
AWS Import Export Snowball	"Windows error: Can't read manifest resource at
Trying to run Snowball and getting error with the manifest path.  Tried moving it but no luck?  any ideas? never used this before..

C:\SnowballClient\bin>snowball.bat start -i 192.168.101.105 -m C:\temp -u xxxx-xxxx-xxxx-xxxxx-xxxxx
Can't read manifest resource at C:\temp. Try restarting the client."
AWS Import Export Snowball	"Re: Windows error: Can't read manifest resource at
Are you giving the full path to the manifest file or just saying c:\temp?
Here is a link to the documentation: http://docs.aws.amazon.com/snowball/latest/ug/using-client.html"
AWS Import Export Snowball	"Re: Windows error: Can't read manifest resource at
hello, i went through all the beginning command and when i tried a copy job i get the same error, 
Can't read manifest resource at (location on C in .aws folder/) my manifest is located in the Downloads folder, not sure why it is redirecting the file or can't find it.

Any help is appreciated."
AWS Import Export Snowball	"Running snowball from application
Hi,
I am thinking about ordering snowball machines in order to import data to S3 from several locations.
The people who are going to operate the copy procedure aren't technical, so I am writing an (javascript) application that will help them initiate the process. 

1) I want to get the updated status of snowball  every couple of seconds, In order to display in the application whether snowball connected and how much free space is left,
for this I will use run every couple of seconds:
snowball start and then snowball status - 
snowball start -i ${ip} -m ${manifestFile} -u ${unlockCode} 
snowball status

Does it safe to run snowball start if snowball already started? 
(And might even copying in the same time)
What this is going to return? Success? Error?

2) I want to  show the status / progress of copy procedure to people using the application.
(how many percent of copy completed or how much time left in order to complete the copy)

Does the status written on the display of the hardware?
Does it somehow possible to get this using the command line?


Thanks,
Vitaly"
AWS Import Export Snowball	"Error starting - The Appliance has Timed Out
Hello,

I'm having connectivity issues with our AWS Snowball device, I am repeatedly receiving an error message when powering on: The appliance has timed out. An internal error has occurred...

I've tried all options in the troubleshooting guide from the knowledge centre:
https://aws.amazon.com/premiumsupport/knowledge-center/troubleshoot-connect-snowball/

And have also tried the options listed here in this forum post:
https://forums.aws.amazon.com/thread.jspa?threadID=254483&tstart=50

Any further advice or troubleshooting options to initialize and connect the device to our network would be greatly appreciated.

Many thanks in advance."
AWS Import Export Snowball	"snowball multipart uploading failed using RESTAPI
Hi there. We're using REST API to upload to snowball appliance. The single put/delete operation works fine, however, the multipart uploading failed with ""403 Forbidden"" error (the init multipart also works fine), we're using the same credential. Any idea? Thanks."
AWS Import Export Snowball	"Re: snowball multipart uploading failed using RESTAPI
After turning on DEBUG and checking the snowball adapter log, found out this is signing issue. Identified and fixed the issue."
AWS Import Export Snowball	"Request submitted has been in ""Job Created"" job status since 6/15/2018
We submitted a 50TB import job request with ID JIDdb3f4de8-9285-4329-ac8c-bdecbf4e47bb on 6/15/2018 and has yet to change state. Is there an expected duration for shipment? Mostly asking because it is getting close to the point where it would have been faster to transfer the data over the internet vs. Snowball."
AWS Import Export Snowball	"Re: Request submitted has been in ""Job Created"" job status since 6/15/2018
I can only suggest contacting support. I had the very same problem. Support replies within a day or so. They can't help me immediately but at least kept me informed. I've got a Snowball with a 2 weeks delay."
AWS Import Export Snowball	"Re: Request submitted has been in ""Job Created"" job status since 6/15/2018
Hi there !

I understand that it took some time for your job to be looked into. 

Usually, orders are typically processed the same day or the next unless the service team has questions about the request.

Apologies for the inconvenience caused.

Best,
Meghna

Edited by: MEGHNA-AWS on Nov 16, 2018 12:14 PM"
AWS Import Export Snowball	"Slow Performance / Messages that the client cannot connect to the snowball
We ran the snow ball client in test mode and it estimated approximately 3 days and 17 Hrs. to copy around 40TB of data. 

The actual snowball device has only copied 3.2 TB in approximately 24 Hours. The device is connected to a 10gig switch and a trace route shows that it is only one hop away from the machine with the data. We are getting messages that the device has failed to copy a file, followed by a message saying that the client cannot communicate with the Snowball and to check the IP Address and try again. However, I have a 2nd instance of the client running, and I am able to run a “snowball status” without any issues.  I am also able to run a continuous ping and do not drop any packets. 

I have attached the failed-files long for review.  It seems like when the client encounters a file with a filename / path that it cannot copy, it stops and waits for like a retry period for like 5 minutes. 

Any insight to this issue would be greatly appreciated."
AWS Import Export Snowball	"Re: Slow Performance / Messages that the client cannot connect to the snowball
Hi rich_limu. 

My apologies for the difficulties.  The speed of copy is dependent on a number of factors, the speed of the source storage that is being copied, the speed of your local network, the available computing power of the machine running the Snowball client, and the size of the files being copied.  The error message you are getting however points to another problem.  Can you verify that you have the latest client software?  Also, if you can PM your job ID and I verify the version of the server that you have received.

Frank"
AWS Import Export Snowball	"Re: Slow Performance / Messages that the client cannot connect to the snowball
Frank thanks for the quick response...

Job ID:  has been sent via PM.
Server has 32GB of ram
client is 1.0.1 Build 5712557332"
AWS Import Export Snowball	"Re: Slow Performance / Messages that the client cannot connect to the snowball
It looks like we are getting an error from JavaAssist 

2016-02-01 09:59:41,536 DEBUG mainhttp://io.netty.util.internal.PlatformDependent: Javassist: unavailable
2016-02-01 09:59:41,536 DEBUG mainhttp://io.netty.util.internal.PlatformDependent: You don't have Javassist in your class path or you don't have enough permission to load dynamically generated classes.  Please check the configuration for better performance.

And this Java Error

2016-02-01 10:03:01,002 DEBUG mainhttp://com.amazon.aws.awsie.snowball.netty.SnowballChannel: Got channel.
2016-02-01 10:03:21,005 INFO  mainhttp://com.amazon.aws.awsie.snowball.retrier.RequestRetrier: Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. attempts=1 Sleep for 100 milliseconds.
2016-02-01 10:03:41,107 INFO  mainhttp://com.amazon.aws.awsie.snowball.retrier.RequestRetrier: Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. attempts=2 Sleep for 200 milliseconds.
2016-02-01 10:04:01,311 INFO  mainhttp://com.amazon.aws.awsie.snowball.retrier.RequestRetrier: Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. attempts=3 Sleep for 400 milliseconds.
2016-02-01 10:04:21,714 INFO  mainhttp://com.amazon.aws.awsie.snowball.retrier.RequestRetrier: Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. attempts=4 Sleep for 800 milliseconds.
2016-02-01 10:04:42,517 INFO  mainhttp://com.amazon.aws.awsie.snowball.retrier.RequestRetrier: Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. attempts=5 Sleep for 1000 milliseconds.
2016-02-01 10:05:03,521 ERROR mainhttp://com.amazon.aws.awsie.snowball.retrier.RequestRetrier: Reached max attempts attempts=6, throwing exception
java.util.concurrent.TimeoutException
	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1771)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
	at com.amazon.aws.awsie.snowball.netty.SnowballFuture.get(SnowballFuture.java:60)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty$2.performRequest(SnowballClientNetty.java:202)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty$2.performRequest(SnowballClientNetty.java:193)
	at com.amazon.aws.awsie.snowball.retrier.RequestRetrier.performRequestAndRetry(RequestRetrier.java:57)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.streamRequest(SnowballClientNetty.java:206)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.streamRequest(SnowballClientNetty.java:182)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.sendRequest(SnowballClientNetty.java:164)
	at com.amazon.aws.awsie.snowball.netty.SnowballClient.deleteObject(SnowballClient.java:147)
	at com.amazon.aws.awsie.snowball.netty.SnowballClient.deleteFileIfExists(SnowballClient.java:164)
	at com.amazon.aws.awsie.snowballclient.service.uploadservice.SnowballClientFileVisitor.visitFile(SnowballClientFileVisitor.java:174)
	at com.amazon.aws.awsie.snowballclient.service.uploadservice.SnowballClientFileVisitor.visitFile(SnowballClientFileVisitor.java:35)
	at java.nio.file.Files.walkFileTree(Files.java:2670)
	at java.nio.file.Files.walkFileTree(Files.java:2742)
	at com.amazon.aws.awsie.snowballclient.service.uploadservice.UploadService.traversePath(UploadService.java:493)
	at com.amazon.aws.awsie.snowballclient.service.uploadservice.UploadService.copyToFolderRecursive(UploadService.java:530)
	at com.amazon.aws.awsie.snowballclient.service.uploadservice.UploadService.transfer(UploadService.java:178)
	at com.amazon.aws.awsie.snowballclient.service.uploadservice.UploadService.copy(UploadService.java:120)
	at com.amazon.aws.awsie.snowballclient.service.JobHandler.handleCopyCommand(JobHandler.java:503)
	at com.amazon.aws.awsie.snowballclient.service.JobHandler.run(JobHandler.java:208)
	at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.main(SnowballClientStarter.java:73)"
AWS Import Export Snowball	"Re: Slow Performance / Messages that the client cannot connect to the snowball
I'm seeing the same problem. Were you able to resolve or workaround this? The Snowball client appears to be essentially broken on Windows."
AWS Import Export Snowball	"Re: Slow Performance / Messages that the client cannot connect to the snowball
Hi Nick.  We have made a number of improvements that may address the issues you are seeing.  Can you send me a PM with your job id so we can verify?

Thanks and my apologies for the difficulties here.
Frank"
AWS Import Export Snowball	"Re: Slow Performance / Messages that the client cannot connect to the snowball
We are seeing this right now as well. Is there any publicly posted solution?
I have already PM'd the JobID.
Thanks.

2018-10-31 14:23:41 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 1000 milliseconds.
2018-10-31 14:23:43 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 1000 milliseconds.
2018-10-31 14:23:43 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 1000 milliseconds.
2018-10-31 14:23:44 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 1000 milliseconds.
2018-10-31 14:23:51 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 1000 milliseconds.
2018-10-31 14:23:54 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 200 milliseconds.
2018-10-31 14:23:54 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 800 milliseconds.
2018-10-31 14:23:56 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 1000 milliseconds.
2018-10-31 14:23:56 ERROR RequestRetrier:95 - Reached max attempts https://forums.aws.amazon.com/, throwing exception
java.util.concurrent.TimeoutException
	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1771)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
	at com.amazon.aws.awsie.snowball.netty.SnowballFuture.get(SnowballFuture.java:60)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty$2.performRequest(SnowballClientNetty.java:237)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty$2.performRequest(SnowballClientNetty.java:227)
	at com.amazon.aws.awsie.snowball.retrier.RequestRetrier.performRequestAndRetry(RequestRetrier.java:47)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.streamRequest(SnowballClientNetty.java:251)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.streamRequest(SnowballClientNetty.java:216)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.sendRequest(SnowballClientNetty.java:201)
	at com.amazon.aws.awsie.snowball.netty.SnowballClient.createObject(SnowballClient.java:355)
	at com.amazon.aws.awsie.snowball.netty.SnowballClient.createFolder(SnowballClient.java:378)
	at com.amazon.aws.awsie.snowballsdk.service.internalservice.SnowballObjectCreator.createObject(SnowballObjectCreator.java:88)
	at com.amazon.aws.awsie.snowballsdk.service.internalservice.SnowballObjectCreator.createObject(SnowballObjectCreator.java:49)
	at com.amazon.aws.awsie.snowballsdk.DirectoryUtilities.createFoldersIfNotExist(DirectoryUtilities.java:109)
	at com.amazon.aws.awsie.snowballsdk.SnowballWritableByteChannel.createObject(SnowballWritableByteChannel.java:142)
	at com.amazon.aws.awsie.snowballsdk.SnowballWritableByteChannel.<init>(SnowballWritableByteChannel.java:125)
	at com.amazon.aws.awsie.snowballsdk.SnowballWritableByteChannel.<init>(SnowballWritableByteChannel.java:80)
	at com.amazon.aws.awsie.snowballsdk.SnowballWritableByteChannel$SnowballWritableByteChannelBuilder.build(SnowballWritableByteChannel.java:72)
	at com.amazon.aws.awsie.snowballsdk.service.UploadService.getWritableChannelToSnowball(UploadService.java:72)
	at com.amazon.aws.awsie.snowballsdk.AmazonSnowballClient.getWritableChannelToSnowball(AmazonSnowballClient.java:210)
	at com.amazon.aws.awsie.snowballsdk.AmazonSnowballClient.getWritableChannelToSnowball(AmazonSnowballClient.java:64)
	at com.aws.snowball.adapter.server.netty.handlers.s3.HttpPutContentHandler.initSnowballChannel(HttpPutContentHandler.java:108)
	at com.aws.snowball.adapter.server.netty.handlers.s3.HttpPutContentHandler.handleContent(HttpPutContentHandler.java:138)
	at com.aws.snowball.adapter.server.netty.handlers.AuthorizableRequestContentHandler.handle(AuthorizableRequestContentHandler.java:67)
	at com.aws.snowball.adapter.server.netty.handlers.SnowballAdapterServerRequestHandler.channelRead0(SnowballAdapterServerRequestHandler.java:56)
	at com.aws.snowball.adapter.server.netty.handlers.SnowballAdapterServerRequestHandler.channelRead0(SnowballAdapterServerRequestHandler.java:25)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:267)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1280)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:890)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:564)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:505)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:419)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:391)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:112)
	at java.lang.Thread.run(Thread.java:745)
2018-10-31 14:23:56 ERROR DirectoryUtilities:120 - There was an unexpected problem creating folder"
AWS Import Export Snowball	"Re: Slow Performance / Messages that the client cannot connect to the snowball
Please see PM posted with some questions"
AWS Import Export Snowball	"Speed
So, we dropped a 10gig NIC into our server and connected it straight to the snowball, which has a static IP.  

I am copying data over now.  21TB in total.

I'm seeing transfer speeds of approx 142MB/s.

Is that inline what what I should see?  Honestly, I was expecting much faster on a 10gig direct connect.

Thanks for any feedback you might offer.

Cliff"
AWS Import Export Snowball	"Re: Speed
Hi there,

Is this for a Snowball or Snowball Edge? We've got some performance tips you can find here for maximizing your throughput:

Snowball: https://docs.aws.amazon.com/snowball/latest/ug/performance.html

Snowball Edge: https://docs.aws.amazon.com/snowball/latest/developer-guide/BestPractices.html

In general, I highly recommend that you batch small files to improve your transfer speeds.

Good luck, let us know how it goes!

-Dan"
AWS Import Export Snowball	"Re: Speed
I swear Dan, you are the only guy at AWS who works!  

It's the snowball, NOT the edge.

Yeah, I thought about batching, and jumbo frames, since it's 21TB and mostly some large (>1TB) vmdks.  Then, on second thought, I ""thought"" I'd just go 10gig and get all kinds of fast transfers.

Sadly, not the case.  I've got all weekend though and since it's direct connect, I'm not killing internal bandwidth so I guess I'll review the notes above and then just let it run.

Thanks"
AWS Import Export Snowball	"Re: Speed
Traffic out of Seattle at rush hour tells me that I'm not the only one keeping the lights on at AWS, haha.

Also, just so you know, jumbo frames aren't supported: https://docs.aws.amazon.com/snowball/latest/ug/limits.html

Had to say it before you tried it out and hit a roadblock.

Take care!

-Dan"
AWS Import Export Snowball	"Re: Speed
You too buddy, and thanks."
AWS Import Export Snowball	"Snowball Import Stalled
I'm looking for support/advice on a job that has been stuck at Importing with no progress for nearly 2 days and after having completed only a very small portion of the import (160GB of 25TB).

Thanks"
AWS Import Export Snowball	"Re: Snowball Import Stalled
Hi there, can you PM me the job ID, and I'll see what I can do to route you through to the right support channels?"
AWS Import Export Snowball	"Re: Snowball Import Stalled
PM sent and thanks for your reply to that; I'm looking forward to your findings and will keep this thread up to date to help anyone else that has a similar issue in the future."
AWS Import Export Snowball	"Snowball start command not working
We received a new snowball after our first one was DOA.

It is connected and pulling an IP so I'm trying the start command.  This is what I'm using:

snowball start -i 192.168.0.20 -m  C:\SnowballClient\JID5a64edbd-3687-414a-af6b-c614ae90057f_manifest.bin -u <UNLOCKCODE>

The manifest.bin is in the c:\snowball folder and I'm running the client from the 

C:\Program Files (x86)\SnowballClient\bin

folder.  The error is can't find the resource manifest at path try restarting the client.

I've verified (at least I think I have) that everything is configured correctly, but alas, no luck.  Does anyone see what I may be doing wrong?

FYI - Doing this on Windows Server 2012 R2.

Thank you.

Cliff

Edited by: Dan@AWS on Oct 18, 2018 3:15 PM"
AWS Import Export Snowball	"Re: Snowball start command not working
Hi there,

I commented out your unlock code. You shouldn't share that.

Sounds like your path is wrong. I'm seeing that you shared two potential paths in your message:

C:\SnowballClient\JID5a64edbd-3687-414a-af6b-c614ae90057f_manifest.bin

and

C:\Program Files (x86)\SnowballClient\bin

I'd nail down where the file is saved, use that as the path, and try again.

This is for a standard Snowball, yes? Or is it for a Snowball Edge?"
AWS Import Export Snowball	"Re: Snowball start command not working
Thanks for that!   

I did a reinstall on the client to c:\snowball and dropped the manifest file in there and it's working as expected.  

I appreciate the quick response as well.  Thanks Dan!"
AWS Import Export Snowball	"Re: Snowball start command not working
No worries, happy to help!"
AWS Import Export Snowball	"Snowball errors on all commands but start
I have the linux client install per documentation. ""snowball mkdir"" gives no feedback when run, and logs give  no information other than a command was run. Snowball cp gives error no matter what. Openssl is installed on host CentOS machine. Need to run ""./snowball cp -r /mnt/oldnfs s3://lti-archive"" all combinations fail.

./snowball mkdir s3://lti-archive/oldnfs

./snowball cp -r /mnt/oldnfs s3://lti-archive/oldnfs
./snowball cp -r /mnt/oldnfs/ s3://lti-archive/oldnfs/
./snowball cp -r /mnt/oldnfs/* s3://lti-archive/oldnfs
./snowball cp -r /mnt/oldnfs/* s3://lti-archive/oldnfs/

 ./snowball cp /mnt/oldnfs/blog.sql s3://lti-archive
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: failed to load the required native library
        at io.netty.handler.ssl.OpenSsl.ensureAvailability(OpenSsl.java:154)
        at io.netty.handler.ssl.OpenSslContext.<init>(OpenSslContext.java:127)
        at io.netty.handler.ssl.OpenSslContext.<init>(OpenSslContext.java:121)
        at io.netty.handler.ssl.OpenSslClientContext.<init>(OpenSslClientContext.java:167)
        at io.netty.handler.ssl.SslContext.newClientContextInternal(SslContext.java:733)
        at io.netty.handler.ssl.SslContextBuilder.build(SslContextBuilder.java:223)
        at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.connect(SnowballClientNetty.java:97)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.initializeClient(JobHandler.java:150)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.queryManifestAndInitializeClientMaybeLite(JobHandler.java:134)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.queryManifestAndInitializeClient(JobHandler.java:116)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.handleCopyCommand(JobHandler.java:464)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.run(JobHandler.java:202)
        at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.run(SnowballClientStarter.java:91)
        at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.main(SnowballClientStarter.java:63)
Caused by: java.lang.UnsatisfiedLinkError: /tmp/libnetty-tcnative6224027303078037940.so: /tmp/libnetty-tcnative6224027303078037940.so: failed to map segment from shared object: Operation not permitted
        at java.lang.ClassLoader$NativeLibrary.load(Native Method)
        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824)
        at java.lang.Runtime.load0(Runtime.java:809)
        at java.lang.System.load(System.java:1086)
        at io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:193)
        at io.netty.handler.ssl.OpenSsl.<clinit>(OpenSsl.java:58)
        at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.connect(SnowballClientNetty.java:92)
        ... 7 more"
AWS Import Export Snowball	"Re: Snowball errors on all commands but start
The linux client requires read/write access to the /tmp. Can you verify the user who is trying to run these commands can access that folder."
AWS Import Export Snowball	"Re: Snowball errors on all commands but start
I know this is kinda late but just for future reference for anyone that runs into this issue like I did, just add the flag: -Djava.io.tmpdir=""/var/tmp"" to the ..bin/snowball script right after -Djava.library.path=""${DIR}/../x86_64"".

PS: doesn't have to be /var/tmp you can specify any location of your choice just ensure the dir exists and your user has rw access to it"
AWS Import Export Snowball	"Running user data on ec2 instances on snowball edge
My main question here is whether userData is run everytime I stop and start an existing instance and what methods are available to debug issues regarding userData . Currently, my userdata is not running when I run a new instance, or when i stop/restart it. The only logs generated by the snowball edge are  SUPPORT logs.
ALso, I did check if the script is present on the snowball edge using aws ec2 describe-instance-attribute and can see it perfectly present.
 How do I debug what's happening here?!"
AWS Import Export Snowball	"Re: Running user data on ec2 instances on snowball edge
Hello,

According to EC2 documentation [1], ""By default user data is executed once, at the first boot of the instance."" The steps to execute user data scripts after initial launch can be found here [2]. 

In order to troubleshoot potential issues with user data not running, we would recommend checking the user data on the instance using metadata service. In order to do this please run ""curl http://169.254.169.254/latest/user-data"" from your EC2 instance. This command should display your user data. If user data is displayed, you can check the cloud-init output log file (/var/log/cloud-init-output.log) for errors with the user data script for further troubleshooting.

Thank you for using Snowball!

[1] https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html#user-data-shell-scripts
[2] https://aws.amazon.com/premiumsupport/knowledge-center/execute-user-data-ec2/"
AWS Import Export Snowball	"New to AWS - Support Question
so, a client of mine ordered a snowball and it arrived.  we need some help since it's not working as expected and seems DOA.  

support costs extra and no one seems to reply on here when we make posts.

is this typical AWS support?"
AWS Import Export Snowball	"Re: New to AWS - Support Question
Hi there,

You can find AWS Support here: https://aws.amazon.com/premiumsupport/compare-plans/ and there are multiple plans you can choose, if costs are a concern. The forums are intended for users to use, to troubleshoot issues with the community or share ideas and projects.

I'm a technical writer on the team, and I check the forums from time to time to see if there are issues I can help with. Can you provide more information about the device in question? Is it a Snowball or a Snowball Edge? What specifically isn't working as you anticipate it would?

-Dan"
AWS Import Export Snowball	"Re: New to AWS - Support Question
Thanks for the reply Dan.

We have a 50TB snowball and it appears to be DOA.  The LCD screen is stuck on timeout and repeatedly asks us to retry.  e have scoured the forums and tried pulling the power and network cables, waiting ten minutes, etc, but nothing works.  It's just stuck on the timeout message.

The snowball DID pull an IP as I can see that in my DHCP lease and I can ping it, but all cli commands report that it cannot communicate with the snowball.

We are stuck and not sure what to do."
AWS Import Export Snowball	"Re: New to AWS - Support Question
Mmm. That sounds not great. Can you send me a private message, and I'll reach out to the team to see what can be done?"
AWS Import Export Snowball	"Snowball not pulling IP, well, not performing as expected
We received our snowball and connected it to our network.  It IS pulling an IP as I can see it in my DHCP lease and I can ping it, but the screen shows a timeout and requests a restart.  We have power cycled the device and still no joy.

I have connected a notebook to the switch port and confirmed that I am pulling an IP.

Any suggestions since all I can do is restart and after several attempts, no joy?

Thanks

Cliff"
AWS Import Export Snowball	"Unable to unlock Snowball Edge device
Hi all,

Just got my Snowball Edge device and having trouble unlocking it. ""unlock-device"" would report my device is unlocking, but the device would just go back to its LOCKED state.

$ ./snowballEdge unlock-device
Your Snowball Edge device is unlocking. You may determine the unlock state of your device using the describe-device command. Your Snowball Edge device will be available for use when it is in the UNLOCKED state.
$ ./snowballEdge wait unlocked
Unlocking /ExecutionException - com.amazonaws.waiters.WaiterUnrecoverableException: Resource never entered the desired state as it failed.
$ ./snowballEdge describe-device
{
  ""DeviceId"" : ""JID8c9f1959-8af9-4fda-a565-7fd0ef14ed61"",
  ""UnlockStatus"" : {
    ""State"" : ""LOCKED""
  }
}


Things I've tried so far:
1. Double check to make sure I got the correct manifest/unlock pair.
2. Use the deprecated ""unlock/status"" command.
3. Run latest snowball client on both Linux/Mac. Mac release: snowball-client-mac-1.0.1-230, Linux release: snowball-client-linux-1.0.1-230

One other observation, ""snowballEdge unlock-device"" would sometimes fail with IncorrectDeviceManifestException, but continues to work if I retry the command with no changes:
$ snowballEdge unlock-device
IncorrectDeviceManifestException -  (Service: AWSSnowballDevice; Status Code: 400; Error Code: IncorrectDeviceManifestException; Request ID: 7ac7702b-9765-402a-8d20-721836f7ea23)
$ snowballEdge unlock-device
IncorrectDeviceManifestException -  (Service: AWSSnowballDevice; Status Code: 400; Error Code: IncorrectDeviceManifestException; Request ID: 31266f82-3192-4c91-b875-f39ff4f42b4a)
$ snowballEdge unlock-device
Your Snowball Edge device is unlocking. You may determine the unlock state of your device using the describe-device command. Your Snowball Edge device will be available for use when it is in the UNLOCKED state.


Any suggestions appreciated.

Thanks,"
AWS Import Export Snowball	"Re: Unable to unlock Snowball Edge device
Hi @breez,
I'm facing the same issue. Were you able to fix this?
-Thanks"
AWS Import Export Snowball	"Re: Unable to unlock Snowball Edge device
Hi Kit7,

What issue are you experiencing, specifically? Have you included your manifest, unlock code, and ip address for the device?"
AWS Import Export Snowball	"Re: Unable to unlock Snowball Edge device
The snowball edge currently on premesis got unlocked initially and had been so for more than a week. Then I booted up some ec2 instances and was playing around a bit. The snowball edge locked itself and now won't unlock. This happened with a second snowball edge on prem as well but that unlocked when I tried unlocking it again. Yes, I am using the correct manifest, unlock code and ip address. 

Any idea why this might happen?"
AWS Import Export Snowball	"Re: Unable to unlock Snowball Edge device
In the case of the first one, it sounds like it's been on for too long without any action taken on it. I'd recommend following these instructions here:

https://docs.aws.amazon.com/snowball/latest/developer-guide/troubleshooting.html#connection-troubleshooting

Specifically:

""Power off the Snowball Edge and then unplug all the cables. Leave the device for 10 minutes, and then reconnect it and start again."""
AWS Import Export Snowball	"SnowballEdge File Transfer
Hello,

I have recently used the regular 80 TB snowball to transfer my data. The command to transfer was snowball cp and then source followed by destination. I now ordered a snowballEdge and the command has now changed. So my question is what is the command to transfer the files. 

I am currently using the file interface but I would like to know the command for future reference. 

Thanks"
AWS Import Export Snowball	"Re: SnowballEdge File Transfer
Hi there,

The Snowball Edge has an S3 endpoint for transferring data. This means that you can use the AWS CLI to transfer data, or one of the AWS SDKs, if you're not using the file interface.

Here's some documentation in the Snowball Edge Developer guide to get you started: https://docs.aws.amazon.com/snowball/latest/developer-guide/using-adapter.html

You would still use cp commands, but they would be more like this:

aws s3 cp <path/to/source/source> <path/to/destination/on/Edge>"
AWS Import Export Snowball	"Re: SnowballEdge File Transfer
Thanks for the help"
AWS Import Export Snowball	"Does the new AWS default encryption work with Snowball?
I'd posted a prior question which was answered (https://forums.aws.amazon.com/message.jspa?messageID=808997), indicating Snowball would encrypt automatically if a policy was set on the target bucket denying updates without encryption.

As jor-el (https://forums.aws.amazon.com/profile.jspa?userID=436790) asked as a followup, the docs are not clear on the later addition of default encryption (https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html): does this also work?

In fact the snowball docs (https://docs.aws.amazon.com/snowball/latest/ug/security.html#encryption) still specifically say you have to set the older style policy.

Thanks for clarifying!

Stephen"
AWS Import Export Snowball	"Sending our own Hard Disk for import
Hi, 

I found this blog post from 2009 about sending our own HD:
https://aws.amazon.com/blogs/aws/send-us-that-data/ 

What I couldn't find is the Address which I send the HD to? I looked in the snowball website and didn't find any link that mention the address. 

Can any one help me? 

Thanks,
Yoni"
AWS Import Export Snowball	"Re: Sending our own Hard Disk for import
Hi there,

If you want to import data into the Amazon Cloud you have a number of options:

1) You can create an import job for Snowball, and transfer your data through a powerful workstation that you own onto the device. We'll then import the data into S3 when you return it.
2) You can create an import job for Snowball Edge, and transfer your data through a typical computer, without the need for a powerful workstation. We'll then import the data into S3 when you return it.
3) The Amazon S3 console allows you to upload data directly using a familiar UI. For more information, see https://docs.aws.amazon.com/AmazonS3/latest/user-guide/upload-objects.html.
4) You can use the AWS CLI to upload files and folders directly into Amazon S3.
5) You can establish an Amazon Direct Connect connection and upload data that way.

Thanks for being an Amazon customer,

-Dan"
AWS Import Export Snowball	"Re: Sending our own Hard Disk for import
When trying to create an import job using Snowball UI I only get the option to obtain a device from AWS. There is no option to send my own device to AWS.

https://us-east-2.console.aws.amazon.com/importexport/home?region=us-east-2#/wizard

Are you sure there is an option to import from my own device?"
AWS Import Export Snowball	"Re: Sending our own Hard Disk for import
No, you can't send in your own hard drive with the Snowball service. You would transfer your files from your hard drive, through a computer, on to the Snowball device, and then send the device back to AWS."
AWS Import Export Snowball	"Use snowball adapter with .net sdk
Hi,

I'm trying to import all my data onto s3 using a snowball, but I need to keep track of some metadata for each file. I have some c# code that uploads the files directly to the bucket with this metadata, which is working fine. I thought I would be able to use the snowball s3 adapter to connect to my snowball and upload data onto from my c# code, because this link https://aws.amazon.com/snowball/tools/ says: ""You can use the S3 Adapter with existing Amazon S3 interfaces like the AWS SDKs, the AWS CLI, or your own custom Amazon S3 REST client.""  

I have the adapter running on http://localhost:8090, so I tried setting that as the endpoint for the Amazon3Client by passing in an AmazonS3Config object with that endpoint as the ServiceUrl, but when I try to upload the file using either a PutObjectRequest or the TransferUtility, I get the following exception:

Amazon.Runtime.AmazonServiceException: A WebException with status NameResolutionFailure was thrown. ---> System.Net.WebException: The remote name could not be resolved: ' BUCKET.localhost'
   at System.Net.HttpWebRequest.GetRequestStream(TransportContext& context)
   at System.Net.HttpWebRequest.GetRequestStream()
   at Amazon.Runtime.Internal.HttpRequest.GetRequestContent()
   at Amazon.Runtime.Internal.HttpHandler`1.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.RedirectHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.Unmarshaller.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.S3.Internal.AmazonS3ResponseHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.ErrorHandler.InvokeSync(IExecutionContext executionContext)
   --- End of inner exception stack trace ---
   at Amazon.Runtime.Internal.WebExceptionHandler.HandleException(IExecutionContext executionContext, WebException exception)
   at Amazon.Runtime.Internal.ExceptionHandler`1.Handle(IExecutionContext executionContext, Exception exception)
   at Amazon.Runtime.Internal.ErrorHandler.ProcessException(IExecutionContext executionContext, Exception exception)
   at Amazon.Runtime.Internal.ErrorHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.CallbackHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.Signer.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.CredentialsRetriever.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.RetryHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.CallbackHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.S3.Internal.AmazonS3KmsHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.EndpointResolver.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.S3.Internal.AmazonS3PostMarshallHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.Marshaller.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.S3.Internal.AmazonS3PreMarshallHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.CallbackHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.S3.Internal.AmazonS3ExceptionHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.ErrorCallbackHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.MetricsHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.RuntimePipeline.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.AmazonServiceClient.Invoke[TRequest,TResponse](TRequest request, IMarshaller`2 marshaller, ResponseUnmarshaller unmarshaller)
   at Amazon.S3.AmazonS3Client.PutObject(PutObjectRequest request)


I tried using my local ip address, as well as 127.0.0.1, but all got the same error. When I use the amazon cli to upload the document to the snowball, using --endpoint to change the endpoint, it worked (with both localhost or my local ip), so apparently it's just something about the .net sdk. But I would really strongly prefer to use my existing c# code than to write up a command line program to do this - it won't be so simple from the cli.

Can anyone help me? 

Thanks"
AWS Import Export Snowball	"Re: Use snowball adapter with .net sdk
Hello, i am curious if you ever got this resolved? I am thinking about using Snowball to move some data from on-prem into S3 and using the code i already have in .net to push the files into Snowball would be nice."
AWS Import Export Snowball	"Re: Use snowball adapter with .net sdk
Hi there,

I spoke with the Snowball dev team about this issue and this is what they told me:

The error thrown in the link provided is related to the URL style in which the S3 Bucket is accessed. The client software they have is accessing the bucket provided in virtual-hosted-style URL as specified in https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro which is why the problem is seen. 

They should be using path-style URL to overcome this problem. For this, they have to set ForcePathStyle property of AmazonS3Config to true while creating AmazonS3Client in .NET  https://docs.aws.amazon.com/sdkfornet/v3/apidocs/Index.html 

If you have other questions, please let us know.

Thanks for being an AWS customer!

-Dan"
AWS Import Export Snowball	"File verifications
Hi, 

We're looking to upload about a PB worth of data using Snowball but are having a hell of time figuring out how best to handle file verifications. The digitized files were delivered to us on portable hard drives with MD5 Checksums. This is great... unfortunately, the etags generated for these same files is different on AWS due to the multipart uploading. We have 10's of thousands of large files, will we have to regenerate checksums using the methods described here?: https://www.savjee.be/2015/10/Verifying-Amazon-S3-multi-part-uploads-with-ETag-hash/

We were hoping to compare the MD5 checksum we have (without regenerating) with the etag generated on AWS, but that seems not possible. 

Thanks,
Jay"
AWS Import Export Snowball	"Slow Transfer speeds using Snowball Edge
Hi All,
We just received our Snowball Edge and configured it based on the guides available from Amazon. The Snowball is connected to our main switch with 10GB SFP+ and clients are accessing through 1GB connections. The main reason we wanted the Edge was for the mountable NFS file system so not so technical users could drag their files onto the Snowball Edge over the network just like an SMB share they are used to.

Once the file interface was set up we went ahead and tested this on windows. We mounted the file share as the guide says using the command line and saw that the snowball AWS bucket was mounted. As a test, tried uploading a 4GB file and transfer speeds were maxed out at 20MBps. Since speeds were slow, we directly connected the snowball to a PC via 1GB RJ45. Transfer speeds were exactly the same. While transferring data I opened up the resource monitor and noticed that the network connection speeds were maxing out at 200Mbps \ 20MBps. To rule out Operating Systems also tried the same exact two tests using the latest version of Mac OS. Transfer speeds were even slower at 6MBps.

I know this cant be network related as I've tried moving the same exacted test file over a 1GB connection to a NAS and saw speeds of 980Mbps \ 112MBps

Any insight is greatly appreciated as I am a bit stumped. Thanks."
AWS Import Export Snowball	"Re: Slow Transfer speeds using Snowball Edge
Did you ever get this resolved? Was unable to do anything on the EC2 images on our Snowball Edge due to extremely slow access out of the Snowball Edge. Couldn't even ping websites efficiently. Sent it back and ordered another - which wouldn't boot. Not pleased."
AWS Import Export Snowball	"Re: Slow Transfer speeds using Snowball Edge
Hi there,

The file interface has additional performance considerations that will result in reduced throughput, as you've seen.

The best practices topic in the documentation (https://docs.aws.amazon.com/snowball/latest/developer-guide/BestPractices.html) can help you get the most performance out of your device. For the highest throughput, you'll want to transfer with the S3 Adapter, with small file batching.

Thanks for being an AWS customer,

-Dan"
AWS Import Export Snowball	"snowball adapter in windows
Hi, this is the first time using a snowball and I'm struggling with getting going. 
We are using CloudBerry and it states we need to install the snowball adapter. 

""For Windows, after downloading the zip file
1) Open the directory where the zip file was downloaded and unzip the file.
2) Run command .\snowball-adapter-win\bin\snowball-adapter.bat with required options""

If I try to run the following with correct IP/manifest path/unlock code 

C:\snowball-adapter-win\bin>./snowball-adapter -i x.x.x.x -m C:\xxx_manifest.bin -u x-x-x-x-x

I get...

'.' is not recognized as an internal or external command,
operable program or batch file.

If I run

C:\snowball-adapter-win\bin>snowball-adapter.bat etc etc

I get

""Failed to start server, check logs for more details""

Any ideas what I'm doing wrong? Where does it log to? I can't find any information relating to this.

Thanks

Jim"
AWS Import Export Snowball	"Re: snowball adapter in windows
For windows you'll need to use the bat file. For Linux and mac you will use the ./snowball-adapter command.

Information on the logs can be found in the documentation: https://docs.aws.amazon.com/snowball/latest/ug/snowball-transfer-adapter.html"
AWS Import Export Snowball	"Re: snowball adapter in windows
Any ideas what I'm doing wrong? Where does it log to? I can't find any information relating to this.

I found this post when I ran into the same super unhelpful error message. After digging around a bunch in documentation and on the filesystem, I found that the logs needed to troubleshoot the error were found in ~/.aws/snowball/logs

In my case, that error was: 

2018-08-15 19:42:57 ERROR S3HttpAdapterStarter:90 - java.lang.IllegalArgumentException: profile file cannot be null

For me, the the root cause was that the adapter was not reading credentials from ENV vars like awscli (or boto3) will. I had to populate the ~/.aws/credentials explicitly.

Hopefully this helps the NEXT person trying to troubleshoot."
AWS Import Export Snowball	"stuck at activating file interface...
Hi, have gone through most of the steps to activate the file interface service on the snowballedge but when starting it up it's stuck on ""Activating"" for quiet some time... any ideas what could be causing this? likely a busted unit?"
AWS Import Export Snowball	"Re: stuck at activating file interface...
Hi there,

It's probably not stuck activating. It can take an hour or more to finish activating. Please don't power-cycle the device while it's activating the file interface. The dev team is working on adding some text to the client or the LCD Display to indicate how long it can take to activate. 

As one of the technical writers for the service, I'll work on getting the docs updated to reflect this as well.

Sorry for the inconvenience,

-Dan"
AWS Import Export Snowball	"Re: stuck at activating file interface...
Not used to any service taking hours to start these days so I assumed something must be wrong. Thanks for the info. We're letting it ""activate"" overnight."
AWS Import Export Snowball	"Re: stuck at activating file interface...
That's good feedback, I'll share it with the dev team. 

Thanks!

-Dan"
AWS Import Export Snowball	"Re: stuck at activating file interface...
We left the unit on overnight in activating state and came into to find in inactive state this morning... that's a bit of an annoying catch-22, seems like it takes forever to activate but will timeout and go to inactive... pls tell us what we're doing wrong. We've had the unit for 4 days already and have been struggling with this one seemingly simple step."
AWS Import Export Snowball	"Re: stuck at activating file interface...
waiting doesn't result in the unit becoming active, instead it goes to inactive..."
AWS Import Export Snowball	"Re: stuck at activating file interface...
Can you message me privately and I'll look into the issue with you? That is unusual behavior."
AWS Import Export Snowball	"Snowball appliance delivered, but still listed as 'In Transit'
Need to unblock job so we can get the credentials.

Thanks,
Sean"
AWS Import Export Snowball	"Re: Snowball appliance delivered, but still listed as 'In Transit'
Hi Sean,

It'd be great if you could PM me your Job Id, so that I look it up internally.

Thanks,
Meghna"
AWS Import Export Snowball	"Re: Snowball appliance delivered, but still listed as 'In Transit'
Hi,

I have sent the JobID in a PM.

Thanks,
Sean"
AWS Import Export Snowball	"Re: Snowball appliance delivered, but still listed as 'In Transit'
Hi,

Any update on this?

Thanks,
Sean"
AWS Import Export Snowball	"Re: Snowball appliance delivered, but still listed as 'In Transit'
Anyone able to assist with this? Appliance is onsite, but we can't do anything.

Thanks,
Sean"
AWS Import Export Snowball	"Snowball Edge Not Freeing Up Space From Deleted Files
Hello,

Our Snowball Edge is not returning space from files that we've deleted. We deleted the files with the aws command line tool, running `aws s3 rm --recursive` on the problematic files/directories. The files are no longer present as determined by `aws s3 ls`. Nonetheless, the space used/free as displayed on the panel of the Snowball Edge has not returned the space. 

We fear that if we return the Snowball, these 'ghost' files will be transferred into S3 and we will be charged for them.

We've tried rebooting the Snowball several times and also left the device idle for a day to see if it was slowly deleting the files, but neither has helped restore the free space.

Thanks for any help,
Sheel

Edited by: sheelc on Jul 12, 2018 4:34 PM"
AWS Import Export Snowball	"Re: Snowball Edge Not Freeing Up Space From Deleted Files
After letting it run for another 24 hours, it looks like the data is now slightly less -- so it seems like it's deleting at a rate of about .5TB/hour. It would be quite costly for us to wait the extra days for the deletion so it would still be good to know that these files have perhaps been ""tombstoned"" for deletion, but we're going to send the Snowball back since it seems like we've properly deleted the files."
AWS Import Export Snowball	"Re: Snowball Edge Not Freeing Up Space From Deleted Files
Hi Sheelc.  The deleted files will not be imported.  

Frank"
AWS Import Export Snowball	"Re: Snowball Edge Not Freeing Up Space From Deleted Files
Excellent, thank you for confirming Frank!"
AWS Import Export Snowball	"Snowball copy operation fails?
Hi, I started copying the data to the Snowball. The copy operation was running for many days (slow interface) and unfortunately ends without any status message, summarizing a backup process. 

Client/OS version:
2018-06-29 13:02:10 INFO  SnowballClientStarter:53 - received commandline args: [cp, -r, /fungen/funhome/db04/RUN, s3://fungen-backup/RUN]
2018-06-29 13:02:10 INFO  SnowballClientStarter:159 - SnowballVersion{version='1.0.1', revision='', build='230', dumpTruckCompatibilityVersion='null'}
2018-06-29 13:02:10 INFO  SnowballClientStarter:166 - OS Name: Linux, OS Version: 3.10.0-693.el7.x86_64, OS Architecture: amd64
2018-06-29 13:02:10 INFO  ClientConstant:110 - Total Physical Memory: 61.947425842285156GB
2018-06-29 13:02:11 INFO  JobHandler:265 - Snowball appliance build version: 2018-03-28.5774009395


In the first iteration I want to copy about 23,4 TB of data:
2018-06-29 13:42:53 INFO  JobHandler:265 - Snowball appliance build version: 2018-03-28.5774009395
2018-06-30 18:21:31 INFO  TransferSpeedLog:151 - Total Files: 38,555,354 Total Size: 23.4 TB


During the copying , 4 files somewhere in the middle if the process can't be copied due to issues with file permission:
2018-07-03 16:57:42 ERROR BatchCopyFileVisitor:48 - /fungen/funhome/db04/RUN/140110_SN934_0118_BD11C8ACXX/Thumbnail_Images/L006/C8.1
java.nio.file.AccessDeniedException: /fungen/funhome/db04/RUN/140110_SN934_0118_BD11C8ACXX/Thumbnail_Images/L006/C8.1


The very last message in the log file I've seen was the following:

2018-07-05 10:38:05 INFO  UploadService:718 - [RUN/RUN_part2887.snowballarchives finished. | Read speed: 0.65 MB/s, write speed: 3.86 MB/s ]


And after that, the status message was not updating and I haven't seen any running process in the memory.
The snowball status
 command shows: Used space 19740.41 GB, so less than expected.

I tried to verify the last operation with snowball -v validate
 operation.
After a short while, the operation exited with OutOfMemoryError error,

Is there any way to test if the copy is finished successfully or continue copy operation from the moment it failed?"
AWS Import Export Snowball	"Snowball import directly into S3 IA or S3 IA One-zone
Looking in the snowball docs I couldn't find an answer to the title's question. Is is possibly to import directly to one of the other storage classes? 

Thanks.

Edited by: ldnee on Jun 7, 2018 7:16 AM"
AWS Import Export Snowball	"Re: Snowball import directly into S3 IA or S3 IA One-zone
Hi,

Unfortunately, it is not possible to import data directly to any other storage class, but S3. However, from S3, you can send data to different storage classes, as per your requirement, using Lifecycle Policies. 

Read more about lifecycle policies here : https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html

Hope this helps.

Thanks,
Meghna"
AWS Import Export Snowball	"pause a copy, change network interface, restart copy
Last Friday, I've got a Snowball delivered. Unfortunately I did not have free SFP+ interface available on server site, to connect a Snowball, therefore I've connected it via a Gigabit Ethernet network cable. 
The test runs at acceptable ~200 MB/s rate, but unfortunately  the actual data transfer is currently working at 40 to 50 MB/s speed.
Therefore, my question is the following: Is it possible to stop (pause) current copy process, stop appliance, disconnect it from LAN, connect Snowball using SFP+ network interface, re-initialize appliance and resume copy procedure form the moment I stopped previously.

Thanks in advance,
Evgeniy"
AWS Import Export Snowball	"""Job Created"" status is not changing for 8 days
Update: I've just got an answer from support directly.

06.06.18 I've submitted an import job (JIDaa7a2b4f-0d3e-488d-be58-17ee861f00f1) and it still stays in ""Job Created"" stage. 
How long does it normally take from when an 80T job is created until it is shipped?

Edited by: FunGen on Jun 15, 2018 7:09 AM"
AWS Import Export Snowball	"Can I use Snowball Edge local storage to migrate data between private DCs?
Hi,
we need to move 200TB of storage between the DCs and I am wondering if I can and allowed to use Snowball Edge ""local compute and storage only"" job to do that?

We would load the data using NFS in DC1, move it ourselves to DC2 (same region), unload and then ship back to AWS for erasure.

a) Are we allowed by AWS to do this?
b) After we loaded the box and powered it off, can we access the data again in DC2?

Can't find in the docs if the above scenario is supported. Any feedback is appreciated!

Thank you!

Alex"
AWS Import Export Snowball	"Re: Can I use Snowball Edge local storage to migrate data between private DCs?
Hi,

did you find the answer to your question? 
I would like as well to transfer the data from DC to DC together with shipping it to AWS

Best"
AWS Import Export Snowball	"Re: Can I use Snowball Edge local storage to migrate data between private DCs?
Hi there,

I'm Dan, the tech writer for Snowball. Currently, in the docs, our official answer is ""Moving a AWS Snowball Edge appliance to an address other than the one specified when the job was created is not allowed and is a violation of the AWS Service Terms.""

Link: https://docs.aws.amazon.com/snowball/latest/developer-guide/limits.html

However, I went and had a chat with some folks here and learned that so long as it doesn't leave the country that it was shipped in, everything should be fine. I'll update the docs to reflect this new development.

Hope that helps,

-Dan"
AWS Import Export Snowball	"Re: Can I use Snowball Edge local storage to migrate data between private DCs?
Thanks for answering me!
Actually, in our case, we have just 2 server rooms located in 2 different building within one campus. 
I wanted to do a backup from both servers anyway and in parallel, if it is possible, move the data from one server to another one with the help of Snowball"
AWS Import Export Snowball	"Re: Can I use Snowball Edge local storage to migrate data between private DCs?
No worries, thanks for being an AWS customer! Good luck with your project"
AWS Import Export Snowball	"Snowball Import files from multiple locations
Hi,

I am new to Snowball and having slightly naive question. Unfortunately I did not find the answer in documentation or here.
Our Linux servers located at two different places and I want to copy data from both. Unfortunately we have problems with LAN connection speed and normally it is not very good idea to transfer big amount of data via the network. Therefore I am wandering, if it is possible to connect  1 Snowball sequentially on different computers: first copy data from one computer and then re-connect the Snowball physically to a different computer?

Best"
AWS Import Export Snowball	"Re: Snowball Import files from multiple locations
Hi FunGen. You can absolutely do this.  The Snowball client and the manifest for your particular Snowball will be need to be on each computer when you are performing the copies.

Frank"
AWS Import Export Snowball	"Re: Snowball Import files from multiple locations
Thanks for you help!"
AWS Import Export Snowball	"S3 Glacier Export via Snowball
Hi there,

I would to export from S3 Glacier class via snowball.
I understand the objects need to be restored to S3 Standard prior, but there is not documentation I could find that explain how many days would the items need to stay restored in Standard?
Would like to minimise that time to avoid S3 Standard storage costs, so if I set to 1 day restore for example, will that be sufficient?

Thank you."
AWS Import Export Snowball	"export snowball will not start
Hi All,
I ordered two files, totalling 1.5Tb to be sent to me via Snowball Export.
The Snowball arrived several days ago and I was finally able to try connecting it this morning
When I powered it on, it started to boot (saying 'please wait'), then started flipping it's e-ink display between  'please wait' and a return shiping label, eventually stopping on the return shipping label.
It sure looks like the snowball has a hardware (power supply?) problem.
Do you have any suggestions, or do I just need to ship it back, and if the latter, how do I get a refund?
I don't think it's the datacenter power or network connections , as I've successfyllu used two snowballs before this.
-thanks"
AWS Import Export Snowball	"Snowball cp operation keeps hanging
I'm trying to get 40TB of data into my snowball.  Everything is set up and communications, and I can start cp jobs just fine.  However after about 30 minutes the cp job just blocks on one thread seemingly forever.  I get the following message in the activity log:

2016-04-13 10:09:58 INFO  BlockingThreadPoolExecutor:116 - 1 task still executing and 0 task still in queue. Sleep for 1 seconds....

But it never wakes up again.  during this time, traffic to the snowball stops, so it's not just trying to copy the file and failing.  

I have  Snowball appliance version: 1.0.1 build 5717619902

Client: RHEL7.2 with 10GB RAM running latest release downloaded from was 3 days ago.

Any ideas what I can do to get the client to keep running?"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hi smithian-elp.  I'm sorry to hear that you are seeing these difficulties.  Can you PM me your job id?  I can look into it.

Frank"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Thanks, I PM'ed you.  For some reason the forum only lets me post one message a day.  Still trying to get this fixed, so any help is appreciated.

It looks like I am running into a memory leak.  I have assigned 48GB of RAM to the client machine and still get OOM errors after a few hours of operation.

Exception in thread ""main"" java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:714)
        at io.netty.util.concurrent.SingleThreadEventExecutor.shutdownGracefully(SingleThreadEventExecutor.java:534)
        at io.netty.util.concurrent.MultithreadEventExecutorGroup.shutdownGracefully(MultithreadEventExecutorGroup.java:146)
        at io.netty.util.concurrent.AbstractEventExecutorGroup.shutdownGracefully(AbstractEventExecutorGroup.java:69)
        at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.disconnect(SnowballClientNetty.java:159)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.run(JobHandler.java:217)
        at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.run(SnowballClientStarter.java:90)
        at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.main(SnowballClientStarter.java:62)

Anything I can do to get this to free up memory properly?"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Thanks smithian-elp.  We will contact you to collect some more data to see how we can fix this for you.  Sorry for these difficulties.

Frank"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hello,

we have the same issue with round about 70TB and currently no solution from AWS Support.
How did you fixed it?

best
Ricardo"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Can you try the latest client version 218? How many CPU cores do you have on the machine that you are using?"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hi awsatericn,

no, only 217, we will try and I give you feedback!

Thanks in advance."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hi awsatericn,

we got also with the new build 217 out of memory. Maybe you have access to the Case ID 4933415291 for more informations.

adm@bak ~$ snowball -w 5 -v cp --debug -l -r /home/backup/Storage/www/sites s3://panthermedia-migration/
Operating in debug mode. Logs are located at /home/.aws/snowball/logs
Pre-checking your source files and folders...
error log saved at /tmp/snowball-429639248710678892/failed-files
Files scanned: 100.843.202
For better performance, we suggest that you batch your small files together to speed up your data transfer. Would you like the Snowball client to batch these files automatically? Y/N (Defaults to Y in 60 seconds):Using Batch upload.
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
        at java.io.UnixFileSystem.list(Native Method)
        at java.io.File.list(File.java:1122)
        at java.io.File.listFiles(File.java:1286)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:469)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.listFiles(FileUtils.java:519)
        at org.apache.commons.io.FileUtils.listFiles(FileUtils.java:684)
        at org.apache.commons.io.FileUtils.iterateFiles(FileUtils.java:703)
        at com.amazon.aws.awsie.snowballclient.service.uploadservice.UploadService.copyWithBatchProcessor(UploadService.java:691)
        at com.amazon.aws.awsie.snowballclient.service.uploadservice.UploadService.transfer(UploadService.java:269)
        at com.amazon.aws.awsie.snowballclient.service.uploadservice.UploadService.copy(UploadService.java:182)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.handleCopyCommand(JobHandler.java:602)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.run(JobHandler.java:225)
        at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.run(SnowballClientStarter.java:89)
        at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.main(SnowballClientStarter.java:61)"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hello Ricardo,

Thank your for reaching out to us again.

I see that you have opened the case 4933415291 with us. I apologize for the inconvenience caused. Our internal team and the engineer assigned to your case is actively working on your concern. If you have any other updates please respond to the case so we can assist you further.

We really appreciate you patience and once again I apologize for the inconvenience caused.

Thanks,
Pavya"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
If you have the memory then the quickest solution is to just locally increase the amount of memory that is allocated. To do so just change the -Xmx7G parameter in the snowball script to a higher number.  See attached file as a reference."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hi awsatericn,

also with -Xmx100G we got OOM."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Something is wrong. It probably won't matter how much RAM you allocate it.
Do you have any cyclic loops using symbolic links? I'm wondering if it's looping over the dataset in a loop."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hi, the snowball client should not consider symlinks, at least according to the documentation. There is also no exclude. We do not create a loop ourselves, see also the post of Mar 28, 2018 4:53 AM."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
We tried also the snowball adapter, but it looks more worse then the snowball client.
Each second copy shows us the following error:  An error occurred (InternalError) when calling the PutObject operation (reached max retries: 4)

Also a recursive ""rm"" is not working in the adapter"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
I sent you a private message."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hi there,
I have the same problem, is there a solution for this?"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hello femrich,

I got currently a new version direct from ""awsatericn"". 
It works better, but not perfect.
Please see your ""private messages"""
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
There is a new public build available at https://aws.amazon.com/snowball/tools/ I would recommend that you give it a try. It's more memory efficient when traversing large directory structures and it doesn't follow symlinks (to prevent against cyclical loops)."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hello,

the new build not solved our problems, because it seems to skip all files, if it finds only one of it copied from a other terminal parallel.

adm@bak$ snowball -w 10 cp -r -b /home/backup/Storage/www/sites/xxx/fotodb/org s3://migration/xxx/fotodb/org

Pre-checking your source files and folders...
Files scanned: 32.199.888
OBJECT_ALREADY_EXISTS: The object already exists.

|Average Speed: 0 MB/s |
|Total Bytes Transferred: 0 MB/46,56 TB |
|Total Transfer Time: 2 sec(s) |
|Total Files Transferred: 0/32.199.888 |
|Total Files Skipped: 32.199.888/32.199.888 |

Now the support told us to try a Snowball Edge instead of the normal Snowball, but the Job status is still ""Job created"" since last Friday"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Try this:
adm@bak$ snowball -w 10 cp -r -b -f /home/backup/Storage/www/sites/xxx/fotodb/org s3://migration/xxx/fotodb/org

You don't need a SnowballEdge to get this working. What I sent you in a private message on April 6th applies to this situation. I'll echo those comments in this thread. 

Looking at your messages it looks like you are in a weird state. If you are running into OBJECT_ALREADY exists errors then that means the object has already been copied (or partially copied). Batching get's complicated because the tool doesn't keep track of state. For example - if you have a folder called ""site"" and it batches the contents it will archive the contents and put them on the snowball. but then if you tell it to archive site again it will give you an exception because the files already exist. It doesn't have the ability to easily skip those files because you could have changed the contents within ""site"".

When doing batch it might be helpful to use the -f flag which will force copying the contents. This will assume something has changed and it will replace the contents on the Snowball."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
This message I get from Support about the Snowball Edge:

AWS Support wrote:
Thanks for writing back.
I went through your correspondence about getting a Snowball Edge device, and got in touch with an expert, and he agreed with an affirmative.
I did some research and also found out that Snowball Edge device will bypass the local resources as it has local compute in it."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Snowball Edge is a great device and when dealing with larger files it's really fast. It will also handle the encryption for you which means it doesn't require a high powered workstation to copy the contents. If you want to use a Snowball Edge then I'd strongly recommend batching of your files: https://docs.aws.amazon.com/snowball/latest/developer-guide/batching-small-files.html 

That said, a regular Snowball will also work.  If you add the force flag -f then it should resolve your concern.

Example:
snowball -w 10 cp -r -b -f /home/backup/Storage/www/sites/xxx/fotodb/org s3://migration/xxx/fotodb/org"
AWS Import Export Snowball	"snowball import stuck at 2.6% for >5 days
Why is the import not making progress, and can it be resolved soon?
This is a proof of concept for my company ultimately storing a couple of petabytes, and the delay's not very reassuring.

the job id is:
JID21b7d616-955e-4839-87bd-165caf5580e3"
AWS Import Export Snowball	"Re: snowball import stuck at 2.6% for >5 days
Let me look into this. I'll respond back by the end of the day via a private message"
AWS Import Export Snowball	"Re: snowball import stuck at 2.6% for >5 days
I have responded with a private message."
AWS Import Export Snowball	"Error creating Import Job request.
Hello Everyone,
I have a 10TB external hard drive full of my data that I intend to import and archive in Glacier.
I have followed the documentation of how to create the import job form this link ""https://docs.aws.amazon.com/AWSImportExport/latest/DG/GSCreateSampleS3ImportRequest.html""
but I'm running into java errors when I run the createJob prompt. see below:
""Version: 2014-12-18
Exception in thread ""main"" java.lang.NoClassDefFoundError: javax/xml/bind/DatatypeConverter
	at com.amazonaws.util.Base64.encodeAsString(Base64.java:39)
	at com.amazonaws.auth.AbstractAWSSigner.signAndBase64Encode(AbstractAWSSigner.java:74)
	at com.amazonaws.auth.AbstractAWSSigner.signAndBase64Encode(AbstractAWSSigner.java:63)
	at com.amazonaws.auth.QueryStringSigner.sign(QueryStringSigner.java:95)
	at com.amazonaws.auth.QueryStringSigner.sign(QueryStringSigner.java:49)
	at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:687)
	at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:467)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:302)
	at com.amazonaws.services.importexport.AmazonImportExportClient.invoke(AmazonImportExportClient.java:655)
	at com.amazonaws.services.importexport.AmazonImportExportClient.createJob(AmazonImportExportClient.java:296)
	at ImportExportWebServiceTool.createJob(ImportExportWebServiceTool.java:849)
	at ImportExportWebServiceTool.main(ImportExportWebServiceTool.java:169)
Caused by: java.lang.ClassNotFoundException: javax.xml.bind.DatatypeConverter
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:190)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:499)
	... 12 more
.......................................................""

I've tried to do it a different computer and had no luck there either.
What do I need to do or change to get this going?

PS. I installed the java SE 10 JDK and this is what pops up when I run java -version in the terminal 
""java version ""10"" 2018-03-20
Java(TM) SE Runtime Environment 18.3 (build 10+46)
Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10+46, mixed mode)"""
AWS Import Export Snowball	"Re: Error creating Import Job request.
Hi Godfrey.  My apologies for the difficulties.  Take a look at AWS Snowball, this product was designed for the kind of transfer you want to do.  If you need to use the import/export disk service, try the AWS CLI and its support for import/export disk, https://docs.aws.amazon.com/cli/latest/reference/importexport/index.html.  

I hope this helps.

Frank"
AWS Import Export Snowball	"Re: Error creating Import Job request.
Thanks Frank this was helpful. AWSCLI is where everyone should go to import CreateJob requests."
AWS Import Export Snowball	"Snowball copy stalled many times on different tries
I have been trying to copy over 50TB of data to a 80TB snowball over ethernet.  I tried to copied all the data over at once, but that failed so I started copying sub directories only with smaller size. however, the copy process stalled eventually at 0MB/s with no activity.

Totals: [Speed: 0 MB/s | 3,854,457/8,331,487 files | 2.37 TB/3.77 TB | Remaining time: 7 hour(s) 2 min(s) 50 sec(s) ]

I hit enter many times and it didn't do anything, so I have to hit ctrl-c and cancel it.  It didn't say failure or something.

I tried a smaller directory and have the same result:
Totals: [Speed: 0 MB/s | 2,329,580/3,343,064 files | 967.53 GB/1,019.36 GB | Remaining time: 21 min(s) 45 sec(s) ]

and more examples:

Totals: [Speed: 0 MB/s | 2,329,580/21,906,718 files | 967.53 GB/10.35 TB | Remaining time: 2 day(s) 3 hour(s) 58 min(s) 56 sec(s) ]
Totals: [Speed: 0 MB/s | 3,854,457/20,492,127 files | 2.37 TB/10.22 TB | Remaining time: 1 day(s) 14 hour(s) 52 min(s) 17 sec(s) ]

Since this is using the small files batch switch by default, I tried to get a listing of what is copied over but look like this in the s3 directory in snowball: vd1_part143.snowballarchives so I don't know how to copy the rest of the deltas over since I can't get a list of successfully copied files on the device.

At this point we are at day 12 of received the snowball and we haven't successfully copied any of our data yet in confidence.  Can someone please assist to see why the copy would stuck at 0MB/s after sometime? 

Alfred"
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
Can I get you to try the latest client (currently version 217). You can download it at https://aws.amazon.com/snowball/tools/

Sorry for any inconvenience this might have caused you. To provide more background. It's a UI glitch, when using batched files, where the Speed goes to 0 at the end of the transfer. If you were to pull up a network monitoring tool you should see a lot of traffic still being transferred to the Snowball device. 

When using batch the client archives a bunch of files into snowballarchives. The archives are copied over to the snowball and then auto extracted during ingestion. 

You do not have to download the latest tool. You current tool should work but do wait for the process to finish copying data over. You are also welcomed to download the latest client which will provide better visibility when using batch."
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
OK, confirmed that we are using 214 version of the client.

However, since last posted this message yesterday, the same copy job that I have left running still show this:

Totals: [Speed: 0 MB/s | 2,329,580/3,343,064 files | 967.53 GB/1,019.36 GB | Remaining time: 21 min(s) 45 sec(s) ]

The copy status have not changed for the last 80+gb or so since I posted this 24 hours ago.

I will use the latest version of the client and try again."
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
Something else might be up if you haven't made any progress for a long period of time. You can certainly try the new client.

Another thing to try is to enable the verbose command ie snowball -v cp -r ... https://docs.aws.amazon.com/snowball/latest/ug/using-client-commands.html#clientverbose

Other things to monitor would be the network traffic. If you ever see a drop and it doesn't pick back up immediately then something might be wrong. You can check the logs https://docs.aws.amazon.com/snowball/latest/ug/using-client.html#snowballlogs and potentially forward them along (best to do in a private message)."
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
Finally.  using client version 217 on verbose mode got this copied over.
--------------------------------------------------------------------------------
|Average Speed: 74.32 MB/s                                                       |
|Total Bytes Transferred: 1,019.36 GB/1,019.36 GB                                |
|Total Transfer Time: 3 hour(s) 54 min(s) 5 sec(s)                               |
|Total Files Transferred: 3,343,064/3,343,064                                    |
--------------------------------------------------------------------------------

I am now trying a bigger size folder.  Thanks."
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
OK.  Using version 217 I was able to finish 2 more copy process.  Marking this as closed.  Thanks."
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
re-opened. still have same problem with v217"
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
Hmm. it's also happening on v217 client:

I have 3 parallel snowball cp commands that have stalled: Here's one of them: (Using the snowball -v for verbose output)

Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: [3,591,784/4,914,169 files | 2.16 TB/2.42 TB | Remaining time: 2 hour(s) 44 min(s)

I check on the network interface and there was no network activity there. 

Also checking top: 
PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND
44495 root 20 0 48.840g 0.020t 9988 S 132.7 16.5 5167:28 java
44903 root 20 0 48.366g 0.021t 10080 S 132.7 17.5 5147:45 java
44354 root 20 0 48.920g 0.022t 10192 S 131.7 17.8 5254:20 java 
There are CPU usage on those 3 snowball jobs, but they are just not doing anything.

How else should we troubleshoot this? Thanks."
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
The general guidance is if the CPU is running then let it run.

How long has it been running with high CPU usage and no network activity?
Are there any recent / active errors in the logs: https://docs.aws.amazon.com/snowball/latest/ug/using-client.html#snowballlogs ?

Since you have verbose running you'll know what the last processed file was. Is the list actively cycling between new files? Are the files that it's processing small or large?"
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
I have sent you a pm since the files name are sensitive, can you follow up further with me with pm? Thanks."
AWS Import Export Snowball	"Migrating Terabytes of On-premise Oracle database to AWS Aurora?
We want to migrate Terabyte size of On-premise Oracle database to AWS Aurora. Can we use snowball to migrate that? If yes, how will be migrate the ongoing changes as well to Aurora?"
AWS Import Export Snowball	"Re: Migrating Terabytes of On-premise Oracle database to AWS Aurora?
Yes you can use snowball to do this. There are a couple of options:

1. You can use Snowball to manual extract and copy data over to Snowball and then Snowball will deliver whatever you put on it to S3.

2. There is an AWS  Database Migration tool that can integrate with snowball for doing something that you are looking for. Take a peek at this article: 
https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html
Here is the announcement link: https://aws.amazon.com/about-aws/whats-new/2017/11/aws-database-migration-service-adds-support-for-aws-snowball/"
AWS Import Export Snowball	"Snowball Import Job stuck at ""Job Created""
I created a snowball import job using the 50 TB snowball for me to migrate about 5 TB of data into AWS GovCloud back on 2/22/18.

It has since been stuck in the ""Job Created"" state. 

I'm new to AWS S3 but it seemed like this would be a quicker process.

Am I forgetting something?

Job ID: JID82d88f55-4b84-4032-8e9f-82fdbf1acc7d

Edited by: TylerMBlueFin on Feb 27, 2018 11:42 AM"
AWS Import Export Snowball	"Re: Snowball Import Job stuck at ""Job Created""
Bump! Been a week now and still in ""Job Created"" Is this normal?

This is a time critical task that I need to complete.

Any feedback will be greatly appreciated.

-Thank you
Tyler"
AWS Import Export Snowball	"Import/Export Disk Job Pending at AWS
This seems to happen to a few folk - I too have an export job still 'Pending'. It has been open since Jan 18 which is when it was shipped, arriving at AMZ within a couple of days but nothing has happened since. Not even a received notification. I have clients waiting for the data and it's getting a bit awkward not being able to let them know when they can have it. 

The job id is: EU-YAEBB"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
Can you private message me your jobid?"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
Sent dm with jobid a couple of days ago but still no response from AWS. The job has been in ""pending"" for nearly two weeks :/"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
Still no help on this issue. No update. Still pending. Still really disappointed.

What's the problem?"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
My job is also still pending since early January... What's going on here?"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
Hi mattynet.  My apologies for the delay here, can you PM me your job ID and I can investigate.

Thanks!

Frank"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
Sent!"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
Hi mattynet.  I replied to your PM.

Frank"
AWS Import Export Snowball	"Snowball to Volume Storage gateway... with a catch
How would one utilize a Volume storage gateway, where the storage gateway is on the local network, and get the bulk of the data up to AWS via snowball?

On our local network we will connect to the gateway server (VMware)over iSCSI from Win2016, and it will be an SMB share.

This means, no snapshots, nor ebs volumes. This is not an EC2 storage gateway."
AWS Import Export Snowball	"Disable --batch processing for large repositories
We are wanting to transfer a fairly small (6TB) repository onto our Snowball, but it has a lot of files. We also don't have a lot of memory available on the systems it's practical for us to use, so want to avoid using --batch.

However, there doesn't seem to be an option to turn it off, and the snowball client currently turns it on automatically once the number of files gets to a certain size. And the option to choose ""N"" when the client suggests that it switch to --batch doesn't seem to work.

Is there any way of turning off --batch entirely?"
AWS Import Export Snowball	"Re: Disable --batch processing for large repositories
The workstation specs outline that a lot of ram is needed. https://docs.aws.amazon.com/snowball/latest/ug/specifications.html#workstationspecs 

I would caution you not to disable batching. It will really slow down your transfer time when you have lots of little files. This is especially true during the ingestion process. The performance difference is often 30 or 40 times faster when batch is enabled.

If you do really want to disable batch, when prompted to disable batch and you press N make sure you also press the enter key. That's the only way it receives the command. It will then tell you that batch is disabled.  You can also disable it by use the --quite command but once again it's strongly recommend to keep batch enabled when there are lots of small files."
AWS Import Export Snowball	"How to use Snowball Import with S3 Server Side Encryption (SSE-S3)
Snowball documentation indicates AWS managed Server Side Encryption  (SSE-S3,
http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html) can be used with a Snowball import job (""AWS Snowball supports server-side encryption with Amazon S3–managed encryption keys (SSE-S3).""  from  http://docs.aws.amazon.com/snowball/latest/ug/security.html).

However, I an unable to find specifics about how we enable this.

Can anyone shed light, and (a) confirm this can be done, and (b) provide specifics of how to specify this?

Thanks -

Stephen"
AWS Import Export Snowball	"Re: How to use Snowball Import with S3 Server Side Encryption (SSE-S3)
Hi Stephensykes.

To have your data imported via Snowball using S3 SSE, simply add a policy to your bucket that enforces that all writes are done via SSE and Snowball will follow that policy.  For information about how to create such a policy, please see http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html

Frank"
AWS Import Export Snowball	"Re: How to use Snowball Import with S3 Server Side Encryption (SSE-S3)
Hi Frank -

It sure would be nice if the documentation for snowball said that.  The policy says to reject (deny) updates that do not request encryption.  That Snowball checks the policy and adjusts its writes to not get rejected is a big leap. 

Not saying I don't believe you; just that this is not implied or even hinted at by the Snowball documentation. It's also a bit hard to test without an expensive and time consuming exercise. Can you point me towards documentation on Snowball that confirms this?  Or can you confirm  that your answer is an official AWS statement of support?  (I know, I'm coming across a bit ... stuffy. But it's about to drive a decision which, if it's not correct, is a pretty big deal.)

Regardless I'd urge AWS to update the docs to make this point clear.

Thanks for understanding, and any further backup to make this a simpler call.

Much appreciated -

Stephen"
AWS Import Export Snowball	"Re: How to use Snowball Import with S3 Server Side Encryption (SSE-S3)
Hi there Stephen,

I'm Dan, the technical writer for the Snowball docs. Thanks for your feedback on the docs. I'll update this topic: http://docs.aws.amazon.com/snowball/latest/ug/security.html#encryption and this topic: http://docs.aws.amazon.com/snowball/latest/developer-guide/security.html#encryption to add the the correct information on how to configure the S3 buckets for SSE-S3 as Frank mentioned.

Thanks,

-Dan"
AWS Import Export Snowball	"Re: How to use Snowball Import with S3 Server Side Encryption (SSE-S3)
Thanks - much appreciated.  I'll take this as gospel and we'll move forward.

Best -

Stephen"
AWS Import Export Snowball	"Re: How to use Snowball Import with S3 Server Side Encryption (SSE-S3)
I had a follow up on this question. If default SSE is enabled on the S3 bucket using the console will this still take effect on the Snowball without the policy in place? Is there a way to check if encryption has been applied on a Snowball device? Thanks."
AWS Import Export Snowball	"Stuck on Preparing shipment
Have been waiting almost two weeks as our snowball job is still stuck on: Preparing shipment. Is this typical? I thought it would go out within a day or two?"
AWS Import Export Snowball	"Stream cp upload with s3 adapter, tar, and gzip
I'm attempting to back up a number of folders directly to two snowball devices with the s3 adapter. Each snowball is connected directly to the server, each to a different 1G interface/ip address. There are 2 lists of folders to back up and their sizes. For each line the following is run:
tar -cf - $folder  |  pigz  | aws s3 --endpoint http://$ip cp --expected-size $size - s3://mybucket/$tgz_name

In small tests this worked fine but it is now failing with vague and intermittent errors.

upload failed: - to s3://mybucket/large_folder.tgz An error occurred (InternalError) when calling the CreateMultipartUpload operation (reached max retries: 4): Failed to initiate multipart upload on snowball


Not all uploads failed and I was able to see them with aws s3 --endpoint http://$ip ls 


I tried stopping the snowball adapter and checking the status with the snowball client which showed that the 180GB were used but only one 5GB file was present.

Anybody familiar with setting up the using tar directly to the snowball device or what might cause ls to not show uploaded files?

Thanks."
AWS Import Export Snowball	"Re: Stream cp upload with s3 adapter, tar, and gzip
When you run an LS it will only show you completed files. 

What you are doing looks like it should work. If you need immediate help I would encourage you to open up a support ticket. I do have a few questions that might help in the debugging process:
Does it always fail with the same folder or if you retry the operation does it work on the second attempt?
If you don't pipe to pigz does it work? 
If you first stage to disk does it work?
How big are the tar.gz files?
Are you running multiple copy commands in parallel?

As a side note, for regular Snowball, there is a batch command which will auto batch in the files and then auto explode them during ingest. You can read more about this here https://forums.aws.amazon.com/ann.jspa?annID=5147"
AWS Import Export Snowball	"Re: Stream cp upload with s3 adapter, tar, and gzip
Thank you very much for the response.
Based on some test runs it looks like there is a OutOfMemory error on the snowball when copying files with an expected size over ~1.1TB (a ~1.1TB files copied successfully but 1.15 failed). After failing, files around 800GB that were successful before would also fail until I restarted the snowball. It seems to be working now on the files with an expected size >= 1.1TB but I would like to avoid splitting the larger files if possible. 

Does it always fail with the same folder or if you retry the operation does it work on the second attempt?
It always will fail and after failing, ones that previously worked no longer worked.
If you don't pipe to pigz does it work? 
No
If you first stage to disk does it work?
No
How big are the tar.gz files?
200GB up to ~4TB. I read that S3 will take up to 5TB objects so I expected that anything under this would be handled by snowball.
Are you running multiple copy commands in parallel?
There is 1 command for each snowball device.

There were a few errors while streaming that I've added below.

Errors in start up (always happens):
java.lang.UnsatisfiedLinkError: no netty-tcnative-linux-x86_64 in java.library.path
java.lang.UnsatisfiedLinkError: no netty-tcnative-linux-x86_64-fedora in java.library.path

Errors in copying (happens intermittently but doesn't prevent upload):
2017-12-03 11:31:15 DEBUG SnowballAdapterServerErrorHandler:62 - com.aws.snowball.adapter.server.netty.handlers.s3.SnowballS3AdapterHttpServerException

Error in copying 3TB file:
2017-12-03 17:55:17 ERROR SnowballAdapterServerErrorHandler:74 - Returning internal error to client 
java.lang.OutOfMemoryError: Java heap space
2017-12-03 17:57:18 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 100 milliseconds.
2017-12-03 17:57:19 DEBUG OpenSslEngine:590 - SSL_read failed: OpenSSL error: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC
2017-12-03 17:57:19 DEBUG SnowballClientNettyHandler:128 - javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC
io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC

Error in files since OOM error (fixed after restarting snowball):

2017-12-03 18:39:17 DEBUG SnowballClientNetty$1:127 - Channel acquired: [id: 0xde6a96ea, L:/10.0.0.2:59256 - R:/10.0.0.10:8080]
2017-12-03 18:39:17 DEBUG SnowballChannel:37 - Waiting for channel
2017-12-03 18:39:17 DEBUG SnowballChannel:44 - Got channel.
2017-12-03 18:39:19 DEBUG OpenSslEngine:590 - SSL_read failed: OpenSSL error: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC
2017-12-03 18:39:19 DEBUG SnowballClientNettyHandler:128 - javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC
io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC
Caused by: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC

2017-12-03 18:39:19 ERROR MultipartUploadService:169 - There was an unexpected problem transferring the chunk data to Snowball
com.amazon.aws.awsie.snowball.exceptions.SnowballClientException: io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC
Caused by: io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC
Caused by: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC

2017-12-03 18:39:19 ERROR BufferedPartWritableByteChannel:67 - com.amazon.aws.awsie.snowballsdk.exception.AmazonSnowballException: Unable to complete operation. Root 
cause is SNOWBALL_SERVER_ERROR; ErrorType: SNOWBALL_SERVER_ERROR
2017-12-03 18:39:19 DEBUG SnowballClientNetty$1:122 - Channel released: [id: 0xde6a96ea, L:/10.0.0.2:59256 - R:/10.0.0.10:8080]

2017-12-03 18:39:20 DEBUG SnowballClientNetty$1:127 - Channel acquired: [id: 0xde6a96ea, L:/10.0.0.2:59256 - R:/10.0.0.10:8080]
2017-12-03 18:39:20 DEBUG SnowballChannel:44 - Got channel.
2017-12-03 18:39:20 INFO  RequestRetrier:88 - Encountered error when making request: com.amazon.aws.awsie.snowball.exceptions.SnowballClientException - io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC. Retrying. https://forums.aws.amazon.com/ Sleep for 100 milliseconds.
2017-12-03 18:39:21 INFO  RequestRetrier:88 - Encountered error when making request: com.amazon.aws.awsie.snowball.exceptions.SnowballClientException - io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC. Retrying. https://forums.aws.amazon.com/ Sleep for 200 milliseconds.
2017-12-03 18:39:21 INFO  RequestRetrier:88 - Encountered error when making request: com.amazon.aws.awsie.snowball.exceptions.SnowballClientException - io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC. Retrying. https://forums.aws.amazon.com/ Sleep for 400 milliseconds.

2017-12-03 18:44:27 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 1000 milliseconds.
2017-12-03 18:44:33 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 100 milliseconds.

Edited by: jor-el on Dec 4, 2017 11:15 AM"
AWS Import Export Snowball	"Re: Stream cp upload with s3 adapter, tar, and gzip
It sounds like you are using Snowball w/ the S3 Adapter and not Snowball Edge?

The Snowball should handle files less than 5TB (parity w/ S3 in the cloud). Using the S3 adapter on Snowball has the potential to overwhelm a Snowball esp if you are using very large files.  If the Snowball is running out of memory, then I'd recommend to play with is the number of parallel threads (max_concurrent_requests) and the chunk size (multipart_chunksize). You can read more about that here  http://docs.aws.amazon.com/cli/latest/topic/s3-config.html. I would tune one or both of the parameters down to relieve some pressure on the Snowball.

The other option to try is to use the native client instead of the S3 Adapter. That will automatically scale down the chunk sizes to prevent overwhelming the Snowball."
AWS Import Export Snowball	"Re: Stream cp upload with s3 adapter, tar, and gzip
It sounds like you are using Snowball w/ the S3 Adapter and not Snowball Edge?  
The other option to try is to use the native client instead of the S3 Adapter. That will automatically scale down the chunk sizes to prevent overwhelming the Snowball.
I am. I'm using the adapter because from what I can tell I can't stream to the s3 client but I'll try that next for the larger files.

I'd recommend to play with is the number of parallel threads (max_concurrent_requests) and the chunk size (multipart_chunksize. http://docs.aws.amazon.com/cli/latest/topic/s3-config.html. I would tune one or both of the parameters down to relieve some pressure on the Snowball.
I will give this a shot.

Thanks for the help. I'm going to let this run for now on the smaller files since its  and will write back the results of the larger files later."
AWS Import Export Snowball	"Re: Stream cp upload with s3 adapter, tar, and gzip
I'd recommend to play with is the number of parallel threads (max_concurrent_requests) and the chunk size (multipart_chunksize).
Also, I haven't tried it yet but I think the --expected-size option modifies these values. Is there any downside to setting the max_queue_size to 20000 or the expected size to 1TB regardless of real size?"
AWS Import Export Snowball	"Re: Stream cp upload with s3 adapter, tar, and gzip
Changing the max_queue_size is fine, that is only a client side change. expected-size can change the chunk size and to some degree you need it to be high. You are limited to 10k chunks which translates to 100 MB chunks if you use all 10k parts for a 1 TB file. When dealing with that large of a file it's probably best to lower the concurrent requests. My recommendation is to turn it down to less than 10 threads (try 5) when dealing with files larger than a terabyte. 

You are correct in that the Snowball client tool doesn't support streaming (it cannot read from stdin). But it is pretty quick and if you have lots of files the new batch command works well. It also does a really good job in the memory management for large files (it's much harder to overwhelm a Snowball with the client tool). It looks like you already have a source folder and you could tell the client to copy that folder onto the Snowball. The main difference is that the files won't be in a single tar file when they arrive in S3."
AWS Import Export Snowball	"Export failing with too many small files
Our snowball job was cancelled after waiting for over a month.  I'm not getting anything from support on what went wrong other than the problem is too many small files.  They suggested ""to zip these files using an ec2 instance to accelerate the transfer,"" but I've had no luck trying to decipher that into an actual list of commands to run.  Can anyone shed some light on what I can do to make this happen?"
AWS Import Export Snowball	"Snowball Ethernet Issue
I'm having an issue with an AWS Snowball that I recently received. Its not getting assigned an IP via DHCP and even being assigned a Static IP, I can't ping or telnet to the device. This is for the RJ45 Ethernet port. The issue I'm having is exactly the same as reported in this post:

https://forums.aws.amazon.com/thread.jspa?messageID=780713򾦩

As he reports that the right LED indicator on the Snowball's RJ45 Ethernet port keeps blinking constantly, even while unplugging the cable, I am experiencing this same thing as well. 

I've tried all the same troubleshooting steps as he did to no avail.

Tried different RJ45 cables, and powercycled the device per the instructions listed here:
https://aws.amazon.com/premiumsupport/knowledge-center/troubleshoot-connect-snowball/

Can you please provide assistance and how I should handle this?

Thanks."
AWS Import Export Snowball	"Snowball delivered but UPS still has it 'in transit'
Hi,
my Snowball was delivered but the UPS tracking site still says in-transit.

Can't continue until the state is changed.

Thanks,
Sean"
AWS Import Export Snowball	"Re: Snowball delivered but UPS still has it 'in transit'
Hi SeanQ.  Please PM me your job id so that I can investigate.

Thank you.

Frank"
AWS Import Export Snowball	"Re: Snowball delivered but UPS still has it 'in transit'
PM'd job ID.

Thanks,
Sean"
AWS Import Export Snowball	"Re: Snowball delivered but UPS still has it 'in transit'
Hello Sean,
Thank you very much. We will get back to you with further details.

Najah"
AWS Import Export Snowball	"Re: Snowball delivered but UPS still has it 'in transit'
Hi Sean.  Sorry again for the delay, you should be unblocked now.

Frank"
AWS Import Export Snowball	"Create a job for direct import to Glacier
I want to transfer 25TB on-prem data to aws glacier. How to import my whole data from snowball to glacier directly without uploading on S3 and how import daily recursive data to glacier over night."
AWS Import Export Snowball	"Re: Create a job for direct import to Glacier
Hi there,

There's no method through which data can be imported from a Snowball into Amazon Glacier without going through Amazon S3 lifecycle rules. Sorry about that.

It also sounds like you've got a question about daily transfers from a Snowball into AWS. This would a challenge, as the shipping speeds (1 day, 2 day, express, standard, etc) are not measured in how soon the Snowball or Snowball Edge would appear at your address after you ordered the job, but how soon the device would get to the address after the job has been processed. Processing can take anywhere from a few hours to a few days, depending on time of day, day of the week, and type of job.

If you have other questions, let us know!

-Dan"
AWS Import Export Snowball	"Re: Create a job for direct import to Glacier
Thanks for your reply Dan, If i choose to go through S3, what would be the process for that? Is it like, I ordered a snowball, copy my data to snowball, return snowball and import the same to my S3 bucket, put transition period of 1day on bucket for transition to glacier and import the data on glacier.
Furthermore, on daily basis, I zip my data, import to s3 and again transfer to glacier. 
Will my uploaded data be browsable or not?
Approx how much is it cost for 25tb complete job?
Can i put my daily data on same glacier bucket?"
AWS Import Export Snowball	"Re: Create a job for direct import to Glacier
ITSupport wrote:
Thanks for your reply Dan, If i choose to go through S3, what would be the process for that? Is it like, I ordered a snowball, copy my data to snowball, return snowball and import the same to my S3 bucket, put transition period of 1day on bucket for transition to glacier and import the data on glacier.

That process sounds about right to me. Here's a link to some documentation about how to configure a lifecycle rule for S3 using the console: http://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html

ITSupport wrote:
Furthermore, on daily basis, I zip my data, import to s3 and again transfer to glacier. 

You can continue to repeat that process as often as you like, but keep in mind that a daily import is highly unlikely due to the factors I called out in my previous message.

ITSupport wrote:
Will my uploaded data be browsable or not?

Here's some documentation on how to get the inventory of a vault: http://docs.aws.amazon.com/amazonglacier/latest/dev/vault-inventory.html

ITSupport wrote:
Approx how much is it cost for 25tb complete job?
Can i put my daily data on same glacier bucket?

I'm not familiar with the pricing data, sorry. Here's a link to the pricing information for both Amazon S3 and for Glacier:

https://aws.amazon.com/s3/pricing/
https://aws.amazon.com/glacier/pricing/

ITSupport wrote:
Can i put my daily data on same glacier bucket?

Amazon Glacier vaults can hold any amount of data, each archive in a vault can hold 40 TB of data.

Hope that helps!

-Dan"
AWS Import Export Snowball	"Deleting Snowball Files
Hey All,

Im looking at trying to figure out how to delete all of my files that i have on my snowball and restart the import process, is there a way to delete the files so that i can upload a different file? Basically i need to start fresh without having to send the snowball back and get a new one. 

When i try snowball rm -r it deletes the destination path but says that the file space is still there.

The file disappears from site and when i run ls it doesn't show that anything is there but if i do a snowball status it still shows that the space is full.

If this is possible to reset the snowball i would love the help."
AWS Import Export Snowball	"Re: Deleting Snowball Files
Hi there,

The 'rm -r' command was the correct one to run to remove your files. This operation can take some time, but the 'ls' command will show you what the state will be when the deletion of those files finishes. Try the status command again and then again in an hour and you should see the used space decreasing.

Let us know if you have any other questions,

-Dan"
AWS Import Export Snowball	"Re: Deleting Snowball Files
Its been a couple days now, and while the files won't appear when doing an ""ls"", the free space has remained the same.

Is there any way to reformat or somehow ""reset"" a snowball?

Thanks!"
AWS Import Export Snowball	"Re: Deleting Snowball Files
Hm. Can you try rebooting the Snowball, unlocking it again, and letting us know if the status command returns the correct size?"
AWS Import Export Snowball	"snowball client ""validate"" command reports apparently random number of file
Environment
 * Centos6 Linux, 
 * snowball client version  1.0.1 build 82

Precondition
copied 594 files totalling 22TB to the device using snowball cp command; the copy finished with no obvious complaint in the logs. Performing `snowball ls` shows the 594 files in the target bucket/prefix:
[root@hpchsmlan01 bin]# ./snowball ls s3://private-a17037da-d031-43e8-a3a7-f7c800c34ab3/EGA_PNET/ |wc -l
594


Action
Validate the destination directory
./snowball validate s3://private-a17037da-d031-43e8-a3a7-f7c800c34ab3/EGA_PNET


Expected behaviour
According to docs at http://docs.aws.amazon.com/AWSImportExport/latest/ug/using-client-commands.html performing snowball validate <path>
 ""If you specify a path, then this command validates the content pointed to by that path and its subdirectories"". 
So I would expect a report on 594 files.

Actual behaviour
The number of files reported is not 594, and indeed it changes (up or down) from run to run of validate command even though there are no background copy/move/delete operations going on:
[root@hpchsmlan01 bin]# for i in `seq 1 3`; do date; ./snowball validate s3://private-a17037da-d031-43e8-a3a7-0c34ab3/EGA_PNET |dos2unix; done
Mon Aug 29 12:30:18 AEST 2016
Validating files on Snowball
 
[Total: 1                     Successful: 1                 Invalid: 0                    Incomplete: 0]           [Total: 268                   Successful: 268               Invalid: 0                    Incomplete: 0]                Mon Aug 29 12:30:23 AEST 2016
Validating files on Snowball
 
[Total: 1                     Successful: 1                 Invalid: 0                    Incomplete: 0]           [Total: 264                   Successful: 264               Invalid: 0                    Incomplete: 0]                Mon Aug 29 12:30:26 AEST 2016
Validating files on Snowball
 
[Total: 1                     Successful: 1                 Invalid: 0                    Incomplete: 0]           [Total: 49                    Successful: 49                Invalid: 0                    Incomplete: 0]           [Total: 49                    Successful: 49                Invalid: 0                    Incomplete: 0]           [Total: 49                    Successful: 49                Invalid: 0                    Incomplete: 0]           [Total: 127                   Successful: 127               Invalid: 0                    Incomplete: 0]           [Total: 470                   Successful: 470               Invalid: 0                    Incomplete: 0] 


Comment
Supplying the individual item paths to `snowball validate` does work for each of the 594 files on the device."
AWS Import Export Snowball	"Re: snowball client ""validate"" command reports apparently random number of file
Can you download the latest version of the client (build 83) for Linux. The was a bug fix to the validator code in build 83. Will you let me know if that works."
AWS Import Export Snowball	"Re: snowball client ""validate"" command reports apparently random number of file
I will download build 83; unfortunately I can't check whether the fix has resolved the issue in this particular case because that Snowball unit has been returned. We will have another onsite soon though, I'll let you know how validate command works then."
AWS Import Export Snowball	"Re: snowball client ""validate"" command reports apparently random number of file
Re-tested with new snowball and new client version; `validate` now works as advertised.

Thanks!"
AWS Import Export Snowball	"Re: snowball client ""validate"" command reports apparently random number of file
While am running Validate command am getting ""FAILED TO VALIDATE OBJECT"" error for few number of fails. 

What I will do for this time? 

Please share your knowledge on this ,thanks in advance."
AWS Import Export Snowball	"Java Exception when copying from Snowball
I'm getting an occasional error when copying from a snowball. I'm using OS X 10.11 El Capitan, with the latest version of the snowball client 1.0.1 Build 146. 

What's going on and what can I do to fix this?

--
2017-08-31 08:50:59 ERROR TreeHashCalculator:156 - There was an unexpected problem finalizing the checksum future
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:192)
        at com.amazon.aws.awsie.frozen.crypto.TreeHashCalculator.calculatedChecksum(TreeHashCalculator.java:153)
        at com.amazon.aws.awsie.frozen.crypto.TreeHashCalculator.validate(TreeHashCalculator.java:138)
        at com.amazon.aws.awsie.snowballclient.service.downloadservice.DownloadReader.validateDownload(DownloadReader.java:150)
        at com.amazon.aws.awsie.snowballclient.service.downloadservice.DownloadReader.performParallelReading(DownloadReader.java:129)
        at com.amazon.aws.awsie.snowballclient.service.downloadservice.DownloadService.downloadToFile(DownloadService.java:347)
        at com.amazon.aws.awsie.snowballclient.service.downloadservice.DownloadService.traverseHelper(DownloadService.java:221)
        at com.amazon.aws.awsie.snowballclient.service.downloadservice.DownloadService$1.run(DownloadService.java:183)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at com.amazon.aws.awsie.frozen.crypto.TreeHashCalculator$ChunkChecksum.access$300(TreeHashCalculator.java:213)
        at com.amazon.aws.awsie.frozen.crypto.TreeHashCalculator$1.call(TreeHashCalculator.java:108)
        at com.amazon.aws.awsie.frozen.crypto.TreeHashCalculator$1.call(TreeHashCalculator.java:72)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        ... 1 more
--"
AWS Import Export Snowball	"Snowball client copy is very slow
I'm running a snowball recursive copy, and it's running very slowly. I'm getting about 60KB/s transfer rate over a 1Gbit/s connection. The Snowball and the client machine are connected to the same switch. What can I do to diagnose this? 

I've found that if I run 2x copies in parallel, the bandwidth used goes up a little to about 80KB/s."
AWS Import Export Snowball	"Re: Snowball client copy is very slow
Hi there,

We have some guidance in the documentation on how to improve your performance with the Snowball client: http://docs.aws.amazon.com/snowball/latest/ug/BestPractices.html

Let us know if that helps

-Dan"
AWS Import Export Snowball	"Re: Snowball client copy is very slow
I ended up writing a script that copied multiple directories off the snowball in parallel. This has sped up the copy process significantly."
AWS Import Export Snowball	"Import/Export Disk Job Pending at AWS
Hi,

I have 4 export disk jobs and the hard drives arrived at AWS on Jul 28, 2017. 3 of the jobs have been completed and the hard drives have been shipping back. However, the remaining one (Job ID: F347B) is still showing pending for almost 3 weeks. Can someone help me check the status and possibly move it along the process?

many thanks,
Patricia"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
Hello Patricia,

Sorry for the delay. 

The job has been complete and drive should be arriving shortly back to you. Hope this helps.

Thanks
Dilip S."
AWS Import Export Snowball	"Snowball listing times out
Hello.
I have a huge amount of documents being transferred to snowball (3.48 million). I am facing two issues in my snowball transfer. 
A) I'd like to ensure the files I copied are in good state for which I ran ""snowball -v validate > validate_output.log""
This ran for an hour or so and showed some meaningless message on the screen and exited. I have a total of 3.48 million files already transferred but it shows ""Total: 23940                 Successful: 23940"". Does not make sense. This operation did not generate any useful debugging info/error.

B)  For reconciliation/verification purposes, I need to check the number of documents that are transferred. For which i executed ""snowball ls s3://mybucketname > list_output.log"" . But 
1. it seems to be very slow (Took about an hour)
2. After an hour, it listed around 62000 and timed out. How do I get all 3.48 million objects to be output?

I have attached the log file for both the problems. Can you please help? Any input is greatly appreciated."
AWS Import Export Snowball	"Re: Snowball listing times out
Hello Baskar,

snowball -v validate command might take some time to complete, and might appear to be stuck from time to time. This effect is common when there are lots of files, and even more so when files are nested within many subfolders. I would recomend rerunning it and leaving the console running.

With regards to snowball ls command, unfortunately there is no easy way to do this recursively. The snowball ls command lists the Snowball contents in the specified path. You can run snowball ls s3://mybucket/myfolder.. and accumulate the output.

Also if you could send me the actual log files including the ones from logs location as a PM would help.
http://docs.aws.amazon.com/snowball/latest/ug/using-client.html#snowballlogs

Thanks,
Dilip S."
AWS Import Export Snowball	"Snowball edge not powering on
Hi folks

We've received a Snowball edge to start bulk data uploads with, but since receiving it, plugging it into power and giving it a network cable, it has not switched on.  The kindle display shows nothing.  the e-ink label does show the delivery address still.

We have tried short, medium and long presses of the power button, but nothing seems to kick it into gear.  The machine itself seems to have power (NIC connectivity, PSU is slightly warm)

Are there any tricks to get it going, at least to a state where it might tell us where to call for support, as I don't believe we should pay for support if the device is faulty.

Many thanks,
Joe"
AWS Import Export Snowball	"Re: Snowball edge not powering on
Hello Joe,

I have opened a Support case on your behalf to help investigate this issue. If you could login to your AWS Console and go to Support Center you should be able to see the Support Case.

I would appreciate if you could help us with the details requested and continue our communication over the support case.

Thanks for understanding.

Dilip S."
AWS Import Export Snowball	"Snowball export EBS snapshot in S3
I'm trying to export an EBS snapshot in an Snowball job, but since the EBS snapshots are not user-visible, the Snowball job interface doesn't show them. Is there a way to do this or do I need to somehow transfer the data from EBS over to a normal S3 bucket so I can get at it?"
AWS Import Export Snowball	"Re: Snowball export EBS snapshot in S3
Hi Stephen I.  AWS Snowball is used to transfer data into and out of S3 so you will need to copy your data to an S3 bucket in order to export it with Snowball.

Frank"
AWS Import Export Snowball	"UPS says delivered but Job Status says NotReceived
Hi, 

UPS Singapore say it delivered on Wednesday, 16 August, but the Job Status has LocationCode: NotReceived. Shipping was from Singapore to Singapore.
This our own drive i.e. Import/Export, not Snowball
JobID: AP-DQY7V
Thanks
Tony"
AWS Import Export Snowball	"Import/Export Disk Stuck in Limbo at AWS
I don't see a forum for Import/Export Disk (as opposed to Import/Export Snowball), so I'm posting here.

I successfully created a job (BJ53J), shipped my drive to AWS, and it arrived 13 days ago. get-status reports ""Your device is at AWS.	Pending	The specified job has not started.""

My understanding was the import usually happens within a day of receipt. Any way to figure out what's going on? I don't have an AWS developer account so can't open a ticket.

Thanks,
Adam

Edited by: adam500 on Nov 17, 2015 11:14 AM"
AWS Import Export Snowball	"Re: Import/Export Disk Stuck in Limbo at AWS
Hi Adam,

Apologies for the delayed start of job BJ53J. I've asked the the Import/Export team to investigate. I/E Jobs normally start quite soon after we receive the storage device - a two week delay is quite unusual.

Kind regards,
Chris"
AWS Import Export Snowball	"Re: Import/Export Disk Stuck in Limbo at AWS
Hi Adam,

I see that job BJ53J has completed. Apologies again that it took much longer than expected.

Kind regards,
Chris"
AWS Import Export Snowball	"Re: Import/Export Disk Stuck in Limbo at AWS
I'm having a similar issue, JobId: E8JHV

It's been InProgress since 4/21 and we really need the devise back.

Let me know what to do, thanks."
AWS Import Export Snowball	"Re: Import/Export Disk Stuck in Limbo at AWS
Hi nickmerwin,

I can see that job E8JHV completed on April 30th but that your device hasn't shipped back.

I've asked engineering to follow this up as a matter of urgency.

We'll post back when we know more.

Richard"
AWS Import Export Snowball	"Re: Import/Export Disk Stuck in Limbo at AWS
Hi Nick,

Your device has been shipped now. My apologies for the delay.

Richard"
AWS Import Export Snowball	"Re: Import/Export Disk Stuck in Limbo at AWS
I also have a similar issue. 

I have an export job ID: F347B that the hard drive has arrived for 3 weeks. But the job still has not started.  

JobId:                       F347B
CreationDate:                Fri Jul 21 10:20:54 PDT 2017
JobType:                     Export
LocationCode:                AtAWS
LocationMessage:             Your device is at AWS.
ProgressCode:                Pending
ProgressMessage:             The specified job has not started.
ErrorCount:                  0
LogBucket:                   null
LogKey:                      null
Carrier:                     null
TrackingNumber:              null

We really need the data exported and the device back to us. Would you please me let me know what to do?

many thanks,
Patricia"
AWS Import Export Snowball	"Copy to Storage Gateway Volume
Is it possible to copy data from a snow ball into a Storage Gateway volume?  We are looking to populate a storage gateway with 17 TBs of backup files that will be used as the basis for additional backups moving forward."
AWS Import Export Snowball	"Re: Copy to Storage Gateway Volume
Yes. Once the Snowball job is complete and the data is in S3:
1. Spin up an EC2 instance and attach an EBS volume
2. Copy the data from S3 to the EBS volume
3. Take a snapshot of the EBS volume
4. Create a SGW volume from the EBS snapshots.
The data you imported through Snowball will now be available on the SGW volume."
AWS Import Export Snowball	"Re: Copy to Storage Gateway Volume
How does this change when you are shipping 100+TB's on multiple Snowballs, considering EBS volumes have a maximum capacity? Will you be able to create the SGW volume with multiple snapshots from those different EBS volumes?"
AWS Import Export Snowball	"Re: Copy to Storage Gateway Volume
The maximum size of a EBS volume is 16 TB. While Storage Gateway support cached volumes up to 32 TB. You can't create a volume from multiple EBS snapshots in either SGW or EBS. So, you need to partition your data into 16 TB tranches to use the above method.

If you wanted to take advantage of the larger SGW volumes, you would need copy to an SGW cached volume on an EC2 Storage Gateway rather than an EBS volume (step 2 above). The steps for this would be:

1. Create an EC2 Storage Gateway (http://docs.aws.amazon.com/storagegateway/latest/userguide/ec2-gateway-common.html)
2. Create a cached volume (up to 32 TB) on this Storage Gateway (http://docs.aws.amazon.com/storagegateway/latest/userguide/GettingStartedCreateVolumes.html)
3. Spin up an EC2 instance and connect to this cached volume over iSCSI (http://docs.aws.amazon.com/storagegateway/latest/userguide/GettingStarted-use-volumes.html#GettingStartedAccessVolumes)
4. On this instance, copy the data from S3 to the iSCSI attached volume.

If you want to access this data from an on-premises SGW you'd then follow the same process as before:

5. Take a snapshot of the SGW cached volume on the EC2 gateway (http://docs.aws.amazon.com/storagegateway/latest/userguide/managing-volumes.html#CreatingSnapshot)
6. On the on-premises SGW, create a cached volume from this EBS snapshots.

Another option if both on-premises and EC2 gateways were in the same AWS Region, is to clone the SGW volume and skip creating the EBS snapshot altogether (http://docs.aws.amazon.com/storagegateway/latest/userguide/managing-volumes.html#clone-volume)"
AWS Import Export Snowball	"Get a Connection Refused error when I try to run snowball start
When I try to run snowballl start with the correct IP, unlock code and manifest (I've checked many times) I get the error ""Connection refused: connect""

The error in the log is:
2017-03-24 15:47:10 ERROR JobHandler:168 - Exception while connecting to Snowball
com.amazon.aws.awsie.snowball.exceptions.SnowballClientException: java.net.ConnectException: Connection refused: connect
	at com.amazon.aws.awsie.snowball.netty.SnowballClientLite.connect(SnowballClientLite.java:119)
	at com.amazon.aws.awsie.snowballclient.service.JobHandler.initializeClient(JobHandler.java:165)
	at com.amazon.aws.awsie.snowballclient.service.JobHandler.queryManifestAndInitializeClientMaybeLite(JobHandler.java:149)
	at com.amazon.aws.awsie.snowballclient.service.JobHandler.queryManifestAndInitializeClientLite(JobHandler.java:135)
	at com.amazon.aws.awsie.snowballclient.service.JobHandler.handleStartCommand(JobHandler.java:299)
	at com.amazon.aws.awsie.snowballclient.service.JobHandler.run(JobHandler.java:221)
	at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.run(SnowballClientStarter.java:88)
	at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.main(SnowballClientStarter.java:60)
Caused by: java.net.ConnectException: Connection refused: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668)
	at sun.security.ssl.SSLSocketImpl.<init>(SSLSocketImpl.java:427)
	at sun.security.ssl.SSLSocketFactoryImpl.createSocket(SSLSocketFactoryImpl.java:88)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientLite.connect(SnowballClientLite.java:115)

Any ideas what I should try?"
AWS Import Export Snowball	"Re: Get a Connection Refused error when I try to run snowball start
Were you ever able to resolve this? I am experiencing the same issue.

I've also verified the manifest and unlock code, that network connection speeds are gigabit, and I can ping the appliance.

Curious if you found a resolution to this."
AWS Import Export Snowball	"Re: Get a Connection Refused error when I try to run snowball start
That error log is saying the client is unable to connect to the snowball. The authentication is failing between the client tool and the snowball. You can try running snowball stop (to clean-up the connection info) and then try running the start command again. If it's still failing then the authentication certificates in the manifest isn't allow you to connect. This could be a result of any of the following:

1. You have the wrong manifest for the Snowball you are trying to connect to. Do you have multiple Snowball devices? If so could you have mixed up the Manifest?
2. There certificate in the manifest is corrupt. You can try downloading the manifest again from the console. It's possible that something went wrong during the downloading of the manifest.
3. The certificate could be corrupt on the Snowball. 

If running snowball stop then start after re-downloading the manifest from the console doesn't work then you'll need to contact AWS."
AWS Import Export Snowball	"Re: Get a Connection Refused error when I try to run snowball start
I can ping the appliance

At the risk of asking the obvious, did you verify that it stops pinging if you unplug it?  This error could potentially be caused by another device unexpectedly using that IP address."
AWS Import Export Snowball	"Import Export GetShippingLabel ""Unrecognized country: Myanmar""
Hi,

When running java -jar lib/AWSImportExportWebServiceTool-1.0.jar GetShippingLabel, getting error
""Unrecognized country: Myanmar""

Is Myanmar not supported? We want to ship an Import/Export disk to Singapore for import into S3.

The shipping calculator at https://awsimportexport.s3.amazonaws.com/aws-import-export-calculator.html has Myanmar in the Return Shipping Destination dropdown.

Thanks
Tony"
AWS Import Export Snowball	"Re: Import Export GetShippingLabel ""Unrecognized country: Myanmar""
Hello Tony,

Unfortunately the best solution to this is to use your own shipping label to send the device to us, and include, inside the package containing the disks, the job-created shipping label with an address in Singapore.  This allows you to ship the package, and allows the service to recognize the job.  

We apologize for the inconvenience.  We know this solution is not ideal, but it will provide the quickest turn around to get your device ingested."
AWS Import Export Snowball	"Re: Import Export GetShippingLabel ""Unrecognized country: Myanmar""
Hi,

Thanks for your response. 

We have the option is to send it to our Singapore office, and generate the job for that office.

However, the ""Generating Your Pre-Paid Shipping Label"" page has this at the bottom:

""Note: If you are having trouble using the pre-paid shipping label, or if you are shipping domestically within Singapore, contact AWS Support Center before shipping your device."" - http://docs.aws.amazon.com/AWSImportExport/latest/DG/GettingYourShippingLabel.html

So if we ship from our Singapore office, there's a different process? And how do we contact AWS Support Center Singapore? Via telephone, in which case what is the number? Or via email, in which case what what email address? Clicking the AWS Support link simply takes us to the basic Case Creation, and we've had no response to our enquiries.

Thanks
Tony"
AWS Import Export Snowball	"Snowball - check transferred file/folder size?
Hi

Is there a command to show me how much file size I have transferred in total to a 50TB Snowball unit? i.e., what the used and remaining capacity is?

thanks!

Matt"
AWS Import Export Snowball	"Re: Snowball - check transferred file/folder size?
If you run the <snowball status> command for the Snowball Client, it'll show you the total size and free space remaining. The docs were just updated with an example output for the status command here: http://docs.aws.amazon.com/snowball/latest/ug/using-client-commands.html#snowball-status-command"
AWS Import Export Snowball	"Snowball Delivered, but UPS doesnt think so
Hi,
Similar to at least one other recent forum poster, my Snowball was delivered but the UPS tracking site still says in-transit.  
Since AWS doesnt release the credentials until UPS delivers the device, I am now stuck."
AWS Import Export Snowball	"Re: Snowball Delivered, but UPS doesnt think so
Hi Jason.  My apologies for these difficulties.  Can you PM your job ID so I can get it's state moved?

Thank you.

Frank"
AWS Import Export Snowball	"Snowball Appliance installation - general question
Hi folks,

A bit of a peculiar  question which I've not found a suitable answer for :

Can an AWS Snowball be placed on its side during operation? 

In the DataCenter where it will be connected, the DataCenter support guys don't want it left on the floor. Turning it on its side would allow them to place it in a rack.

So is there a limitation on its orientation?

Thanks!"
AWS Import Export Snowball	"Re: Snowball Appliance installation - general question
Hi JasonMann.  The Snowball should operate just fine when placed on its side.

Frank"
AWS Import Export Snowball	"Expected run time for Snowball validation command
Hi, 
I'm trying to export ~20TB of data from local storage to Amazon S3 using their Snowball appliance. The copy finished successfully, and we are doing the validation before mailing the appliance back. However, the validation seems to get ""stuck"" after running for about 45 minutes. We terminated the first validation, and the 2nd try seems stuck at the same spot. Right now, status says that total = 17029 Successful = 17029 and invalid and incomplete = 0. It has had this same status since about 45 minutes after the start. We've gotten no errors. It's been running for a total of 3 hours.

I'm wondering these things: 
Is the validation doing a checksum on every file? 
Is there an expected time for validation on ~20TB of data? 
Will it show some errors at some point if it has a problem?

More information: 
Total files = 827,940 
95% of files are video or .jpg files 
Largest individual file is 80GB 
Command run = snowball -v validate s3://path/

Thanks in advance!"
AWS Import Export Snowball	"Re: Expected run time for Snowball validation command
So, we found out the timing. We waited it out overnight. We started at 3:45pm and it finished by 10am the next day. It never got any errors. So, maybe it just seems ""stuck"" when it hits a really large file."
AWS Import Export Snowball	"Re: Expected run time for Snowball validation command
Thanks for your feedback on the validation. Were you using the verbose option for the command? That's how I typically see the progress in real time for commands I execute in the Snowball client. Here's a link to the docs with the verbose listing: http://docs.aws.amazon.com/snowball/latest/ug/using-client-commands.html#clientverbose"
AWS Import Export Snowball	"snowball edge nfs lambda - nfs file copy results in which s3 events
The snowball edge can have lambda functions that can be configured to fire on s3 events.  When files are copied to the snowball edge using the nfs interface will s3 events be fired?  What events?  Configuring Amazon S3 Event Notifications: http://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html#supported-notification-event-types mentions these events: s3:ObjectCreated:Put, s3:ObjectCreated:Post, s3:ObjectCreated:Copy, s3:ObjectCreated:CompleteMultipartUpload"
AWS Import Export Snowball	"Re: snowball edge nfs lambda - nfs file copy results in which s3 events
Hi there,

Yes, using the file interface (the nfs mount) you can trigger Lambda functions. From the docs:

""The local compute functionality is AWS Lambda powered by AWS Greengrass, and can automatically run Python-language code in response to Amazon S3 PUT object action API calls to the AWS Snowball Edge appliance.""

and 

""PUT object actions can occur through the file interface (with a write operation), the AWS CLI (with a s3 cp command), or programmatically through one of the SDKs or a REST application of your own design. All of these S3 PUT object actions will trigger an associated function on the specified bucket.""

You can find more information on the file interface and the AWS Lambda powered by AWS Greengrass functionality here: http://docs.aws.amazon.com/snowball/latest/developer-guide/using-lambda.html and here: http://docs.aws.amazon.com/snowball/latest/developer-guide/using-fileinterface.html"
AWS Import Export Snowball	"Re: snowball edge nfs lambda - nfs file copy results in which s3 events
I was hoping to trigger a lambda function to create a checksum file when the nfs file was closed.  I think you are saying that the lambda function will be called on each write.  There will be a lot of writes for the file, right?"
AWS Import Export Snowball	"Re: snowball edge nfs lambda - nfs file copy results in which s3 events
Yes, the Lambda function would be triggered on each write (S3 Put object action http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html) to the bucket associated with your Lambda function."
AWS Import Export Snowball	"On Mac, can't find connected network folder to copy to snowball.
Hello All - 
I have my snowball up and running, but the client can't seem to see the folder of the networked drive I want to copy everything off of.  I have it mounted in my finder, but no combination of paths I use get it to the folder.  
I would think this would work:  snowball cp -r /rtpnfil03.rti.ns/vs/Video_Archive s3://rtivideoarchive
or some combination would work?  rtpnfil03.rti.ns is the name of the networked drive.  ""vs"" is the name of the mounted share, and ""Video_Archive"" is the name of the folder.   I've tried every combination I can think of, like 
snowball cp -r smb://rtpnfil03.rti.ns/vs/Video_Archive s3://rtivideoarchive
to
snowball cp -r /vs s3://rtivideoarchive
and anything in-between.  
I always get this error.  ""Pre-checking your source files and folders...
Cannot locate the specified file(s) or folder(s):""

Any ideas? Also the ""-r"" is to copy all the sub-directories....right?

Thanks everyone!
J"
AWS Import Export Snowball	"Re: On Mac, can't find connected network folder to copy to snowball.
Just tested copying from my local machine, and it worked no problem......still no love over the network."
AWS Import Export Snowball	"Re: On Mac, can't find connected network folder to copy to snowball.
Sorry!!! Think I figured it out.  Teaching myself as i go."
AWS Import Export Snowball	"Error on Mac - Can't read manifest resource
Hey All,

First time snowballer.  So I believe I have everything set up correctly; but when I run the Start Snowball script -i, -m, -u I get this error:

""Can't read manifest resource at /Downloads/JID2c7d5454-0d4d-4d6d-a6c2-55e371a6fe0a_manifest.bin. Try restarting the client.""

From here, I'm lost.  Any ideas?  I'm pretty savvy around a computer, but not an IT professional.  
Did see a thread where someone had a similar problem, but after an OS update they were all good. There are no more updates available for my OS.  I'm running El Capitan 10.11.16. 

This is the message I got when I installed the client - and this seems to have gone correctly. 

Running sudo copy commands
Password:
rm: /opt/aws/snowball: No such file or directory
Copying files to /opt/aws/snowball.
Snowball command is linkted at /usr/local/bin/snowball.
Install finished.

Any help would be greatly appreciated! 
Thanks."
AWS Import Export Snowball	"Re: Error on Mac - Can't read manifest resource
Hi 10engines.  Looking at the message it appears that you might have the incorrect path for the manifest file.  Typically on a Mac the path to the download file is something like /Users/<your user name>/Downloads.  In the example you give below it looks like you might have left off the first part of that path.

I hope this is helpful.

Frank"
AWS Import Export Snowball	"Re: Error on Mac - Can't read manifest resource
Thanks so much!  Exactly what I was doing wrong. Working now!!!"
AWS Import Export Snowball	"Re: Error on Mac - Can't read manifest resource
Ugh - next problem (if you're still there Frank) - 
Have it up and running, but the client can't seem to see the folder of the networked drive I want to copy everything off of.  I have it mounted in my finder, but no combination of paths I use get it to the folder.  
I would think this would work:  snowball cp -r /rtpnfil03.rti.ns/vs/Video_Archive s3://rtivideoarchive
or some combination would work?  rtpnfil03.rti.ns is the name of the networked drive.  ""vs"" is the name of the mounted share, and ""Video_Archive"" is the name of the folder.   I've tried every combination I can think of, like 
snowball cp -r smb://rtpnfil03.rti.ns/vs/Video_Archive s3://rtivideoarchive
to
snowball cp -r /vs s3://rtivideoarchive
and anything in-between.  
I always get this error.  ""Pre-checking your source files and folders...
Cannot locate the specified file(s) or folder(s):""

Any ideas? Also the ""-r"" is to copy all the sub-directories....right?

I'll post as a separate topic in case that works better. 

J"
AWS Import Export Snowball	"I get the following error running test command
Hello, 

I get the following error when I run a test command - 

Internal Error Message Begins: InvalidPathException-Illegal char <:> at index 2:

Here is my command - 

snowball test --recursive ""M:\DTM Data\EU_DTM"" s3://coremapdata/raw/elv/5m

The location s3://coremapdata/raw/elv/5m exists on the snowball I have confirmed with snowball ls"
AWS Import Export Snowball	"Re: I get the following error running test command
Hi there,

After a little bit of digging, I found that you don’t need to include the ‘s3://…’ part for the test command, or the quotes around your local path. Can you try the following command to see if it works:
snowball test --recursive M:\DTM Data\EU_DTM"
AWS Import Export Snowball	"Re: I get the following error running test command
I had to put """" around the file path but it worked, taking the s3:// bit out.  

The copy seems to be working now. 

Thank you"
AWS Import Export Snowball	"snowball edge testing lambda functions that process files loaded via nfs
I'm going to get a snowball edge and load it full of data using the nfs capability.  I would like to run a lambda function on the files in some of the directories.  I want to test the functions before ordering the snowball edge.  Any suggestions?"
AWS Import Export Snowball	"Re: snowball edge testing lambda functions that process files loaded via nfs
Hi there,
Currently, we don’t have a method for testing AWS GreenGrass Lambda functions that would run on a Snowball Edge in the cloud. I’ve made a note of your comment in a feature request with the team. But you can test your function against an Amazon S3 bucket that you have in the cloud, triggered by a PutObject operation. This will allow you to test the core functionality of the function, outside of the constraints put in place by the Snowball Edge. 

But before you do too much testing or create the Snowball Edge job, I have two quick recommendations for you:

1)	Get in on the AWS GreenGrass Limited Preview here: https://aws.amazon.com/greengrass/
2)	Read everything on this page: http://docs.aws.amazon.com/snowball/latest/developer-guide/using-lambda.html

There are limitations and considerations when using AWS GreenGrass Lambda functions on a Snowball Edge, so in place of testing, I’d recommend keeping those in mind when you create your functions.
Also, do note that when a function is on a Snowball Edge, it can’t be changed. You’ll have to create a new job with the new/updated function."
AWS Import Export Snowball	"Snowball Ethernet issue
Hi, 

I'm having issues connectivity issues to the AWS Snowball device, trying to connect the device using RJ45 Ethernet cable but it is not getting assigned an IP through DHCP. Tried connecting to multiple switches and using multiple cables but all with the same result. Trying to assign a static IP to the device also did not help as pinging the assigned IP or trying to connect to the device using the Snowball Client results in timeouts.

Something strange I noticed was that the right LED indicator on the snowball's Ethernet card keeps blinking constantly, even if there are not ethernet cables attached to it. Connecting an ethernet cable does not change the behavior of the indicators and the left LED indicator never lights up. Not sure whether this is expected or whether this is some kind of fault from the Ethernet card itself

A summary of the different options I've tried to connect the device can be found below:

1. Tried connecting to a patch panel. DHCP on AWS snowball did not get an IP. Trying to assign a static IP from the device itself did not work either.
2. Tried connecting to a normal switch directly to limit the number of loops, but same behavior
3. Tried following the troubleshooting on the website for all instances: https://aws.amazon.com/premiumsupport/knowledge-center/troubleshoot-connect-snowball/
4. Tried restarting the device a couple of times after connecting to the different devices I mentioned above, as listed on the troubleshooting page.
5. Tried different RJ45 cables, including the cable which came with the device, and also a CAT5 cable as mentioned in the troubleshooting page, and again no change in behavior was noticed.

Can you please provide some assistance in this matter and advice on what might be the possible cause of this behavior.

Thanks"
AWS Import Export Snowball	"Re: Snowball Ethernet issue
Hi jonacara.  If you are still seeing these problems, please PM your job id so I can investigate further.

Frank"
AWS Import Export Snowball	"Re: Snowball Ethernet issue
Hi Frank, yes unfortunately still having this issue. I've PM'ed you the Job ID"
AWS Import Export Snowball	"Snowball importing issue
Hello,

I use Snowball and the status of the JOB is Importing.
However, as far as Cloudwatch metrics is concerned, 
the NumberOfObjects for the S3 bucket does not change after 3/26.

Does Import processing work?
Or is the Cloudwatch metrics strange?

The job ID is ""JIDc9279805-b114-47ec-8fa2-eb98304bb51e""

Thank you."
AWS Import Export Snowball	"Snowball delivered, but UPS site says 'in transit'.
Got our Snowball 5 days ago.  UPS site has not updated status to 'delivered'.
I can't get the unlock credentials for the snowball before it hits 'delivered' status.
Any way around this?  Ticket has been open with UPS for 8 hours now, so maybe they'll update/fix it over night."
AWS Import Export Snowball	"Re: Snowball delivered, but UPS site says 'in transit'.
Hi jnathlich12.  Sorry for these difficulties, can you PM me your job ID so we can investigate?

Thank you.

Frank"
AWS Import Export Snowball	"Re: Snowball delivered, but UPS site says 'in transit'.
Hi jnathlich12, I have sent you a PM. Can you please check and confirm us in the forums. We will be waiting for your response.

Najah"
AWS Import Export Snowball	"How long does it take before a job goes to preparing?
I submitted an import job on Thursday last week (JID019d6081-a95f-49a8-8dc1-448ee3254a0d) and it still say ""Job Created"". How long does it normally take from when an 80T job is created until it is shipped?

I wish I had known it was going to take this long, I'm worried it might arrive while I'm away on vacation."
AWS Import Export Snowball	"Re: How long does it take before a job goes to preparing?
So it took a little more than four business days to before the appliance was prepared. It's now sat in between ""Prepare Appliance"" and ""In Transit to You"" for a day. I saw in the faq that they only ship them once a day, so I imagine it'll go out today/tonight; but if it's tonight, then why didn't it go out last night? shrug With the two-day shipping selected, I suspect I'll get it Friday this week or Monday next, which is more than a week from order to receive."
AWS Import Export Snowball	"Re: How long does it take before a job goes to preparing?
Hi Chad.  I'll look into this delay, it is not typical.  I'll update you with what we find.

Frank"
AWS Import Export Snowball	"Re: How long does it take before a job goes to preparing?
Hi Chad.  I have sent you a private message with so details about the processing of your job.

Frank"
AWS Import Export Snowball	"Snowball importing progress has stopped for more than 72 hours.
Hello,

We use Snowball to transfer data from the local server to S3.

Snowball is returned to AWS, Job status becomes ""Importing""
Copying was proceeding, but it did not proceed further after proceeding to the status below.

Total files transferred 15310197
99.765% complete

I do not know when it stopped, but it has stopped for more than 72 hours.

How should we deal with this?"
AWS Import Export Snowball	"Re: Snowball importing progress has stopped for more than 72 hours.
Hi fujita.  This typically means that an engineer is looking into your job, can you PM me your job id, the id that begins with ""JID"" so I can look into it?

Thanks!
Frank"
AWS Import Export Snowball	"Re: Snowball importing progress has stopped for more than 72 hours.
Thank you for your response.

I sent a JID as a private message.

Thanks."
AWS Import Export Snowball	"Estimated delivery time for Snowball job?
I'm in the US, and just created a Snowball job. What is the typical delivery time?

It's Saturday now and I pretty much need it by Tuesday, as I'm leaving the country for a month on Wednesday."
AWS Import Export Snowball	"Amazon API Details Invalid
Hi folks. I am trying to set up an Amazon Import tool on my Wordpress website. I enter my Affiliate ID, Secret key and Access key to no avail. Continuously receive - Amazon API Details Invalid. We could not connect to Amazon because your Amazon import settings are incorrect. I have downloaded numerous keys under what I think is the Root account since I have never created any other. This tool seems to be hitting the keys as the ""last used"" time keeps changing to when I am trying to import. Any thought? Feedback would be greatly appreciated. Thanks!"
AWS Import Export Snowball	"Re: Amazon API Details Invalid
Hi hmsnzTelecine 

I was able to locate your case. You’ll find it’s been updated by visiting the Support Center here: http://amzn.to/1Nsiekn

Regards
Shaun C"
AWS Import Export Snowball	"No clue how to download everything from AWS, and then shut down
Hi all, I inherited this AWS set up and I have montly billing going for a website that will never come about... so I want to shut it down.  But before I shut it down I just want to download everything so I can save it locally.

I have no clue how to do this, and have been spending a great amount of time trying to figure it out.

Help please!"
AWS Import Export Snowball	"Re: No clue how to download everything from AWS, and then shut down
It depends a lot on your AWS set up. Is it a single server? Are you using RDS?
I recently migrated an entire site from another cloud provider (Rackspace Cloud) to AWS; my approach was basically to tarball the relevant paths (/var/sites, /var/www, /etc for any possible config files I might miss), pg_dump the database and then download all that stuff. I would untar all of that on a separate directory on the new server and copy the needed config files to the real /etc, the working paths to their corresponding place and just fire up the server.

If you know all of the components, you might simply tarball the specifics, or go for ""everything but /proc"" and download it. If your setup is using RDS, you can do a mysqldump or pg_dump (depending on what RDBMS your setup is using) to keep a copy of the whole database."
AWS Import Export Snowball	"The KMS key you selected is invalid
When I get to the end of the process of ordering the Snowball appliance I the message.
The KMS key you selected is invalid.

I only have one bucket.  I can not find anything wrong."
AWS Import Export Snowball	"Re: The KMS key you selected is invalid
Sorry for the difficulty you're having. If you're trying to order a Snowball device in one of the US West regions, for the time being you will have to create a KMS key in US East (N. Virginia) and select that key for your job. Then you should be able to proceed with the order."
AWS Import Export Snowball	"md5 generated on Import
If we copy our objects to Snowball with the snowball client (vs. the S3 Adaptor), will the MD5 Etags get generated when the objects are imported when the snowball is returned to Amazon?"
AWS Import Export Snowball	"Snowball Export Issues
I am attempting to export 40TB of data from a snowball to my local network, and have been having a number of issues.

Currently I am exporting using the following command:
snowball -v cp -r <src> <dst>

My snowball client version is:
Snowball client version: 1.0.1 Build 124

The snowball client is bursting network traffic. It will start with a 0MB/s transfer rate, burst up to ~110MB/s then drop back to 0MB/s prior to completing the file transfer, as shown below.

Current File: 0% Totals: http://3.86 GB/4.25 GB                                                                            Current File: 5% Totals: http://3.88 GB/4.25 GB                                                                           Current File: 29% Totals: http://3.97 GB/4.25 GB                                                                          Current File: 54% Totals: http://4.08 GB/4.25 GB                                                                         Current File: 63% Totals: http://4.11 GB/4.25 GB                                                                          Current File: 63% Totals: http://4.11 GB/4.25 GB                                                                           Current File: 63% Totals: http://4.11 GB/4.25 GB                                                                           Current File: 63% Totals: http://4.11 GB/4.25 GB                                                                           Current File: 63% Totals: http://4.11 GB/4.25 GB 

I realize that the transfer is a CPU intensive job, but it is not currently maxing the CPU. Here are the relevant logs from top:
top - 08:09:03 up 3 days, 23:17,  6 users,  load average: 14.88, 12.98, 9.15
Tasks: 242 total,   1 running, 241 sleeping,   0 stopped,   0 zombie
%Cpu0  : 29.7 us,  1.0 sy,  0.0 ni, 38.0 id, 31.3 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu1  : 31.8 us,  1.3 sy,  0.0 ni, 34.1 id, 32.8 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu2  : 29.1 us,  0.7 sy,  0.0 ni, 35.3 id, 29.5 wa,  0.0 hi,  5.5 si,  0.0 st
%Cpu3  : 42.1 us,  0.0 sy,  0.0 ni, 39.1 id, 18.7 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu4  : 30.0 us,  0.7 sy,  0.0 ni, 34.7 id, 34.7 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu5  : 26.4 us,  1.0 sy,  0.0 ni, 56.2 id, 16.4 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu6  : 27.5 us,  0.7 sy,  0.0 ni, 71.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu7  : 76.1 us,  0.0 sy,  0.0 ni, 23.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:  32897956 total, 23936920 used,  8961036 free,   405304 buffers
KiB Swap: 33442812 total,   308960 used, 33133852 free. 15764872 cached Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
10419 user    20   0 12.792g 3.595g  23868 S 304.9 11.5  10:29.54 java
11106 user    20   0   29208   3060   2432 R   0.3  0.0   0:00.07 top
    1 root      20   0  185184   4336   2868 S   0.0  0.0   0:08.20 systemd
...

What could be causing this bursting behavior? Is there anything that can be done to speed up the overall transfer times? (I will run multiple instances as soon as I can correct the bursting behavior issues)"
AWS Import Export Snowball	"Re: Snowball Export Issues
There are generally two phases that happen: encryption and transferring. The encryption phase needs a lot of CPU and the transfer phase needs bandwidth.  It is common to see the CPU spike and drop as the data is often changing between the encryption and transferring phase.

There are many ways to speed up the transfer of data to a snowball. Many of them are describe in the following documentation: http://docs.aws.amazon.com/snowball/latest/ug/performance.html"
AWS Import Export Snowball	"Emulator for snowball device
It would be very useful for partner developers to have an emulator available for the snowball appliance device, especially given that a snowball device can not be kept on premises for more than 90 days."
AWS Import Export Snowball	"Re: Emulator for snowball device
Thank you for your feedback. I will let them team know about your request."
AWS Import Export Snowball	"Snowball with Cloudberry Backup
Hi all,
We're going to order one Snowball to send data from our file servers to S3.
We've been looking for Cloudberry Backup that supports Snowballs:
https://www.cloudberrylab.com/blog/working-with-aws-snowball-in-cloudberry-backup/

I only used Cloudberry Explorer, but not backup.
Any suggestions? What are the pros and cons?"
AWS Import Export Snowball	"Re: Snowball with Cloudberry Backup
Snowball is designed to import data directly into s3. You can order a snowball and manually copy files from your file server to the snowball. After you send the snowball back it will be imported into S3.

Here are a few links that might be helpful for you:
https://aws.amazon.com/snowball/getting-started/
https://aws.amazon.com/blogs/aws/aws-importexport-snowball-transfer-1-petabyte-per-week-using-amazon-owned-storage-appliances/"
AWS Import Export Snowball	"N Virginia - Error Snowball is not available in your area. Contact AWS
I am in dire need of getting a clients data imported to AWS. The data is close to 10TB and it would take forever to upload. I have tried to arrange for a snowball to be shipped to me but keep receiving this message:

Error
Snowball is not available in your area. Contact AWS Support for more details.

I am in Tulsa OK 74137 and have tried logging into different US regions and ordering a snowball from each region. All say the same error."
AWS Import Export Snowball	"Re: N Virginia - Error Snowball is not available in your area. Contact AWS
Hi cmasty.  Unfortunately we are not able to send you a Snowball device.  You are welcome to use our AWS Import/Export Disk service to import your data.  This service, which you can read more about at https://aws.amazon.com/snowball/disk/, is one where you send us a disk drive with your data to be imported.

My apologies for the difficulties.

Frank"
AWS Import Export Snowball	"Does Snowball support CloudTrail logging?
Does Snowball support CloudTrail logging and by Extension CloudWatch Alarms?

http://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events-supported-services.html

I believe the answer is no, just wanted to verify as I could not find a definitive no.

We have SNS notifications set up.

Thanks."
AWS Import Export Snowball	"Re: Does Snowball support CloudTrail logging?
That is a good idea, but it doesn't not support cloud trails or cloud watch."
AWS Import Export Snowball	"how do I use multiple snowballs from the same user account?
Hi,

I have multiple snowballs that I'd like to use from the same user account - I've tried the obvious trick of setting HOME to a different location, but it doesn't work. I get the 'manifest already extracted' which I presume is because of the existing manifest for the first snowball. Thanks.

Cheers, Cos."
AWS Import Export Snowball	"Re: how do I use multiple snowballs from the same user account?
I'm assuming you want to use multiple snowballs from the same account at the same time. If so then it's possible but a little tricky. If you don't need to access them at the same time then it's easy. Just run snowball stop and then snowball start with the new snowball.

To support two snowballs at the same time you'll need to modify the snowball script (or snowball.bat if on windows). Inside the file on the last line you'll need to add the java flag: user.home=<some path to where the manifest will be stored>. The path will need to be unique for each snowball that you are trying to connect to. 

https://forums.aws.amazon.com/
if you are on Mac and you wanted to change the home location to /tmp you could add the following -Duser.home=/tmp to the last line in the snowball script. See below for full context. Note the -D flag will need to go after the call to java. 

DYLD_LIBRARY_PATH=""${DIR}/../x86_64"" ${DIR}/../jre/Contents/Home/bin/java -Duser.home=/tmp -Xmx7G -Djava.library.path=""${DIR}/../x86_64"" -cp ""${DIR}/../jarfarm/protobuf-java-3.0.x.jar"":""${DIR}/../jarfarm/*"" com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter ""$@""

Save this file as a new file (example snowball2). Now when you want to connect to the two different snowballs you can use snowball and snowball2 to communicate with the two different snowball devices at the same time.

I hope this helps."
AWS Import Export Snowball	"Snowball job hasn't moved forward in more than a month
Hi There,

I created a snowball job on 25-Nov-2016. It is close to 40 days the status is still stuck in Job Created. Though this is not an urgent thing it is very important for us migrate all the data from local machines to S3. Can you please help why the job hasn't moved forward?

Region: Mumbai (ap-south-1)
The JobID (if that helps is): JIDa1f069f7-c9ce-4329-b7f3-847594762513
Appliance capacity requested for: 80TB."
AWS Import Export Snowball	"Re: Snowball job hasn't moved forward in more than a month
Hello gantir,
I've sent you a private message regarding your order.  Please have a look.

Thank you,
Ryan - AWS Import/Export team"
AWS Import Export Snowball	"snowball cp behavior
Hi, realized snowball client's cp isn't like typical GNU cp, but glad to hear it's goal is to more closely mimic AWS CLI's cp.

Noticed while running scripts to copy data using snowball's cp command - directory to directory, if the dataset hasn't changed, cp will will confirm the data set is the same and move on. However, if snowball cp is invoked again, and any of the data has changed anywhere in the dataset, cp will start from scratch and re-transfer the entire dataset (can be PB mind you), not just the changes (as one might think). 

Has anyone else confirmed that this is the case?
Thanks!"
AWS Import Export Snowball	"Re: snowball cp behavior
Hello JamesT21@, 
I have queried our snowball service team to look into the issue. We will get back to you as soon as we get further information from them. In the meantime if you have other issues feel free to update.

Cheers
Najah"
AWS Import Export Snowball	"Re: snowball cp behavior
Hello,
Thanks for your patience. According to our Snowball team, Whenever a new copy operation is performed all the source files are pre-checked before they are copied to the Snowball. When the client encounters a file which is already present on the Snowball, it will skip copying it and move on to the next file. Please note that the client decides a file has ""changed"" when there is a mismatch of last modified date or file size between the source file and the one on the Snowball.

I hope this clarifies the behavior you were observing."
AWS Import Export Snowball	"US East snowball edge availability?
Hi there. We're well into planning a very large data import job to S3 and we plan to use a 5-node cluster of Snowball Edges. I know the Edge is a brand-new product so are they generally available at this point for US east regions? We are on a tight timeframe and hope to get them shipped within the next week. If we are likely to have to wait more than a few days we would probably want to change our plans to use original Snowballs instead.

I know the precise answer to my question will depend on what is available the day I actually place the order so I just want some guidance as to whether they are generally ""in stock"" and available now or not.

Thanks,

-joel"
AWS Import Export Snowball	"Re: US East snowball edge availability?
Hi joelp.  Snowball Edge devices are available from US East regions but they are in limited supply.  If your need is time sensitive, you may want to order the 80TB versions of the original Snowballs.

Frank"
AWS Import Export Snowball	"snowball job stuck in ""job created"" state
We're trying to evaluate AWS and need to move quite a bit of data.

A Snowball job was created two days ago and is still in the ""Job created"" state. ( 	JID744803a0-0057-4c67-bed5-196cc45f004a)

So far there have been 6 (successful and unsuccessful) attempts to contact support.
Sometimes a tech says he's contacting the snowball team and we never hear back.
Sometimes the phone disconnects

Let's try the forum! 

Update: the job is tied to a different account... it seems I can't post in the forum from that account!"
AWS Import Export Snowball	"Re: snowball job stuck in ""job created"" state
Hi rmeden2.  I checked on your job and it has been provisioned to be shipped.

My apologies for any difficulties.

Frank"
AWS Import Export Snowball	"Re: snowball job stuck in ""job created"" state
Thanks... I wonder if the problem was there were no snowballs available... if so, a better status would be nice."
AWS Import Export Snowball	"Snowball Export Job at 100% complete for two weeks
I have an export job which has been at 100% complete for about 2 weeks now.  

The Job ID is: JID5bc78916-3473-4969-bcb8-528c90da9664

Can someone take a look at this for me?  Thank you."
AWS Import Export Snowball	"Re: Snowball Export Job at 100% complete for two weeks
Hello,

I have reached out to the engineering team for an update on the mentioned job id,  as soon as we have any information from the team we will update you as well. 
Highly appreciate your patience. 

Regards,
Ashwin A."
AWS Import Export Snowball	"Re: Snowball Export Job at 100% complete for two weeks
Thanks.  Shortly after posting this I received an email which stated that there was a problem and it was being worked on. Within a day the problem was resolved and the Snowball device was being shipped.

I wanted to update this post, but could not find it again until now when I received a thread update email.

Thanks again."
AWS Import Export Snowball	"Amazon Snowball Issues - The appliance has timed out
This is the 2nd time I have used the service and unfortunately the 2nd time I have had issues..

I am unable to get past 'The applicance has timed out'. I have tried powering off, leaving unplugged for a few minutes and plugging back in, all to no avail.

When I get the 'The appliance has timed out' error, the only option is to 'restart interface', and there isn't anything possible after this.

Hope someone can help!"
AWS Import Export Snowball	"Cannot create export job due to ""bucket markers"" ???
We're having trouble creating a Snowball export job (see the attached image).  Every time we try to create the job we get the following error message:

""The ARN for the IAM role you provided can't access one or more of your specified bucket markers.""

What does that even mean?  What is a ""bucket marker""?  This particular bucket has no special policies on it, does not use versioning, and does not have any pending multi-part uploads.  We've given the IAM role full administrative permissions to the entire account and we still get this error message!

The account for this request is 552637259102."
AWS Import Export Snowball	"Import/Export GetShipInfo ""Failure to retrieve PDF""
hi, in using import/export all of a sudden GetShipInfo API is returning errors. 
java -jar lib/AWSImportExportWebServiceTool-1.0.jar GetShipInfo ZTUN5
Version: 2014-12-18
..Failure to retrieve PDF.  Retrying in 1 seconds.
Failure to retrieve PDF.  Retrying in 2 seconds.
Failure to retrieve PDF.  Retrying in 4 seconds.
Failure to retrieve PDF.  Retrying in 8 seconds.

(This was working fine before...)
What to do ?"
AWS Import Export Snowball	"Re: Import/Export GetShipInfo ""Failure to retrieve PDF""
Hi Thomas.  My apologies for these difficulties. If you can PM me your job ID, I can investigate further to find out what might be wrong.

Thank you.

Frank"
AWS Import Export Snowball	"Re: Import/Export GetShipInfo ""Failure to retrieve PDF""
Job ID is YZSWG 
 tom"
AWS Import Export Snowball	"Snowball Import - Consuming 100%+ CPU, No files transferred
We are trying to transfer 20TB from Snowball device to local storage. 

Using client snowball-client-linux-1.0.1-115.tar.gz on Linux OEL Linux 3.10.0-327.36.1.el7.x86_64, on a 16 core, 64GB RAM machine.

The command is ""snowball-client-linux-1.0.1-115/bin/snowball cp -r s3://<obfusicatedname1> /nfs_share"".

This job is running since Sat (56 hours so far), but has not copied anything so far, however the CPU is running 100%+ most of the time. The logs don't show anything credible other than  occasional messages as seen below.

""connection: reset by peer"" 
""2016-10-24 15:04:25 ERROR ExceptionHandler:222 - com.amazon.aws.awsie.snowballsdk.exception.AmazonSnowballException; ErrorType: INTERNAL_ERROR""
""java.util.concurrent.ExecutionException: com.amazon.aws.awsie.snowballsdk.exception.AmazonSnowballException; ErrorType: INTERNAL_ERROR""

I would appreciate help from the engineering team, because so far any request for help through the support channel has not been useful."
AWS Import Export Snowball	"Re: Snowball Import - Consuming 100%+ CPU, No files transferred
Can you provide the logs and your JobID? You can either respond to the thread or send them in private message.

You can try running in verbose mode. It should show you which files are being actively copied.
Can you try exporting a smaller subset to see if that works?"
AWS Import Export Snowball	"Re: Snowball Import - Consuming 100%+ CPU, No files transferred
Thanks for responding. 

I did the same copy command for a specific subdirectory earlier and that finished copying within a couple days. So I know it works for a specific subdirectory. I did notice that there was a state when the CPU spiked for few hours without any data being copied, however the job started copying eventually after few hours. 


What is the switch for verbose ? The online documentation showed no switch for verbose. 
I do not have access to JobId (it was generated through another AWS account managed by vendor)
How do I send logs privately ?"
AWS Import Export Snowball	"Re: Snowball Import - Consuming 100%+ CPU, No files transferred
If you click on my name you will be taken to my profile page. From there on the right hand side is the option to send a private message.

To get the jobID you can just copy the manifest's file name. It has the jobID in it."
AWS Import Export Snowball	"Re: Snowball Import - Consuming 100%+ CPU, No files transferred
awsatericn - can you please confirm you are a AWS employee from Amazon? I want to make sure you are before sharing private information. 

Regards,Sam"
AWS Import Export Snowball	"Snowball export - Stuck in ""Job Creating""
Hi,

I'm trying to export a S3 bucket through Snowball but it's been stuck in the ""Job Created"" state for 48 hours now. The bucket is only around 4TB. Is this a normal duration?

The Job is: JID2ccfa77b-fa4b-472a-8271-6aca09b41f23

Thanks,
Alan

Edited by: alanclelland on Oct 12, 2016 3:36 AM"
AWS Import Export Snowball	"Re: Snowball export - Stuck in ""Job Creating""
Hi alanclelland.  I wanted to let you know that we have checked on the status of your job and it has been processed to be shipped on Thursday.

Frank"
AWS Import Export Snowball	"Re: Snowball export - Stuck in ""Job Creating""
Perfect! 

Thanks,
Alan"
AWS Import Export Snowball	"Snowball list throws TimeoutException
I am trying to get the list of files in a specific folder in snowball for validation purposes. I have 80K files in this folder, when I tried to run snowball ls, I am seeing ""SocketTimeoutException: Read timed out"" in logs. In screen, I am seeing ""Can't communicate with Snowball. Confirm the Snowball's IP address and try again"". Sometimes it listed 38K, 48K and 50K and then timed out or sometimes it throws timeout straight away. I have attached the log files (I ran snowball ls with and without debug mode).

I use Snowball client Version: 1.0.1, build: 68 for Mac.

Any idea how do I resolve this ? Thanks in advance.

Edited by: Baskar on Aug 16, 2016 4:09 PM"
AWS Import Export Snowball	"Re: Snowball list throws TimeoutException
Do you know about how many files are in a directory?

You can increase the timeout threshold. It will require you to modify the starting script which is typical installed at /opt/aws/snowball/bin/snowball

You will need to pass in -Dsnowball.timeout=120 (the value is in seconds, if there is a really high number of files 100k+ in a folder then you'll want to increase this number).

I've attached an example of a modified snowball file with the timeout increased to 2 minutes (120 seconds). Note I had to rename the file to snowball.txt in order to upload the file."
AWS Import Export Snowball	"Re: Snowball list throws TimeoutException
Thank you for the reply.
There were 80000 files in the directory. I will try to increase timeout and see if this works.
Appreciate your help."
AWS Import Export Snowball	"Re: Snowball list throws TimeoutException
This worked for me after increasing time out. Sometimes 120 seconds failed, I had to increate up to 300 seconds."
AWS Import Export Snowball	"New Snowball Client - Copy Command Syntax update
As of version 107 for Windows, 85 for Linux and 79 for Mac the snowball client's copy command syntax has been updated to be more aligned with S3's copy command syntax. The details are further describe in User Guide:

http://docs.aws.amazon.com/AWSImportExport/latest/ug/copy-command-syntax.html"
AWS Import Export Snowball	"Re: New Snowball Client - Copy Command Syntax update
The latest client is now 115 (for Windows, Linux, and Mac). It is recommended that everyone upgrades to the latest version. You can download the update from https://aws.amazon.com/importexport/tools/"
AWS Import Export Snowball	"Snowball questions
Hi. Hopefully these are easy to answer:

1) We have 2 branch offices in the UK that I need to import data from into S3. Can I order a snowball to Office 1, do an import, unplug it, drive it down to Office 2, do an import and then ship it back from Office 2?

2) Our EC2 instances, and S3 buckets are all in the eu-west-1 (Ireland) region. Can I have a snowball from eu-west-1 shipped to the UK, and then ship it back so that the data is in the correct region for S3?
Thanks!"
AWS Import Export Snowball	"Re: Snowball questions
EDITED TO CORRECT #1

Hi there,

1) Yes, minor correction, you can move devices once you receive them, you just can't move the\ devices m outside of the the EU (or just 'country' if for a country not in the EU) where it was received.
2) Yes, you can have a device shipped from eu-west-1 to the UK (currently). From the docs ""For data transfers in the EU regions, we only ship devices to the EU member countries listed following: Austria, Belgium, Bulgaria, Croatia, Republic of Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, Italy, Ireland, Latvia, Lithuania, Luxembourg, Malta, Netherlands, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, Sweden, and the UK.""

Hope that helps!

-Dan

Edited by: Dan@AWS on Jan 21, 2019 10:26 AM"
AWS Import Export Snowball	"Re: Snowball questions
I was wrong on #1, I've edited it with the correct information. Yes, it can be moved. So yes and yes!"
AWS Import Export Snowball	"Re: Snowball questions
Great, thanks for the responds."
AWS Import Export Snowball	"Snowball Export job stuck in Job created for 6 days.
Hi,

I have an export job created on Dec, 28th 2018 (today is Jan, 3rd 2019) and it seems it is stuck in ""Job created"" status. Is this a normal duration? I mean, how long does it take to change from ""Job created"" status to next step (Preparing appliance)? Weeks? Months? 

I searched for more details on aws documentations but looks like no information is available regarding this subject (ETA, Duration). 

Job ID: JID915cca63-b6f9-40a2-9d53-94348ad9b382

Thanks,
Pedro"
AWS Import Export Snowball	"Re: Snowball Export job stuck in Job created for 6 days.
Hi,

For future reference, and in hoping this helps anyone that may face similar situation.

It seems using Forum for this sort of issues, delay in processing snow ball request, may work better instead of opening a ticket in support !?!?!?!?!.

I got response from snowball team and they opened another ticket to address our request (snowball export job).

Best Regards,

Pedro"
AWS Import Export Snowball	"Windows error: Can't read manifest resource at
Trying to run Snowball and getting error with the manifest path.  Tried moving it but no luck?  any ideas? never used this before..

C:\SnowballClient\bin>snowball.bat start -i 192.168.101.105 -m C:\temp -u xxxx-xxxx-xxxx-xxxxx-xxxxx
Can't read manifest resource at C:\temp. Try restarting the client."
AWS Import Export Snowball	"Re: Windows error: Can't read manifest resource at
Are you giving the full path to the manifest file or just saying c:\temp?
Here is a link to the documentation: http://docs.aws.amazon.com/snowball/latest/ug/using-client.html"
AWS Import Export Snowball	"Re: Windows error: Can't read manifest resource at
hello, i went through all the beginning command and when i tried a copy job i get the same error, 
Can't read manifest resource at (location on C in .aws folder/) my manifest is located in the Downloads folder, not sure why it is redirecting the file or can't find it.

Any help is appreciated."
AWS Import Export Snowball	"Running snowball from application
Hi,
I am thinking about ordering snowball machines in order to import data to S3 from several locations.
The people who are going to operate the copy procedure aren't technical, so I am writing an (javascript) application that will help them initiate the process. 

1) I want to get the updated status of snowball  every couple of seconds, In order to display in the application whether snowball connected and how much free space is left,
for this I will use run every couple of seconds:
snowball start and then snowball status - 
snowball start -i ${ip} -m ${manifestFile} -u ${unlockCode} 
snowball status

Does it safe to run snowball start if snowball already started? 
(And might even copying in the same time)
What this is going to return? Success? Error?

2) I want to  show the status / progress of copy procedure to people using the application.
(how many percent of copy completed or how much time left in order to complete the copy)

Does the status written on the display of the hardware?
Does it somehow possible to get this using the command line?


Thanks,
Vitaly"
AWS Import Export Snowball	"Error starting - The Appliance has Timed Out
Hello,

I'm having connectivity issues with our AWS Snowball device, I am repeatedly receiving an error message when powering on: The appliance has timed out. An internal error has occurred...

I've tried all options in the troubleshooting guide from the knowledge centre:
https://aws.amazon.com/premiumsupport/knowledge-center/troubleshoot-connect-snowball/

And have also tried the options listed here in this forum post:
https://forums.aws.amazon.com/thread.jspa?threadID=254483&tstart=50

Any further advice or troubleshooting options to initialize and connect the device to our network would be greatly appreciated.

Many thanks in advance."
AWS Import Export Snowball	"snowball multipart uploading failed using RESTAPI
Hi there. We're using REST API to upload to snowball appliance. The single put/delete operation works fine, however, the multipart uploading failed with ""403 Forbidden"" error (the init multipart also works fine), we're using the same credential. Any idea? Thanks."
AWS Import Export Snowball	"Re: snowball multipart uploading failed using RESTAPI
After turning on DEBUG and checking the snowball adapter log, found out this is signing issue. Identified and fixed the issue."
AWS Import Export Snowball	"Request submitted has been in ""Job Created"" job status since 6/15/2018
We submitted a 50TB import job request with ID JIDdb3f4de8-9285-4329-ac8c-bdecbf4e47bb on 6/15/2018 and has yet to change state. Is there an expected duration for shipment? Mostly asking because it is getting close to the point where it would have been faster to transfer the data over the internet vs. Snowball."
AWS Import Export Snowball	"Re: Request submitted has been in ""Job Created"" job status since 6/15/2018
I can only suggest contacting support. I had the very same problem. Support replies within a day or so. They can't help me immediately but at least kept me informed. I've got a Snowball with a 2 weeks delay."
AWS Import Export Snowball	"Re: Request submitted has been in ""Job Created"" job status since 6/15/2018
Hi there !

I understand that it took some time for your job to be looked into. 

Usually, orders are typically processed the same day or the next unless the service team has questions about the request.

Apologies for the inconvenience caused.

Best,
Meghna

Edited by: MEGHNA-AWS on Nov 16, 2018 12:14 PM"
AWS Import Export Snowball	"Slow Performance / Messages that the client cannot connect to the snowball
We ran the snow ball client in test mode and it estimated approximately 3 days and 17 Hrs. to copy around 40TB of data. 

The actual snowball device has only copied 3.2 TB in approximately 24 Hours. The device is connected to a 10gig switch and a trace route shows that it is only one hop away from the machine with the data. We are getting messages that the device has failed to copy a file, followed by a message saying that the client cannot communicate with the Snowball and to check the IP Address and try again. However, I have a 2nd instance of the client running, and I am able to run a “snowball status” without any issues.  I am also able to run a continuous ping and do not drop any packets. 

I have attached the failed-files long for review.  It seems like when the client encounters a file with a filename / path that it cannot copy, it stops and waits for like a retry period for like 5 minutes. 

Any insight to this issue would be greatly appreciated."
AWS Import Export Snowball	"Re: Slow Performance / Messages that the client cannot connect to the snowball
Hi rich_limu. 

My apologies for the difficulties.  The speed of copy is dependent on a number of factors, the speed of the source storage that is being copied, the speed of your local network, the available computing power of the machine running the Snowball client, and the size of the files being copied.  The error message you are getting however points to another problem.  Can you verify that you have the latest client software?  Also, if you can PM your job ID and I verify the version of the server that you have received.

Frank"
AWS Import Export Snowball	"Re: Slow Performance / Messages that the client cannot connect to the snowball
Frank thanks for the quick response...

Job ID:  has been sent via PM.
Server has 32GB of ram
client is 1.0.1 Build 5712557332"
AWS Import Export Snowball	"Re: Slow Performance / Messages that the client cannot connect to the snowball
It looks like we are getting an error from JavaAssist 

2016-02-01 09:59:41,536 DEBUG mainhttp://io.netty.util.internal.PlatformDependent: Javassist: unavailable
2016-02-01 09:59:41,536 DEBUG mainhttp://io.netty.util.internal.PlatformDependent: You don't have Javassist in your class path or you don't have enough permission to load dynamically generated classes.  Please check the configuration for better performance.

And this Java Error

2016-02-01 10:03:01,002 DEBUG mainhttp://com.amazon.aws.awsie.snowball.netty.SnowballChannel: Got channel.
2016-02-01 10:03:21,005 INFO  mainhttp://com.amazon.aws.awsie.snowball.retrier.RequestRetrier: Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. attempts=1 Sleep for 100 milliseconds.
2016-02-01 10:03:41,107 INFO  mainhttp://com.amazon.aws.awsie.snowball.retrier.RequestRetrier: Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. attempts=2 Sleep for 200 milliseconds.
2016-02-01 10:04:01,311 INFO  mainhttp://com.amazon.aws.awsie.snowball.retrier.RequestRetrier: Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. attempts=3 Sleep for 400 milliseconds.
2016-02-01 10:04:21,714 INFO  mainhttp://com.amazon.aws.awsie.snowball.retrier.RequestRetrier: Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. attempts=4 Sleep for 800 milliseconds.
2016-02-01 10:04:42,517 INFO  mainhttp://com.amazon.aws.awsie.snowball.retrier.RequestRetrier: Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. attempts=5 Sleep for 1000 milliseconds.
2016-02-01 10:05:03,521 ERROR mainhttp://com.amazon.aws.awsie.snowball.retrier.RequestRetrier: Reached max attempts attempts=6, throwing exception
java.util.concurrent.TimeoutException
	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1771)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
	at com.amazon.aws.awsie.snowball.netty.SnowballFuture.get(SnowballFuture.java:60)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty$2.performRequest(SnowballClientNetty.java:202)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty$2.performRequest(SnowballClientNetty.java:193)
	at com.amazon.aws.awsie.snowball.retrier.RequestRetrier.performRequestAndRetry(RequestRetrier.java:57)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.streamRequest(SnowballClientNetty.java:206)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.streamRequest(SnowballClientNetty.java:182)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.sendRequest(SnowballClientNetty.java:164)
	at com.amazon.aws.awsie.snowball.netty.SnowballClient.deleteObject(SnowballClient.java:147)
	at com.amazon.aws.awsie.snowball.netty.SnowballClient.deleteFileIfExists(SnowballClient.java:164)
	at com.amazon.aws.awsie.snowballclient.service.uploadservice.SnowballClientFileVisitor.visitFile(SnowballClientFileVisitor.java:174)
	at com.amazon.aws.awsie.snowballclient.service.uploadservice.SnowballClientFileVisitor.visitFile(SnowballClientFileVisitor.java:35)
	at java.nio.file.Files.walkFileTree(Files.java:2670)
	at java.nio.file.Files.walkFileTree(Files.java:2742)
	at com.amazon.aws.awsie.snowballclient.service.uploadservice.UploadService.traversePath(UploadService.java:493)
	at com.amazon.aws.awsie.snowballclient.service.uploadservice.UploadService.copyToFolderRecursive(UploadService.java:530)
	at com.amazon.aws.awsie.snowballclient.service.uploadservice.UploadService.transfer(UploadService.java:178)
	at com.amazon.aws.awsie.snowballclient.service.uploadservice.UploadService.copy(UploadService.java:120)
	at com.amazon.aws.awsie.snowballclient.service.JobHandler.handleCopyCommand(JobHandler.java:503)
	at com.amazon.aws.awsie.snowballclient.service.JobHandler.run(JobHandler.java:208)
	at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.main(SnowballClientStarter.java:73)"
AWS Import Export Snowball	"Re: Slow Performance / Messages that the client cannot connect to the snowball
I'm seeing the same problem. Were you able to resolve or workaround this? The Snowball client appears to be essentially broken on Windows."
AWS Import Export Snowball	"Re: Slow Performance / Messages that the client cannot connect to the snowball
Hi Nick.  We have made a number of improvements that may address the issues you are seeing.  Can you send me a PM with your job id so we can verify?

Thanks and my apologies for the difficulties here.
Frank"
AWS Import Export Snowball	"Re: Slow Performance / Messages that the client cannot connect to the snowball
We are seeing this right now as well. Is there any publicly posted solution?
I have already PM'd the JobID.
Thanks.

2018-10-31 14:23:41 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 1000 milliseconds.
2018-10-31 14:23:43 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 1000 milliseconds.
2018-10-31 14:23:43 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 1000 milliseconds.
2018-10-31 14:23:44 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 1000 milliseconds.
2018-10-31 14:23:51 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 1000 milliseconds.
2018-10-31 14:23:54 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 200 milliseconds.
2018-10-31 14:23:54 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 800 milliseconds.
2018-10-31 14:23:56 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 1000 milliseconds.
2018-10-31 14:23:56 ERROR RequestRetrier:95 - Reached max attempts https://forums.aws.amazon.com/, throwing exception
java.util.concurrent.TimeoutException
	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1771)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
	at com.amazon.aws.awsie.snowball.netty.SnowballFuture.get(SnowballFuture.java:60)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty$2.performRequest(SnowballClientNetty.java:237)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty$2.performRequest(SnowballClientNetty.java:227)
	at com.amazon.aws.awsie.snowball.retrier.RequestRetrier.performRequestAndRetry(RequestRetrier.java:47)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.streamRequest(SnowballClientNetty.java:251)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.streamRequest(SnowballClientNetty.java:216)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.sendRequest(SnowballClientNetty.java:201)
	at com.amazon.aws.awsie.snowball.netty.SnowballClient.createObject(SnowballClient.java:355)
	at com.amazon.aws.awsie.snowball.netty.SnowballClient.createFolder(SnowballClient.java:378)
	at com.amazon.aws.awsie.snowballsdk.service.internalservice.SnowballObjectCreator.createObject(SnowballObjectCreator.java:88)
	at com.amazon.aws.awsie.snowballsdk.service.internalservice.SnowballObjectCreator.createObject(SnowballObjectCreator.java:49)
	at com.amazon.aws.awsie.snowballsdk.DirectoryUtilities.createFoldersIfNotExist(DirectoryUtilities.java:109)
	at com.amazon.aws.awsie.snowballsdk.SnowballWritableByteChannel.createObject(SnowballWritableByteChannel.java:142)
	at com.amazon.aws.awsie.snowballsdk.SnowballWritableByteChannel.<init>(SnowballWritableByteChannel.java:125)
	at com.amazon.aws.awsie.snowballsdk.SnowballWritableByteChannel.<init>(SnowballWritableByteChannel.java:80)
	at com.amazon.aws.awsie.snowballsdk.SnowballWritableByteChannel$SnowballWritableByteChannelBuilder.build(SnowballWritableByteChannel.java:72)
	at com.amazon.aws.awsie.snowballsdk.service.UploadService.getWritableChannelToSnowball(UploadService.java:72)
	at com.amazon.aws.awsie.snowballsdk.AmazonSnowballClient.getWritableChannelToSnowball(AmazonSnowballClient.java:210)
	at com.amazon.aws.awsie.snowballsdk.AmazonSnowballClient.getWritableChannelToSnowball(AmazonSnowballClient.java:64)
	at com.aws.snowball.adapter.server.netty.handlers.s3.HttpPutContentHandler.initSnowballChannel(HttpPutContentHandler.java:108)
	at com.aws.snowball.adapter.server.netty.handlers.s3.HttpPutContentHandler.handleContent(HttpPutContentHandler.java:138)
	at com.aws.snowball.adapter.server.netty.handlers.AuthorizableRequestContentHandler.handle(AuthorizableRequestContentHandler.java:67)
	at com.aws.snowball.adapter.server.netty.handlers.SnowballAdapterServerRequestHandler.channelRead0(SnowballAdapterServerRequestHandler.java:56)
	at com.aws.snowball.adapter.server.netty.handlers.SnowballAdapterServerRequestHandler.channelRead0(SnowballAdapterServerRequestHandler.java:25)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:267)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1280)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:890)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:564)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:505)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:419)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:391)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:112)
	at java.lang.Thread.run(Thread.java:745)
2018-10-31 14:23:56 ERROR DirectoryUtilities:120 - There was an unexpected problem creating folder"
AWS Import Export Snowball	"Re: Slow Performance / Messages that the client cannot connect to the snowball
Please see PM posted with some questions"
AWS Import Export Snowball	"Speed
So, we dropped a 10gig NIC into our server and connected it straight to the snowball, which has a static IP.  

I am copying data over now.  21TB in total.

I'm seeing transfer speeds of approx 142MB/s.

Is that inline what what I should see?  Honestly, I was expecting much faster on a 10gig direct connect.

Thanks for any feedback you might offer.

Cliff"
AWS Import Export Snowball	"Re: Speed
Hi there,

Is this for a Snowball or Snowball Edge? We've got some performance tips you can find here for maximizing your throughput:

Snowball: https://docs.aws.amazon.com/snowball/latest/ug/performance.html

Snowball Edge: https://docs.aws.amazon.com/snowball/latest/developer-guide/BestPractices.html

In general, I highly recommend that you batch small files to improve your transfer speeds.

Good luck, let us know how it goes!

-Dan"
AWS Import Export Snowball	"Re: Speed
I swear Dan, you are the only guy at AWS who works!  

It's the snowball, NOT the edge.

Yeah, I thought about batching, and jumbo frames, since it's 21TB and mostly some large (>1TB) vmdks.  Then, on second thought, I ""thought"" I'd just go 10gig and get all kinds of fast transfers.

Sadly, not the case.  I've got all weekend though and since it's direct connect, I'm not killing internal bandwidth so I guess I'll review the notes above and then just let it run.

Thanks"
AWS Import Export Snowball	"Re: Speed
Traffic out of Seattle at rush hour tells me that I'm not the only one keeping the lights on at AWS, haha.

Also, just so you know, jumbo frames aren't supported: https://docs.aws.amazon.com/snowball/latest/ug/limits.html

Had to say it before you tried it out and hit a roadblock.

Take care!

-Dan"
AWS Import Export Snowball	"Re: Speed
You too buddy, and thanks."
AWS Import Export Snowball	"Snowball Import Stalled
I'm looking for support/advice on a job that has been stuck at Importing with no progress for nearly 2 days and after having completed only a very small portion of the import (160GB of 25TB).

Thanks"
AWS Import Export Snowball	"Re: Snowball Import Stalled
Hi there, can you PM me the job ID, and I'll see what I can do to route you through to the right support channels?"
AWS Import Export Snowball	"Re: Snowball Import Stalled
PM sent and thanks for your reply to that; I'm looking forward to your findings and will keep this thread up to date to help anyone else that has a similar issue in the future."
AWS Import Export Snowball	"Snowball start command not working
We received a new snowball after our first one was DOA.

It is connected and pulling an IP so I'm trying the start command.  This is what I'm using:

snowball start -i 192.168.0.20 -m  C:\SnowballClient\JID5a64edbd-3687-414a-af6b-c614ae90057f_manifest.bin -u <UNLOCKCODE>

The manifest.bin is in the c:\snowball folder and I'm running the client from the 

C:\Program Files (x86)\SnowballClient\bin

folder.  The error is can't find the resource manifest at path try restarting the client.

I've verified (at least I think I have) that everything is configured correctly, but alas, no luck.  Does anyone see what I may be doing wrong?

FYI - Doing this on Windows Server 2012 R2.

Thank you.

Cliff

Edited by: Dan@AWS on Oct 18, 2018 3:15 PM"
AWS Import Export Snowball	"Re: Snowball start command not working
Hi there,

I commented out your unlock code. You shouldn't share that.

Sounds like your path is wrong. I'm seeing that you shared two potential paths in your message:

C:\SnowballClient\JID5a64edbd-3687-414a-af6b-c614ae90057f_manifest.bin

and

C:\Program Files (x86)\SnowballClient\bin

I'd nail down where the file is saved, use that as the path, and try again.

This is for a standard Snowball, yes? Or is it for a Snowball Edge?"
AWS Import Export Snowball	"Re: Snowball start command not working
Thanks for that!   

I did a reinstall on the client to c:\snowball and dropped the manifest file in there and it's working as expected.  

I appreciate the quick response as well.  Thanks Dan!"
AWS Import Export Snowball	"Re: Snowball start command not working
No worries, happy to help!"
AWS Import Export Snowball	"Snowball errors on all commands but start
I have the linux client install per documentation. ""snowball mkdir"" gives no feedback when run, and logs give  no information other than a command was run. Snowball cp gives error no matter what. Openssl is installed on host CentOS machine. Need to run ""./snowball cp -r /mnt/oldnfs s3://lti-archive"" all combinations fail.

./snowball mkdir s3://lti-archive/oldnfs

./snowball cp -r /mnt/oldnfs s3://lti-archive/oldnfs
./snowball cp -r /mnt/oldnfs/ s3://lti-archive/oldnfs/
./snowball cp -r /mnt/oldnfs/* s3://lti-archive/oldnfs
./snowball cp -r /mnt/oldnfs/* s3://lti-archive/oldnfs/

 ./snowball cp /mnt/oldnfs/blog.sql s3://lti-archive
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: failed to load the required native library
        at io.netty.handler.ssl.OpenSsl.ensureAvailability(OpenSsl.java:154)
        at io.netty.handler.ssl.OpenSslContext.<init>(OpenSslContext.java:127)
        at io.netty.handler.ssl.OpenSslContext.<init>(OpenSslContext.java:121)
        at io.netty.handler.ssl.OpenSslClientContext.<init>(OpenSslClientContext.java:167)
        at io.netty.handler.ssl.SslContext.newClientContextInternal(SslContext.java:733)
        at io.netty.handler.ssl.SslContextBuilder.build(SslContextBuilder.java:223)
        at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.connect(SnowballClientNetty.java:97)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.initializeClient(JobHandler.java:150)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.queryManifestAndInitializeClientMaybeLite(JobHandler.java:134)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.queryManifestAndInitializeClient(JobHandler.java:116)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.handleCopyCommand(JobHandler.java:464)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.run(JobHandler.java:202)
        at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.run(SnowballClientStarter.java:91)
        at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.main(SnowballClientStarter.java:63)
Caused by: java.lang.UnsatisfiedLinkError: /tmp/libnetty-tcnative6224027303078037940.so: /tmp/libnetty-tcnative6224027303078037940.so: failed to map segment from shared object: Operation not permitted
        at java.lang.ClassLoader$NativeLibrary.load(Native Method)
        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824)
        at java.lang.Runtime.load0(Runtime.java:809)
        at java.lang.System.load(System.java:1086)
        at io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:193)
        at io.netty.handler.ssl.OpenSsl.<clinit>(OpenSsl.java:58)
        at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.connect(SnowballClientNetty.java:92)
        ... 7 more"
AWS Import Export Snowball	"Re: Snowball errors on all commands but start
The linux client requires read/write access to the /tmp. Can you verify the user who is trying to run these commands can access that folder."
AWS Import Export Snowball	"Re: Snowball errors on all commands but start
I know this is kinda late but just for future reference for anyone that runs into this issue like I did, just add the flag: -Djava.io.tmpdir=""/var/tmp"" to the ..bin/snowball script right after -Djava.library.path=""${DIR}/../x86_64"".

PS: doesn't have to be /var/tmp you can specify any location of your choice just ensure the dir exists and your user has rw access to it"
AWS Import Export Snowball	"Running user data on ec2 instances on snowball edge
My main question here is whether userData is run everytime I stop and start an existing instance and what methods are available to debug issues regarding userData . Currently, my userdata is not running when I run a new instance, or when i stop/restart it. The only logs generated by the snowball edge are  SUPPORT logs.
ALso, I did check if the script is present on the snowball edge using aws ec2 describe-instance-attribute and can see it perfectly present.
 How do I debug what's happening here?!"
AWS Import Export Snowball	"Re: Running user data on ec2 instances on snowball edge
Hello,

According to EC2 documentation [1], ""By default user data is executed once, at the first boot of the instance."" The steps to execute user data scripts after initial launch can be found here [2]. 

In order to troubleshoot potential issues with user data not running, we would recommend checking the user data on the instance using metadata service. In order to do this please run ""curl http://169.254.169.254/latest/user-data"" from your EC2 instance. This command should display your user data. If user data is displayed, you can check the cloud-init output log file (/var/log/cloud-init-output.log) for errors with the user data script for further troubleshooting.

Thank you for using Snowball!

[1] https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html#user-data-shell-scripts
[2] https://aws.amazon.com/premiumsupport/knowledge-center/execute-user-data-ec2/"
AWS Import Export Snowball	"New to AWS - Support Question
so, a client of mine ordered a snowball and it arrived.  we need some help since it's not working as expected and seems DOA.  

support costs extra and no one seems to reply on here when we make posts.

is this typical AWS support?"
AWS Import Export Snowball	"Re: New to AWS - Support Question
Hi there,

You can find AWS Support here: https://aws.amazon.com/premiumsupport/compare-plans/ and there are multiple plans you can choose, if costs are a concern. The forums are intended for users to use, to troubleshoot issues with the community or share ideas and projects.

I'm a technical writer on the team, and I check the forums from time to time to see if there are issues I can help with. Can you provide more information about the device in question? Is it a Snowball or a Snowball Edge? What specifically isn't working as you anticipate it would?

-Dan"
AWS Import Export Snowball	"Re: New to AWS - Support Question
Thanks for the reply Dan.

We have a 50TB snowball and it appears to be DOA.  The LCD screen is stuck on timeout and repeatedly asks us to retry.  e have scoured the forums and tried pulling the power and network cables, waiting ten minutes, etc, but nothing works.  It's just stuck on the timeout message.

The snowball DID pull an IP as I can see that in my DHCP lease and I can ping it, but all cli commands report that it cannot communicate with the snowball.

We are stuck and not sure what to do."
AWS Import Export Snowball	"Re: New to AWS - Support Question
Mmm. That sounds not great. Can you send me a private message, and I'll reach out to the team to see what can be done?"
AWS Import Export Snowball	"Snowball not pulling IP, well, not performing as expected
We received our snowball and connected it to our network.  It IS pulling an IP as I can see it in my DHCP lease and I can ping it, but the screen shows a timeout and requests a restart.  We have power cycled the device and still no joy.

I have connected a notebook to the switch port and confirmed that I am pulling an IP.

Any suggestions since all I can do is restart and after several attempts, no joy?

Thanks

Cliff"
AWS Import Export Snowball	"Unable to unlock Snowball Edge device
Hi all,

Just got my Snowball Edge device and having trouble unlocking it. ""unlock-device"" would report my device is unlocking, but the device would just go back to its LOCKED state.

$ ./snowballEdge unlock-device
Your Snowball Edge device is unlocking. You may determine the unlock state of your device using the describe-device command. Your Snowball Edge device will be available for use when it is in the UNLOCKED state.
$ ./snowballEdge wait unlocked
Unlocking /ExecutionException - com.amazonaws.waiters.WaiterUnrecoverableException: Resource never entered the desired state as it failed.
$ ./snowballEdge describe-device
{
  ""DeviceId"" : ""JID8c9f1959-8af9-4fda-a565-7fd0ef14ed61"",
  ""UnlockStatus"" : {
    ""State"" : ""LOCKED""
  }
}


Things I've tried so far:
1. Double check to make sure I got the correct manifest/unlock pair.
2. Use the deprecated ""unlock/status"" command.
3. Run latest snowball client on both Linux/Mac. Mac release: snowball-client-mac-1.0.1-230, Linux release: snowball-client-linux-1.0.1-230

One other observation, ""snowballEdge unlock-device"" would sometimes fail with IncorrectDeviceManifestException, but continues to work if I retry the command with no changes:
$ snowballEdge unlock-device
IncorrectDeviceManifestException -  (Service: AWSSnowballDevice; Status Code: 400; Error Code: IncorrectDeviceManifestException; Request ID: 7ac7702b-9765-402a-8d20-721836f7ea23)
$ snowballEdge unlock-device
IncorrectDeviceManifestException -  (Service: AWSSnowballDevice; Status Code: 400; Error Code: IncorrectDeviceManifestException; Request ID: 31266f82-3192-4c91-b875-f39ff4f42b4a)
$ snowballEdge unlock-device
Your Snowball Edge device is unlocking. You may determine the unlock state of your device using the describe-device command. Your Snowball Edge device will be available for use when it is in the UNLOCKED state.


Any suggestions appreciated.

Thanks,"
AWS Import Export Snowball	"Re: Unable to unlock Snowball Edge device
Hi @breez,
I'm facing the same issue. Were you able to fix this?
-Thanks"
AWS Import Export Snowball	"Re: Unable to unlock Snowball Edge device
Hi Kit7,

What issue are you experiencing, specifically? Have you included your manifest, unlock code, and ip address for the device?"
AWS Import Export Snowball	"Re: Unable to unlock Snowball Edge device
The snowball edge currently on premesis got unlocked initially and had been so for more than a week. Then I booted up some ec2 instances and was playing around a bit. The snowball edge locked itself and now won't unlock. This happened with a second snowball edge on prem as well but that unlocked when I tried unlocking it again. Yes, I am using the correct manifest, unlock code and ip address. 

Any idea why this might happen?"
AWS Import Export Snowball	"Re: Unable to unlock Snowball Edge device
In the case of the first one, it sounds like it's been on for too long without any action taken on it. I'd recommend following these instructions here:

https://docs.aws.amazon.com/snowball/latest/developer-guide/troubleshooting.html#connection-troubleshooting

Specifically:

""Power off the Snowball Edge and then unplug all the cables. Leave the device for 10 minutes, and then reconnect it and start again."""
AWS Import Export Snowball	"SnowballEdge File Transfer
Hello,

I have recently used the regular 80 TB snowball to transfer my data. The command to transfer was snowball cp and then source followed by destination. I now ordered a snowballEdge and the command has now changed. So my question is what is the command to transfer the files. 

I am currently using the file interface but I would like to know the command for future reference. 

Thanks"
AWS Import Export Snowball	"Re: SnowballEdge File Transfer
Hi there,

The Snowball Edge has an S3 endpoint for transferring data. This means that you can use the AWS CLI to transfer data, or one of the AWS SDKs, if you're not using the file interface.

Here's some documentation in the Snowball Edge Developer guide to get you started: https://docs.aws.amazon.com/snowball/latest/developer-guide/using-adapter.html

You would still use cp commands, but they would be more like this:

aws s3 cp <path/to/source/source> <path/to/destination/on/Edge>"
AWS Import Export Snowball	"Re: SnowballEdge File Transfer
Thanks for the help"
AWS Import Export Snowball	"Does the new AWS default encryption work with Snowball?
I'd posted a prior question which was answered (https://forums.aws.amazon.com/message.jspa?messageID=808997), indicating Snowball would encrypt automatically if a policy was set on the target bucket denying updates without encryption.

As jor-el (https://forums.aws.amazon.com/profile.jspa?userID=436790) asked as a followup, the docs are not clear on the later addition of default encryption (https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html): does this also work?

In fact the snowball docs (https://docs.aws.amazon.com/snowball/latest/ug/security.html#encryption) still specifically say you have to set the older style policy.

Thanks for clarifying!

Stephen"
AWS Import Export Snowball	"Sending our own Hard Disk for import
Hi, 

I found this blog post from 2009 about sending our own HD:
https://aws.amazon.com/blogs/aws/send-us-that-data/ 

What I couldn't find is the Address which I send the HD to? I looked in the snowball website and didn't find any link that mention the address. 

Can any one help me? 

Thanks,
Yoni"
AWS Import Export Snowball	"Re: Sending our own Hard Disk for import
Hi there,

If you want to import data into the Amazon Cloud you have a number of options:

1) You can create an import job for Snowball, and transfer your data through a powerful workstation that you own onto the device. We'll then import the data into S3 when you return it.
2) You can create an import job for Snowball Edge, and transfer your data through a typical computer, without the need for a powerful workstation. We'll then import the data into S3 when you return it.
3) The Amazon S3 console allows you to upload data directly using a familiar UI. For more information, see https://docs.aws.amazon.com/AmazonS3/latest/user-guide/upload-objects.html.
4) You can use the AWS CLI to upload files and folders directly into Amazon S3.
5) You can establish an Amazon Direct Connect connection and upload data that way.

Thanks for being an Amazon customer,

-Dan"
AWS Import Export Snowball	"Re: Sending our own Hard Disk for import
When trying to create an import job using Snowball UI I only get the option to obtain a device from AWS. There is no option to send my own device to AWS.

https://us-east-2.console.aws.amazon.com/importexport/home?region=us-east-2#/wizard

Are you sure there is an option to import from my own device?"
AWS Import Export Snowball	"Re: Sending our own Hard Disk for import
No, you can't send in your own hard drive with the Snowball service. You would transfer your files from your hard drive, through a computer, on to the Snowball device, and then send the device back to AWS."
AWS Import Export Snowball	"Use snowball adapter with .net sdk
Hi,

I'm trying to import all my data onto s3 using a snowball, but I need to keep track of some metadata for each file. I have some c# code that uploads the files directly to the bucket with this metadata, which is working fine. I thought I would be able to use the snowball s3 adapter to connect to my snowball and upload data onto from my c# code, because this link https://aws.amazon.com/snowball/tools/ says: ""You can use the S3 Adapter with existing Amazon S3 interfaces like the AWS SDKs, the AWS CLI, or your own custom Amazon S3 REST client.""  

I have the adapter running on http://localhost:8090, so I tried setting that as the endpoint for the Amazon3Client by passing in an AmazonS3Config object with that endpoint as the ServiceUrl, but when I try to upload the file using either a PutObjectRequest or the TransferUtility, I get the following exception:

Amazon.Runtime.AmazonServiceException: A WebException with status NameResolutionFailure was thrown. ---> System.Net.WebException: The remote name could not be resolved: ' BUCKET.localhost'
   at System.Net.HttpWebRequest.GetRequestStream(TransportContext& context)
   at System.Net.HttpWebRequest.GetRequestStream()
   at Amazon.Runtime.Internal.HttpRequest.GetRequestContent()
   at Amazon.Runtime.Internal.HttpHandler`1.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.RedirectHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.Unmarshaller.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.S3.Internal.AmazonS3ResponseHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.ErrorHandler.InvokeSync(IExecutionContext executionContext)
   --- End of inner exception stack trace ---
   at Amazon.Runtime.Internal.WebExceptionHandler.HandleException(IExecutionContext executionContext, WebException exception)
   at Amazon.Runtime.Internal.ExceptionHandler`1.Handle(IExecutionContext executionContext, Exception exception)
   at Amazon.Runtime.Internal.ErrorHandler.ProcessException(IExecutionContext executionContext, Exception exception)
   at Amazon.Runtime.Internal.ErrorHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.CallbackHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.Signer.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.CredentialsRetriever.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.RetryHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.CallbackHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.S3.Internal.AmazonS3KmsHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.EndpointResolver.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.S3.Internal.AmazonS3PostMarshallHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.Marshaller.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.S3.Internal.AmazonS3PreMarshallHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.CallbackHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.S3.Internal.AmazonS3ExceptionHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.ErrorCallbackHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.MetricsHandler.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.Internal.RuntimePipeline.InvokeSync(IExecutionContext executionContext)
   at Amazon.Runtime.AmazonServiceClient.Invoke[TRequest,TResponse](TRequest request, IMarshaller`2 marshaller, ResponseUnmarshaller unmarshaller)
   at Amazon.S3.AmazonS3Client.PutObject(PutObjectRequest request)


I tried using my local ip address, as well as 127.0.0.1, but all got the same error. When I use the amazon cli to upload the document to the snowball, using --endpoint to change the endpoint, it worked (with both localhost or my local ip), so apparently it's just something about the .net sdk. But I would really strongly prefer to use my existing c# code than to write up a command line program to do this - it won't be so simple from the cli.

Can anyone help me? 

Thanks"
AWS Import Export Snowball	"Re: Use snowball adapter with .net sdk
Hello, i am curious if you ever got this resolved? I am thinking about using Snowball to move some data from on-prem into S3 and using the code i already have in .net to push the files into Snowball would be nice."
AWS Import Export Snowball	"Re: Use snowball adapter with .net sdk
Hi there,

I spoke with the Snowball dev team about this issue and this is what they told me:

The error thrown in the link provided is related to the URL style in which the S3 Bucket is accessed. The client software they have is accessing the bucket provided in virtual-hosted-style URL as specified in https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro which is why the problem is seen. 

They should be using path-style URL to overcome this problem. For this, they have to set ForcePathStyle property of AmazonS3Config to true while creating AmazonS3Client in .NET  https://docs.aws.amazon.com/sdkfornet/v3/apidocs/Index.html 

If you have other questions, please let us know.

Thanks for being an AWS customer!

-Dan"
AWS Import Export Snowball	"File verifications
Hi, 

We're looking to upload about a PB worth of data using Snowball but are having a hell of time figuring out how best to handle file verifications. The digitized files were delivered to us on portable hard drives with MD5 Checksums. This is great... unfortunately, the etags generated for these same files is different on AWS due to the multipart uploading. We have 10's of thousands of large files, will we have to regenerate checksums using the methods described here?: https://www.savjee.be/2015/10/Verifying-Amazon-S3-multi-part-uploads-with-ETag-hash/

We were hoping to compare the MD5 checksum we have (without regenerating) with the etag generated on AWS, but that seems not possible. 

Thanks,
Jay"
AWS Import Export Snowball	"Slow Transfer speeds using Snowball Edge
Hi All,
We just received our Snowball Edge and configured it based on the guides available from Amazon. The Snowball is connected to our main switch with 10GB SFP+ and clients are accessing through 1GB connections. The main reason we wanted the Edge was for the mountable NFS file system so not so technical users could drag their files onto the Snowball Edge over the network just like an SMB share they are used to.

Once the file interface was set up we went ahead and tested this on windows. We mounted the file share as the guide says using the command line and saw that the snowball AWS bucket was mounted. As a test, tried uploading a 4GB file and transfer speeds were maxed out at 20MBps. Since speeds were slow, we directly connected the snowball to a PC via 1GB RJ45. Transfer speeds were exactly the same. While transferring data I opened up the resource monitor and noticed that the network connection speeds were maxing out at 200Mbps \ 20MBps. To rule out Operating Systems also tried the same exact two tests using the latest version of Mac OS. Transfer speeds were even slower at 6MBps.

I know this cant be network related as I've tried moving the same exacted test file over a 1GB connection to a NAS and saw speeds of 980Mbps \ 112MBps

Any insight is greatly appreciated as I am a bit stumped. Thanks."
AWS Import Export Snowball	"Re: Slow Transfer speeds using Snowball Edge
Did you ever get this resolved? Was unable to do anything on the EC2 images on our Snowball Edge due to extremely slow access out of the Snowball Edge. Couldn't even ping websites efficiently. Sent it back and ordered another - which wouldn't boot. Not pleased."
AWS Import Export Snowball	"Re: Slow Transfer speeds using Snowball Edge
Hi there,

The file interface has additional performance considerations that will result in reduced throughput, as you've seen.

The best practices topic in the documentation (https://docs.aws.amazon.com/snowball/latest/developer-guide/BestPractices.html) can help you get the most performance out of your device. For the highest throughput, you'll want to transfer with the S3 Adapter, with small file batching.

Thanks for being an AWS customer,

-Dan"
AWS Import Export Snowball	"snowball adapter in windows
Hi, this is the first time using a snowball and I'm struggling with getting going. 
We are using CloudBerry and it states we need to install the snowball adapter. 

""For Windows, after downloading the zip file
1) Open the directory where the zip file was downloaded and unzip the file.
2) Run command .\snowball-adapter-win\bin\snowball-adapter.bat with required options""

If I try to run the following with correct IP/manifest path/unlock code 

C:\snowball-adapter-win\bin>./snowball-adapter -i x.x.x.x -m C:\xxx_manifest.bin -u x-x-x-x-x

I get...

'.' is not recognized as an internal or external command,
operable program or batch file.

If I run

C:\snowball-adapter-win\bin>snowball-adapter.bat etc etc

I get

""Failed to start server, check logs for more details""

Any ideas what I'm doing wrong? Where does it log to? I can't find any information relating to this.

Thanks

Jim"
AWS Import Export Snowball	"Re: snowball adapter in windows
For windows you'll need to use the bat file. For Linux and mac you will use the ./snowball-adapter command.

Information on the logs can be found in the documentation: https://docs.aws.amazon.com/snowball/latest/ug/snowball-transfer-adapter.html"
AWS Import Export Snowball	"Re: snowball adapter in windows
Any ideas what I'm doing wrong? Where does it log to? I can't find any information relating to this.

I found this post when I ran into the same super unhelpful error message. After digging around a bunch in documentation and on the filesystem, I found that the logs needed to troubleshoot the error were found in ~/.aws/snowball/logs

In my case, that error was: 

2018-08-15 19:42:57 ERROR S3HttpAdapterStarter:90 - java.lang.IllegalArgumentException: profile file cannot be null

For me, the the root cause was that the adapter was not reading credentials from ENV vars like awscli (or boto3) will. I had to populate the ~/.aws/credentials explicitly.

Hopefully this helps the NEXT person trying to troubleshoot."
AWS Import Export Snowball	"stuck at activating file interface...
Hi, have gone through most of the steps to activate the file interface service on the snowballedge but when starting it up it's stuck on ""Activating"" for quiet some time... any ideas what could be causing this? likely a busted unit?"
AWS Import Export Snowball	"Re: stuck at activating file interface...
Hi there,

It's probably not stuck activating. It can take an hour or more to finish activating. Please don't power-cycle the device while it's activating the file interface. The dev team is working on adding some text to the client or the LCD Display to indicate how long it can take to activate. 

As one of the technical writers for the service, I'll work on getting the docs updated to reflect this as well.

Sorry for the inconvenience,

-Dan"
AWS Import Export Snowball	"Re: stuck at activating file interface...
Not used to any service taking hours to start these days so I assumed something must be wrong. Thanks for the info. We're letting it ""activate"" overnight."
AWS Import Export Snowball	"Re: stuck at activating file interface...
That's good feedback, I'll share it with the dev team. 

Thanks!

-Dan"
AWS Import Export Snowball	"Re: stuck at activating file interface...
We left the unit on overnight in activating state and came into to find in inactive state this morning... that's a bit of an annoying catch-22, seems like it takes forever to activate but will timeout and go to inactive... pls tell us what we're doing wrong. We've had the unit for 4 days already and have been struggling with this one seemingly simple step."
AWS Import Export Snowball	"Re: stuck at activating file interface...
waiting doesn't result in the unit becoming active, instead it goes to inactive..."
AWS Import Export Snowball	"Re: stuck at activating file interface...
Can you message me privately and I'll look into the issue with you? That is unusual behavior."
AWS Import Export Snowball	"Snowball appliance delivered, but still listed as 'In Transit'
Need to unblock job so we can get the credentials.

Thanks,
Sean"
AWS Import Export Snowball	"Re: Snowball appliance delivered, but still listed as 'In Transit'
Hi Sean,

It'd be great if you could PM me your Job Id, so that I look it up internally.

Thanks,
Meghna"
AWS Import Export Snowball	"Re: Snowball appliance delivered, but still listed as 'In Transit'
Hi,

I have sent the JobID in a PM.

Thanks,
Sean"
AWS Import Export Snowball	"Re: Snowball appliance delivered, but still listed as 'In Transit'
Hi,

Any update on this?

Thanks,
Sean"
AWS Import Export Snowball	"Re: Snowball appliance delivered, but still listed as 'In Transit'
Anyone able to assist with this? Appliance is onsite, but we can't do anything.

Thanks,
Sean"
AWS Import Export Snowball	"Snowball Edge Not Freeing Up Space From Deleted Files
Hello,

Our Snowball Edge is not returning space from files that we've deleted. We deleted the files with the aws command line tool, running `aws s3 rm --recursive` on the problematic files/directories. The files are no longer present as determined by `aws s3 ls`. Nonetheless, the space used/free as displayed on the panel of the Snowball Edge has not returned the space. 

We fear that if we return the Snowball, these 'ghost' files will be transferred into S3 and we will be charged for them.

We've tried rebooting the Snowball several times and also left the device idle for a day to see if it was slowly deleting the files, but neither has helped restore the free space.

Thanks for any help,
Sheel

Edited by: sheelc on Jul 12, 2018 4:34 PM"
AWS Import Export Snowball	"Re: Snowball Edge Not Freeing Up Space From Deleted Files
After letting it run for another 24 hours, it looks like the data is now slightly less -- so it seems like it's deleting at a rate of about .5TB/hour. It would be quite costly for us to wait the extra days for the deletion so it would still be good to know that these files have perhaps been ""tombstoned"" for deletion, but we're going to send the Snowball back since it seems like we've properly deleted the files."
AWS Import Export Snowball	"Re: Snowball Edge Not Freeing Up Space From Deleted Files
Hi Sheelc.  The deleted files will not be imported.  

Frank"
AWS Import Export Snowball	"Re: Snowball Edge Not Freeing Up Space From Deleted Files
Excellent, thank you for confirming Frank!"
AWS Import Export Snowball	"Snowball copy operation fails?
Hi, I started copying the data to the Snowball. The copy operation was running for many days (slow interface) and unfortunately ends without any status message, summarizing a backup process. 

Client/OS version:
2018-06-29 13:02:10 INFO  SnowballClientStarter:53 - received commandline args: [cp, -r, /fungen/funhome/db04/RUN, s3://fungen-backup/RUN]
2018-06-29 13:02:10 INFO  SnowballClientStarter:159 - SnowballVersion{version='1.0.1', revision='', build='230', dumpTruckCompatibilityVersion='null'}
2018-06-29 13:02:10 INFO  SnowballClientStarter:166 - OS Name: Linux, OS Version: 3.10.0-693.el7.x86_64, OS Architecture: amd64
2018-06-29 13:02:10 INFO  ClientConstant:110 - Total Physical Memory: 61.947425842285156GB
2018-06-29 13:02:11 INFO  JobHandler:265 - Snowball appliance build version: 2018-03-28.5774009395


In the first iteration I want to copy about 23,4 TB of data:
2018-06-29 13:42:53 INFO  JobHandler:265 - Snowball appliance build version: 2018-03-28.5774009395
2018-06-30 18:21:31 INFO  TransferSpeedLog:151 - Total Files: 38,555,354 Total Size: 23.4 TB


During the copying , 4 files somewhere in the middle if the process can't be copied due to issues with file permission:
2018-07-03 16:57:42 ERROR BatchCopyFileVisitor:48 - /fungen/funhome/db04/RUN/140110_SN934_0118_BD11C8ACXX/Thumbnail_Images/L006/C8.1
java.nio.file.AccessDeniedException: /fungen/funhome/db04/RUN/140110_SN934_0118_BD11C8ACXX/Thumbnail_Images/L006/C8.1


The very last message in the log file I've seen was the following:

2018-07-05 10:38:05 INFO  UploadService:718 - [RUN/RUN_part2887.snowballarchives finished. | Read speed: 0.65 MB/s, write speed: 3.86 MB/s ]


And after that, the status message was not updating and I haven't seen any running process in the memory.
The snowball status
 command shows: Used space 19740.41 GB, so less than expected.

I tried to verify the last operation with snowball -v validate
 operation.
After a short while, the operation exited with OutOfMemoryError error,

Is there any way to test if the copy is finished successfully or continue copy operation from the moment it failed?"
AWS Import Export Snowball	"Snowball import directly into S3 IA or S3 IA One-zone
Looking in the snowball docs I couldn't find an answer to the title's question. Is is possibly to import directly to one of the other storage classes? 

Thanks.

Edited by: ldnee on Jun 7, 2018 7:16 AM"
AWS Import Export Snowball	"Re: Snowball import directly into S3 IA or S3 IA One-zone
Hi,

Unfortunately, it is not possible to import data directly to any other storage class, but S3. However, from S3, you can send data to different storage classes, as per your requirement, using Lifecycle Policies. 

Read more about lifecycle policies here : https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html

Hope this helps.

Thanks,
Meghna"
AWS Import Export Snowball	"pause a copy, change network interface, restart copy
Last Friday, I've got a Snowball delivered. Unfortunately I did not have free SFP+ interface available on server site, to connect a Snowball, therefore I've connected it via a Gigabit Ethernet network cable. 
The test runs at acceptable ~200 MB/s rate, but unfortunately  the actual data transfer is currently working at 40 to 50 MB/s speed.
Therefore, my question is the following: Is it possible to stop (pause) current copy process, stop appliance, disconnect it from LAN, connect Snowball using SFP+ network interface, re-initialize appliance and resume copy procedure form the moment I stopped previously.

Thanks in advance,
Evgeniy"
AWS Import Export Snowball	"""Job Created"" status is not changing for 8 days
Update: I've just got an answer from support directly.

06.06.18 I've submitted an import job (JIDaa7a2b4f-0d3e-488d-be58-17ee861f00f1) and it still stays in ""Job Created"" stage. 
How long does it normally take from when an 80T job is created until it is shipped?

Edited by: FunGen on Jun 15, 2018 7:09 AM"
AWS Import Export Snowball	"Can I use Snowball Edge local storage to migrate data between private DCs?
Hi,
we need to move 200TB of storage between the DCs and I am wondering if I can and allowed to use Snowball Edge ""local compute and storage only"" job to do that?

We would load the data using NFS in DC1, move it ourselves to DC2 (same region), unload and then ship back to AWS for erasure.

a) Are we allowed by AWS to do this?
b) After we loaded the box and powered it off, can we access the data again in DC2?

Can't find in the docs if the above scenario is supported. Any feedback is appreciated!

Thank you!

Alex"
AWS Import Export Snowball	"Re: Can I use Snowball Edge local storage to migrate data between private DCs?
Hi,

did you find the answer to your question? 
I would like as well to transfer the data from DC to DC together with shipping it to AWS

Best"
AWS Import Export Snowball	"Re: Can I use Snowball Edge local storage to migrate data between private DCs?
Hi there,

I'm Dan, the tech writer for Snowball. Currently, in the docs, our official answer is ""Moving a AWS Snowball Edge appliance to an address other than the one specified when the job was created is not allowed and is a violation of the AWS Service Terms.""

Link: https://docs.aws.amazon.com/snowball/latest/developer-guide/limits.html

However, I went and had a chat with some folks here and learned that so long as it doesn't leave the country that it was shipped in, everything should be fine. I'll update the docs to reflect this new development.

Hope that helps,

-Dan"
AWS Import Export Snowball	"Re: Can I use Snowball Edge local storage to migrate data between private DCs?
Thanks for answering me!
Actually, in our case, we have just 2 server rooms located in 2 different building within one campus. 
I wanted to do a backup from both servers anyway and in parallel, if it is possible, move the data from one server to another one with the help of Snowball"
AWS Import Export Snowball	"Re: Can I use Snowball Edge local storage to migrate data between private DCs?
No worries, thanks for being an AWS customer! Good luck with your project"
AWS Import Export Snowball	"Snowball Import files from multiple locations
Hi,

I am new to Snowball and having slightly naive question. Unfortunately I did not find the answer in documentation or here.
Our Linux servers located at two different places and I want to copy data from both. Unfortunately we have problems with LAN connection speed and normally it is not very good idea to transfer big amount of data via the network. Therefore I am wandering, if it is possible to connect  1 Snowball sequentially on different computers: first copy data from one computer and then re-connect the Snowball physically to a different computer?

Best"
AWS Import Export Snowball	"Re: Snowball Import files from multiple locations
Hi FunGen. You can absolutely do this.  The Snowball client and the manifest for your particular Snowball will be need to be on each computer when you are performing the copies.

Frank"
AWS Import Export Snowball	"Re: Snowball Import files from multiple locations
Thanks for you help!"
AWS Import Export Snowball	"S3 Glacier Export via Snowball
Hi there,

I would to export from S3 Glacier class via snowball.
I understand the objects need to be restored to S3 Standard prior, but there is not documentation I could find that explain how many days would the items need to stay restored in Standard?
Would like to minimise that time to avoid S3 Standard storage costs, so if I set to 1 day restore for example, will that be sufficient?

Thank you."
AWS Import Export Snowball	"export snowball will not start
Hi All,
I ordered two files, totalling 1.5Tb to be sent to me via Snowball Export.
The Snowball arrived several days ago and I was finally able to try connecting it this morning
When I powered it on, it started to boot (saying 'please wait'), then started flipping it's e-ink display between  'please wait' and a return shiping label, eventually stopping on the return shipping label.
It sure looks like the snowball has a hardware (power supply?) problem.
Do you have any suggestions, or do I just need to ship it back, and if the latter, how do I get a refund?
I don't think it's the datacenter power or network connections , as I've successfyllu used two snowballs before this.
-thanks"
AWS Import Export Snowball	"Snowball cp operation keeps hanging
I'm trying to get 40TB of data into my snowball.  Everything is set up and communications, and I can start cp jobs just fine.  However after about 30 minutes the cp job just blocks on one thread seemingly forever.  I get the following message in the activity log:

2016-04-13 10:09:58 INFO  BlockingThreadPoolExecutor:116 - 1 task still executing and 0 task still in queue. Sleep for 1 seconds....

But it never wakes up again.  during this time, traffic to the snowball stops, so it's not just trying to copy the file and failing.  

I have  Snowball appliance version: 1.0.1 build 5717619902

Client: RHEL7.2 with 10GB RAM running latest release downloaded from was 3 days ago.

Any ideas what I can do to get the client to keep running?"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hi smithian-elp.  I'm sorry to hear that you are seeing these difficulties.  Can you PM me your job id?  I can look into it.

Frank"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Thanks, I PM'ed you.  For some reason the forum only lets me post one message a day.  Still trying to get this fixed, so any help is appreciated.

It looks like I am running into a memory leak.  I have assigned 48GB of RAM to the client machine and still get OOM errors after a few hours of operation.

Exception in thread ""main"" java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:714)
        at io.netty.util.concurrent.SingleThreadEventExecutor.shutdownGracefully(SingleThreadEventExecutor.java:534)
        at io.netty.util.concurrent.MultithreadEventExecutorGroup.shutdownGracefully(MultithreadEventExecutorGroup.java:146)
        at io.netty.util.concurrent.AbstractEventExecutorGroup.shutdownGracefully(AbstractEventExecutorGroup.java:69)
        at com.amazon.aws.awsie.snowball.netty.SnowballClientNetty.disconnect(SnowballClientNetty.java:159)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.run(JobHandler.java:217)
        at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.run(SnowballClientStarter.java:90)
        at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.main(SnowballClientStarter.java:62)

Anything I can do to get this to free up memory properly?"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Thanks smithian-elp.  We will contact you to collect some more data to see how we can fix this for you.  Sorry for these difficulties.

Frank"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hello,

we have the same issue with round about 70TB and currently no solution from AWS Support.
How did you fixed it?

best
Ricardo"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Can you try the latest client version 218? How many CPU cores do you have on the machine that you are using?"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hi awsatericn,

no, only 217, we will try and I give you feedback!

Thanks in advance."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hi awsatericn,

we got also with the new build 217 out of memory. Maybe you have access to the Case ID 4933415291 for more informations.

adm@bak ~$ snowball -w 5 -v cp --debug -l -r /home/backup/Storage/www/sites s3://panthermedia-migration/
Operating in debug mode. Logs are located at /home/.aws/snowball/logs
Pre-checking your source files and folders...
error log saved at /tmp/snowball-429639248710678892/failed-files
Files scanned: 100.843.202
For better performance, we suggest that you batch your small files together to speed up your data transfer. Would you like the Snowball client to batch these files automatically? Y/N (Defaults to Y in 60 seconds):Using Batch upload.
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
        at java.io.UnixFileSystem.list(Native Method)
        at java.io.File.list(File.java:1122)
        at java.io.File.listFiles(File.java:1286)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:469)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.innerListFiles(FileUtils.java:477)
        at org.apache.commons.io.FileUtils.listFiles(FileUtils.java:519)
        at org.apache.commons.io.FileUtils.listFiles(FileUtils.java:684)
        at org.apache.commons.io.FileUtils.iterateFiles(FileUtils.java:703)
        at com.amazon.aws.awsie.snowballclient.service.uploadservice.UploadService.copyWithBatchProcessor(UploadService.java:691)
        at com.amazon.aws.awsie.snowballclient.service.uploadservice.UploadService.transfer(UploadService.java:269)
        at com.amazon.aws.awsie.snowballclient.service.uploadservice.UploadService.copy(UploadService.java:182)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.handleCopyCommand(JobHandler.java:602)
        at com.amazon.aws.awsie.snowballclient.service.JobHandler.run(JobHandler.java:225)
        at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.run(SnowballClientStarter.java:89)
        at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.main(SnowballClientStarter.java:61)"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hello Ricardo,

Thank your for reaching out to us again.

I see that you have opened the case 4933415291 with us. I apologize for the inconvenience caused. Our internal team and the engineer assigned to your case is actively working on your concern. If you have any other updates please respond to the case so we can assist you further.

We really appreciate you patience and once again I apologize for the inconvenience caused.

Thanks,
Pavya"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
If you have the memory then the quickest solution is to just locally increase the amount of memory that is allocated. To do so just change the -Xmx7G parameter in the snowball script to a higher number.  See attached file as a reference."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hi awsatericn,

also with -Xmx100G we got OOM."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Something is wrong. It probably won't matter how much RAM you allocate it.
Do you have any cyclic loops using symbolic links? I'm wondering if it's looping over the dataset in a loop."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hi, the snowball client should not consider symlinks, at least according to the documentation. There is also no exclude. We do not create a loop ourselves, see also the post of Mar 28, 2018 4:53 AM."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
We tried also the snowball adapter, but it looks more worse then the snowball client.
Each second copy shows us the following error:  An error occurred (InternalError) when calling the PutObject operation (reached max retries: 4)

Also a recursive ""rm"" is not working in the adapter"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
I sent you a private message."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hi there,
I have the same problem, is there a solution for this?"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hello femrich,

I got currently a new version direct from ""awsatericn"". 
It works better, but not perfect.
Please see your ""private messages"""
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
There is a new public build available at https://aws.amazon.com/snowball/tools/ I would recommend that you give it a try. It's more memory efficient when traversing large directory structures and it doesn't follow symlinks (to prevent against cyclical loops)."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Hello,

the new build not solved our problems, because it seems to skip all files, if it finds only one of it copied from a other terminal parallel.

adm@bak$ snowball -w 10 cp -r -b /home/backup/Storage/www/sites/xxx/fotodb/org s3://migration/xxx/fotodb/org

Pre-checking your source files and folders...
Files scanned: 32.199.888
OBJECT_ALREADY_EXISTS: The object already exists.

|Average Speed: 0 MB/s |
|Total Bytes Transferred: 0 MB/46,56 TB |
|Total Transfer Time: 2 sec(s) |
|Total Files Transferred: 0/32.199.888 |
|Total Files Skipped: 32.199.888/32.199.888 |

Now the support told us to try a Snowball Edge instead of the normal Snowball, but the Job status is still ""Job created"" since last Friday"
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Try this:
adm@bak$ snowball -w 10 cp -r -b -f /home/backup/Storage/www/sites/xxx/fotodb/org s3://migration/xxx/fotodb/org

You don't need a SnowballEdge to get this working. What I sent you in a private message on April 6th applies to this situation. I'll echo those comments in this thread. 

Looking at your messages it looks like you are in a weird state. If you are running into OBJECT_ALREADY exists errors then that means the object has already been copied (or partially copied). Batching get's complicated because the tool doesn't keep track of state. For example - if you have a folder called ""site"" and it batches the contents it will archive the contents and put them on the snowball. but then if you tell it to archive site again it will give you an exception because the files already exist. It doesn't have the ability to easily skip those files because you could have changed the contents within ""site"".

When doing batch it might be helpful to use the -f flag which will force copying the contents. This will assume something has changed and it will replace the contents on the Snowball."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
This message I get from Support about the Snowball Edge:

AWS Support wrote:
Thanks for writing back.
I went through your correspondence about getting a Snowball Edge device, and got in touch with an expert, and he agreed with an affirmative.
I did some research and also found out that Snowball Edge device will bypass the local resources as it has local compute in it."
AWS Import Export Snowball	"Re: Snowball cp operation keeps hanging
Snowball Edge is a great device and when dealing with larger files it's really fast. It will also handle the encryption for you which means it doesn't require a high powered workstation to copy the contents. If you want to use a Snowball Edge then I'd strongly recommend batching of your files: https://docs.aws.amazon.com/snowball/latest/developer-guide/batching-small-files.html 

That said, a regular Snowball will also work.  If you add the force flag -f then it should resolve your concern.

Example:
snowball -w 10 cp -r -b -f /home/backup/Storage/www/sites/xxx/fotodb/org s3://migration/xxx/fotodb/org"
AWS Import Export Snowball	"snowball import stuck at 2.6% for >5 days
Why is the import not making progress, and can it be resolved soon?
This is a proof of concept for my company ultimately storing a couple of petabytes, and the delay's not very reassuring.

the job id is:
JID21b7d616-955e-4839-87bd-165caf5580e3"
AWS Import Export Snowball	"Re: snowball import stuck at 2.6% for >5 days
Let me look into this. I'll respond back by the end of the day via a private message"
AWS Import Export Snowball	"Re: snowball import stuck at 2.6% for >5 days
I have responded with a private message."
AWS Import Export Snowball	"Error creating Import Job request.
Hello Everyone,
I have a 10TB external hard drive full of my data that I intend to import and archive in Glacier.
I have followed the documentation of how to create the import job form this link ""https://docs.aws.amazon.com/AWSImportExport/latest/DG/GSCreateSampleS3ImportRequest.html""
but I'm running into java errors when I run the createJob prompt. see below:
""Version: 2014-12-18
Exception in thread ""main"" java.lang.NoClassDefFoundError: javax/xml/bind/DatatypeConverter
	at com.amazonaws.util.Base64.encodeAsString(Base64.java:39)
	at com.amazonaws.auth.AbstractAWSSigner.signAndBase64Encode(AbstractAWSSigner.java:74)
	at com.amazonaws.auth.AbstractAWSSigner.signAndBase64Encode(AbstractAWSSigner.java:63)
	at com.amazonaws.auth.QueryStringSigner.sign(QueryStringSigner.java:95)
	at com.amazonaws.auth.QueryStringSigner.sign(QueryStringSigner.java:49)
	at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:687)
	at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:467)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:302)
	at com.amazonaws.services.importexport.AmazonImportExportClient.invoke(AmazonImportExportClient.java:655)
	at com.amazonaws.services.importexport.AmazonImportExportClient.createJob(AmazonImportExportClient.java:296)
	at ImportExportWebServiceTool.createJob(ImportExportWebServiceTool.java:849)
	at ImportExportWebServiceTool.main(ImportExportWebServiceTool.java:169)
Caused by: java.lang.ClassNotFoundException: javax.xml.bind.DatatypeConverter
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:190)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:499)
	... 12 more
.......................................................""

I've tried to do it a different computer and had no luck there either.
What do I need to do or change to get this going?

PS. I installed the java SE 10 JDK and this is what pops up when I run java -version in the terminal 
""java version ""10"" 2018-03-20
Java(TM) SE Runtime Environment 18.3 (build 10+46)
Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10+46, mixed mode)"""
AWS Import Export Snowball	"Re: Error creating Import Job request.
Hi Godfrey.  My apologies for the difficulties.  Take a look at AWS Snowball, this product was designed for the kind of transfer you want to do.  If you need to use the import/export disk service, try the AWS CLI and its support for import/export disk, https://docs.aws.amazon.com/cli/latest/reference/importexport/index.html.  

I hope this helps.

Frank"
AWS Import Export Snowball	"Re: Error creating Import Job request.
Thanks Frank this was helpful. AWSCLI is where everyone should go to import CreateJob requests."
AWS Import Export Snowball	"Snowball copy stalled many times on different tries
I have been trying to copy over 50TB of data to a 80TB snowball over ethernet.  I tried to copied all the data over at once, but that failed so I started copying sub directories only with smaller size. however, the copy process stalled eventually at 0MB/s with no activity.

Totals: [Speed: 0 MB/s | 3,854,457/8,331,487 files | 2.37 TB/3.77 TB | Remaining time: 7 hour(s) 2 min(s) 50 sec(s) ]

I hit enter many times and it didn't do anything, so I have to hit ctrl-c and cancel it.  It didn't say failure or something.

I tried a smaller directory and have the same result:
Totals: [Speed: 0 MB/s | 2,329,580/3,343,064 files | 967.53 GB/1,019.36 GB | Remaining time: 21 min(s) 45 sec(s) ]

and more examples:

Totals: [Speed: 0 MB/s | 2,329,580/21,906,718 files | 967.53 GB/10.35 TB | Remaining time: 2 day(s) 3 hour(s) 58 min(s) 56 sec(s) ]
Totals: [Speed: 0 MB/s | 3,854,457/20,492,127 files | 2.37 TB/10.22 TB | Remaining time: 1 day(s) 14 hour(s) 52 min(s) 17 sec(s) ]

Since this is using the small files batch switch by default, I tried to get a listing of what is copied over but look like this in the s3 directory in snowball: vd1_part143.snowballarchives so I don't know how to copy the rest of the deltas over since I can't get a list of successfully copied files on the device.

At this point we are at day 12 of received the snowball and we haven't successfully copied any of our data yet in confidence.  Can someone please assist to see why the copy would stuck at 0MB/s after sometime? 

Alfred"
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
Can I get you to try the latest client (currently version 217). You can download it at https://aws.amazon.com/snowball/tools/

Sorry for any inconvenience this might have caused you. To provide more background. It's a UI glitch, when using batched files, where the Speed goes to 0 at the end of the transfer. If you were to pull up a network monitoring tool you should see a lot of traffic still being transferred to the Snowball device. 

When using batch the client archives a bunch of files into snowballarchives. The archives are copied over to the snowball and then auto extracted during ingestion. 

You do not have to download the latest tool. You current tool should work but do wait for the process to finish copying data over. You are also welcomed to download the latest client which will provide better visibility when using batch."
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
OK, confirmed that we are using 214 version of the client.

However, since last posted this message yesterday, the same copy job that I have left running still show this:

Totals: [Speed: 0 MB/s | 2,329,580/3,343,064 files | 967.53 GB/1,019.36 GB | Remaining time: 21 min(s) 45 sec(s) ]

The copy status have not changed for the last 80+gb or so since I posted this 24 hours ago.

I will use the latest version of the client and try again."
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
Something else might be up if you haven't made any progress for a long period of time. You can certainly try the new client.

Another thing to try is to enable the verbose command ie snowball -v cp -r ... https://docs.aws.amazon.com/snowball/latest/ug/using-client-commands.html#clientverbose

Other things to monitor would be the network traffic. If you ever see a drop and it doesn't pick back up immediately then something might be wrong. You can check the logs https://docs.aws.amazon.com/snowball/latest/ug/using-client.html#snowballlogs and potentially forward them along (best to do in a private message)."
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
Finally.  using client version 217 on verbose mode got this copied over.
--------------------------------------------------------------------------------
|Average Speed: 74.32 MB/s                                                       |
|Total Bytes Transferred: 1,019.36 GB/1,019.36 GB                                |
|Total Transfer Time: 3 hour(s) 54 min(s) 5 sec(s)                               |
|Total Files Transferred: 3,343,064/3,343,064                                    |
--------------------------------------------------------------------------------

I am now trying a bigger size folder.  Thanks."
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
OK.  Using version 217 I was able to finish 2 more copy process.  Marking this as closed.  Thanks."
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
re-opened. still have same problem with v217"
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
Hmm. it's also happening on v217 client:

I have 3 parallel snowball cp commands that have stalled: Here's one of them: (Using the snowball -v for verbose output)

Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: http://2.16 TB/2.42 TB         Current File: 0% Totals: [3,591,784/4,914,169 files | 2.16 TB/2.42 TB | Remaining time: 2 hour(s) 44 min(s)

I check on the network interface and there was no network activity there. 

Also checking top: 
PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND
44495 root 20 0 48.840g 0.020t 9988 S 132.7 16.5 5167:28 java
44903 root 20 0 48.366g 0.021t 10080 S 132.7 17.5 5147:45 java
44354 root 20 0 48.920g 0.022t 10192 S 131.7 17.8 5254:20 java 
There are CPU usage on those 3 snowball jobs, but they are just not doing anything.

How else should we troubleshoot this? Thanks."
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
The general guidance is if the CPU is running then let it run.

How long has it been running with high CPU usage and no network activity?
Are there any recent / active errors in the logs: https://docs.aws.amazon.com/snowball/latest/ug/using-client.html#snowballlogs ?

Since you have verbose running you'll know what the last processed file was. Is the list actively cycling between new files? Are the files that it's processing small or large?"
AWS Import Export Snowball	"Re: Snowball copy stalled many times on different tries
I have sent you a pm since the files name are sensitive, can you follow up further with me with pm? Thanks."
AWS Import Export Snowball	"Migrating Terabytes of On-premise Oracle database to AWS Aurora?
We want to migrate Terabyte size of On-premise Oracle database to AWS Aurora. Can we use snowball to migrate that? If yes, how will be migrate the ongoing changes as well to Aurora?"
AWS Import Export Snowball	"Re: Migrating Terabytes of On-premise Oracle database to AWS Aurora?
Yes you can use snowball to do this. There are a couple of options:

1. You can use Snowball to manual extract and copy data over to Snowball and then Snowball will deliver whatever you put on it to S3.

2. There is an AWS  Database Migration tool that can integrate with snowball for doing something that you are looking for. Take a peek at this article: 
https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html
Here is the announcement link: https://aws.amazon.com/about-aws/whats-new/2017/11/aws-database-migration-service-adds-support-for-aws-snowball/"
AWS Import Export Snowball	"Snowball Import Job stuck at ""Job Created""
I created a snowball import job using the 50 TB snowball for me to migrate about 5 TB of data into AWS GovCloud back on 2/22/18.

It has since been stuck in the ""Job Created"" state. 

I'm new to AWS S3 but it seemed like this would be a quicker process.

Am I forgetting something?

Job ID: JID82d88f55-4b84-4032-8e9f-82fdbf1acc7d

Edited by: TylerMBlueFin on Feb 27, 2018 11:42 AM"
AWS Import Export Snowball	"Re: Snowball Import Job stuck at ""Job Created""
Bump! Been a week now and still in ""Job Created"" Is this normal?

This is a time critical task that I need to complete.

Any feedback will be greatly appreciated.

-Thank you
Tyler"
AWS Import Export Snowball	"Import/Export Disk Job Pending at AWS
This seems to happen to a few folk - I too have an export job still 'Pending'. It has been open since Jan 18 which is when it was shipped, arriving at AMZ within a couple of days but nothing has happened since. Not even a received notification. I have clients waiting for the data and it's getting a bit awkward not being able to let them know when they can have it. 

The job id is: EU-YAEBB"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
Can you private message me your jobid?"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
Sent dm with jobid a couple of days ago but still no response from AWS. The job has been in ""pending"" for nearly two weeks :/"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
Still no help on this issue. No update. Still pending. Still really disappointed.

What's the problem?"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
My job is also still pending since early January... What's going on here?"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
Hi mattynet.  My apologies for the delay here, can you PM me your job ID and I can investigate.

Thanks!

Frank"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
Sent!"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
Hi mattynet.  I replied to your PM.

Frank"
AWS Import Export Snowball	"Snowball to Volume Storage gateway... with a catch
How would one utilize a Volume storage gateway, where the storage gateway is on the local network, and get the bulk of the data up to AWS via snowball?

On our local network we will connect to the gateway server (VMware)over iSCSI from Win2016, and it will be an SMB share.

This means, no snapshots, nor ebs volumes. This is not an EC2 storage gateway."
AWS Import Export Snowball	"Disable --batch processing for large repositories
We are wanting to transfer a fairly small (6TB) repository onto our Snowball, but it has a lot of files. We also don't have a lot of memory available on the systems it's practical for us to use, so want to avoid using --batch.

However, there doesn't seem to be an option to turn it off, and the snowball client currently turns it on automatically once the number of files gets to a certain size. And the option to choose ""N"" when the client suggests that it switch to --batch doesn't seem to work.

Is there any way of turning off --batch entirely?"
AWS Import Export Snowball	"Re: Disable --batch processing for large repositories
The workstation specs outline that a lot of ram is needed. https://docs.aws.amazon.com/snowball/latest/ug/specifications.html#workstationspecs 

I would caution you not to disable batching. It will really slow down your transfer time when you have lots of little files. This is especially true during the ingestion process. The performance difference is often 30 or 40 times faster when batch is enabled.

If you do really want to disable batch, when prompted to disable batch and you press N make sure you also press the enter key. That's the only way it receives the command. It will then tell you that batch is disabled.  You can also disable it by use the --quite command but once again it's strongly recommend to keep batch enabled when there are lots of small files."
AWS Import Export Snowball	"How to use Snowball Import with S3 Server Side Encryption (SSE-S3)
Snowball documentation indicates AWS managed Server Side Encryption  (SSE-S3,
http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html) can be used with a Snowball import job (""AWS Snowball supports server-side encryption with Amazon S3–managed encryption keys (SSE-S3).""  from  http://docs.aws.amazon.com/snowball/latest/ug/security.html).

However, I an unable to find specifics about how we enable this.

Can anyone shed light, and (a) confirm this can be done, and (b) provide specifics of how to specify this?

Thanks -

Stephen"
AWS Import Export Snowball	"Re: How to use Snowball Import with S3 Server Side Encryption (SSE-S3)
Hi Stephensykes.

To have your data imported via Snowball using S3 SSE, simply add a policy to your bucket that enforces that all writes are done via SSE and Snowball will follow that policy.  For information about how to create such a policy, please see http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html

Frank"
AWS Import Export Snowball	"Re: How to use Snowball Import with S3 Server Side Encryption (SSE-S3)
Hi Frank -

It sure would be nice if the documentation for snowball said that.  The policy says to reject (deny) updates that do not request encryption.  That Snowball checks the policy and adjusts its writes to not get rejected is a big leap. 

Not saying I don't believe you; just that this is not implied or even hinted at by the Snowball documentation. It's also a bit hard to test without an expensive and time consuming exercise. Can you point me towards documentation on Snowball that confirms this?  Or can you confirm  that your answer is an official AWS statement of support?  (I know, I'm coming across a bit ... stuffy. But it's about to drive a decision which, if it's not correct, is a pretty big deal.)

Regardless I'd urge AWS to update the docs to make this point clear.

Thanks for understanding, and any further backup to make this a simpler call.

Much appreciated -

Stephen"
AWS Import Export Snowball	"Re: How to use Snowball Import with S3 Server Side Encryption (SSE-S3)
Hi there Stephen,

I'm Dan, the technical writer for the Snowball docs. Thanks for your feedback on the docs. I'll update this topic: http://docs.aws.amazon.com/snowball/latest/ug/security.html#encryption and this topic: http://docs.aws.amazon.com/snowball/latest/developer-guide/security.html#encryption to add the the correct information on how to configure the S3 buckets for SSE-S3 as Frank mentioned.

Thanks,

-Dan"
AWS Import Export Snowball	"Re: How to use Snowball Import with S3 Server Side Encryption (SSE-S3)
Thanks - much appreciated.  I'll take this as gospel and we'll move forward.

Best -

Stephen"
AWS Import Export Snowball	"Re: How to use Snowball Import with S3 Server Side Encryption (SSE-S3)
I had a follow up on this question. If default SSE is enabled on the S3 bucket using the console will this still take effect on the Snowball without the policy in place? Is there a way to check if encryption has been applied on a Snowball device? Thanks."
AWS Import Export Snowball	"Stuck on Preparing shipment
Have been waiting almost two weeks as our snowball job is still stuck on: Preparing shipment. Is this typical? I thought it would go out within a day or two?"
AWS Import Export Snowball	"Stream cp upload with s3 adapter, tar, and gzip
I'm attempting to back up a number of folders directly to two snowball devices with the s3 adapter. Each snowball is connected directly to the server, each to a different 1G interface/ip address. There are 2 lists of folders to back up and their sizes. For each line the following is run:
tar -cf - $folder  |  pigz  | aws s3 --endpoint http://$ip cp --expected-size $size - s3://mybucket/$tgz_name

In small tests this worked fine but it is now failing with vague and intermittent errors.

upload failed: - to s3://mybucket/large_folder.tgz An error occurred (InternalError) when calling the CreateMultipartUpload operation (reached max retries: 4): Failed to initiate multipart upload on snowball


Not all uploads failed and I was able to see them with aws s3 --endpoint http://$ip ls 


I tried stopping the snowball adapter and checking the status with the snowball client which showed that the 180GB were used but only one 5GB file was present.

Anybody familiar with setting up the using tar directly to the snowball device or what might cause ls to not show uploaded files?

Thanks."
AWS Import Export Snowball	"Re: Stream cp upload with s3 adapter, tar, and gzip
When you run an LS it will only show you completed files. 

What you are doing looks like it should work. If you need immediate help I would encourage you to open up a support ticket. I do have a few questions that might help in the debugging process:
Does it always fail with the same folder or if you retry the operation does it work on the second attempt?
If you don't pipe to pigz does it work? 
If you first stage to disk does it work?
How big are the tar.gz files?
Are you running multiple copy commands in parallel?

As a side note, for regular Snowball, there is a batch command which will auto batch in the files and then auto explode them during ingest. You can read more about this here https://forums.aws.amazon.com/ann.jspa?annID=5147"
AWS Import Export Snowball	"Re: Stream cp upload with s3 adapter, tar, and gzip
Thank you very much for the response.
Based on some test runs it looks like there is a OutOfMemory error on the snowball when copying files with an expected size over ~1.1TB (a ~1.1TB files copied successfully but 1.15 failed). After failing, files around 800GB that were successful before would also fail until I restarted the snowball. It seems to be working now on the files with an expected size >= 1.1TB but I would like to avoid splitting the larger files if possible. 

Does it always fail with the same folder or if you retry the operation does it work on the second attempt?
It always will fail and after failing, ones that previously worked no longer worked.
If you don't pipe to pigz does it work? 
No
If you first stage to disk does it work?
No
How big are the tar.gz files?
200GB up to ~4TB. I read that S3 will take up to 5TB objects so I expected that anything under this would be handled by snowball.
Are you running multiple copy commands in parallel?
There is 1 command for each snowball device.

There were a few errors while streaming that I've added below.

Errors in start up (always happens):
java.lang.UnsatisfiedLinkError: no netty-tcnative-linux-x86_64 in java.library.path
java.lang.UnsatisfiedLinkError: no netty-tcnative-linux-x86_64-fedora in java.library.path

Errors in copying (happens intermittently but doesn't prevent upload):
2017-12-03 11:31:15 DEBUG SnowballAdapterServerErrorHandler:62 - com.aws.snowball.adapter.server.netty.handlers.s3.SnowballS3AdapterHttpServerException

Error in copying 3TB file:
2017-12-03 17:55:17 ERROR SnowballAdapterServerErrorHandler:74 - Returning internal error to client 
java.lang.OutOfMemoryError: Java heap space
2017-12-03 17:57:18 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 100 milliseconds.
2017-12-03 17:57:19 DEBUG OpenSslEngine:590 - SSL_read failed: OpenSSL error: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC
2017-12-03 17:57:19 DEBUG SnowballClientNettyHandler:128 - javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC
io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC

Error in files since OOM error (fixed after restarting snowball):

2017-12-03 18:39:17 DEBUG SnowballClientNetty$1:127 - Channel acquired: [id: 0xde6a96ea, L:/10.0.0.2:59256 - R:/10.0.0.10:8080]
2017-12-03 18:39:17 DEBUG SnowballChannel:37 - Waiting for channel
2017-12-03 18:39:17 DEBUG SnowballChannel:44 - Got channel.
2017-12-03 18:39:19 DEBUG OpenSslEngine:590 - SSL_read failed: OpenSSL error: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC
2017-12-03 18:39:19 DEBUG SnowballClientNettyHandler:128 - javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC
io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC
Caused by: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC

2017-12-03 18:39:19 ERROR MultipartUploadService:169 - There was an unexpected problem transferring the chunk data to Snowball
com.amazon.aws.awsie.snowball.exceptions.SnowballClientException: io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC
Caused by: io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC
Caused by: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC

2017-12-03 18:39:19 ERROR BufferedPartWritableByteChannel:67 - com.amazon.aws.awsie.snowballsdk.exception.AmazonSnowballException: Unable to complete operation. Root 
cause is SNOWBALL_SERVER_ERROR; ErrorType: SNOWBALL_SERVER_ERROR
2017-12-03 18:39:19 DEBUG SnowballClientNetty$1:122 - Channel released: [id: 0xde6a96ea, L:/10.0.0.2:59256 - R:/10.0.0.10:8080]

2017-12-03 18:39:20 DEBUG SnowballClientNetty$1:127 - Channel acquired: [id: 0xde6a96ea, L:/10.0.0.2:59256 - R:/10.0.0.10:8080]
2017-12-03 18:39:20 DEBUG SnowballChannel:44 - Got channel.
2017-12-03 18:39:20 INFO  RequestRetrier:88 - Encountered error when making request: com.amazon.aws.awsie.snowball.exceptions.SnowballClientException - io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC. Retrying. https://forums.aws.amazon.com/ Sleep for 100 milliseconds.
2017-12-03 18:39:21 INFO  RequestRetrier:88 - Encountered error when making request: com.amazon.aws.awsie.snowball.exceptions.SnowballClientException - io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC. Retrying. https://forums.aws.amazon.com/ Sleep for 200 milliseconds.
2017-12-03 18:39:21 INFO  RequestRetrier:88 - Encountered error when making request: com.amazon.aws.awsie.snowball.exceptions.SnowballClientException - io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: error:100003fc:SSL routines:OPENSSL_internal:SSLV3_ALERT_BAD_RECORD_MAC. Retrying. https://forums.aws.amazon.com/ Sleep for 400 milliseconds.

2017-12-03 18:44:27 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 1000 milliseconds.
2017-12-03 18:44:33 INFO  RequestRetrier:88 - Encountered error when making request: java.util.concurrent.TimeoutException - null. Retrying. https://forums.aws.amazon.com/ Sleep for 100 milliseconds.

Edited by: jor-el on Dec 4, 2017 11:15 AM"
AWS Import Export Snowball	"Re: Stream cp upload with s3 adapter, tar, and gzip
It sounds like you are using Snowball w/ the S3 Adapter and not Snowball Edge?

The Snowball should handle files less than 5TB (parity w/ S3 in the cloud). Using the S3 adapter on Snowball has the potential to overwhelm a Snowball esp if you are using very large files.  If the Snowball is running out of memory, then I'd recommend to play with is the number of parallel threads (max_concurrent_requests) and the chunk size (multipart_chunksize). You can read more about that here  http://docs.aws.amazon.com/cli/latest/topic/s3-config.html. I would tune one or both of the parameters down to relieve some pressure on the Snowball.

The other option to try is to use the native client instead of the S3 Adapter. That will automatically scale down the chunk sizes to prevent overwhelming the Snowball."
AWS Import Export Snowball	"Re: Stream cp upload with s3 adapter, tar, and gzip
It sounds like you are using Snowball w/ the S3 Adapter and not Snowball Edge?  
The other option to try is to use the native client instead of the S3 Adapter. That will automatically scale down the chunk sizes to prevent overwhelming the Snowball.
I am. I'm using the adapter because from what I can tell I can't stream to the s3 client but I'll try that next for the larger files.

I'd recommend to play with is the number of parallel threads (max_concurrent_requests) and the chunk size (multipart_chunksize. http://docs.aws.amazon.com/cli/latest/topic/s3-config.html. I would tune one or both of the parameters down to relieve some pressure on the Snowball.
I will give this a shot.

Thanks for the help. I'm going to let this run for now on the smaller files since its  and will write back the results of the larger files later."
AWS Import Export Snowball	"Re: Stream cp upload with s3 adapter, tar, and gzip
I'd recommend to play with is the number of parallel threads (max_concurrent_requests) and the chunk size (multipart_chunksize).
Also, I haven't tried it yet but I think the --expected-size option modifies these values. Is there any downside to setting the max_queue_size to 20000 or the expected size to 1TB regardless of real size?"
AWS Import Export Snowball	"Re: Stream cp upload with s3 adapter, tar, and gzip
Changing the max_queue_size is fine, that is only a client side change. expected-size can change the chunk size and to some degree you need it to be high. You are limited to 10k chunks which translates to 100 MB chunks if you use all 10k parts for a 1 TB file. When dealing with that large of a file it's probably best to lower the concurrent requests. My recommendation is to turn it down to less than 10 threads (try 5) when dealing with files larger than a terabyte. 

You are correct in that the Snowball client tool doesn't support streaming (it cannot read from stdin). But it is pretty quick and if you have lots of files the new batch command works well. It also does a really good job in the memory management for large files (it's much harder to overwhelm a Snowball with the client tool). It looks like you already have a source folder and you could tell the client to copy that folder onto the Snowball. The main difference is that the files won't be in a single tar file when they arrive in S3."
AWS Import Export Snowball	"Export failing with too many small files
Our snowball job was cancelled after waiting for over a month.  I'm not getting anything from support on what went wrong other than the problem is too many small files.  They suggested ""to zip these files using an ec2 instance to accelerate the transfer,"" but I've had no luck trying to decipher that into an actual list of commands to run.  Can anyone shed some light on what I can do to make this happen?"
AWS Import Export Snowball	"Snowball Ethernet Issue
I'm having an issue with an AWS Snowball that I recently received. Its not getting assigned an IP via DHCP and even being assigned a Static IP, I can't ping or telnet to the device. This is for the RJ45 Ethernet port. The issue I'm having is exactly the same as reported in this post:

https://forums.aws.amazon.com/thread.jspa?messageID=780713򾦩

As he reports that the right LED indicator on the Snowball's RJ45 Ethernet port keeps blinking constantly, even while unplugging the cable, I am experiencing this same thing as well. 

I've tried all the same troubleshooting steps as he did to no avail.

Tried different RJ45 cables, and powercycled the device per the instructions listed here:
https://aws.amazon.com/premiumsupport/knowledge-center/troubleshoot-connect-snowball/

Can you please provide assistance and how I should handle this?

Thanks."
AWS Import Export Snowball	"Snowball delivered but UPS still has it 'in transit'
Hi,
my Snowball was delivered but the UPS tracking site still says in-transit.

Can't continue until the state is changed.

Thanks,
Sean"
AWS Import Export Snowball	"Re: Snowball delivered but UPS still has it 'in transit'
Hi SeanQ.  Please PM me your job id so that I can investigate.

Thank you.

Frank"
AWS Import Export Snowball	"Re: Snowball delivered but UPS still has it 'in transit'
PM'd job ID.

Thanks,
Sean"
AWS Import Export Snowball	"Re: Snowball delivered but UPS still has it 'in transit'
Hello Sean,
Thank you very much. We will get back to you with further details.

Najah"
AWS Import Export Snowball	"Re: Snowball delivered but UPS still has it 'in transit'
Hi Sean.  Sorry again for the delay, you should be unblocked now.

Frank"
AWS Import Export Snowball	"Create a job for direct import to Glacier
I want to transfer 25TB on-prem data to aws glacier. How to import my whole data from snowball to glacier directly without uploading on S3 and how import daily recursive data to glacier over night."
AWS Import Export Snowball	"Re: Create a job for direct import to Glacier
Hi there,

There's no method through which data can be imported from a Snowball into Amazon Glacier without going through Amazon S3 lifecycle rules. Sorry about that.

It also sounds like you've got a question about daily transfers from a Snowball into AWS. This would a challenge, as the shipping speeds (1 day, 2 day, express, standard, etc) are not measured in how soon the Snowball or Snowball Edge would appear at your address after you ordered the job, but how soon the device would get to the address after the job has been processed. Processing can take anywhere from a few hours to a few days, depending on time of day, day of the week, and type of job.

If you have other questions, let us know!

-Dan"
AWS Import Export Snowball	"Re: Create a job for direct import to Glacier
Thanks for your reply Dan, If i choose to go through S3, what would be the process for that? Is it like, I ordered a snowball, copy my data to snowball, return snowball and import the same to my S3 bucket, put transition period of 1day on bucket for transition to glacier and import the data on glacier.
Furthermore, on daily basis, I zip my data, import to s3 and again transfer to glacier. 
Will my uploaded data be browsable or not?
Approx how much is it cost for 25tb complete job?
Can i put my daily data on same glacier bucket?"
AWS Import Export Snowball	"Re: Create a job for direct import to Glacier
ITSupport wrote:
Thanks for your reply Dan, If i choose to go through S3, what would be the process for that? Is it like, I ordered a snowball, copy my data to snowball, return snowball and import the same to my S3 bucket, put transition period of 1day on bucket for transition to glacier and import the data on glacier.

That process sounds about right to me. Here's a link to some documentation about how to configure a lifecycle rule for S3 using the console: http://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html

ITSupport wrote:
Furthermore, on daily basis, I zip my data, import to s3 and again transfer to glacier. 

You can continue to repeat that process as often as you like, but keep in mind that a daily import is highly unlikely due to the factors I called out in my previous message.

ITSupport wrote:
Will my uploaded data be browsable or not?

Here's some documentation on how to get the inventory of a vault: http://docs.aws.amazon.com/amazonglacier/latest/dev/vault-inventory.html

ITSupport wrote:
Approx how much is it cost for 25tb complete job?
Can i put my daily data on same glacier bucket?

I'm not familiar with the pricing data, sorry. Here's a link to the pricing information for both Amazon S3 and for Glacier:

https://aws.amazon.com/s3/pricing/
https://aws.amazon.com/glacier/pricing/

ITSupport wrote:
Can i put my daily data on same glacier bucket?

Amazon Glacier vaults can hold any amount of data, each archive in a vault can hold 40 TB of data.

Hope that helps!

-Dan"
AWS Import Export Snowball	"Deleting Snowball Files
Hey All,

Im looking at trying to figure out how to delete all of my files that i have on my snowball and restart the import process, is there a way to delete the files so that i can upload a different file? Basically i need to start fresh without having to send the snowball back and get a new one. 

When i try snowball rm -r it deletes the destination path but says that the file space is still there.

The file disappears from site and when i run ls it doesn't show that anything is there but if i do a snowball status it still shows that the space is full.

If this is possible to reset the snowball i would love the help."
AWS Import Export Snowball	"Re: Deleting Snowball Files
Hi there,

The 'rm -r' command was the correct one to run to remove your files. This operation can take some time, but the 'ls' command will show you what the state will be when the deletion of those files finishes. Try the status command again and then again in an hour and you should see the used space decreasing.

Let us know if you have any other questions,

-Dan"
AWS Import Export Snowball	"Re: Deleting Snowball Files
Its been a couple days now, and while the files won't appear when doing an ""ls"", the free space has remained the same.

Is there any way to reformat or somehow ""reset"" a snowball?

Thanks!"
AWS Import Export Snowball	"Re: Deleting Snowball Files
Hm. Can you try rebooting the Snowball, unlocking it again, and letting us know if the status command returns the correct size?"
AWS Import Export Snowball	"snowball client ""validate"" command reports apparently random number of file
Environment
 * Centos6 Linux, 
 * snowball client version  1.0.1 build 82

Precondition
copied 594 files totalling 22TB to the device using snowball cp command; the copy finished with no obvious complaint in the logs. Performing `snowball ls` shows the 594 files in the target bucket/prefix:
[root@hpchsmlan01 bin]# ./snowball ls s3://private-a17037da-d031-43e8-a3a7-f7c800c34ab3/EGA_PNET/ |wc -l
594


Action
Validate the destination directory
./snowball validate s3://private-a17037da-d031-43e8-a3a7-f7c800c34ab3/EGA_PNET


Expected behaviour
According to docs at http://docs.aws.amazon.com/AWSImportExport/latest/ug/using-client-commands.html performing snowball validate <path>
 ""If you specify a path, then this command validates the content pointed to by that path and its subdirectories"". 
So I would expect a report on 594 files.

Actual behaviour
The number of files reported is not 594, and indeed it changes (up or down) from run to run of validate command even though there are no background copy/move/delete operations going on:
[root@hpchsmlan01 bin]# for i in `seq 1 3`; do date; ./snowball validate s3://private-a17037da-d031-43e8-a3a7-0c34ab3/EGA_PNET |dos2unix; done
Mon Aug 29 12:30:18 AEST 2016
Validating files on Snowball
 
[Total: 1                     Successful: 1                 Invalid: 0                    Incomplete: 0]           [Total: 268                   Successful: 268               Invalid: 0                    Incomplete: 0]                Mon Aug 29 12:30:23 AEST 2016
Validating files on Snowball
 
[Total: 1                     Successful: 1                 Invalid: 0                    Incomplete: 0]           [Total: 264                   Successful: 264               Invalid: 0                    Incomplete: 0]                Mon Aug 29 12:30:26 AEST 2016
Validating files on Snowball
 
[Total: 1                     Successful: 1                 Invalid: 0                    Incomplete: 0]           [Total: 49                    Successful: 49                Invalid: 0                    Incomplete: 0]           [Total: 49                    Successful: 49                Invalid: 0                    Incomplete: 0]           [Total: 49                    Successful: 49                Invalid: 0                    Incomplete: 0]           [Total: 127                   Successful: 127               Invalid: 0                    Incomplete: 0]           [Total: 470                   Successful: 470               Invalid: 0                    Incomplete: 0] 


Comment
Supplying the individual item paths to `snowball validate` does work for each of the 594 files on the device."
AWS Import Export Snowball	"Re: snowball client ""validate"" command reports apparently random number of file
Can you download the latest version of the client (build 83) for Linux. The was a bug fix to the validator code in build 83. Will you let me know if that works."
AWS Import Export Snowball	"Re: snowball client ""validate"" command reports apparently random number of file
I will download build 83; unfortunately I can't check whether the fix has resolved the issue in this particular case because that Snowball unit has been returned. We will have another onsite soon though, I'll let you know how validate command works then."
AWS Import Export Snowball	"Re: snowball client ""validate"" command reports apparently random number of file
Re-tested with new snowball and new client version; `validate` now works as advertised.

Thanks!"
AWS Import Export Snowball	"Re: snowball client ""validate"" command reports apparently random number of file
While am running Validate command am getting ""FAILED TO VALIDATE OBJECT"" error for few number of fails. 

What I will do for this time? 

Please share your knowledge on this ,thanks in advance."
AWS Import Export Snowball	"Java Exception when copying from Snowball
I'm getting an occasional error when copying from a snowball. I'm using OS X 10.11 El Capitan, with the latest version of the snowball client 1.0.1 Build 146. 

What's going on and what can I do to fix this?

--
2017-08-31 08:50:59 ERROR TreeHashCalculator:156 - There was an unexpected problem finalizing the checksum future
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:192)
        at com.amazon.aws.awsie.frozen.crypto.TreeHashCalculator.calculatedChecksum(TreeHashCalculator.java:153)
        at com.amazon.aws.awsie.frozen.crypto.TreeHashCalculator.validate(TreeHashCalculator.java:138)
        at com.amazon.aws.awsie.snowballclient.service.downloadservice.DownloadReader.validateDownload(DownloadReader.java:150)
        at com.amazon.aws.awsie.snowballclient.service.downloadservice.DownloadReader.performParallelReading(DownloadReader.java:129)
        at com.amazon.aws.awsie.snowballclient.service.downloadservice.DownloadService.downloadToFile(DownloadService.java:347)
        at com.amazon.aws.awsie.snowballclient.service.downloadservice.DownloadService.traverseHelper(DownloadService.java:221)
        at com.amazon.aws.awsie.snowballclient.service.downloadservice.DownloadService$1.run(DownloadService.java:183)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at com.amazon.aws.awsie.frozen.crypto.TreeHashCalculator$ChunkChecksum.access$300(TreeHashCalculator.java:213)
        at com.amazon.aws.awsie.frozen.crypto.TreeHashCalculator$1.call(TreeHashCalculator.java:108)
        at com.amazon.aws.awsie.frozen.crypto.TreeHashCalculator$1.call(TreeHashCalculator.java:72)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        ... 1 more
--"
AWS Import Export Snowball	"Snowball client copy is very slow
I'm running a snowball recursive copy, and it's running very slowly. I'm getting about 60KB/s transfer rate over a 1Gbit/s connection. The Snowball and the client machine are connected to the same switch. What can I do to diagnose this? 

I've found that if I run 2x copies in parallel, the bandwidth used goes up a little to about 80KB/s."
AWS Import Export Snowball	"Re: Snowball client copy is very slow
Hi there,

We have some guidance in the documentation on how to improve your performance with the Snowball client: http://docs.aws.amazon.com/snowball/latest/ug/BestPractices.html

Let us know if that helps

-Dan"
AWS Import Export Snowball	"Re: Snowball client copy is very slow
I ended up writing a script that copied multiple directories off the snowball in parallel. This has sped up the copy process significantly."
AWS Import Export Snowball	"Import/Export Disk Job Pending at AWS
Hi,

I have 4 export disk jobs and the hard drives arrived at AWS on Jul 28, 2017. 3 of the jobs have been completed and the hard drives have been shipping back. However, the remaining one (Job ID: F347B) is still showing pending for almost 3 weeks. Can someone help me check the status and possibly move it along the process?

many thanks,
Patricia"
AWS Import Export Snowball	"Re: Import/Export Disk Job Pending at AWS
Hello Patricia,

Sorry for the delay. 

The job has been complete and drive should be arriving shortly back to you. Hope this helps.

Thanks
Dilip S."
AWS Import Export Snowball	"Snowball listing times out
Hello.
I have a huge amount of documents being transferred to snowball (3.48 million). I am facing two issues in my snowball transfer. 
A) I'd like to ensure the files I copied are in good state for which I ran ""snowball -v validate > validate_output.log""
This ran for an hour or so and showed some meaningless message on the screen and exited. I have a total of 3.48 million files already transferred but it shows ""Total: 23940                 Successful: 23940"". Does not make sense. This operation did not generate any useful debugging info/error.

B)  For reconciliation/verification purposes, I need to check the number of documents that are transferred. For which i executed ""snowball ls s3://mybucketname > list_output.log"" . But 
1. it seems to be very slow (Took about an hour)
2. After an hour, it listed around 62000 and timed out. How do I get all 3.48 million objects to be output?

I have attached the log file for both the problems. Can you please help? Any input is greatly appreciated."
AWS Import Export Snowball	"Re: Snowball listing times out
Hello Baskar,

snowball -v validate command might take some time to complete, and might appear to be stuck from time to time. This effect is common when there are lots of files, and even more so when files are nested within many subfolders. I would recomend rerunning it and leaving the console running.

With regards to snowball ls command, unfortunately there is no easy way to do this recursively. The snowball ls command lists the Snowball contents in the specified path. You can run snowball ls s3://mybucket/myfolder.. and accumulate the output.

Also if you could send me the actual log files including the ones from logs location as a PM would help.
http://docs.aws.amazon.com/snowball/latest/ug/using-client.html#snowballlogs

Thanks,
Dilip S."
AWS Import Export Snowball	"Snowball edge not powering on
Hi folks

We've received a Snowball edge to start bulk data uploads with, but since receiving it, plugging it into power and giving it a network cable, it has not switched on.  The kindle display shows nothing.  the e-ink label does show the delivery address still.

We have tried short, medium and long presses of the power button, but nothing seems to kick it into gear.  The machine itself seems to have power (NIC connectivity, PSU is slightly warm)

Are there any tricks to get it going, at least to a state where it might tell us where to call for support, as I don't believe we should pay for support if the device is faulty.

Many thanks,
Joe"
AWS Import Export Snowball	"Re: Snowball edge not powering on
Hello Joe,

I have opened a Support case on your behalf to help investigate this issue. If you could login to your AWS Console and go to Support Center you should be able to see the Support Case.

I would appreciate if you could help us with the details requested and continue our communication over the support case.

Thanks for understanding.

Dilip S."
AWS Import Export Snowball	"Snowball export EBS snapshot in S3
I'm trying to export an EBS snapshot in an Snowball job, but since the EBS snapshots are not user-visible, the Snowball job interface doesn't show them. Is there a way to do this or do I need to somehow transfer the data from EBS over to a normal S3 bucket so I can get at it?"
AWS Import Export Snowball	"Re: Snowball export EBS snapshot in S3
Hi Stephen I.  AWS Snowball is used to transfer data into and out of S3 so you will need to copy your data to an S3 bucket in order to export it with Snowball.

Frank"
AWS Import Export Snowball	"UPS says delivered but Job Status says NotReceived
Hi, 

UPS Singapore say it delivered on Wednesday, 16 August, but the Job Status has LocationCode: NotReceived. Shipping was from Singapore to Singapore.
This our own drive i.e. Import/Export, not Snowball
JobID: AP-DQY7V
Thanks
Tony"
AWS Import Export Snowball	"Import/Export Disk Stuck in Limbo at AWS
I don't see a forum for Import/Export Disk (as opposed to Import/Export Snowball), so I'm posting here.

I successfully created a job (BJ53J), shipped my drive to AWS, and it arrived 13 days ago. get-status reports ""Your device is at AWS.	Pending	The specified job has not started.""

My understanding was the import usually happens within a day of receipt. Any way to figure out what's going on? I don't have an AWS developer account so can't open a ticket.

Thanks,
Adam

Edited by: adam500 on Nov 17, 2015 11:14 AM"
AWS Import Export Snowball	"Re: Import/Export Disk Stuck in Limbo at AWS
Hi Adam,

Apologies for the delayed start of job BJ53J. I've asked the the Import/Export team to investigate. I/E Jobs normally start quite soon after we receive the storage device - a two week delay is quite unusual.

Kind regards,
Chris"
AWS Import Export Snowball	"Re: Import/Export Disk Stuck in Limbo at AWS
Hi Adam,

I see that job BJ53J has completed. Apologies again that it took much longer than expected.

Kind regards,
Chris"
AWS Import Export Snowball	"Re: Import/Export Disk Stuck in Limbo at AWS
I'm having a similar issue, JobId: E8JHV

It's been InProgress since 4/21 and we really need the devise back.

Let me know what to do, thanks."
AWS Import Export Snowball	"Re: Import/Export Disk Stuck in Limbo at AWS
Hi nickmerwin,

I can see that job E8JHV completed on April 30th but that your device hasn't shipped back.

I've asked engineering to follow this up as a matter of urgency.

We'll post back when we know more.

Richard"
AWS Import Export Snowball	"Re: Import/Export Disk Stuck in Limbo at AWS
Hi Nick,

Your device has been shipped now. My apologies for the delay.

Richard"
AWS Import Export Snowball	"Re: Import/Export Disk Stuck in Limbo at AWS
I also have a similar issue. 

I have an export job ID: F347B that the hard drive has arrived for 3 weeks. But the job still has not started.  

JobId:                       F347B
CreationDate:                Fri Jul 21 10:20:54 PDT 2017
JobType:                     Export
LocationCode:                AtAWS
LocationMessage:             Your device is at AWS.
ProgressCode:                Pending
ProgressMessage:             The specified job has not started.
ErrorCount:                  0
LogBucket:                   null
LogKey:                      null
Carrier:                     null
TrackingNumber:              null

We really need the data exported and the device back to us. Would you please me let me know what to do?

many thanks,
Patricia"
AWS Import Export Snowball	"Copy to Storage Gateway Volume
Is it possible to copy data from a snow ball into a Storage Gateway volume?  We are looking to populate a storage gateway with 17 TBs of backup files that will be used as the basis for additional backups moving forward."
AWS Import Export Snowball	"Re: Copy to Storage Gateway Volume
Yes. Once the Snowball job is complete and the data is in S3:
1. Spin up an EC2 instance and attach an EBS volume
2. Copy the data from S3 to the EBS volume
3. Take a snapshot of the EBS volume
4. Create a SGW volume from the EBS snapshots.
The data you imported through Snowball will now be available on the SGW volume."
AWS Import Export Snowball	"Re: Copy to Storage Gateway Volume
How does this change when you are shipping 100+TB's on multiple Snowballs, considering EBS volumes have a maximum capacity? Will you be able to create the SGW volume with multiple snapshots from those different EBS volumes?"
AWS Import Export Snowball	"Re: Copy to Storage Gateway Volume
The maximum size of a EBS volume is 16 TB. While Storage Gateway support cached volumes up to 32 TB. You can't create a volume from multiple EBS snapshots in either SGW or EBS. So, you need to partition your data into 16 TB tranches to use the above method.

If you wanted to take advantage of the larger SGW volumes, you would need copy to an SGW cached volume on an EC2 Storage Gateway rather than an EBS volume (step 2 above). The steps for this would be:

1. Create an EC2 Storage Gateway (http://docs.aws.amazon.com/storagegateway/latest/userguide/ec2-gateway-common.html)
2. Create a cached volume (up to 32 TB) on this Storage Gateway (http://docs.aws.amazon.com/storagegateway/latest/userguide/GettingStartedCreateVolumes.html)
3. Spin up an EC2 instance and connect to this cached volume over iSCSI (http://docs.aws.amazon.com/storagegateway/latest/userguide/GettingStarted-use-volumes.html#GettingStartedAccessVolumes)
4. On this instance, copy the data from S3 to the iSCSI attached volume.

If you want to access this data from an on-premises SGW you'd then follow the same process as before:

5. Take a snapshot of the SGW cached volume on the EC2 gateway (http://docs.aws.amazon.com/storagegateway/latest/userguide/managing-volumes.html#CreatingSnapshot)
6. On the on-premises SGW, create a cached volume from this EBS snapshots.

Another option if both on-premises and EC2 gateways were in the same AWS Region, is to clone the SGW volume and skip creating the EBS snapshot altogether (http://docs.aws.amazon.com/storagegateway/latest/userguide/managing-volumes.html#clone-volume)"
AWS Import Export Snowball	"Get a Connection Refused error when I try to run snowball start
When I try to run snowballl start with the correct IP, unlock code and manifest (I've checked many times) I get the error ""Connection refused: connect""

The error in the log is:
2017-03-24 15:47:10 ERROR JobHandler:168 - Exception while connecting to Snowball
com.amazon.aws.awsie.snowball.exceptions.SnowballClientException: java.net.ConnectException: Connection refused: connect
	at com.amazon.aws.awsie.snowball.netty.SnowballClientLite.connect(SnowballClientLite.java:119)
	at com.amazon.aws.awsie.snowballclient.service.JobHandler.initializeClient(JobHandler.java:165)
	at com.amazon.aws.awsie.snowballclient.service.JobHandler.queryManifestAndInitializeClientMaybeLite(JobHandler.java:149)
	at com.amazon.aws.awsie.snowballclient.service.JobHandler.queryManifestAndInitializeClientLite(JobHandler.java:135)
	at com.amazon.aws.awsie.snowballclient.service.JobHandler.handleStartCommand(JobHandler.java:299)
	at com.amazon.aws.awsie.snowballclient.service.JobHandler.run(JobHandler.java:221)
	at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.run(SnowballClientStarter.java:88)
	at com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter.main(SnowballClientStarter.java:60)
Caused by: java.net.ConnectException: Connection refused: connect
	at java.net.DualStackPlainSocketImpl.connect0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:79)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668)
	at sun.security.ssl.SSLSocketImpl.<init>(SSLSocketImpl.java:427)
	at sun.security.ssl.SSLSocketFactoryImpl.createSocket(SSLSocketFactoryImpl.java:88)
	at com.amazon.aws.awsie.snowball.netty.SnowballClientLite.connect(SnowballClientLite.java:115)

Any ideas what I should try?"
AWS Import Export Snowball	"Re: Get a Connection Refused error when I try to run snowball start
Were you ever able to resolve this? I am experiencing the same issue.

I've also verified the manifest and unlock code, that network connection speeds are gigabit, and I can ping the appliance.

Curious if you found a resolution to this."
AWS Import Export Snowball	"Re: Get a Connection Refused error when I try to run snowball start
That error log is saying the client is unable to connect to the snowball. The authentication is failing between the client tool and the snowball. You can try running snowball stop (to clean-up the connection info) and then try running the start command again. If it's still failing then the authentication certificates in the manifest isn't allow you to connect. This could be a result of any of the following:

1. You have the wrong manifest for the Snowball you are trying to connect to. Do you have multiple Snowball devices? If so could you have mixed up the Manifest?
2. There certificate in the manifest is corrupt. You can try downloading the manifest again from the console. It's possible that something went wrong during the downloading of the manifest.
3. The certificate could be corrupt on the Snowball. 

If running snowball stop then start after re-downloading the manifest from the console doesn't work then you'll need to contact AWS."
AWS Import Export Snowball	"Re: Get a Connection Refused error when I try to run snowball start
I can ping the appliance

At the risk of asking the obvious, did you verify that it stops pinging if you unplug it?  This error could potentially be caused by another device unexpectedly using that IP address."
AWS Import Export Snowball	"Import Export GetShippingLabel ""Unrecognized country: Myanmar""
Hi,

When running java -jar lib/AWSImportExportWebServiceTool-1.0.jar GetShippingLabel, getting error
""Unrecognized country: Myanmar""

Is Myanmar not supported? We want to ship an Import/Export disk to Singapore for import into S3.

The shipping calculator at https://awsimportexport.s3.amazonaws.com/aws-import-export-calculator.html has Myanmar in the Return Shipping Destination dropdown.

Thanks
Tony"
AWS Import Export Snowball	"Re: Import Export GetShippingLabel ""Unrecognized country: Myanmar""
Hello Tony,

Unfortunately the best solution to this is to use your own shipping label to send the device to us, and include, inside the package containing the disks, the job-created shipping label with an address in Singapore.  This allows you to ship the package, and allows the service to recognize the job.  

We apologize for the inconvenience.  We know this solution is not ideal, but it will provide the quickest turn around to get your device ingested."
AWS Import Export Snowball	"Re: Import Export GetShippingLabel ""Unrecognized country: Myanmar""
Hi,

Thanks for your response. 

We have the option is to send it to our Singapore office, and generate the job for that office.

However, the ""Generating Your Pre-Paid Shipping Label"" page has this at the bottom:

""Note: If you are having trouble using the pre-paid shipping label, or if you are shipping domestically within Singapore, contact AWS Support Center before shipping your device."" - http://docs.aws.amazon.com/AWSImportExport/latest/DG/GettingYourShippingLabel.html

So if we ship from our Singapore office, there's a different process? And how do we contact AWS Support Center Singapore? Via telephone, in which case what is the number? Or via email, in which case what what email address? Clicking the AWS Support link simply takes us to the basic Case Creation, and we've had no response to our enquiries.

Thanks
Tony"
AWS Import Export Snowball	"Snowball - check transferred file/folder size?
Hi

Is there a command to show me how much file size I have transferred in total to a 50TB Snowball unit? i.e., what the used and remaining capacity is?

thanks!

Matt"
AWS Import Export Snowball	"Re: Snowball - check transferred file/folder size?
If you run the <snowball status> command for the Snowball Client, it'll show you the total size and free space remaining. The docs were just updated with an example output for the status command here: http://docs.aws.amazon.com/snowball/latest/ug/using-client-commands.html#snowball-status-command"
AWS Import Export Snowball	"Snowball Delivered, but UPS doesnt think so
Hi,
Similar to at least one other recent forum poster, my Snowball was delivered but the UPS tracking site still says in-transit.  
Since AWS doesnt release the credentials until UPS delivers the device, I am now stuck."
AWS Import Export Snowball	"Re: Snowball Delivered, but UPS doesnt think so
Hi Jason.  My apologies for these difficulties.  Can you PM your job ID so I can get it's state moved?

Thank you.

Frank"
AWS Import Export Snowball	"Snowball Appliance installation - general question
Hi folks,

A bit of a peculiar  question which I've not found a suitable answer for :

Can an AWS Snowball be placed on its side during operation? 

In the DataCenter where it will be connected, the DataCenter support guys don't want it left on the floor. Turning it on its side would allow them to place it in a rack.

So is there a limitation on its orientation?

Thanks!"
AWS Import Export Snowball	"Re: Snowball Appliance installation - general question
Hi JasonMann.  The Snowball should operate just fine when placed on its side.

Frank"
AWS Import Export Snowball	"Expected run time for Snowball validation command
Hi, 
I'm trying to export ~20TB of data from local storage to Amazon S3 using their Snowball appliance. The copy finished successfully, and we are doing the validation before mailing the appliance back. However, the validation seems to get ""stuck"" after running for about 45 minutes. We terminated the first validation, and the 2nd try seems stuck at the same spot. Right now, status says that total = 17029 Successful = 17029 and invalid and incomplete = 0. It has had this same status since about 45 minutes after the start. We've gotten no errors. It's been running for a total of 3 hours.

I'm wondering these things: 
Is the validation doing a checksum on every file? 
Is there an expected time for validation on ~20TB of data? 
Will it show some errors at some point if it has a problem?

More information: 
Total files = 827,940 
95% of files are video or .jpg files 
Largest individual file is 80GB 
Command run = snowball -v validate s3://path/

Thanks in advance!"
AWS Import Export Snowball	"Re: Expected run time for Snowball validation command
So, we found out the timing. We waited it out overnight. We started at 3:45pm and it finished by 10am the next day. It never got any errors. So, maybe it just seems ""stuck"" when it hits a really large file."
AWS Import Export Snowball	"Re: Expected run time for Snowball validation command
Thanks for your feedback on the validation. Were you using the verbose option for the command? That's how I typically see the progress in real time for commands I execute in the Snowball client. Here's a link to the docs with the verbose listing: http://docs.aws.amazon.com/snowball/latest/ug/using-client-commands.html#clientverbose"
AWS Import Export Snowball	"snowball edge nfs lambda - nfs file copy results in which s3 events
The snowball edge can have lambda functions that can be configured to fire on s3 events.  When files are copied to the snowball edge using the nfs interface will s3 events be fired?  What events?  Configuring Amazon S3 Event Notifications: http://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html#supported-notification-event-types mentions these events: s3:ObjectCreated:Put, s3:ObjectCreated:Post, s3:ObjectCreated:Copy, s3:ObjectCreated:CompleteMultipartUpload"
AWS Import Export Snowball	"Re: snowball edge nfs lambda - nfs file copy results in which s3 events
Hi there,

Yes, using the file interface (the nfs mount) you can trigger Lambda functions. From the docs:

""The local compute functionality is AWS Lambda powered by AWS Greengrass, and can automatically run Python-language code in response to Amazon S3 PUT object action API calls to the AWS Snowball Edge appliance.""

and 

""PUT object actions can occur through the file interface (with a write operation), the AWS CLI (with a s3 cp command), or programmatically through one of the SDKs or a REST application of your own design. All of these S3 PUT object actions will trigger an associated function on the specified bucket.""

You can find more information on the file interface and the AWS Lambda powered by AWS Greengrass functionality here: http://docs.aws.amazon.com/snowball/latest/developer-guide/using-lambda.html and here: http://docs.aws.amazon.com/snowball/latest/developer-guide/using-fileinterface.html"
AWS Import Export Snowball	"Re: snowball edge nfs lambda - nfs file copy results in which s3 events
I was hoping to trigger a lambda function to create a checksum file when the nfs file was closed.  I think you are saying that the lambda function will be called on each write.  There will be a lot of writes for the file, right?"
AWS Import Export Snowball	"Re: snowball edge nfs lambda - nfs file copy results in which s3 events
Yes, the Lambda function would be triggered on each write (S3 Put object action http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html) to the bucket associated with your Lambda function."
AWS Import Export Snowball	"On Mac, can't find connected network folder to copy to snowball.
Hello All - 
I have my snowball up and running, but the client can't seem to see the folder of the networked drive I want to copy everything off of.  I have it mounted in my finder, but no combination of paths I use get it to the folder.  
I would think this would work:  snowball cp -r /rtpnfil03.rti.ns/vs/Video_Archive s3://rtivideoarchive
or some combination would work?  rtpnfil03.rti.ns is the name of the networked drive.  ""vs"" is the name of the mounted share, and ""Video_Archive"" is the name of the folder.   I've tried every combination I can think of, like 
snowball cp -r smb://rtpnfil03.rti.ns/vs/Video_Archive s3://rtivideoarchive
to
snowball cp -r /vs s3://rtivideoarchive
and anything in-between.  
I always get this error.  ""Pre-checking your source files and folders...
Cannot locate the specified file(s) or folder(s):""

Any ideas? Also the ""-r"" is to copy all the sub-directories....right?

Thanks everyone!
J"
AWS Import Export Snowball	"Re: On Mac, can't find connected network folder to copy to snowball.
Just tested copying from my local machine, and it worked no problem......still no love over the network."
AWS Import Export Snowball	"Re: On Mac, can't find connected network folder to copy to snowball.
Sorry!!! Think I figured it out.  Teaching myself as i go."
AWS Import Export Snowball	"Error on Mac - Can't read manifest resource
Hey All,

First time snowballer.  So I believe I have everything set up correctly; but when I run the Start Snowball script -i, -m, -u I get this error:

""Can't read manifest resource at /Downloads/JID2c7d5454-0d4d-4d6d-a6c2-55e371a6fe0a_manifest.bin. Try restarting the client.""

From here, I'm lost.  Any ideas?  I'm pretty savvy around a computer, but not an IT professional.  
Did see a thread where someone had a similar problem, but after an OS update they were all good. There are no more updates available for my OS.  I'm running El Capitan 10.11.16. 

This is the message I got when I installed the client - and this seems to have gone correctly. 

Running sudo copy commands
Password:
rm: /opt/aws/snowball: No such file or directory
Copying files to /opt/aws/snowball.
Snowball command is linkted at /usr/local/bin/snowball.
Install finished.

Any help would be greatly appreciated! 
Thanks."
AWS Import Export Snowball	"Re: Error on Mac - Can't read manifest resource
Hi 10engines.  Looking at the message it appears that you might have the incorrect path for the manifest file.  Typically on a Mac the path to the download file is something like /Users/<your user name>/Downloads.  In the example you give below it looks like you might have left off the first part of that path.

I hope this is helpful.

Frank"
AWS Import Export Snowball	"Re: Error on Mac - Can't read manifest resource
Thanks so much!  Exactly what I was doing wrong. Working now!!!"
AWS Import Export Snowball	"Re: Error on Mac - Can't read manifest resource
Ugh - next problem (if you're still there Frank) - 
Have it up and running, but the client can't seem to see the folder of the networked drive I want to copy everything off of.  I have it mounted in my finder, but no combination of paths I use get it to the folder.  
I would think this would work:  snowball cp -r /rtpnfil03.rti.ns/vs/Video_Archive s3://rtivideoarchive
or some combination would work?  rtpnfil03.rti.ns is the name of the networked drive.  ""vs"" is the name of the mounted share, and ""Video_Archive"" is the name of the folder.   I've tried every combination I can think of, like 
snowball cp -r smb://rtpnfil03.rti.ns/vs/Video_Archive s3://rtivideoarchive
to
snowball cp -r /vs s3://rtivideoarchive
and anything in-between.  
I always get this error.  ""Pre-checking your source files and folders...
Cannot locate the specified file(s) or folder(s):""

Any ideas? Also the ""-r"" is to copy all the sub-directories....right?

I'll post as a separate topic in case that works better. 

J"
AWS Import Export Snowball	"I get the following error running test command
Hello, 

I get the following error when I run a test command - 

Internal Error Message Begins: InvalidPathException-Illegal char <:> at index 2:

Here is my command - 

snowball test --recursive ""M:\DTM Data\EU_DTM"" s3://coremapdata/raw/elv/5m

The location s3://coremapdata/raw/elv/5m exists on the snowball I have confirmed with snowball ls"
AWS Import Export Snowball	"Re: I get the following error running test command
Hi there,

After a little bit of digging, I found that you don’t need to include the ‘s3://…’ part for the test command, or the quotes around your local path. Can you try the following command to see if it works:
snowball test --recursive M:\DTM Data\EU_DTM"
AWS Import Export Snowball	"Re: I get the following error running test command
I had to put """" around the file path but it worked, taking the s3:// bit out.  

The copy seems to be working now. 

Thank you"
AWS Import Export Snowball	"snowball edge testing lambda functions that process files loaded via nfs
I'm going to get a snowball edge and load it full of data using the nfs capability.  I would like to run a lambda function on the files in some of the directories.  I want to test the functions before ordering the snowball edge.  Any suggestions?"
AWS Import Export Snowball	"Re: snowball edge testing lambda functions that process files loaded via nfs
Hi there,
Currently, we don’t have a method for testing AWS GreenGrass Lambda functions that would run on a Snowball Edge in the cloud. I’ve made a note of your comment in a feature request with the team. But you can test your function against an Amazon S3 bucket that you have in the cloud, triggered by a PutObject operation. This will allow you to test the core functionality of the function, outside of the constraints put in place by the Snowball Edge. 

But before you do too much testing or create the Snowball Edge job, I have two quick recommendations for you:

1)	Get in on the AWS GreenGrass Limited Preview here: https://aws.amazon.com/greengrass/
2)	Read everything on this page: http://docs.aws.amazon.com/snowball/latest/developer-guide/using-lambda.html

There are limitations and considerations when using AWS GreenGrass Lambda functions on a Snowball Edge, so in place of testing, I’d recommend keeping those in mind when you create your functions.
Also, do note that when a function is on a Snowball Edge, it can’t be changed. You’ll have to create a new job with the new/updated function."
AWS Import Export Snowball	"Snowball Ethernet issue
Hi, 

I'm having issues connectivity issues to the AWS Snowball device, trying to connect the device using RJ45 Ethernet cable but it is not getting assigned an IP through DHCP. Tried connecting to multiple switches and using multiple cables but all with the same result. Trying to assign a static IP to the device also did not help as pinging the assigned IP or trying to connect to the device using the Snowball Client results in timeouts.

Something strange I noticed was that the right LED indicator on the snowball's Ethernet card keeps blinking constantly, even if there are not ethernet cables attached to it. Connecting an ethernet cable does not change the behavior of the indicators and the left LED indicator never lights up. Not sure whether this is expected or whether this is some kind of fault from the Ethernet card itself

A summary of the different options I've tried to connect the device can be found below:

1. Tried connecting to a patch panel. DHCP on AWS snowball did not get an IP. Trying to assign a static IP from the device itself did not work either.
2. Tried connecting to a normal switch directly to limit the number of loops, but same behavior
3. Tried following the troubleshooting on the website for all instances: https://aws.amazon.com/premiumsupport/knowledge-center/troubleshoot-connect-snowball/
4. Tried restarting the device a couple of times after connecting to the different devices I mentioned above, as listed on the troubleshooting page.
5. Tried different RJ45 cables, including the cable which came with the device, and also a CAT5 cable as mentioned in the troubleshooting page, and again no change in behavior was noticed.

Can you please provide some assistance in this matter and advice on what might be the possible cause of this behavior.

Thanks"
AWS Import Export Snowball	"Re: Snowball Ethernet issue
Hi jonacara.  If you are still seeing these problems, please PM your job id so I can investigate further.

Frank"
AWS Import Export Snowball	"Re: Snowball Ethernet issue
Hi Frank, yes unfortunately still having this issue. I've PM'ed you the Job ID"
AWS Import Export Snowball	"Snowball importing issue
Hello,

I use Snowball and the status of the JOB is Importing.
However, as far as Cloudwatch metrics is concerned, 
the NumberOfObjects for the S3 bucket does not change after 3/26.

Does Import processing work?
Or is the Cloudwatch metrics strange?

The job ID is ""JIDc9279805-b114-47ec-8fa2-eb98304bb51e""

Thank you."
AWS Import Export Snowball	"Snowball delivered, but UPS site says 'in transit'.
Got our Snowball 5 days ago.  UPS site has not updated status to 'delivered'.
I can't get the unlock credentials for the snowball before it hits 'delivered' status.
Any way around this?  Ticket has been open with UPS for 8 hours now, so maybe they'll update/fix it over night."
AWS Import Export Snowball	"Re: Snowball delivered, but UPS site says 'in transit'.
Hi jnathlich12.  Sorry for these difficulties, can you PM me your job ID so we can investigate?

Thank you.

Frank"
AWS Import Export Snowball	"Re: Snowball delivered, but UPS site says 'in transit'.
Hi jnathlich12, I have sent you a PM. Can you please check and confirm us in the forums. We will be waiting for your response.

Najah"
AWS Import Export Snowball	"How long does it take before a job goes to preparing?
I submitted an import job on Thursday last week (JID019d6081-a95f-49a8-8dc1-448ee3254a0d) and it still say ""Job Created"". How long does it normally take from when an 80T job is created until it is shipped?

I wish I had known it was going to take this long, I'm worried it might arrive while I'm away on vacation."
AWS Import Export Snowball	"Re: How long does it take before a job goes to preparing?
So it took a little more than four business days to before the appliance was prepared. It's now sat in between ""Prepare Appliance"" and ""In Transit to You"" for a day. I saw in the faq that they only ship them once a day, so I imagine it'll go out today/tonight; but if it's tonight, then why didn't it go out last night? shrug With the two-day shipping selected, I suspect I'll get it Friday this week or Monday next, which is more than a week from order to receive."
AWS Import Export Snowball	"Re: How long does it take before a job goes to preparing?
Hi Chad.  I'll look into this delay, it is not typical.  I'll update you with what we find.

Frank"
AWS Import Export Snowball	"Re: How long does it take before a job goes to preparing?
Hi Chad.  I have sent you a private message with so details about the processing of your job.

Frank"
AWS Import Export Snowball	"Snowball importing progress has stopped for more than 72 hours.
Hello,

We use Snowball to transfer data from the local server to S3.

Snowball is returned to AWS, Job status becomes ""Importing""
Copying was proceeding, but it did not proceed further after proceeding to the status below.

Total files transferred 15310197
99.765% complete

I do not know when it stopped, but it has stopped for more than 72 hours.

How should we deal with this?"
AWS Import Export Snowball	"Re: Snowball importing progress has stopped for more than 72 hours.
Hi fujita.  This typically means that an engineer is looking into your job, can you PM me your job id, the id that begins with ""JID"" so I can look into it?

Thanks!
Frank"
AWS Import Export Snowball	"Re: Snowball importing progress has stopped for more than 72 hours.
Thank you for your response.

I sent a JID as a private message.

Thanks."
AWS Import Export Snowball	"Estimated delivery time for Snowball job?
I'm in the US, and just created a Snowball job. What is the typical delivery time?

It's Saturday now and I pretty much need it by Tuesday, as I'm leaving the country for a month on Wednesday."
AWS Import Export Snowball	"Amazon API Details Invalid
Hi folks. I am trying to set up an Amazon Import tool on my Wordpress website. I enter my Affiliate ID, Secret key and Access key to no avail. Continuously receive - Amazon API Details Invalid. We could not connect to Amazon because your Amazon import settings are incorrect. I have downloaded numerous keys under what I think is the Root account since I have never created any other. This tool seems to be hitting the keys as the ""last used"" time keeps changing to when I am trying to import. Any thought? Feedback would be greatly appreciated. Thanks!"
AWS Import Export Snowball	"Re: Amazon API Details Invalid
Hi hmsnzTelecine 

I was able to locate your case. You’ll find it’s been updated by visiting the Support Center here: http://amzn.to/1Nsiekn

Regards
Shaun C"
AWS Import Export Snowball	"No clue how to download everything from AWS, and then shut down
Hi all, I inherited this AWS set up and I have montly billing going for a website that will never come about... so I want to shut it down.  But before I shut it down I just want to download everything so I can save it locally.

I have no clue how to do this, and have been spending a great amount of time trying to figure it out.

Help please!"
AWS Import Export Snowball	"Re: No clue how to download everything from AWS, and then shut down
It depends a lot on your AWS set up. Is it a single server? Are you using RDS?
I recently migrated an entire site from another cloud provider (Rackspace Cloud) to AWS; my approach was basically to tarball the relevant paths (/var/sites, /var/www, /etc for any possible config files I might miss), pg_dump the database and then download all that stuff. I would untar all of that on a separate directory on the new server and copy the needed config files to the real /etc, the working paths to their corresponding place and just fire up the server.

If you know all of the components, you might simply tarball the specifics, or go for ""everything but /proc"" and download it. If your setup is using RDS, you can do a mysqldump or pg_dump (depending on what RDBMS your setup is using) to keep a copy of the whole database."
AWS Import Export Snowball	"The KMS key you selected is invalid
When I get to the end of the process of ordering the Snowball appliance I the message.
The KMS key you selected is invalid.

I only have one bucket.  I can not find anything wrong."
AWS Import Export Snowball	"Re: The KMS key you selected is invalid
Sorry for the difficulty you're having. If you're trying to order a Snowball device in one of the US West regions, for the time being you will have to create a KMS key in US East (N. Virginia) and select that key for your job. Then you should be able to proceed with the order."
AWS Import Export Snowball	"md5 generated on Import
If we copy our objects to Snowball with the snowball client (vs. the S3 Adaptor), will the MD5 Etags get generated when the objects are imported when the snowball is returned to Amazon?"
AWS Import Export Snowball	"Snowball Export Issues
I am attempting to export 40TB of data from a snowball to my local network, and have been having a number of issues.

Currently I am exporting using the following command:
snowball -v cp -r <src> <dst>

My snowball client version is:
Snowball client version: 1.0.1 Build 124

The snowball client is bursting network traffic. It will start with a 0MB/s transfer rate, burst up to ~110MB/s then drop back to 0MB/s prior to completing the file transfer, as shown below.

Current File: 0% Totals: http://3.86 GB/4.25 GB                                                                            Current File: 5% Totals: http://3.88 GB/4.25 GB                                                                           Current File: 29% Totals: http://3.97 GB/4.25 GB                                                                          Current File: 54% Totals: http://4.08 GB/4.25 GB                                                                         Current File: 63% Totals: http://4.11 GB/4.25 GB                                                                          Current File: 63% Totals: http://4.11 GB/4.25 GB                                                                           Current File: 63% Totals: http://4.11 GB/4.25 GB                                                                           Current File: 63% Totals: http://4.11 GB/4.25 GB                                                                           Current File: 63% Totals: http://4.11 GB/4.25 GB 

I realize that the transfer is a CPU intensive job, but it is not currently maxing the CPU. Here are the relevant logs from top:
top - 08:09:03 up 3 days, 23:17,  6 users,  load average: 14.88, 12.98, 9.15
Tasks: 242 total,   1 running, 241 sleeping,   0 stopped,   0 zombie
%Cpu0  : 29.7 us,  1.0 sy,  0.0 ni, 38.0 id, 31.3 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu1  : 31.8 us,  1.3 sy,  0.0 ni, 34.1 id, 32.8 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu2  : 29.1 us,  0.7 sy,  0.0 ni, 35.3 id, 29.5 wa,  0.0 hi,  5.5 si,  0.0 st
%Cpu3  : 42.1 us,  0.0 sy,  0.0 ni, 39.1 id, 18.7 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu4  : 30.0 us,  0.7 sy,  0.0 ni, 34.7 id, 34.7 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu5  : 26.4 us,  1.0 sy,  0.0 ni, 56.2 id, 16.4 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu6  : 27.5 us,  0.7 sy,  0.0 ni, 71.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu7  : 76.1 us,  0.0 sy,  0.0 ni, 23.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:  32897956 total, 23936920 used,  8961036 free,   405304 buffers
KiB Swap: 33442812 total,   308960 used, 33133852 free. 15764872 cached Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
10419 user    20   0 12.792g 3.595g  23868 S 304.9 11.5  10:29.54 java
11106 user    20   0   29208   3060   2432 R   0.3  0.0   0:00.07 top
    1 root      20   0  185184   4336   2868 S   0.0  0.0   0:08.20 systemd
...

What could be causing this bursting behavior? Is there anything that can be done to speed up the overall transfer times? (I will run multiple instances as soon as I can correct the bursting behavior issues)"
AWS Import Export Snowball	"Re: Snowball Export Issues
There are generally two phases that happen: encryption and transferring. The encryption phase needs a lot of CPU and the transfer phase needs bandwidth.  It is common to see the CPU spike and drop as the data is often changing between the encryption and transferring phase.

There are many ways to speed up the transfer of data to a snowball. Many of them are describe in the following documentation: http://docs.aws.amazon.com/snowball/latest/ug/performance.html"
AWS Import Export Snowball	"Emulator for snowball device
It would be very useful for partner developers to have an emulator available for the snowball appliance device, especially given that a snowball device can not be kept on premises for more than 90 days."
AWS Import Export Snowball	"Re: Emulator for snowball device
Thank you for your feedback. I will let them team know about your request."
AWS Import Export Snowball	"Snowball with Cloudberry Backup
Hi all,
We're going to order one Snowball to send data from our file servers to S3.
We've been looking for Cloudberry Backup that supports Snowballs:
https://www.cloudberrylab.com/blog/working-with-aws-snowball-in-cloudberry-backup/

I only used Cloudberry Explorer, but not backup.
Any suggestions? What are the pros and cons?"
AWS Import Export Snowball	"Re: Snowball with Cloudberry Backup
Snowball is designed to import data directly into s3. You can order a snowball and manually copy files from your file server to the snowball. After you send the snowball back it will be imported into S3.

Here are a few links that might be helpful for you:
https://aws.amazon.com/snowball/getting-started/
https://aws.amazon.com/blogs/aws/aws-importexport-snowball-transfer-1-petabyte-per-week-using-amazon-owned-storage-appliances/"
AWS Import Export Snowball	"N Virginia - Error Snowball is not available in your area. Contact AWS
I am in dire need of getting a clients data imported to AWS. The data is close to 10TB and it would take forever to upload. I have tried to arrange for a snowball to be shipped to me but keep receiving this message:

Error
Snowball is not available in your area. Contact AWS Support for more details.

I am in Tulsa OK 74137 and have tried logging into different US regions and ordering a snowball from each region. All say the same error."
AWS Import Export Snowball	"Re: N Virginia - Error Snowball is not available in your area. Contact AWS
Hi cmasty.  Unfortunately we are not able to send you a Snowball device.  You are welcome to use our AWS Import/Export Disk service to import your data.  This service, which you can read more about at https://aws.amazon.com/snowball/disk/, is one where you send us a disk drive with your data to be imported.

My apologies for the difficulties.

Frank"
AWS Import Export Snowball	"Does Snowball support CloudTrail logging?
Does Snowball support CloudTrail logging and by Extension CloudWatch Alarms?

http://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events-supported-services.html

I believe the answer is no, just wanted to verify as I could not find a definitive no.

We have SNS notifications set up.

Thanks."
AWS Import Export Snowball	"Re: Does Snowball support CloudTrail logging?
That is a good idea, but it doesn't not support cloud trails or cloud watch."
AWS Import Export Snowball	"how do I use multiple snowballs from the same user account?
Hi,

I have multiple snowballs that I'd like to use from the same user account - I've tried the obvious trick of setting HOME to a different location, but it doesn't work. I get the 'manifest already extracted' which I presume is because of the existing manifest for the first snowball. Thanks.

Cheers, Cos."
AWS Import Export Snowball	"Re: how do I use multiple snowballs from the same user account?
I'm assuming you want to use multiple snowballs from the same account at the same time. If so then it's possible but a little tricky. If you don't need to access them at the same time then it's easy. Just run snowball stop and then snowball start with the new snowball.

To support two snowballs at the same time you'll need to modify the snowball script (or snowball.bat if on windows). Inside the file on the last line you'll need to add the java flag: user.home=<some path to where the manifest will be stored>. The path will need to be unique for each snowball that you are trying to connect to. 

https://forums.aws.amazon.com/
if you are on Mac and you wanted to change the home location to /tmp you could add the following -Duser.home=/tmp to the last line in the snowball script. See below for full context. Note the -D flag will need to go after the call to java. 

DYLD_LIBRARY_PATH=""${DIR}/../x86_64"" ${DIR}/../jre/Contents/Home/bin/java -Duser.home=/tmp -Xmx7G -Djava.library.path=""${DIR}/../x86_64"" -cp ""${DIR}/../jarfarm/protobuf-java-3.0.x.jar"":""${DIR}/../jarfarm/*"" com.amazon.aws.awsie.snowballclient.starter.SnowballClientStarter ""$@""

Save this file as a new file (example snowball2). Now when you want to connect to the two different snowballs you can use snowball and snowball2 to communicate with the two different snowball devices at the same time.

I hope this helps."
AWS Import Export Snowball	"Snowball job hasn't moved forward in more than a month
Hi There,

I created a snowball job on 25-Nov-2016. It is close to 40 days the status is still stuck in Job Created. Though this is not an urgent thing it is very important for us migrate all the data from local machines to S3. Can you please help why the job hasn't moved forward?

Region: Mumbai (ap-south-1)
The JobID (if that helps is): JIDa1f069f7-c9ce-4329-b7f3-847594762513
Appliance capacity requested for: 80TB."
AWS Import Export Snowball	"Re: Snowball job hasn't moved forward in more than a month
Hello gantir,
I've sent you a private message regarding your order.  Please have a look.

Thank you,
Ryan - AWS Import/Export team"
AWS Import Export Snowball	"snowball cp behavior
Hi, realized snowball client's cp isn't like typical GNU cp, but glad to hear it's goal is to more closely mimic AWS CLI's cp.

Noticed while running scripts to copy data using snowball's cp command - directory to directory, if the dataset hasn't changed, cp will will confirm the data set is the same and move on. However, if snowball cp is invoked again, and any of the data has changed anywhere in the dataset, cp will start from scratch and re-transfer the entire dataset (can be PB mind you), not just the changes (as one might think). 

Has anyone else confirmed that this is the case?
Thanks!"
AWS Import Export Snowball	"Re: snowball cp behavior
Hello JamesT21@, 
I have queried our snowball service team to look into the issue. We will get back to you as soon as we get further information from them. In the meantime if you have other issues feel free to update.

Cheers
Najah"
AWS Import Export Snowball	"Re: snowball cp behavior
Hello,
Thanks for your patience. According to our Snowball team, Whenever a new copy operation is performed all the source files are pre-checked before they are copied to the Snowball. When the client encounters a file which is already present on the Snowball, it will skip copying it and move on to the next file. Please note that the client decides a file has ""changed"" when there is a mismatch of last modified date or file size between the source file and the one on the Snowball.

I hope this clarifies the behavior you were observing."
AWS Import Export Snowball	"US East snowball edge availability?
Hi there. We're well into planning a very large data import job to S3 and we plan to use a 5-node cluster of Snowball Edges. I know the Edge is a brand-new product so are they generally available at this point for US east regions? We are on a tight timeframe and hope to get them shipped within the next week. If we are likely to have to wait more than a few days we would probably want to change our plans to use original Snowballs instead.

I know the precise answer to my question will depend on what is available the day I actually place the order so I just want some guidance as to whether they are generally ""in stock"" and available now or not.

Thanks,

-joel"
AWS Import Export Snowball	"Re: US East snowball edge availability?
Hi joelp.  Snowball Edge devices are available from US East regions but they are in limited supply.  If your need is time sensitive, you may want to order the 80TB versions of the original Snowballs.

Frank"
AWS Import Export Snowball	"snowball job stuck in ""job created"" state
We're trying to evaluate AWS and need to move quite a bit of data.

A Snowball job was created two days ago and is still in the ""Job created"" state. ( 	JID744803a0-0057-4c67-bed5-196cc45f004a)

So far there have been 6 (successful and unsuccessful) attempts to contact support.
Sometimes a tech says he's contacting the snowball team and we never hear back.
Sometimes the phone disconnects

Let's try the forum! 

Update: the job is tied to a different account... it seems I can't post in the forum from that account!"
AWS Import Export Snowball	"Re: snowball job stuck in ""job created"" state
Hi rmeden2.  I checked on your job and it has been provisioned to be shipped.

My apologies for any difficulties.

Frank"
AWS Import Export Snowball	"Re: snowball job stuck in ""job created"" state
Thanks... I wonder if the problem was there were no snowballs available... if so, a better status would be nice."
AWS Import Export Snowball	"Snowball Export Job at 100% complete for two weeks
I have an export job which has been at 100% complete for about 2 weeks now.  

The Job ID is: JID5bc78916-3473-4969-bcb8-528c90da9664

Can someone take a look at this for me?  Thank you."
AWS Import Export Snowball	"Re: Snowball Export Job at 100% complete for two weeks
Hello,

I have reached out to the engineering team for an update on the mentioned job id,  as soon as we have any information from the team we will update you as well. 
Highly appreciate your patience. 

Regards,
Ashwin A."
AWS Import Export Snowball	"Re: Snowball Export Job at 100% complete for two weeks
Thanks.  Shortly after posting this I received an email which stated that there was a problem and it was being worked on. Within a day the problem was resolved and the Snowball device was being shipped.

I wanted to update this post, but could not find it again until now when I received a thread update email.

Thanks again."
AWS Import Export Snowball	"Amazon Snowball Issues - The appliance has timed out
This is the 2nd time I have used the service and unfortunately the 2nd time I have had issues..

I am unable to get past 'The applicance has timed out'. I have tried powering off, leaving unplugged for a few minutes and plugging back in, all to no avail.

When I get the 'The appliance has timed out' error, the only option is to 'restart interface', and there isn't anything possible after this.

Hope someone can help!"
AWS Import Export Snowball	"Cannot create export job due to ""bucket markers"" ???
We're having trouble creating a Snowball export job (see the attached image).  Every time we try to create the job we get the following error message:

""The ARN for the IAM role you provided can't access one or more of your specified bucket markers.""

What does that even mean?  What is a ""bucket marker""?  This particular bucket has no special policies on it, does not use versioning, and does not have any pending multi-part uploads.  We've given the IAM role full administrative permissions to the entire account and we still get this error message!

The account for this request is 552637259102."
AWS Import Export Snowball	"Import/Export GetShipInfo ""Failure to retrieve PDF""
hi, in using import/export all of a sudden GetShipInfo API is returning errors. 
java -jar lib/AWSImportExportWebServiceTool-1.0.jar GetShipInfo ZTUN5
Version: 2014-12-18
..Failure to retrieve PDF.  Retrying in 1 seconds.
Failure to retrieve PDF.  Retrying in 2 seconds.
Failure to retrieve PDF.  Retrying in 4 seconds.
Failure to retrieve PDF.  Retrying in 8 seconds.

(This was working fine before...)
What to do ?"
AWS Import Export Snowball	"Re: Import/Export GetShipInfo ""Failure to retrieve PDF""
Hi Thomas.  My apologies for these difficulties. If you can PM me your job ID, I can investigate further to find out what might be wrong.

Thank you.

Frank"
AWS Import Export Snowball	"Re: Import/Export GetShipInfo ""Failure to retrieve PDF""
Job ID is YZSWG 
 tom"
AWS Import Export Snowball	"Snowball Import - Consuming 100%+ CPU, No files transferred
We are trying to transfer 20TB from Snowball device to local storage. 

Using client snowball-client-linux-1.0.1-115.tar.gz on Linux OEL Linux 3.10.0-327.36.1.el7.x86_64, on a 16 core, 64GB RAM machine.

The command is ""snowball-client-linux-1.0.1-115/bin/snowball cp -r s3://<obfusicatedname1> /nfs_share"".

This job is running since Sat (56 hours so far), but has not copied anything so far, however the CPU is running 100%+ most of the time. The logs don't show anything credible other than  occasional messages as seen below.

""connection: reset by peer"" 
""2016-10-24 15:04:25 ERROR ExceptionHandler:222 - com.amazon.aws.awsie.snowballsdk.exception.AmazonSnowballException; ErrorType: INTERNAL_ERROR""
""java.util.concurrent.ExecutionException: com.amazon.aws.awsie.snowballsdk.exception.AmazonSnowballException; ErrorType: INTERNAL_ERROR""

I would appreciate help from the engineering team, because so far any request for help through the support channel has not been useful."
AWS Import Export Snowball	"Re: Snowball Import - Consuming 100%+ CPU, No files transferred
Can you provide the logs and your JobID? You can either respond to the thread or send them in private message.

You can try running in verbose mode. It should show you which files are being actively copied.
Can you try exporting a smaller subset to see if that works?"
AWS Import Export Snowball	"Re: Snowball Import - Consuming 100%+ CPU, No files transferred
Thanks for responding. 

I did the same copy command for a specific subdirectory earlier and that finished copying within a couple days. So I know it works for a specific subdirectory. I did notice that there was a state when the CPU spiked for few hours without any data being copied, however the job started copying eventually after few hours. 


What is the switch for verbose ? The online documentation showed no switch for verbose. 
I do not have access to JobId (it was generated through another AWS account managed by vendor)
How do I send logs privately ?"
AWS Import Export Snowball	"Re: Snowball Import - Consuming 100%+ CPU, No files transferred
If you click on my name you will be taken to my profile page. From there on the right hand side is the option to send a private message.

To get the jobID you can just copy the manifest's file name. It has the jobID in it."
AWS Import Export Snowball	"Re: Snowball Import - Consuming 100%+ CPU, No files transferred
awsatericn - can you please confirm you are a AWS employee from Amazon? I want to make sure you are before sharing private information. 

Regards,Sam"
AWS Import Export Snowball	"Snowball export - Stuck in ""Job Creating""
Hi,

I'm trying to export a S3 bucket through Snowball but it's been stuck in the ""Job Created"" state for 48 hours now. The bucket is only around 4TB. Is this a normal duration?

The Job is: JID2ccfa77b-fa4b-472a-8271-6aca09b41f23

Thanks,
Alan

Edited by: alanclelland on Oct 12, 2016 3:36 AM"
AWS Import Export Snowball	"Re: Snowball export - Stuck in ""Job Creating""
Hi alanclelland.  I wanted to let you know that we have checked on the status of your job and it has been processed to be shipped on Thursday.

Frank"
AWS Import Export Snowball	"Re: Snowball export - Stuck in ""Job Creating""
Perfect! 

Thanks,
Alan"
AWS Import Export Snowball	"Snowball list throws TimeoutException
I am trying to get the list of files in a specific folder in snowball for validation purposes. I have 80K files in this folder, when I tried to run snowball ls, I am seeing ""SocketTimeoutException: Read timed out"" in logs. In screen, I am seeing ""Can't communicate with Snowball. Confirm the Snowball's IP address and try again"". Sometimes it listed 38K, 48K and 50K and then timed out or sometimes it throws timeout straight away. I have attached the log files (I ran snowball ls with and without debug mode).

I use Snowball client Version: 1.0.1, build: 68 for Mac.

Any idea how do I resolve this ? Thanks in advance.

Edited by: Baskar on Aug 16, 2016 4:09 PM"
AWS Import Export Snowball	"Re: Snowball list throws TimeoutException
Do you know about how many files are in a directory?

You can increase the timeout threshold. It will require you to modify the starting script which is typical installed at /opt/aws/snowball/bin/snowball

You will need to pass in -Dsnowball.timeout=120 (the value is in seconds, if there is a really high number of files 100k+ in a folder then you'll want to increase this number).

I've attached an example of a modified snowball file with the timeout increased to 2 minutes (120 seconds). Note I had to rename the file to snowball.txt in order to upload the file."
AWS Import Export Snowball	"Re: Snowball list throws TimeoutException
Thank you for the reply.
There were 80000 files in the directory. I will try to increase timeout and see if this works.
Appreciate your help."
AWS Import Export Snowball	"Re: Snowball list throws TimeoutException
This worked for me after increasing time out. Sometimes 120 seconds failed, I had to increate up to 300 seconds."
AWS Import Export Snowball	"New Snowball Client - Copy Command Syntax update
As of version 107 for Windows, 85 for Linux and 79 for Mac the snowball client's copy command syntax has been updated to be more aligned with S3's copy command syntax. The details are further describe in User Guide:

http://docs.aws.amazon.com/AWSImportExport/latest/ug/copy-command-syntax.html"
AWS Import Export Snowball	"Re: New Snowball Client - Copy Command Syntax update
The latest client is now 115 (for Windows, Linux, and Mac). It is recommended that everyone upgrades to the latest version. You can download the update from https://aws.amazon.com/importexport/tools/"
