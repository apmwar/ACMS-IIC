label	description
Amazon Athena	"Unable to retrieve the client encryption materials
Hi all, I am having trouble querying client-encrypted S3 data using Amazon S3. The table has the has_encrypted_data=true flag set, and the little key icon next to it in the console. The data is encrypted with an KMS customer managed key, which my user appears to have Decrypt permission on, but I still receive the error: Unable to retrieve the client encryption materials. An example failing query is Query Id: ecaadf0a-5844-4599-a307-04d93f7758bb. Thank you."
Amazon Athena	"Athena Null Values
Using DMS to get data from Oracle to S3. Need Athena to display NULL when values are not present instead of empty string."
Amazon Athena	"Re: Athena Null Values
Hello,

The NULL values in the source gets converted to empty/blank at the S3 target. This is the current behavior of the DMS.

In order to migrate the ‘NULL’ values from the source as ’NULL’ to the target, please modify the S3 endpoint and add below ECA (extra connection attribute) 
csvNullValue=NULL; 

This will ensure that the NULL values are kept as it is in the S# backup and thus the same will be queried by Athena. 

Hope this helps. Thanks."
Amazon Athena	"Athena CREATE TABLE cannot reference structured parquet
Hi,
I have generated parquet files. The partial schema of the files is as follows. 
root
 |-- variant: struct (nullable = true)
 |    |-- variantErrorProbability: integer (nullable = true)
 |    |-- contig: struct (nullable = true)
 |    |    |-- contigName: string (nullable = true)
 |    |    |-- contigLength: long (nullable = true)
 |    |    |-- contigMD5: string (nullable = true)
 |    |    |-- referenceURL: string (nullable = true)
 |    |    |-- assembly: string (nullable = true)
 |    |    |-- species: string (nullable = true)
 |    |    |-- referenceIndex: integer (nullable = true)
 |    |-- start: long (nullable = true)
 |    |-- end: long (nullable = true)

How do i reference contigName in an Athena CREATE TABLE. The following DDL does not seem to work. Do I need some kind of structure in the DDL ? Thanks for the help.

CREATE EXTERNAL TABLE IF NOT EXISTS test.test (
  `contigName` string 
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
WITH SERDEPROPERTIES (
  'serialization.format' = '1'
) LOCATION 's3://cr2-n/'
TBLPROPERTIES ('has_encrypted_data'='false')

Thanks"
Amazon Athena	"Issues with running query in Athena from XML files imported from s3
HIVE_METASTORE_ERROR: com.facebook.presto.spi.PrestoException: Error: : expected at the position 34 of 'struct<_env:string,_oub:string,oub\:record:struct<oub\:measure:struct<oub\:geographical.area:string,oub\:geographical.area.sid:int,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:justification.regulation.id:string,oub\:justification.regulation.role:int,oub\:measure.generating.regulation.id:string,oub\:measure.generating.regulation.role:int,oub\:measure.sid:int,oub\:measure.type:int,oub\:stopped.flag:int,oub\:validity.end.date:string,oub\:validity.start.date:string,oub\:ordernumber:int,oub\:additional.code:int,oub\:additional.code.sid:int,oub\:additional.code.type:string,oub\:export.refund.nomenclature.sid:int,oub\:reduction.indicator:int>,oub\:record.code:int,oub\:record.sequence.number:int,oub\:subrecord.code:int,oub\:transaction.id:int,oub\:update.type:int,oub\:measure.component:struct<oub\:duty.amount:double,oub\:duty.expression.id:int,oub\:measure.sid:int,oub\:measurement.unit.code:string,oub\:monetary.unit.code:string,oub\:measurement.unit.qualifier.code:string>,oub\:modification.regulation:struct<oub\:approved.flag:int,oub\:base.regulation.id:string,oub\:base.regulation.role:int,oub\:information.text:string,oub\:modification.regulation.id:string,oub\:modification.regulation.role:int,oub\:officialjournal.number:string,oub\:officialjournal.page:int,oub\:published.date:string,oub\:replacement.indicator:int,oub\:stopped.flag:int,oub\:validity.start.date:string,oub\:effective.end.date:string,oub\:explicit.abrogation.regulation.id:string,oub\:explicit.abrogation.regulation.role:int,oub\:validity.end.date:string>,oub\:regulation.replacement:struct<oub\:measure.type.id:int,oub\:replaced.regulation.id:string,oub\:replaced.regulation.role:int,oub\:replacing.regulation.id:string,oub\:replacing.regulation.role:int,oub\:geographical.area.id:string>,oub\:explicit.abrogation.regulation:struct<oub\:abrogation.date:string,oub\:approved.flag:int,oub\:explicit.abrogation.regulation.id:string,oub\:explicit.abrogation.regulation.role:int,oub\:information.text:string,oub\:replacement.indicator:int,oub\:officialjournal.number:string,oub\:officialjournal.page:int,oub\:published.date:string>,oub\:measure.excluded.geographical.area:struct<oub\:excluded.geographical.area:string,oub\:geographical.area.sid:int,oub\:measure.sid:int>,oub\:quota.definition:struct<oub\:critical.state:string,oub\:critical.threshold:int,oub\:description:string,oub\:initial.volume:bigint,oub\:maximum.precision:int,oub\:measurement.unit.code:string,oub\:quota.definition.sid:int,oub\:quota.order.number.id:int,oub\:quota.order.number.sid:int,oub\:validity.end.date:string,oub\:validity.start.date:string,oub\:volume:bigint,oub\:measurement.unit.qualifier.code:string,oub\:monetary.unit.code:string>,oub\:quota.blocking.period:struct<oub\:blocking.end.date:string,oub\:blocking.period.type:int,oub\:blocking.start.date:string,oub\:quota.blocking.period.sid:int,oub\:quota.definition.sid:int,oub\:description:string>,oub\:quota.balance.event:struct<oub\:imported.amount:int,oub\:last.import.date.in.allocation:string,oub\:new.balance:bigint,oub\:occurrence.timestamp:string,oub\:old.balance:bigint,oub\:quota.definition.sid:int>,oub\:quota.critical.event:struct<oub\:critical.state:string,oub\:critical.state.change.date:string,oub\:occurrence.timestamp:string,oub\:quota.definition.sid:int>,oub\:quota.reopening.event:struct<oub\:occurrence.timestamp:string,oub\:quota.definition.sid:int,oub\:reopening.date:string>,oub\:base.regulation:struct<oub\:approved.flag:int,oub\:base.regulation.id:string,oub\:base.regulation.role:int,oub\:community.code:int,oub\:information.text:string,oub\:officialjournal.number:string,oub\:officialjournal.page:int,oub\:published.date:string,oub\:regulation.group.id:string,oub\:replacement.indicator:int,oub\:stopped.flag:int,oub\:validity.start.date:string,oub\:effective.end.date:string,oub\:explicit.abrogation.regulation.id:string,oub\:explicit.abrogation.regulation.role:int,oub\:validity.end.date:string,oub\:antidumping.regulation.role:int,oub\:related.antidumping.regulation.id:string>,oub\:measure.condition:struct<oub\:action.code:int,oub\:certificate.code:int,oub\:certificate.type.code:string,oub\:component.sequence.number:int,oub\:condition.code:string,oub\:measure.condition.sid:int,oub\:measure.sid:int,oub\:condition.duty.amount:double,oub\:condition.measurement.unit.code:string,oub\:condition.measurement.unit.qualifier.code:string,oub\:condition.monetary.unit.code:string>,oub\:footnote.association.measure:struct<oub\:footnote.id:int,oub\:footnote.type.id:string,oub\:measure.sid:int>,oub\:measure.condition.component:struct<oub\:duty.amount:double,oub\:duty.expression.id:int,oub\:measure.condition.sid:int,oub\:measurement.unit.code:string,oub\:measurement.unit.qualifier.code:string,oub\:monetary.unit.code:string>,oub\:quota.unblocking.event:struct<oub\:occurrence.timestamp:string,oub\:quota.definition.sid:int,oub\:unblocking.date:string>,oub\:quota.exhaustion.event:struct<oub\:exhaustion.date:string,oub\:occurrence.timestamp:string,oub\:quota.definition.sid:int>,oub\:goods.nomenclature:struct<oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:producline.suffix:int,oub\:statistical.indicator:int,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:goods.nomenclature.successor:struct<oub\:absorbed.goods.nomenclature.item.id:bigint,oub\:absorbed.productline.suffix:int,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:productline.suffix:int>,oub\:additional.code:struct<oub\:additional.code:int,oub\:additional.code.sid:int,oub\:additional.code.type.id:string,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:footnote:struct<oub\:footnote.id:int,oub\:footnote.type.id:string,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:nomenclature.group.membership:struct<oub\:goods.nomenclature.group.id:int,oub\:goods.nomenclature.group.type:string,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:productline.suffix:int,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:footnote.description.period:struct<oub\:footnote.description.period.sid:int,oub\:footnote.id:int,oub\:footnote.type.id:string,oub\:validity.start.date:string>,oub\:footnote.description:struct<oub\:description:string,oub\:footnote.description.period.sid:int,oub\:footnote.id:int,oub\:footnote.type.id:string,oub\:language.id:string>,oub\:goods.nomenclature.description.period:struct<oub\:goods.nomenclature.description.period.sid:int,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:productline.suffix:int,oub\:validity.start.date:string>,oub\:goods.nomenclature.description:struct<oub\:description:string,oub\:goods.nomenclature.description.period.sid:int,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:language.id:string,oub\:productline.suffix:int>,oub\:duty.expression.description:struct<oub\:description:string,oub\:duty.expression.id:int,oub\:language.id:string>,oub\:additional.code.description:struct<oub\:additional.code:int,oub\:additional.code.description.period.sid:int,oub\:additional.code.sid:int,oub\:additional.code.type.id:string,oub\:description:string,oub\:language.id:string>,oub\:quota.unsuspension.event:struct<oub\:occurrence.timestamp:string,oub\:quota.definition.sid:int,oub\:unsuspension.date:string>,oub\:export.refund.nomenclature.description.period:struct<oub\:additional.code.type:int,oub\:export.refund.code:int,oub\:export.refund.nomenclature.description.period.sid:int,oub\:export.refund.nomenclature.sid:int,oub\:goods.nomenclature.item.id:int,oub\:productline.suffix:int,oub\:validity.start.date:string>,oub\:export.refund.nomenclature.description:struct<oub\:additional.code.type:int,oub\:description:string,oub\:export.refund.code:int,oub\:export.refund.nomenclature.description.period.sid:int,oub\:export.refund.nomenclature.sid:int,oub\:goods.nomenclature.item.id:int,oub\:language.id:string,oub\:productline.suffix:int>,oub\:goods.nomenclature.indents:struct<oub\:goods.nomenclature.indent.sid:int,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:number.indents:int,oub\:productline.suffix:int,oub\:validity.start.date:string>,oub\:goods.nomenclature.origin:struct<oub\:derived.goods.nomenclature.item.id:bigint,oub\:derived.productline.suffix:int,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:productline.suffix:int>,oub\:quota.order.number:struct<oub\:quota.order.number.id:int,oub\:quota.order.number.sid:int,oub\:validity.start.date:string,oub\:validity.end.date:string>,oub\:quota.order.number.origin:struct<oub\:geographical.area.id:string,oub\:geographical.area.sid:int,oub\:quota.order.number.origin.sid:int,oub\:quota.order.number.sid:int,oub\:validity.start.date:string,oub\:validity.end.date:string>,oub\:certificate.description:struct<oub\:certificate.code:int,oub\:certificate.description.period.sid:int,oub\:certificate.type.code:string,oub\:description:string,oub\:language.id:string>,oub\:full.temporary.stop.regulation:struct<oub\:approved.flag:int,oub\:effective.enddate:string,oub\:full.temporary.stop.regulation.id:string,oub\:full.temporary.stop.regulation.role:int,oub\:information.text:string,oub\:officialjournal.number:string,oub\:officialjournal.page:int,oub\:published.date:string,oub\:replacement.indicator:int,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:additional.code.description.period:struct<oub\:additional.code:int,oub\:additional.code.description.period.sid:int,oub\:additional.code.sid:int,oub\:additional.code.type.id:string,oub\:validity.start.date:string>,oub\:certificate:struct<oub\:certificate.code:int,oub\:certificate.type.code:string,oub\:validity.start.date:string,oub\:validity.end.date:string>,oub\:certificate.description.period:struct<oub\:certificate.code:int,oub\:certificate.description.period.sid:int,oub\:certificate.type.code:string,oub\:validity.start.date:string>,oub\:geographical.area.membership:struct<oub\:geographical.area.group.sid:int,oub\:geographical.area.sid:int,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:geographical.area:struct<oub\:geographical.area.id:string,oub\:geographical.area.sid:int,oub\:geographical.code:int,oub\:validity.start.date:string,oub\:validity.end.date:string>,oub\:geographical.area.description.period:struct<oub\:geographical.area.description.period.sid:int,oub\:geographical.area.id:string,oub\:geographical.area.sid:int,oub\:validity.start.date:string>,oub\:geographical.area.description:struct<oub\:description:string,oub\:geographical.area.description.period.sid:int,oub\:geographical.area.id:string,oub\:geographical.area.sid:int,oub\:language.id:string>,oub\:fts.regulation.action:struct<oub\:fts.regulation.id:string,oub\:fts.regulation.role:int,oub\:stopped.regulation.id:string,oub\:stopped.regulation.role:int>,oub\:monetary.exchange.period:struct<oub\:monetary.exchange.period.sid:int,oub\:parent.monetary.unit.code:string,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:monetary.exchange.rate:struct<oub\:child.monetary.unit.code:string,oub\:exchange.rate:double,oub\:monetary.exchange.period.sid:int>,oub\:measure.partial.temporary.stop:struct<oub\:abrogation.regulation.id:string,oub\:abrogation.regulation.officialjournal.number:string,oub\:abrogation.regulation.officialjournal.page:int,oub\:measure.sid:int,oub\:partial.temporary.stop.regulation.id:string,oub\:partial.temporary.stop.regulation.officialjournal.number:string,oub\:partial.temporary.stop.regulation.officialjournal.page:int,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:measurement.unit.qualifier:struct<oub\:measurement.unit.qualifier.code:string,oub\:validity.start.date:string>,oub\:measurement.unit.qualifier.description:struct<oub\:description:string,oub\:language.id:string,oub\:measurement.unit.qualifier.code:string>,oub\:measurement:struct<oub\:measurement.unit.code:string,oub\:measurement.unit.qualifier.code:string,oub\:validity.start.date:string>,oub\:quota.association:struct<oub\:coefficient:double,oub\:main.quota.definition.sid:int,oub\:relation.type:string,oub\:sub.quota.definition.sid:int>,oub\:quota.suspension.period:struct<oub\:description:string,oub\:quota.definition.sid:int,oub\:quota.suspension.period.sid:int,oub\:suspension.end.date:string,oub\:suspension.start.date:string>,oub\:footnote.association.goods.nomenclature:struct<oub\:footnote.id:int,oub\:footnote.type:string,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:productline.suffix:int,oub\:validity.start.date:string>,oub\:regulation.group:struct<oub\:regulation.group.id:string,oub\:validity.start.date:string>,oub\:export.refund.nomenclature.indents:struct<oub\:additional.code.type:int,oub\:export.refund.code:int,oub\:export.refund.nomenclature.indents.sid:int,oub\:export.refund.nomenclature.sid:int,oub\:goods.nomenclature.item.id:int,oub\:number.export.refund.nomenclature.indents:int,oub\:productline.suffix:int,oub\:validity.start.date:string>,oub\:export.refund.nomenclature:struct<oub\:additional.code.type:int,oub\:export.refund.code:int,oub\:export.refund.nomenclature.sid:int,oub\:goods.nomenclature.item.id:int,oub\:goods.nomenclature.sid:int,oub\:productline.suffix:int,oub\:validity.start.date:string>,oub\:measure.type:struct<oub\:measure.component.applicable.code:int,oub\:measure.explosion.level:int,oub\:measure.type.id:int,oub\:measure.type.series.id:string,oub\:order.number.capture.code:int,oub\:origin.dest.code:int,oub\:priority.code:int,oub\:trade.movement.code:int,oub\:validity.start.date:string>,oub\:measure.type.description:struct<oub\:description:string,oub\:language.id:string,oub\:measure.type.id:int>,oub\:measure.action:struct<oub\:action.code:int,oub\:validity.start.date:string>,oub\:measure.action.description:struct<oub\:action.code:int,oub\:description:string,oub\:language.id:string>,oub\:measure.condition.code:struct<oub\:condition.code:string,oub\:validity.start.date:string>,oub\:measure.condition.code.description:struct<oub\:condition.code:string,oub\:description:string,oub\:language.id:string>,oub\:monetary.unit:struct<oub\:monetary.unit.code:string,oub\:validity.start.date:string,oub\:validity.end.date:string>,oub\:monetary.unit.description:struct<oub\:description:string,oub\:language.id:string,oub\:monetary.unit.code:string>,oub\:additional.code.type:struct<oub\:additional.code.type.id:string,oub\:application.code:int,oub\:validity.start.date:string>,oub\:additional.code.type.description:struct<oub\:additional.code.type.id:string,oub\:description:string,oub\:language.id:string>,oub\:additional.code.type.measure.type:struct<oub\:additional.code.type.id:string,oub\:measure.type.id:int,oub\:validity.start.date:string>,oub\:language:struct<oub\:language.id:string,oub\:validity.start.date:string>,oub\:language.description:struct<oub\:description:string,oub\:language.code.id:string,oub\:language.id:string>,oub\:certificate.type.description:struct<oub\:certificate.type.code:string,oub\:description:string,oub\:language.id:string>>>' but '\' is found. (Service: null; Status Code: 0; Error Code: null; Request ID: null)
This query ran against the ""tarifftaric"" database, unless qualified by the query. Please post the error message on our forum or contact customer support with Query Id: 4e399ffc-0fb3-45ea-b96f-cd1e074bc507.



This is my schema for the struct

struct<_env:string,_oub:string,oub\:record:struct<oub\:measure:struct<oub\:geographical.area:string,oub\:geographical.area.sid:int,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:justification.regulation.id:string,oub\:justification.regulation.role:int,oub\:measure.generating.regulation.id:string,oub\:measure.generating.regulation.role:int,oub\:measure.sid:int,oub\:measure.type:int,oub\:stopped.flag:int,oub\:validity.end.date:string,oub\:validity.start.date:string,oub\:ordernumber:int,oub\:additional.code:int,oub\:additional.code.sid:int,oub\:additional.code.type:string,oub\:export.refund.nomenclature.sid:int,oub\:reduction.indicator:int>,oub\:record.code:int,oub\:record.sequence.number:int,oub\:subrecord.code:int,oub\:transaction.id:int,oub\:update.type:int,oub\:measure.component:struct<oub\:duty.amount:double,oub\:duty.expression.id:int,oub\:measure.sid:int,oub\:measurement.unit.code:string,oub\:monetary.unit.code:string,oub\:measurement.unit.qualifier.code:string>,oub\:modification.regulation:struct<oub\:approved.flag:int,oub\:base.regulation.id:string,oub\:base.regulation.role:int,oub\:information.text:string,oub\:modification.regulation.id:string,oub\:modification.regulation.role:int,oub\:officialjournal.number:string,oub\:officialjournal.page:int,oub\:published.date:string,oub\:replacement.indicator:int,oub\:stopped.flag:int,oub\:validity.start.date:string,oub\:effective.end.date:string,oub\:explicit.abrogation.regulation.id:string,oub\:explicit.abrogation.regulation.role:int,oub\:validity.end.date:string>,oub\:regulation.replacement:struct<oub\:measure.type.id:int,oub\:replaced.regulation.id:string,oub\:replaced.regulation.role:int,oub\:replacing.regulation.id:string,oub\:replacing.regulation.role:int,oub\:geographical.area.id:string>,oub\:explicit.abrogation.regulation:struct<oub\:abrogation.date:string,oub\:approved.flag:int,oub\:explicit.abrogation.regulation.id:string,oub\:explicit.abrogation.regulation.role:int,oub\:information.text:string,oub\:replacement.indicator:int,oub\:officialjournal.number:string,oub\:officialjournal.page:int,oub\:published.date:string>,oub\:measure.excluded.geographical.area:struct<oub\:excluded.geographical.area:string,oub\:geographical.area.sid:int,oub\:measure.sid:int>,oub\:quota.definition:struct<oub\:critical.state:string,oub\:critical.threshold:int,oub\:description:string,oub\:initial.volume:bigint,oub\:maximum.precision:int,oub\:measurement.unit.code:string,oub\:quota.definition.sid:int,oub\:quota.order.number.id:int,oub\:quota.order.number.sid:int,oub\:validity.end.date:string,oub\:validity.start.date:string,oub\:volume:bigint,oub\:measurement.unit.qualifier.code:string,oub\:monetary.unit.code:string>,oub\:quota.blocking.period:struct<oub\:blocking.end.date:string,oub\:blocking.period.type:int,oub\:blocking.start.date:string,oub\:quota.blocking.period.sid:int,oub\:quota.definition.sid:int,oub\:description:string>,oub\:quota.balance.event:struct<oub\:imported.amount:int,oub\:last.import.date.in.allocation:string,oub\:new.balance:bigint,oub\:occurrence.timestamp:string,oub\:old.balance:bigint,oub\:quota.definition.sid:int>,oub\:quota.critical.event:struct<oub\:critical.state:string,oub\:critical.state.change.date:string,oub\:occurrence.timestamp:string,oub\:quota.definition.sid:int>,oub\:quota.reopening.event:struct<oub\:occurrence.timestamp:string,oub\:quota.definition.sid:int,oub\:reopening.date:string>,oub\:base.regulation:struct<oub\:approved.flag:int,oub\:base.regulation.id:string,oub\:base.regulation.role:int,oub\:community.code:int,oub\:information.text:string,oub\:officialjournal.number:string,oub\:officialjournal.page:int,oub\:published.date:string,oub\:regulation.group.id:string,oub\:replacement.indicator:int,oub\:stopped.flag:int,oub\:validity.start.date:string,oub\:effective.end.date:string,oub\:explicit.abrogation.regulation.id:string,oub\:explicit.abrogation.regulation.role:int,oub\:validity.end.date:string,oub\:antidumping.regulation.role:int,oub\:related.antidumping.regulation.id:string>,oub\:measure.condition:struct<oub\:action.code:int,oub\:certificate.code:int,oub\:certificate.type.code:string,oub\:component.sequence.number:int,oub\:condition.code:string,oub\:measure.condition.sid:int,oub\:measure.sid:int,oub\:condition.duty.amount:double,oub\:condition.measurement.unit.code:string,oub\:condition.measurement.unit.qualifier.code:string,oub\:condition.monetary.unit.code:string>,oub\:footnote.association.measure:struct<oub\:footnote.id:int,oub\:footnote.type.id:string,oub\:measure.sid:int>,oub\:measure.condition.component:struct<oub\:duty.amount:double,oub\:duty.expression.id:int,oub\:measure.condition.sid:int,oub\:measurement.unit.code:string,oub\:measurement.unit.qualifier.code:string,oub\:monetary.unit.code:string>,oub\:quota.unblocking.event:struct<oub\:occurrence.timestamp:string,oub\:quota.definition.sid:int,oub\:unblocking.date:string>,oub\:quota.exhaustion.event:struct<oub\:exhaustion.date:string,oub\:occurrence.timestamp:string,oub\:quota.definition.sid:int>,oub\:goods.nomenclature:struct<oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:producline.suffix:int,oub\:statistical.indicator:int,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:goods.nomenclature.successor:struct<oub\:absorbed.goods.nomenclature.item.id:bigint,oub\:absorbed.productline.suffix:int,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:productline.suffix:int>,oub\:additional.code:struct<oub\:additional.code:int,oub\:additional.code.sid:int,oub\:additional.code.type.id:string,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:footnote:struct<oub\:footnote.id:int,oub\:footnote.type.id:string,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:nomenclature.group.membership:struct<oub\:goods.nomenclature.group.id:int,oub\:goods.nomenclature.group.type:string,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:productline.suffix:int,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:footnote.description.period:struct<oub\:footnote.description.period.sid:int,oub\:footnote.id:int,oub\:footnote.type.id:string,oub\:validity.start.date:string>,oub\:footnote.description:struct<oub\:description:string,oub\:footnote.description.period.sid:int,oub\:footnote.id:int,oub\:footnote.type.id:string,oub\:language.id:string>,oub\:goods.nomenclature.description.period:struct<oub\:goods.nomenclature.description.period.sid:int,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:productline.suffix:int,oub\:validity.start.date:string>,oub\:goods.nomenclature.description:struct<oub\:description:string,oub\:goods.nomenclature.description.period.sid:int,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:language.id:string,oub\:productline.suffix:int>,oub\:duty.expression.description:struct<oub\:description:string,oub\:duty.expression.id:int,oub\:language.id:string>,oub\:additional.code.description:struct<oub\:additional.code:int,oub\:additional.code.description.period.sid:int,oub\:additional.code.sid:int,oub\:additional.code.type.id:string,oub\:description:string,oub\:language.id:string>,oub\:quota.unsuspension.event:struct<oub\:occurrence.timestamp:string,oub\:quota.definition.sid:int,oub\:unsuspension.date:string>,oub\:export.refund.nomenclature.description.period:struct<oub\:additional.code.type:int,oub\:export.refund.code:int,oub\:export.refund.nomenclature.description.period.sid:int,oub\:export.refund.nomenclature.sid:int,oub\:goods.nomenclature.item.id:int,oub\:productline.suffix:int,oub\:validity.start.date:string>,oub\:export.refund.nomenclature.description:struct<oub\:additional.code.type:int,oub\:description:string,oub\:export.refund.code:int,oub\:export.refund.nomenclature.description.period.sid:int,oub\:export.refund.nomenclature.sid:int,oub\:goods.nomenclature.item.id:int,oub\:language.id:string,oub\:productline.suffix:int>,oub\:goods.nomenclature.indents:struct<oub\:goods.nomenclature.indent.sid:int,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:number.indents:int,oub\:productline.suffix:int,oub\:validity.start.date:string>,oub\:goods.nomenclature.origin:struct<oub\:derived.goods.nomenclature.item.id:bigint,oub\:derived.productline.suffix:int,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:productline.suffix:int>,oub\:quota.order.number:struct<oub\:quota.order.number.id:int,oub\:quota.order.number.sid:int,oub\:validity.start.date:string,oub\:validity.end.date:string>,oub\:quota.order.number.origin:struct<oub\:geographical.area.id:string,oub\:geographical.area.sid:int,oub\:quota.order.number.origin.sid:int,oub\:quota.order.number.sid:int,oub\:validity.start.date:string,oub\:validity.end.date:string>,oub\:certificate.description:struct<oub\:certificate.code:int,oub\:certificate.description.period.sid:int,oub\:certificate.type.code:string,oub\:description:string,oub\:language.id:string>,oub\:full.temporary.stop.regulation:struct<oub\:approved.flag:int,oub\:effective.enddate:string,oub\:full.temporary.stop.regulation.id:string,oub\:full.temporary.stop.regulation.role:int,oub\:information.text:string,oub\:officialjournal.number:string,oub\:officialjournal.page:int,oub\:published.date:string,oub\:replacement.indicator:int,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:additional.code.description.period:struct<oub\:additional.code:int,oub\:additional.code.description.period.sid:int,oub\:additional.code.sid:int,oub\:additional.code.type.id:string,oub\:validity.start.date:string>,oub\:certificate:struct<oub\:certificate.code:int,oub\:certificate.type.code:string,oub\:validity.start.date:string,oub\:validity.end.date:string>,oub\:certificate.description.period:struct<oub\:certificate.code:int,oub\:certificate.description.period.sid:int,oub\:certificate.type.code:string,oub\:validity.start.date:string>,oub\:geographical.area.membership:struct<oub\:geographical.area.group.sid:int,oub\:geographical.area.sid:int,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:geographical.area:struct<oub\:geographical.area.id:string,oub\:geographical.area.sid:int,oub\:geographical.code:int,oub\:validity.start.date:string,oub\:validity.end.date:string>,oub\:geographical.area.description.period:struct<oub\:geographical.area.description.period.sid:int,oub\:geographical.area.id:string,oub\:geographical.area.sid:int,oub\:validity.start.date:string>,oub\:geographical.area.description:struct<oub\:description:string,oub\:geographical.area.description.period.sid:int,oub\:geographical.area.id:string,oub\:geographical.area.sid:int,oub\:language.id:string>,oub\:fts.regulation.action:struct<oub\:fts.regulation.id:string,oub\:fts.regulation.role:int,oub\:stopped.regulation.id:string,oub\:stopped.regulation.role:int>,oub\:monetary.exchange.period:struct<oub\:monetary.exchange.period.sid:int,oub\:parent.monetary.unit.code:string,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:monetary.exchange.rate:struct<oub\:child.monetary.unit.code:string,oub\:exchange.rate:double,oub\:monetary.exchange.period.sid:int>,oub\:measure.partial.temporary.stop:struct<oub\:abrogation.regulation.id:string,oub\:abrogation.regulation.officialjournal.number:string,oub\:abrogation.regulation.officialjournal.page:int,oub\:measure.sid:int,oub\:partial.temporary.stop.regulation.id:string,oub\:partial.temporary.stop.regulation.officialjournal.number:string,oub\:partial.temporary.stop.regulation.officialjournal.page:int,oub\:validity.end.date:string,oub\:validity.start.date:string>,oub\:measurement.unit.qualifier:struct<oub\:measurement.unit.qualifier.code:string,oub\:validity.start.date:string>,oub\:measurement.unit.qualifier.description:struct<oub\:description:string,oub\:language.id:string,oub\:measurement.unit.qualifier.code:string>,oub\:measurement:struct<oub\:measurement.unit.code:string,oub\:measurement.unit.qualifier.code:string,oub\:validity.start.date:string>,oub\:quota.association:struct<oub\:coefficient:double,oub\:main.quota.definition.sid:int,oub\:relation.type:string,oub\:sub.quota.definition.sid:int>,oub\:quota.suspension.period:struct<oub\:description:string,oub\:quota.definition.sid:int,oub\:quota.suspension.period.sid:int,oub\:suspension.end.date:string,oub\:suspension.start.date:string>,oub\:footnote.association.goods.nomenclature:struct<oub\:footnote.id:int,oub\:footnote.type:string,oub\:goods.nomenclature.item.id:bigint,oub\:goods.nomenclature.sid:int,oub\:productline.suffix:int,oub\:validity.start.date:string>,oub\:regulation.group:struct<oub\:regulation.group.id:string,oub\:validity.start.date:string>,oub\:export.refund.nomenclature.indents:struct<oub\:additional.code.type:int,oub\:export.refund.code:int,oub\:export.refund.nomenclature.indents.sid:int,oub\:export.refund.nomenclature.sid:int,oub\:goods.nomenclature.item.id:int,oub\:number.export.refund.nomenclature.indents:int,oub\:productline.suffix:int,oub\:validity.start.date:string>,oub\:export.refund.nomenclature:struct<oub\:additional.code.type:int,oub\:export.refund.code:int,oub\:export.refund.nomenclature.sid:int,oub\:goods.nomenclature.item.id:int,oub\:goods.nomenclature.sid:int,oub\:productline.suffix:int,oub\:validity.start.date:string>,oub\:measure.type:struct<oub\:measure.component.applicable.code:int,oub\:measure.explosion.level:int,oub\:measure.type.id:int,oub\:measure.type.series.id:string,oub\:order.number.capture.code:int,oub\:origin.dest.code:int,oub\:priority.code:int,oub\:trade.movement.code:int,oub\:validity.start.date:string>,oub\:measure.type.description:struct<oub\:description:string,oub\:language.id:string,oub\:measure.type.id:int>,oub\:measure.action:struct<oub\:action.code:int,oub\:validity.start.date:string>,oub\:measure.action.description:struct<oub\:action.code:int,oub\:description:string,oub\:language.id:string>,oub\:measure.condition.code:struct<oub\:condition.code:string,oub\:validity.start.date:string>,oub\:measure.condition.code.description:struct<oub\:condition.code:string,oub\:description:string,oub\:language.id:string>,oub\:monetary.unit:struct<oub\:monetary.unit.code:string,oub\:validity.start.date:string,oub\:validity.end.date:string>,oub\:monetary.unit.description:struct<oub\:description:string,oub\:language.id:string,oub\:monetary.unit.code:string>,oub\:additional.code.type:struct<oub\:additional.code.type.id:string,oub\:application.code:int,oub\:validity.start.date:string>,oub\:additional.code.type.description:struct<oub\:additional.code.type.id:string,oub\:description:string,oub\:language.id:string>,oub\:additional.code.type.measure.type:struct<oub\:additional.code.type.id:string,oub\:measure.type.id:int,oub\:validity.start.date:string>,oub\:language:struct<oub\:language.id:string,oub\:validity.start.date:string>,oub\:language.description:struct<oub\:description:string,oub\:language.code.id:string,oub\:language.id:string>,oub\:certificate.type.description:struct<oub\:certificate.type.code:string,oub\:description:string,oub\:language.id:string>>>


Edited by: Bit Zesty Ltd on Feb 20, 2019 8:33 AM"
Amazon Athena	"java.lang.IllegalThreadStateException occurred occasionally when querying
I am getting ""java.sql.SQLException: [JDBC Driver]null"" caused by ""java.lang.IllegalThreadStateException"" when querying via Athena JDBC Driver.

I was doing simple queries such as ""select * from mydatabase.companyfunding limit 10"" or ""show tables in mydatabase;"". I think the exception happens when a query is executed around a minute after a successful query. 

After a lot of googling, I only found a thread about a Presto issue that looks similar to mine https://github.com/prestodb/presto/issues/4748 . Since Athena is built on Presto, I wonder if they are related. 

I start a connection via com.simba.athena.jdbc.DataSource and execute a query such as follow:
	  DataSource ds = new com.simba.athena.jdbc.DataSource();
	  ds.setURL(athenaUrl);
	  source = ds.getPooledConnection();
	  if (conn == null) { conn = source.getConnection(); } 

	  String sql = ""show tables in ""+ databaseName;
	  ps = conn.prepareStatement(sql);
	  ResultSet rs = ps.executeQuery();

          while (rs.next()) {
              //Retrieve table column.
              String name = rs.getString(""tab_name"");

              //Display values.
              System.out.println(""Name: "" + name);
          }
          rs.close();
          conn.close();
	  conn = null;

The Athena Driver version is AthenaJDBC42_2.0.6.jar and AWS SDK version is aws-java-sdk-1.11.292.

The Stack trace is such as follow:
		Stack Trace:
		java.sql.SQLException: [JDBC Driver]null
			at java.lang.ThreadGroup.addUnstarted:867
			at java.lang.Thread.init:405
			at java.lang.Thread.init:349
			at java.lang.Thread.<init>:678
			at java.util.concurrent.Executors$DefaultThreadFactory.newThread:613
			at java.util.concurrent.ThreadPoolExecutor$Worker.<init>:619
			at java.util.concurrent.ThreadPoolExecutor.addWorker:932
			at java.util.concurrent.ThreadPoolExecutor.execute:1378
			at java.util.concurrent.AbstractExecutorService.submit:112
			at com.simba.athena.athena.dataengine.AJStreamResultSet.<init>
			at com.simba.athena.athena.dataengine.AJQueryExecutor.execute
			at com.simba.athena.jdbc.common.SPreparedStatement.executeWithParams
			at com.simba.athena.jdbc.common.SPreparedStatement.executeQuery
			at com.simba.athena.jdbc.jdbc42.S42PreparedStatementHandle.executeQuery
			at com.example.MyAthenaJDBCApp$MyThread.run

		Stack Trace:
		java.lang.IllegalThreadStateException
			at java.lang.ThreadGroup.addUnstarted:867
			at java.lang.Thread.init:405
			at java.lang.Thread.init:349
			at java.lang.Thread.<init>:678
			at java.util.concurrent.Executors$DefaultThreadFactory.newThread:613
			at java.util.concurrent.ThreadPoolExecutor$Worker.<init>:619
			at java.util.concurrent.ThreadPoolExecutor.addWorker:932
			at java.util.concurrent.ThreadPoolExecutor.execute:1378
			at java.util.concurrent.AbstractExecutorService.submit:112
			at com.simba.athena.athena.dataengine.AJStreamResultSet.<init>
			at com.simba.athena.athena.dataengine.AJQueryExecutor.execute
			at com.simba.athena.jdbc.common.SPreparedStatement.executeWithParams
			at com.simba.athena.jdbc.common.SPreparedStatement.executeQuery
			at com.simba.athena.jdbc.jdbc42.S42PreparedStatementHandle.executeQuery
			at com.example.MyAthenaJDBCApp$MyThread.run

Edited by: YuriAY on Feb 17, 2019 11:14 PM"
Amazon Athena	"Athena MAP of MAP column type?
Hello guys,
I have this table inferred via GlUE:

CREATE EXTERNAL TABLE `total_kas_com`(
  `feed_date` string COMMENT 'from deserializer', 
  `total_system_count` struct<oas:int,ods:int,mav:int,wav:int,ids:int,vul:int,kas:int> COMMENT 'from deserializer', 
  `country_system_count` struct<oas:map<string,int>,ods:map<string,int>,mav:map<string,int>,wav:map<string,int>,ids:map<string,int>,vul:map<string,int>,kas:map<string,int>> COMMENT 'from deserializer', 
  `country_total_count` map<string,int> COMMENT 'from deserializer')
PARTITIONED BY ( 
  `year` string, 
  `month` string, 
  `day` string, 
  `hour` string)
ROW FORMAT SERDE 
  'org.openx.data.jsonserde.JsonSerDe' 
WITH SERDEPROPERTIES ( 
  'paths'='Country_System_Count,Country_Total_count,Feed_Date,Total_System_Count') 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'

When I query via Athena I get this odd error message:

HIVE_METASTORE_ERROR: com.facebook.presto.spi.PrestoException: Error: type expected at the position 0 of 'MAP<STRING,INT>' but 'MAP' is found. (Service: null; Status Code: 0; Error Code: null; Request ID: null)

which doesn't make any sense, the schema is pretty basic and the map definition seems correct...

Cheers."
Amazon Athena	"Re: Athena MAP of MAP column type?
Hi,
I see that you are facing the below error when trying to query data with type struct:
HIVE_METASTORE_ERROR: com.facebook.presto.spi.PrestoException: Error: type expected at the position 0 of 'MAP<STRING,INT>' but 'MAP' is found. (Service: null; Status Code: 0; Error Code: null; Request ID: null)

As Athena supports only lowercase characers can we please try once by using all lowercase.
Link: https://docs.aws.amazon.com/athena/latest/ug/tables-databases-columns-names.html#table-names-and-table-column-names-in-ate-must-be-lowercase

Can we please try to make the changes according to the example below.
STRUCT<raw_text:ARRAY<string>>
to
struct<raw_text:array<string>>

Apologies for any incoonvenience caused as there might be examples in Athena documentation wherein uppercase letters are used and this is considered by the internal team.

I hope above helps. If you are facing any query related issues you can definitely contact AWS Support for more in-depth investigation.

Have a AWSome day"
Amazon Athena	"Re: Athena MAP of MAP column type?
I checked the schema in the Glue console and everything is lower cased (already).
The table definition is all already lower cased as you can see.
Is this some sort of bug?"
Amazon Athena	"HIVE_BAD_DATA: Error parsing field value
I have an S3 bucket folder of simple TSV data setup in AWS Glue. There seems to be a problem with some specific records, however, the error message does not provide any clues as to what (unlike similar errors about JSON or empty strings). Even more perplexing, I can query for the specific value in the error message itself just fine. The schema is simply (string,string,bigint). No fancy JSON or avro structures, etc.

The first error is along the lines of:
HIVE_BAD_DATA: Error parsing field value 'http://na.op.gg/summoner/userName=' for field 2: For input string: ""http://na.op.gg/summoner/userName=""


For a query like:
Select c.*, f.adj_count
FROM imp_forecast f
join canonicalized_urls c on f.url_hash = c.c_url_id
where f.adj_count > 10000
order by f.adj_count desc
limit 1000

(query ef349c38-1952-4410-bfe0-3b3b0fb8e314)

Strangely, a query like this query works:
Select c.c_url, f.adj_count
FROM imp_forecast f
join canonicalized_urls c on f.url_hash = c.c_url_id
where c.c_url = 'http://na.op.gg/summoner/userName='
order by f.adj_count desc
limit 1000

(query 9c00368e-5740-4929-b923-59a710caf142)

Most queries trying to sidestep this record still fail with the same error. This one gets past the bad 'na.op.gg' record above, but then fails on another one:
select c.*, f.adj_count
FROM imp_forecast f
join canonicalized_urls c on f.url_hash = c.c_url_id
where adj_count > 10000 and url  'http://na.op.gg/summoner/userName='
order by adj_count desc
limit 1000

(query 97f4f023-5011-42de-a012-f53502afdd54)

Error: HIVE_BAD_DATA: Error parsing field value 'https://globalnews.ca/search/9.' for field 2: For input string: ""https://globalnews.ca/search/9.""


The table config is similar to others I've used without issue. 

{
    ""Name"": ""canonicalized_urls"",
    ""StorageDescriptor"": {
        ""Columns"": [
            {
                ""Name"": ""url"",
                ""Type"": ""string""
            },
            {
                ""Name"": ""c_url"",
                ""Type"": ""string""
            },
            {
                ""Name"": ""c_url_id"",
                ""Type"": ""bigint""
            }
        ],
        ""Location"": ""s3://our_s3_bucket/canonicalized_urls/"",
        ""InputFormat"": ""org.apache.hadoop.mapred.TextInputFormat"",
        ""OutputFormat"": ""org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat"",
        ""Compressed"": false,
        ""NumberOfBuckets"": -1,
        ""SerdeInfo"": {
            ""SerializationLibrary"": ""org.apache.hadoop.hive.serde2.OpenCSVSerde"",
            ""Parameters"": {
                ""separatorChar"": ""\\t"",
                ""quoteChar"": ""\\"""",
                ""escapeChar"": ""\\\""
            }
        },
        ""BucketColumns"": [],
        ""SortColumns"": [],
        ""Parameters"": {
            ""classification"": ""csv"",
            ""compressionType"": ""none"",
            ""typeOfData"": ""file""
        },
        ""StoredAsSubDirectories"": false
    },
    ""PartitionKeys"": [
        {
            ""Name"": ""partition_0"",
            ""Type"": ""string""
        }
    ],
    ""TableType"": ""EXTERNAL_TABLE"",
    ""Parameters"": {
        ""classification"": ""csv"",
        ""compressionType"": ""none"",
        ""typeOfData"": ""file""
    }
}


Has anyone else run into this before? Am I overlooking some glitch in my table configuration?"
Amazon Athena	"Re: HIVE_BAD_DATA: Error parsing field value
This is happening to me now... have you been able to resolve?"
Amazon Athena	"Help: Athena Nested Query
I'm trying to query an s3 bucket with Athena, and have configured the schema with glue successfully so that I get a preview of the table that lists 10 items. The objects are complex, including nested formatting and I'm having trouble querying by an id that is in the middle of the data. Below is an example item, I've shortened it greatly but am hoping to get an idea of what a query would look like to return objects/items with a particular id (here id='1234567898').

{items=[{group={information=https://forums.aws.amazon.com/}
I believe the object didn't display correctly, so here is an image of the example obj.
http://tinypic.com/r/255qhi8/9

Edited by: ldew on Feb 13, 2019 3:33 PM"
Amazon Athena	"Unable to verify/create output bucket <bucket_name>
Hi,

We are using the latest AWS JDK and running queries has been working. However today we kept seeing this error message, and we did not change anything. We are sure that we do have permission to the bucket.
Here is the stack trace:

Unable to verify/create output bucket <bucket_name> (Service: AmazonAthena; Status Code: 400; Error Code: InvalidRequestException; Request ID: 2fa92e90-793a-11e7-b87c-d3ddc27cf1d3): com.amazonaws.services.athena.model.InvalidRequestException
com.amazonaws.services.athena.model.InvalidRequestException: Unable to verify/create output bucket <bucket_name> (Service: AmazonAthena; Status Code: 400; Error Code: InvalidRequestException; Request ID: 2fa92e90-793a-11e7-b87c-d3ddc27cf1d3)
at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1587)
at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1257)
at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1029)
at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:741)
at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:715)
at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:697)
at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:665)
at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:647)
at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:511)
at com.amazonaws.services.athena.AmazonAthenaClient.doInvoke(AmazonAthenaClient.java:802)
at com.amazonaws.services.athena.AmazonAthenaClient.invoke(AmazonAthenaClient.java:778)
at com.amazonaws.services.athena.AmazonAthenaClient.executeStartQueryExecution(AmazonAthenaClient.java:684)
at com.amazonaws.services.athena.AmazonAthenaClient.startQueryExecution(AmazonAthenaClient.java:660)
...

How should we resolve this issue? Thanks!"
Amazon Athena	"Re: Unable to verify/create output bucket <bucket_name>
After waiting for an hour and running the same code, it started working again.

This makes me think that there are some weird issues with Athena. Could you guys please investigate?

Thanks!"
Amazon Athena	"Re: Unable to verify/create output bucket <bucket_name>
Hello, 

Sorry for the issues. Has this issue happened again ? We will investigate this."
Amazon Athena	"Re: Unable to verify/create output bucket <bucket_name>
This happened to me as well. I get sporadic errors saying ""Unable to verify/create output bucket "". Any idea on whats going on?"
Amazon Athena	"Re: Unable to verify/create output bucket <bucket_name>
I'm seeing this too. We have a relatively small set of parquet files under a single simple athena schema. 

The error happens from a python client as well as from an IDE like SquirrellSQL. The difference is the python code connects and disconnects for every query. The queries are very simple, like ""select * from schema.table limit 100""

The error goes away after a while. 

The error seems to be more frequent when our servers are adding parquet files in the mornings. We are only running ""repair table"" once a day after our daily ingestion completes. 

def athena_query(sql):

    conn = connect(s3_staging_dir=stage_dir, region_name=region, profile_name='prd')
    datals = []
    try:
        with conn.cursor() as cursor:
            cursor.execute(""""""{x}"""""".format(x=sql))
            for row in cursor:
                datals.append(row)
    finally:
        conn.close()

    return datals"
Amazon Athena	"Re: Unable to verify/create output bucket <bucket_name>
Hello,

This is happening again. Please help!"
Amazon Athena	"Re: Unable to verify/create output bucket <bucket_name>
Encountering the same issue when using boto3 to execute a query. Any solutions?"
Amazon Athena	"Re: Unable to verify/create output bucket <bucket_name>
+1, Also seeing this error sporadically"
Amazon Athena	"Re: Unable to verify/create output bucket <bucket_name>
having the same issue here running the query from web console."
Amazon Athena	"Re: Unable to verify/create output bucket <bucket_name>
Same issue, absolutely non-deterministic, please fix!"
Amazon Athena	"HIVE_METASTORE_ERROR - type expected at the position 0 ... but STRUCT found
I have an Athena table querying parquet data from S3. The following is the create DDL :
CREATE EXTERNAL TABLE `testactions`(
  `_id` string, 
  `access_key` string, 
  `user` STRUCT<name:STRING,email:STRING>)
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION
  's3://bucket/prefix/'
TBLPROPERTIES (
  'classification'='parquet')


When I try to query this data, I get the following error, regardless of the query run :
Your query has the following error(s):
 
HIVE_METASTORE_ERROR: com.facebook.presto.spi.PrestoException: Error: type expected at the position 0 of 'STRUCT<name:STRING,email:STRING>' but 'STRUCT' is found. (Service: null; Status Code: 0; Error Code: null; Request ID: null)


Does parquet format in Athena not support structs?"
Amazon Athena	"Re: HIVE_METASTORE_ERROR - type expected at the position 0 ... but STRUCT found
If I don't remember wrong, AWS athena doesn't support struct just support separate types and map"
Amazon Athena	"Re: HIVE_METASTORE_ERROR - type expected at the position 0 ... but STRUCT found
If it doesn't support structs, why does it have the same in create table DDL? Also, the docs specify that struct is a valid data type.

Ref - https://docs.aws.amazon.com/athena/latest/ug/data-types.html"
Amazon Athena	"Re: HIVE_METASTORE_ERROR - type expected at the position 0 ... but STRUCT found
When I use Athena, when at the step add column type. I don't found any struct type but map found. I don't know why but in my opinion, you shoud seperate it for easy use."
Amazon Athena	"Re: HIVE_METASTORE_ERROR - type expected at the position 0 ... but STRUCT found
Hi,
I see that you are facing the below error when trying to query data with type struct:
HIVE_METASTORE_ERROR: com.facebook.presto.spi.PrestoException: Error: type expected at the position 0 of 'STRUCT<name:STRING,email:STRING>' but 'STRUCT' is found. (Service: null; Status Code: 0; Error Code: null; Request ID: null)

As Athena supports only lowercase characers can we please try once by using all lowercase. 
Link: https://docs.aws.amazon.com/athena/latest/ug/tables-databases-columns-names.html#table-names-and-table-column-names-in-ate-must-be-lowercase

Example to change the column types:
STRUCT<raw_text:ARRAY<string>>
to 
struct<raw_text:array<string>>

Apologies for any incoonvenience caused as there might be examples in Athena documentation wherein uppercase letters are used and this is considered by the internal team. 

I hope above helps. If you are facing any query related issues you can definitely contact AWS Support for more in-depth investigation.

Have a AWSome day"
Amazon Athena	"Re: HIVE_METASTORE_ERROR - type expected at the position 0 ... but STRUCT found
Oh, thank you Harsh. This was indeed the fix. We changed the case of the type definition in Glue, and it now works. Along with the documentation, schema examples in Glue should also be updated. Thanks!"
Amazon Athena	"'updating' athena partition question
Been going through the ""Analyze your Amazon CloudFront access logs at scale"" - to get an idea of Athena - great intro.
I'm trying to code the case where a partition in the columnar (parquet) table needs to be updated (a log file has come in late or we need to add historic log files), which is briefly mentioned in the blog post.  I think the steps would be:
1.  Add the new log file to partitioned space in the partitioned_gz table
2.  Delete partition in the 'parquet' table 
3.  Rerun the CTAS statement to 'copy' the partition data from partitioned_gz to the parquet tables

How should the parquet partition be deleted? just delete the files from S3 or is there data in athena metadata that needs to be updated?"
Amazon Athena	"Analysis of S3 Access Logs using Athena
Hello,

Has anyone tried analyzing the S3 bucket access logs using Athena, this post in particular?
https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/

I tried this but I am not 100% sure if this is working as expected, After following the steps in the above link, my table contains 14 million rows and out of that 2.5 million rows are null. 

Not sure if it is not parsing correctly or if the rows are actually null. Is there any other method that can be used for parsing S3 logs? Any help would be appreciated.

Thank you.

Edited by: d021 on Feb 10, 2019 1:16 PM"
Amazon Athena	"How to skip non-matching groups?
Hi,

I created a regex for my S3 log messages and checked it works with Athena. My messages don't always have the same (meta-)data. However the regex covers all cases. Some data appears only in some messages causing applying regex produces trash columns. See an example below. I don't want to have the trash in my table. I can create a table without these columns, but if I try to query the table, I get HIVE_CURSOR_ERROR: Number of matching groups doesn't match the number of columns. Only if I add the trash columns to my table it works. OK, I can create a view selecting proper columns, but I don't want to keep the trash in my table.

Example:
regex: ( method <<(^>*)>>|)( status <<(^>*)>>|)
log request msg: method <<GET>>
log response msg: status <<200>>
I want to have just two columns method and status.
But applying the regex produces four groups: method, method <<GET>>,  status, status <<200>>.
So, I created a table with four columns for these four groups. Then I created a view selecting just method and status columns.
I don't like it. I'd prefer to have a table with just two columns.

Is it possible to skip non-matching groups at querying to match the number of columns?
If not, what other options do I have?"
Amazon Athena	"Re: How to skip non-matching groups?
this regex (?: method <<([^>]*)>>|)(?: status <<([^>]*)>>|)
 produces only two groups method and status."
Amazon Athena	"HIVE_METASTORE_ERROR expected 'STRING' but 'STRING' is found
I've been unable to get any query to work against my AWS Glue Partitioned table. The error I'm getting is HIVE_METASTORE_ERROR: com.facebook.presto.spi.PrestoException: Error: type expected at the position 0 of 'STRING' but 'STRING' is found. (Service: null; Status Code: 0; Error Code: null; Request ID: null)


I've found one other thread that brings up the fact that the database name and table cannot have characters other than alphanumeric and underscores. So, I made sure the database name, table name and all column names adhere to this restriction. The only object that does not adhere to this restriction is my s3 bucket name which would be very difficult to change.

Here are the table definitions, parquet-tools dumps of the data and an AWS Query ID that failed.
AWS Glue Table Definition
-------------------
{
    ""Table"": {
        ""UpdateTime"": 1545845064.0, 
        ""PartitionKeys"": [
            {
                ""Comment"": ""call_time year"", 
                ""Type"": ""INT"", 
                ""Name"": ""date_year""
            }, 
            {
                ""Comment"": ""call_time month"", 
                ""Type"": ""INT"", 
                ""Name"": ""date_month""
            }, 
            {
                ""Comment"": ""call_time day"", 
                ""Type"": ""INT"", 
                ""Name"": ""date_day""
            }
        ], 
        ""StorageDescriptor"": {
            ""OutputFormat"": ""org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat"", 
            ""SortColumns"": [], 
            ""InputFormat"": ""org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat"", 
            ""SerdeInfo"": {
                ""SerializationLibrary"": ""org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe"", 
                ""Name"": ""ser_de_info_system_admin_created"", 
                ""Parameters"": {
                    ""serialization.format"": ""1""
                }
            }, 
            ""BucketColumns"": [], 
            ""Parameters"": {}, 
            ""Location"": ""s3://ph-data-lake-cududfs2z3xveg5t/curated/system/admin_created/"", 
            ""NumberOfBuckets"": 0, 
            ""StoredAsSubDirectories"": false, 
            ""Columns"": [
                {
                    ""Comment"": ""Unique user ID"", 
                    ""Type"": ""STRING"", 
                    ""Name"": ""user_id""
                }, 
                {
                    ""Comment"": ""Unique group ID"", 
                    ""Type"": ""STRING"", 
                    ""Name"": ""group_id""
                }, 
                {
                    ""Comment"": ""Date and time the message was published"", 
                    ""Type"": ""TIMESTAMP"", 
                    ""Name"": ""call_time""
                }, 
                {
                    ""Comment"": ""call_time year"", 
                    ""Type"": ""INT"", 
                    ""Name"": ""date_year""
                }, 
                {
                    ""Comment"": ""call_time month"", 
                    ""Type"": ""INT"", 
                    ""Name"": ""date_month""
                }, 
                {
                    ""Comment"": ""call_time day"", 
                    ""Type"": ""INT"", 
                    ""Name"": ""date_day""
                }, 
                {
                    ""Comment"": ""Given name for user"", 
                    ""Type"": ""STRING"", 
                    ""Name"": ""given_name""
                }, 
                {
                    ""Comment"": ""IANA time zone for user"", 
                    ""Type"": ""STRING"", 
                    ""Name"": ""time_zone""
                }, 
                {
                    ""Comment"": ""Name that links to geneaology"", 
                    ""Type"": ""STRING"", 
                    ""Name"": ""family_name""
                }, 
                {
                    ""Comment"": ""Email address for user"", 
                    ""Type"": ""STRING"", 
                    ""Name"": ""email""
                }, 
                {
                    ""Comment"": ""RFC BCP 47 code set in this user's profile language and region"", 
                    ""Type"": ""STRING"", 
                    ""Name"": ""language""
                }, 
                {
                    ""Comment"": ""Phone number including ITU-T ITU-T E.164 country codes"", 
                    ""Type"": ""STRING"", 
                    ""Name"": ""phone""
                }, 
                {
                    ""Comment"": ""Date user was created"", 
                    ""Type"": ""TIMESTAMP"", 
                    ""Name"": ""date_created""
                }, 
                {
                    ""Comment"": ""User role"", 
                    ""Type"": ""STRING"", 
                    ""Name"": ""role""
                }, 
                {
                    ""Comment"": ""Provider dashboard preferences"", 
                    ""Type"": ""STRUCT<portal_welcome_done:BOOLEAN,weekend_digests:BOOLEAN,patients_hidden:BOOLEAN,last_announcement:STRING>"", 
                    ""Name"": ""preferences""
                }, 
                {
                    ""Comment"": ""Provider notification settings"", 
                    ""Type"": ""STRUCT<digest_email:BOOLEAN>"", 
                    ""Name"": ""notifications""
                }
            ], 
            ""Compressed"": true
        }, 
        ""Parameters"": {
            ""classification"": ""parquet"", 
            ""parquet.compress"": ""SNAPPY""
        }, 
        ""Description"": ""System wide admin_created messages"", 
        ""Name"": ""system_admin_created"", 
        ""TableType"": ""EXTERNAL_TABLE"", 
        ""Retention"": 0
    }
}


AWS Athena schema
CREATE EXTERNAL TABLE `system_admin_created`(
  `user_id` STRING COMMENT 'Unique user ID', 
  `group_id` STRING COMMENT 'Unique group ID', 
  `call_time` TIMESTAMP COMMENT 'Date and time the message was published', 
  `date_year` INT COMMENT 'call_time year', 
  `date_month` INT COMMENT 'call_time month', 
  `date_day` INT COMMENT 'call_time day', 
  `given_name` STRING COMMENT 'Given name for user', 
  `time_zone` STRING COMMENT 'IANA time zone for user', 
  `family_name` STRING COMMENT 'Name that links to geneaology', 
  `email` STRING COMMENT 'Email address for user', 
  `language` STRING COMMENT 'RFC BCP 47 code set in this user\'s profile language and region', 
  `phone` STRING COMMENT 'Phone number including ITU-T ITU-T E.164 country codes', 
  `date_created` TIMESTAMP COMMENT 'Date user was created', 
  `role` STRING COMMENT 'User role', 
  `preferences` STRUCT<portal_welcome_done:BOOLEAN,weekend_digests:BOOLEAN,patients_hidden:BOOLEAN,last_announcement:STRING> COMMENT 'Provider dashboard preferences', 
  `notifications` STRUCT<digest_email:BOOLEAN> COMMENT 'Provider notification settings')
PARTITIONED BY ( 
  `date_year` INT COMMENT 'call_time year', 
  `date_month` INT COMMENT 'call_time month', 
  `date_day` INT COMMENT 'call_time day')
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION
  's3://ph-data-lake-cududfs2z3xveg5t/curated/system/admin_created/'
TBLPROPERTIES (
  'classification'='parquet', 
  'parquet.compress'='SNAPPY')


parquet-tools cat
role = admin
date_created = 2018-01-11T14:40:23.142Z
preferences:
.patients_hidden = false
.weekend_digests = true
.portal_welcome_done = true
email = foo.barr+123@example.com
notifications:
.digest_email = true
group_id = 5a5399df23a804001aa25227
given_name = foo
call_time = 2018-01-11T14:40:23.000Z
time_zone = US/Pacific
family_name = bar
language = en-US
user_id = 5a5777572060a700170240c3


parquet-tools schema
message spark_schema {
  optional binary role (UTF8);
  optional binary date_created (UTF8);
  optional group preferences {
    optional boolean patients_hidden;
    optional boolean weekend_digests;
    optional boolean portal_welcome_done;
    optional binary last_announcement (UTF8);
  }
  optional binary email (UTF8);
  optional group notifications {
    optional boolean digest_email;
  }
  optional binary group_id (UTF8);
  optional binary given_name (UTF8);
  optional binary call_time (UTF8);
  optional binary time_zone (UTF8);
  optional binary family_name (UTF8);
  optional binary language (UTF8);
  optional binary user_id (UTF8);
  optional binary phone (UTF8);
}


AWS Query ID

cea9abce-3176-443a-b4fa-15e189a923b3"
Amazon Athena	"Re: HIVE_METASTORE_ERROR expected 'STRING' but 'STRING' is found
use test file and run a crawler against it. Check whether you were able to reproduce the error. It looks like schema mismatch .It could be some of the file has incorrect parquet schema so , try on single file which you verified the schema."
Amazon Athena	"Re: HIVE_METASTORE_ERROR expected 'STRING' but 'STRING' is found
Hi,
I see that you are facing the below error when trying to query data with type struct:
HIVE_METASTORE_ERROR expected 'STRING' but 'STRING' is found

As Athena supports only lowercase characers can we please try once by using all lowercase.
Link: https://docs.aws.amazon.com/athena/latest/ug/tables-databases-columns-names.html#table-names-and-table-column-names-in-ate-must-be-lowercase

Can we please try to make the changes according to the example below and see if this helps here:
STRUCT<raw_text:ARRAY<string>>
to
struct<raw_text:array<string>>

I hope above helps. If you are facing any query related issues you can definitely contact AWS Support for more in-depth investigation.

Have a AWSome day"
Amazon Athena	"Query exhausted resources at this scale factor.
Query exhausted resources at this scale factor. You may need to manually clean the data at location ...

This query ran against the ""default"" database, unless qualified by the query. Please post the error message on our forum or contact customer support with Query Id: 3d3ff552-1b30-45fd-b5e6-a330809ba51d."
Amazon Athena	"Re: Query exhausted resources at this scale factor.
it usually happens due to worker overloaded as Athena keeps the data in memory. you need to reduce the input data size (if you join 10 days data then try 5 days data) . remove order by, approx_distinct instead of distinct etc..

https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/

Edited by: Shivan on Jan 14, 2019 2:38 AM"
Amazon Athena	"Re: Query exhausted resources at this scale factor.
Hi,

I understand that you are getting ""Query exhausted resources at this scale factor"" error sometime, while the same query execute successfully at other times.

As you might be aware that, Athena is a Serverless technology i.e. It make use of shared resources available with AWS and hence, when large amount of queries are submitted by users concurrently around the world at the same time, sometimes resource exhaustion take place. Our Athena service team has identified this as a known issue and we are working towards providing better flexibility. However, this error is transient in nature unless, you get same error everytime you submit a query. 
As you mentioned that sometimes query execute successfully, you can submit the query again and it will be successful. However, if you repeatedly get the same error, then it might be hitting Athena Hard Memory Limit and you might need to partition your data and optimize the query further as mentioned in Athena Best Practices  https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/.

However, to defer occurrence of such issue, you can find suggestions below : 
==================
1) Avoid submitting queries at the end of an hour. If query fails, Back off exponentially by some minutes and try to submit query again.
2) I highly recommend to adopt Amazon Athena best practices ( https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/) to optimize your query.
3) Use columnar formatted data which can drastically reduce the resource consumption.
==================

However, this issue can be due to other specific reasons as well. If you still face any issue with Amazon Athena, please provide us with the query id and region in which you are executing the query. I'd be glad to help you.

Have an AWSome day!"
Amazon Athena	"Athena ""Generate Create Table DDL"" call results in a Java Exception
Hi,

I'm getting an java exception when describing the ddl for a table that I have. The table's ddl uses a '-' for one of its keys. I believe this is a bug.

Here is an example of a record:
{""updated_at"": ""2019-01-17T00:51:17Z"", ""tenant_guid"": ""123456708"", ""payload"": {""2017"": {""UpdatedAt"": ""2019-01-17T00:51:17Z"", ""Members"": [{""Summary"": [{""DataID"": ""Members"", ""WithDocs"": 0, ""Total"": 0}], ""Reviewed"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""WithoutOpportunity"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""Documents"": [{""Summary"": [{""DataID"": ""Documents"", ""Acquired"": 0}], ""Reviewed"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""WithoutOpportunity"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""CodeActions"": [{""Summary"": [{""DataID"": ""Code Actions"", ""Suggested"": 0}], ""Accepted"": [{""Percent"": 0, ""ActualValue"": 0}], ""Rejected"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""ManAdd"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""CodesLinkedToClaim"": [{""Summary"": [{""DataID"": ""Codes Linked to a Claim"", ""Accepted_Added"": 0}], ""Linked"": [{""Percent"": 0, ""ActualValue"": 0}], ""NotLinked"": [{""Percent"": 0, ""ActualValue"": 0}]}]}, ""2017-2018"": {""UpdatedAt"": ""2019-01-17T00:51:17Z"", ""Members"": [{""Summary"": [{""DataID"": ""Members"", ""WithDocs"": 0, ""Total"": 0}], ""Reviewed"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""WithoutOpportunity"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""Documents"": [{""Summary"": [{""DataID"": ""Documents"", ""Acquired"": 0}], ""Reviewed"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""WithoutOpportunity"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""CodeActions"": [{""Summary"": [{""DataID"": ""Code Actions"", ""Suggested"": 0}], ""Accepted"": [{""Percent"": 0, ""ActualValue"": 0}], ""Rejected"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""ManAdd"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""CodesLinkedToClaim"": [{""Summary"": [{""DataID"": ""Codes Linked to a Claim"", ""Accepted_Added"": 0}], ""Linked"": [{""Percent"": 0, ""ActualValue"": 0}], ""NotLinked"": [{""Percent"": 0, ""ActualValue"": 0}]}]}, ""2017-2019"": {""UpdatedAt"": ""2019-01-17T00:51:17Z"", ""Members"": [{""Summary"": [{""DataID"": ""Members"", ""WithDocs"": 0, ""Total"": 0}], ""Reviewed"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""WithoutOpportunity"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""Documents"": [{""Summary"": [{""DataID"": ""Documents"", ""Acquired"": 0}], ""Reviewed"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""WithoutOpportunity"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""CodeActions"": [{""Summary"": [{""DataID"": ""Code Actions"", ""Suggested"": 0}], ""Accepted"": [{""Percent"": 0, ""ActualValue"": 0}], ""Rejected"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""ManAdd"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""CodesLinkedToClaim"": [{""Summary"": [{""DataID"": ""Codes Linked to a Claim"", ""Accepted_Added"": 0}], ""Linked"": [{""Percent"": 0, ""ActualValue"": 0}], ""NotLinked"": [{""Percent"": 0, ""ActualValue"": 0}]}]}, ""2018"": {""UpdatedAt"": ""2019-01-17T00:51:17Z"", ""Members"": [{""Summary"": [{""DataID"": ""Members"", ""WithDocs"": 0, ""Total"": 0}], ""Reviewed"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""WithoutOpportunity"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""Documents"": [{""Summary"": [{""DataID"": ""Documents"", ""Acquired"": 0}], ""Reviewed"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""WithoutOpportunity"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""CodeActions"": [{""Summary"": [{""DataID"": ""Code Actions"", ""Suggested"": 0}], ""Accepted"": [{""Percent"": 0, ""ActualValue"": 0}], ""Rejected"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""ManAdd"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""CodesLinkedToClaim"": [{""Summary"": [{""DataID"": ""Codes Linked to a Claim"", ""Accepted_Added"": 0}], ""Linked"": [{""Percent"": 0, ""ActualValue"": 0}], ""NotLinked"": [{""Percent"": 0, ""ActualValue"": 0}]}]}, ""2018-2019"": {""UpdatedAt"": ""2019-01-17T00:51:17Z"", ""Members"": [{""Summary"": [{""DataID"": ""Members"", ""WithDocs"": 0, ""Total"": 0}], ""Reviewed"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""WithoutOpportunity"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""Documents"": [{""Summary"": [{""DataID"": ""Documents"", ""Acquired"": 0}], ""Reviewed"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""WithoutOpportunity"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""CodeActions"": [{""Summary"": [{""DataID"": ""Code Actions"", ""Suggested"": 0}], ""Accepted"": [{""Percent"": 0, ""ActualValue"": 0}], ""Rejected"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""ManAdd"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""CodesLinkedToClaim"": [{""Summary"": [{""DataID"": ""Codes Linked to a Claim"", ""Accepted_Added"": 0}], ""Linked"": [{""Percent"": 0, ""ActualValue"": 0}], ""NotLinked"": [{""Percent"": 0, ""ActualValue"": 0}]}]}, ""2019"": {""UpdatedAt"": ""2019-01-17T00:51:17Z"", ""Members"": [{""Summary"": [{""DataID"": ""Members"", ""WithDocs"": 0, ""Total"": 0}], ""Reviewed"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""WithoutOpportunity"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""Documents"": [{""Summary"": [{""DataID"": ""Documents"", ""Acquired"": 0}], ""Reviewed"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""WithoutOpportunity"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""CodeActions"": [{""Summary"": [{""DataID"": ""Code Actions"", ""Suggested"": 0}], ""Accepted"": [{""Percent"": 0, ""ActualValue"": 0}], ""Rejected"": [{""Percent"": 0, ""ActualValue"": 0}], ""Remaining"": [{""Percent"": 0, ""ActualValue"": 0}], ""ManAdd"": [{""Percent"": 0, ""ActualValue"": 0}]}], ""CodesLinkedToClaim"": [{""Summary"": [{""DataID"": ""Codes Linked to a Claim"", ""Accepted_Added"": 0}], ""Linked"": [{""Percent"": 0, ""ActualValue"": 0}], ""NotLinked"": [{""Percent"": 0, ""ActualValue"": 0}]}]}}, ""dashboard_type"": ""scout_progress""}


Your query has the following error(s):
 
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.IllegalArgumentException: Error: : expected at the position 1025 of 'string:string:struct<2017:struct<UpdatedAt:string,Members:array<struct<Summary:array<struct<DataID:string,WithDocs:int,Total:int>>,Reviewed:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,WithoutOpportunity:array<struct<Percent:int,ActualValue:int>>>>,Documents:array<struct<Summary:array<struct<DataID:string,Acquired:int>>,Reviewed:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,WithoutOpportunity:array<struct<Percent:int,ActualValue:int>>>>,CodeActions:array<struct<Summary:array<struct<DataID:string,Suggested:int>>,Accepted:array<struct<Percent:int,ActualValue:int>>,Rejected:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,ManAdd:array<struct<Percent:int,ActualValue:int>>>>,CodesLinkedToClaim:array<struct<Summary:array<struct<DataID:string,Accepted_Added:int>>,Linked:array<struct<Percent:int,ActualValue:int>>,NotLinked:array<struct<Percent:int,ActualValue:int>>>>>,2017-2018:struct<UpdatedAt:string,Members:array<struct<Summary:array<struct<DataID:string,WithDocs:int,Total:int>>,Reviewed:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,WithoutOpportunity:array<struct<Percent:int,ActualValue:int>>>>,Documents:array<struct<Summary:array<struct<DataID:string,Acquired:int>>,Reviewed:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,WithoutOpportunity:array<struct<Percent:int,ActualValue:int>>>>,CodeActions:array<struct<Summary:array<struct<DataID:string,Suggested:int>>,Accepted:array<struct<Percent:int,ActualValue:int>>,Rejected:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,ManAdd:array<struct<Percent:int,ActualValue:int>>>>,CodesLinkedToClaim:array<struct<Summary:array<struct<DataID:string,Accepted_Added:int>>,Linked:array<struct<Percent:int,ActualValue:int>>,NotLinked:array<struct<Percent:int,ActualValue:int>>>>>,2017-2019:struct<UpdatedAt:string,Members:array<struct<Summary:array<struct<DataID:string,WithDocs:int,Total:int>>,Reviewed:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,WithoutOpportunity:array<struct<Percent:int,ActualValue:int>>>>,Documents:array<struct<Summary:array<struct<DataID:string,Acquired:int>>,Reviewed:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,WithoutOpportunity:array<struct<Percent:int,ActualValue:int>>>>,CodeActions:array<struct<Summary:array<struct<DataID:string,Suggested:int>>,Accepted:array<struct<Percent:int,ActualValue:int>>,Rejected:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,ManAdd:array<struct<Percent:int,ActualValue:int>>>>,CodesLinkedToClaim:array<struct<Summary:array<struct<DataID:string,Accepted_Added:int>>,Linked:array<struct<Percent:int,ActualValue:int>>,NotLinked:array<struct<Percent:int,ActualValue:int>>>>>,2018:struct<UpdatedAt:string,Members:array<struct<Summary:array<struct<DataID:string,WithDocs:int,Total:int>>,Reviewed:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,WithoutOpportunity:array<struct<Percent:int,ActualValue:int>>>>,Documents:array<struct<Summary:array<struct<DataID:string,Acquired:int>>,Reviewed:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,WithoutOpportunity:array<struct<Percent:int,ActualValue:int>>>>,CodeActions:array<struct<Summary:array<struct<DataID:string,Suggested:int>>,Accepted:array<struct<Percent:int,ActualValue:int>>,Rejected:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,ManAdd:array<struct<Percent:int,ActualValue:int>>>>,CodesLinkedToClaim:array<struct<Summary:array<struct<DataID:string,Accepted_Added:int>>,Linked:array<struct<Percent:int,ActualValue:int>>,NotLinked:array<struct<Percent:int,ActualValue:int>>>>>,2018-2019:struct<UpdatedAt:string,Members:array<struct<Summary:array<struct<DataID:string,WithDocs:int,Total:int>>,Reviewed:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,WithoutOpportunity:array<struct<Percent:int,ActualValue:int>>>>,Documents:array<struct<Summary:array<struct<DataID:string,Acquired:int>>,Reviewed:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,WithoutOpportunity:array<struct<Percent:int,ActualValue:int>>>>,CodeActions:array<struct<Summary:array<struct<DataID:string,Suggested:int>>,Accepted:array<struct<Percent:int,ActualValue:int>>,Rejected:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,ManAdd:array<struct<Percent:int,ActualValue:int>>>>,CodesLinkedToClaim:array<struct<Summary:array<struct<DataID:string,Accepted_Added:int>>,Linked:array<struct<Percent:int,ActualValue:int>>,NotLinked:array<struct<Percent:int,ActualValue:int>>>>>,2019:struct<UpdatedAt:string,Members:array<struct<Summary:array<struct<DataID:string,WithDocs:int,Total:int>>,Reviewed:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,WithoutOpportunity:array<struct<Percent:int,ActualValue:int>>>>,Documents:array<struct<Summary:array<struct<DataID:string,Acquired:int>>,Reviewed:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,WithoutOpportunity:array<struct<Percent:int,ActualValue:int>>>>,CodeActions:array<struct<Summary:array<struct<DataID:string,Suggested:int>>,Accepted:array<struct<Percent:int,ActualValue:int>>,Rejected:array<struct<Percent:int,ActualValue:int>>,Remaining:array<struct<Percent:int,ActualValue:int>>,ManAdd:array<struct<Percent:int,ActualValue:int>>>>,CodesLinkedToClaim:array<struct<Summary:array<struct<DataID:string,Accepted_Added:int>>,Linked:array<struct<Percent:int,ActualValue:int>>,NotLinked:array<struct<Percent:int,ActualValue:int>>>>>>:string' but '-' is found.
 
This query ran against the ""rascout"" database, unless qualified by the query. Please post the error message on our forum or contact customer support with Query Id: a66ffc54-81a3-4ef5-b2b2-0e33befb0b0f."
Amazon Athena	"Re: Athena ""Generate Create Table DDL"" call results in a Java Exception
did you create a table in Athena ? 
Athena does not support ""-"" however, datacatalog shared by other services like glue,emr and redshift which might not have same  validation hence you may get above error."
Amazon Athena	"Re: Athena ""Generate Create Table DDL"" call results in a Java Exception
The table was created with glue. Should Athena support ""-"" for consistency???"
Amazon Athena	"Re: Athena ""Generate Create Table DDL"" call results in a Java Exception
Hi,

Athena table, view, database, and column names cannot contain special characters, other than underscore (_).

Link : https://docs.aws.amazon.com/athena/latest/ug/tables-databases-columns-names.html

To work with special characters within the statements we can try to use backtics for the columns or table names. Example:
We can use backtics to enclose table, view, or column names that begin with an underscore. For example:
CREATE TABLE `_myunderscoretable` (
`_id` string,
`_index`string,
…

Similarly for the select statement as well we can use backtics when having a “-“ (or any other special character). Example:
SELECT * FROM 
`fpa-dev`
LIMIT 10

Regarding the describe table behavior with tables having special characters within the nested columns you can use column mapping within the SerDe properties of the table. We can handle forbidden characters with the help of mapping. Please refer the below blog for using the mapping parameter within the serve properties:
https://aws.amazon.com/blogs/big-data/create-tables-in-amazon-athena-from-nested-json-and-mappings-using-jsonserde/

I hope this helps but in case you have any doubts or any queries/issues when using Athena that need more in-depth analysis you can definitely contact AWS Support. And we will be glad to assist as always.

Have a AWSome Day"
Amazon Athena	"Athena -- Query exhausted resources at this scale factor
Hello,

I received the following error while executing a query using Athena:

Your query has the following error(s):

Query exhausted resources at this scale factor

This query ran against the ""REDACTED"" database, unless qualified by the query. Please post the error message on our forum or contact customer support with Query Id: 8a74b2c4-90ad-4e04-9968-61434bf78899.

Any insight into the meaning of this error would be much appreciated.

Cheers,
Dave"
Amazon Athena	"Re: Athena -- Query exhausted resources at this scale factor
Hi Dave,

I too am an Athena customer so this is not an authoritative statement.

However, when I have seen the ""Query exhausted resources at this scale factor"" error, and I have seen quite a few of them, it usually has meant that the query plan was too big for the Presto cluster running the query.  

A couple of things have helped some occurrences of the error:

1. Try to reduce the resource required by intermediate results in the plan:
     a. Reduce the number of columns projected.
     b. Try to split the query into 2 or more queries and materialize the any the earlier parts in a permanent table.
     c. Look hard to see if plan stalling operation like sorts on subqueries can be eliminated.
2. Split the query into smaller data increments.
3. Try different join orders.

I think Athena is still on a Presto version before the cost based optimizer (CBO) is available in Athena and before statistics are likely populated in the data catalog for the tables you're using.  
That's the biggest hope for these issues going forward, but as I see it there's alot of work that needs to be done to Athena to make it CBO ready. 

I hope this helps,
-Kurt"
Amazon Athena	"Re: Athena -- Query exhausted resources at this scale factor
Hi Kurt,

Thanks for the reply and the suggestions.  I wish the ""scale factor"" was less obscure and that it could be increased to handle the queries I want to execute.  Until then, I've broken up the queries as you suggested, which works fine.

Appreciate the response.

Cheers,
Dave"
Amazon Athena	"Tableau won't load data from Athena connection
Hello, I'm receiving the following error below in Tableau when I attempt to ""update now"" via live conneection. I've already established the Athena connection in Tableau by successfully entering in the credentials, signing in, and I'm pointing to the proper database and table. In addition, within Athena's query editor service I'm able to query the data as well which is stored in S3.

I'm not sure why I'm getting a ""bad connection"" error and an AthenaJDBC error because I have the appropriate driver and Java version installed on my machine. Another thing to add is that I was able to view the data perfectly fine in Tableau last week, but today I made some changes to data types of certain columns in the table that I created using AWS Glue. 

If someone could chime in that would be awesome.


An error occurred while communicating with data source 'aws_fhwa_trafficvolume_data (fhwa-traffic)'.

Bad Connection: Tableau could not connect to the data source.
com.tableausoftware.jdbc.TableauJDBCException: Exception in runQuery for query: SELECT 1 AS ""number_of_records"",
  ""aws_fhwa_trafficvo"".""data_date_volume"" AS ""data_date_volume"",
  ""aws_fhwa_trafficvo"".""day_of_data"" AS ""day_of_data"",
  ""aws_fhwa_trafficvo"".""day_of_week"" AS ""day_of_week"",
  ""aws_fhwa_trafficvo"".""direction_of_travel"" AS ""direction_of_trave"",
  ""aws_fhwa_trafficvo"".""for_the_filter"" AS ""for_the_filter"",
  ""aws_fhwa_trafficvo"".""functional_classification_code"" AS ""functional_classif"",
  ""aws_fhwa_trafficvo"".""hour_bins"" AS ""hour_bins"",
  ""aws_fhwa_trafficvo"".""lane_of_travel"" AS ""lane_of_travel"",
  ""aws_fhwa_trafficvo"".""latitude"" AS ""latitude"",
  ""aws_fhwa_trafficvo"".""longitude"" AS ""longitude"",
  ""aws_fhwa_trafficvo"".""month_of_data"" AS ""month_of_data"",
  ""aws_fhwa_trafficvo"".""number_of_lanes_in_the_direction_indicted"" AS ""number_of_lanes_in"",
  ""aws_fhwa_trafficvo"".""record_type"" AS ""record_type"",
  ""aws_fhwa_trafficvo"".""region_classification"" AS ""region_classificat"",
  ""aws_fhwa_trafficvo"".""state"" AS ""state"",
  ""aws_fhwa_trafficvo"".""station_id"" AS ""station_id"",
  ""aws_fhwa_trafficvo"".""station_type"" AS ""station_type"",
  ""aws_fhwa_trafficvo"".""vehicle_traffic"" AS ""vehicle_traffic"",
  ""aws_fhwa_trafficvo"".""year_of_data"" AS ""year_of_data""
FROM ""fhwa-traffic"".""aws_fhwa_trafficvolume_data"" ""aws_fhwa_trafficvo""
LIMIT 1000
https://forums.aws.amazon.com/https://forums.aws.amazon.com/(100122) An error has occurred. Details: Exception during column initialization: com.simba.athena.support.exceptions.GeneralException: AWS_CLIENT_ERR.
There was a Java error.
SELECT 1 AS ""number_of_records"",
  ""aws_fhwa_trafficvo"".""data_date_volume"" AS ""data_date_volume"",
  ""aws_fhwa_trafficvo"".""day_of_data"" AS ""day_of_data"",
  ""aws_fhwa_trafficvo"".""day_of_week"" AS ""day_of_week"",
  ""aws_fhwa_trafficvo"".""direction_of_travel"" AS ""direction_of_trave"",
  ""aws_fhwa_trafficvo"".""for_the_filter"" AS ""for_the_filter"",
  ""aws_fhwa_trafficvo"".""functional_classification_code"" AS ""functional_classif"",
  ""aws_fhwa_trafficvo"".""hour_bins"" AS ""hour_bins"",
  ""aws_fhwa_trafficvo"".""lane_of_travel"" AS ""lane_of_travel"",
  ""aws_fhwa_trafficvo"".""latitude"" AS ""latitude"",
  ""aws_fhwa_trafficvo"".""longitude"" AS ""longitude"",
  ""aws_fhwa_trafficvo"".""month_of_data"" AS ""month_of_data"",
  ""aws_fhwa_trafficvo"".""number_of_lanes_in_the_direction_indicted"" AS ""number_of_lanes_in"",
  ""aws_fhwa_trafficvo"".""record_type"" AS ""record_type"",
  ""aws_fhwa_trafficvo"".""region_classification"" AS ""region_classificat"",
  ""aws_fhwa_trafficvo"".""state"" AS ""state"",
  ""aws_fhwa_trafficvo"".""station_id"" AS ""station_id"",
  ""aws_fhwa_trafficvo"".""station_type"" AS ""station_type"",
  ""aws_fhwa_trafficvo"".""vehicle_traffic"" AS ""vehicle_traffic"",
  ""aws_fhwa_trafficvo"".""year_of_data"" AS ""year_of_data""
FROM ""fhwa-traffic"".""aws_fhwa_trafficvolume_data"" ""aws_fhwa_trafficvo""
LIMIT 1000"
Amazon Athena	"Re: Tableau won't load data from Athena connection
To troubleshoot above issue, please carry out the following steps on your Tableau Desktop host machine:

1) If installing latest Java does not resolve the issue, download an older version of the JDBC driver from 
https://s3.amazonaws.com/athena-downloads/drivers/JDBC/SimbaAthenaJDBC_2.0.2/AthenaJDBC41_2.0.2.jar

2) Move the downloaded .jar file to C:\Program Files\Tableau\Drivers

3) Remove any other Athena JDBC drivers in the same folder.

<Replacing the latest Athena JDBC driver with older one>

Best regards,
Aji Chacko

Edited by: achacko on Feb 5, 2019 1:30 AM"
Amazon Athena	"Error when trying to run Create table as Select
I ran the same CTAS query twice, and both times the data was scanned and errors were thrown in the middle of writing the output data.

the first time this error occurred:
HIVE_WRITER_CLOSE_ERROR: Error committing write: java.lang.IllegalStateException: Reached max limit of upload attempts for part. You may need to manually clean the data at location


the second time I ran the same query, this error occurred:
GENERIC_INTERNAL_ERROR: Unable to execute HTTP request: Read timed out. You may need to manually clean the data at location


Both times, 148 GB of data was scanned and errorred out after writing ~108 GB of data

Any insights into why this happened?

Thanks."
Amazon Athena	"Re: Error when trying to run Create table as Select
I'm also sporadically seeing these errors, after removing the data and retrying the query it tends to work but it's annoying.

HIVE_WRITER_CLOSE_ERROR: Error committing write: java.lang.IllegalStateException: Reached max limit of upload attempts for part. You may need to manually clean the data at location 's3://xxx' before retrying. Athena will not delete data in your account.


Any insight into what is happening? I can provide multiple query ids where this has happened if it helps."
Amazon Athena	"PreparedStatements with parameters with Athena JDBC driver
I'm hoping there's something I'm missing here, but looking for information on this I was stunned to realise that prepared statements with parameters don't appear to be supported in the Athena JDBC Driver.

This is standard practice in all JDBC code; in fact, not using prepared statements for parameterized queries in JDBC is considered extremely bad form, e.g. allowing for SQL injection attacks.

Is there something I'm missing here (e.g. some particular syntax different from standard JDBC)? If not, when will support for this be added?"
Amazon Athena	"Athena query with Regex
I need to query results from S3 bucket which contains logs from SSM Command. The output from Run Command contains several steps (download package, downlaod script and script output itself). This is example from one of the Run:
4.9.3252.0,i-094a085ds23412,us-east-1,32143523432
EC2ConfigVersion:4.9.3252.0

----------ERROR-------
failed to run commands: exit status 1

When I try to run just simple Select * from table I got mixedup results and only 1 row is correct which in the case below is row 4.
Results
 	ec2config	instanceid	awsregion	awsaccount
 	ec2config	instanceid	awsregion	awsaccount
1	Content downloaded to C:\programdata\infor-automation\EC2ConfigUpdate\Infra-UpdateEC2Config_Athena.ps1			
2	failed to run commands: exit status 1			
3	Content downloaded to C:\programdata\infor-automation\EC2ConfigUpdate\EC2Install.zip			
4	4.9.3252.0	i-094a085ds23412	us-east-1	32143523432
5	EC2ConfigVersion:4.9.3252.0			

I thought about some regex select but it does not work for me:
SELECT REGEXP_EXTRACT(ec2athena.ec2athena, ""('^,*')"", 2)"
Amazon Athena	"Re: Athena query with Regex
I have written an workaroung
SELECT *
FROM ec2athena.ec2athena where instanceid like 'i%' and (ec2config like '4%' or ec2config like '3%')"
Amazon Athena	"How to receive JSON in query result?
Hello,
Let's say, I have messages in s3 with following structure:

{ ""msg_type"": ""foo"", ""msg_body"": { ""str_field"": ""hello"", ""int_field"": 1 } }

When perform following query in Athena:

SELECT * FROM foo_table

it always returns numbers in quotes (see ""int_field"": ""1""):

{""int_field"":""1"",""str_field"":""hello""}

If I modify message like this one:

{ ""msg_type"": ""foo2"", ""msg_body"": ""{ \""str_field\"": \""hello\"", \""int_field\"": 1 }"" }

it returns normal JSON object:

{ ""str_field"": ""hello"", ""int_field"": 1 }

And if I use function ""json_extract"" it returns only numerations of records:
select json_extract(msg_body, '$.msg_body') as msg from foo_table
1
2
...

So, my questions are:
1) is it normal behavior?
2) how to perform queries, so I can receive normal JSON objects, because I need to copy-paste it for testing?

P.S. I was looking at https://engineering.skybettingandgaming.com/2015/01/20/parsing-json-in-hive/, but I've got following error: Function get_json_object not registered

Edited by: buniak on Jan 28, 2019 9:18 AM"
Amazon Athena	"HIVE_BAD_DATA in Athena
Hi all!

We are using AWS Glue and Athena to process multiple json files from S3 bucket.

This is the DDL for the table
CREATE EXTERNAL TABLE `product2_20181227`(
  `array` array<struct<attributes:struct<type:string,url:string>,id:string,name:string,productcode:string,description:string,isactive:boolean,createddate:string,createdbyid:string,lastmodifieddate:string,lastmodifiedbyid:string,systemmodstamp:string,family:string,currencyisocode:string,isdeleted:boolean,isarchived:boolean,required_amperage__c:string,article_id__c:string,importance_product__c:string,report_family__c:string,charged_monthly__c:boolean,custom_installation_package__c:boolean,dynamic_load_balacing__c:boolean,extra_crawlspace__c:boolean,ral_product__c:boolean,standard_installation_pack__c:boolean,subscription__c:boolean,extra_installation_costs__c:boolean,lease_package__c:boolean,icu_compact__c:boolean,icu_eve__c:boolean,dismantling__c:boolean,mounting__c:boolean,the_new_motion_chargepoint__c:boolean,name__c:string,product_description__c:string,ethernet__c:boolean,dontshowtheuser__c:boolean,allowedcolors__c:string,customprice__c:boolean,horizon_id__c:string,ral__c:double,lastreferenceddate:string,attached_cable__c:string,suitable_for__c:string,number_of_charging_units__c:string>> COMMENT 'from deserializer')
ROW FORMAT SERDE 
  'org.openx.data.jsonserde.JsonSerDe' 
WITH SERDEPROPERTIES ( 
  'paths'='array') 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  's3://mybucket/dir1/dir2/'
TBLPROPERTIES (
  'CrawlerSchemaDeserializerVersion'='1.0', 
  'CrawlerSchemaSerializerVersion'='1.0', 
  'UPDATED_BY_CRAWLER'='sample-table-stuff-2', 
  'averageRecordSize'='11075', 
  'classification'='json', 
  'compressionType'='none', 
  'objectCount'='199', 
  'recordCount'='199', 
  'sizeKey'='2213342', 
  'typeOfData'='file')


This is the query id - c995fb5b-ae44-4000-9396-a8333c81f904.

The error i get is this one.
HIVE_BAD_DATA: Error parsing field value for field 0: org.openx.data.jsonserde.json.JSONObject cannot be cast to org.openx.data.jsonserde.json.JSONArray

Is there any way i can find what exactly is happening during parsing?"
Amazon Athena	"Re: HIVE_BAD_DATA in Athena
it seems like you have invalid json . Athena does not support below format
https://forums.aws.amazon.com/

Json should be like below format and there should not be any embedded new line .
{""a"":""df""}"
Amazon Athena	"Re: HIVE_BAD_DATA in Athena
Hello,

I see that you are facing ""HIVE_BAD_DATA: Error parsing field value"".

When we look at this error it states that Athena is either incorrectly understanding the data as integer for one of the columns. Which means that Athena is thinking that there is some data type mismatch in one of the data files.  Unfortunately, Athena will not accommodate a query where the data type is not strictly the same in schema and actual data file. Even if any one of the record in the file has incorrect data it will give this error. 

Unfortunayley thre is no direct way to find out which row or line this is erroring on too other than the error message provided above. If you would like help with further investigation please provide the below details by opening a support case with AWS:

1. some sample data files
2. DDL of the table

Have a Great Day"
Amazon Athena	"Cross Account Athena Access returns 403
Hello,

I log on to AWS console using IAM credentials in a Member Account in our AWS Organization. Then, I assume a role in another member account that is our Analytics AWS account. In this account, I am attempting to use Athena to create and run queries on an S3 bucket in a third AWS account that does not belong to us - it belongs to our customer. I am able to create the table just fine. However, all my select queries are failing with a single error Access Denied 403 to one of the files in the customer's S3 bucket. The role in our Analytics account has full permissions to S3, Glue, Athena. The bucket in the customer's AWS Account has public access. 

What permission is missing?

Here is a request id that I captured from Athena if it helps debug. 

Please help!

QS1VQ4vpQ968g7rt5rvh1YQlhBPrDGeOL0D3f3g/xsLgmTI5Rmg4skLfk2j29zxFVlYuIVIYT6Q= 

Thanks,
Sachin

Edited by: sdole on Sep 20, 2018 8:14 PM"
Amazon Athena	"Re: Cross Account Athena Access returns 403
Make sure that the bucket policy is added to give access to your account in a third-party bucket. Refer here:

https://docs.aws.amazon.com/athena/latest/ug/access.html#cross-account-permissions

You can verify it by running aws s3 cp <thirdpartyucket> . 

if you can access, then Athena would be able to access as well. because Athena assumes your credentials to read the objects in Amazon S3.

Edited by: JM-AWS on Jan 2, 2019 11:17 AM"
Amazon Athena	"Re: Cross Account Athena Access returns 403
Hello,

Regarding cross account access in Athena. Yes you can follow the below link to configure the S3 bucket policy accordingly by providing access to the user in the source account.
Link: https://docs.aws.amazon.com/athena/latest/ug/cross-account-permissions.html

If the S3 object are pushed tot he bucket by the destination account or the account other than the one who is accessing then we will also need to update the ACL of the S3 object as the object owner and the bucket owner will be different otherwise. 
Due to this also we can face access denied issue when trying to access a particular object.
Basically we will have to update the ACL of the object and grant access to the account accessing the s3 object.[1]

can you please check if this helps resolve your issue. Or you can definitely contact AWS Support for further investigation.

References:
[1] https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html

Additional references:
https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-owner-access/"
Amazon Athena	"Athena S3 Slow Down Error
Hi - I was running two Athena queries in parallel on a partitioned dataset and one of the queries failed with the following error:

Slow Down (Service: Amazon S3; Status Code: 503; Error Code: 503 Slow Down; Request ID: 51E29BB324E3419A; S3 Extended Request ID: h8zjpGgP/CS3/RGtpO2zgxLojcbgwTBoooitG3dErmWmO44+FlBEGEBRFQ7TNc+y2Z8OtoTE07A=)

It asked that I post the following: Query Id: f104dffe-ce4d-4d10-a8aa-35709703120c.

I imagine this is related to the partition checking, but Athena/S3 really should coordinate better to allow for multiple parallel queries of this nature.

Any suggestions besides throttling to one query at a time?

Thanks!"
Amazon Athena	"Re: Athena S3 Slow Down Error
How many underlying files were there that were being scanned? I can't say I've seen this before but this would be coming from S3 rate limiting, see here - https://docs.aws.amazon.com/AmazonS3/latest/dev/ErrorBestPractices.html#UsingErrorsSlowDown

I believe internally the Presto code (that Athena uses) should be using exponential backoff retries in certain conditions (not sure about that particular one though). I'm wondering if there's just something with the amount of files you have though that would make this more likely to happen. Was the query filtering in the where clause to only hit a particular partition? How many objects/files in S3 is that partition comprised of? There's currently some scalability issues if you have many small files although there's a presentation from ReInvent that says AWS is working to make that better. Also, how many partitions do you have?

Internally, Presto will use S3 List to list all the objects under the matching partition(s). Then from there that will generate splits where the split calls S3 Get. Since Athena internally is massively parallel the rate at which those could happen could be high although I still wouldn't expect this to happen unless your partition has 10k+ files or something large?"
Amazon Athena	"Re: Athena S3 Slow Down Error
Hello,

I see that you have observed a number of slowdown errors when making requests to your S3 buckets.

It seems that you are reaching S3 request rate limits which are 100/s for PUT/LIST/DELETE and 300/s for GETs.

By default, S3 will scale automatically to support very high request rates. When your request rate is scaling, S3 automatically partitions your buckets as needed to support higher request rates. From the start, this should easily accommodate a request level of 100 PUT/LIST/DELETE requests per second or more than 300 GET requests per second without issue, and grow from there. 

However, if S3 is being overwhelmed by a high request volume you would start to see 5XX errors or sometimes 200 as well, requesting you to slow down / try later. Considering the distributed nature of the S3 service these errors are sometimes expected but rarely generated.

When SlowDown responses are seen, you should exponentially backoff and retry appropriately to give S3 enough time to partition your bucket further based on your keyspace design. During this time, errors can still occur but it is best to retry while S3 auto-partitions your bucket based on the requests it's receiving.

Could you please provide me the S3 request IDs and Host IDs of the request for which you received an S3 slowdown error? These request IDs will allow me to do a log dive at my end.

There are two main methods for reducing this type of slowdown error. The second can only be implemented once the first method is in place.

Method 1:
Customers can use a suitable random hash/prefix schema on their buckets. When this is in place, you will see a requested capacity that is far better than the numbers quoted above- in the 1000s of requests per second area. You can find details of recommendations for a suitable schema in our documentation here:

http://docs.aws.amazon.com/AmazonS3/latest/dev/request-rate-perf-considerations.html

Method 2:
If you have a random hash/prefix schema in place and your volume of Requests still exceed what S3 is capable of with this schema in place, we can then submit a request directly to our S3 Engineering Team and have them manually partition the Bucket. This would allow for a TPS of a couple of thousands per second. 

**It must be noted, this can only be achieved on buckets that have a suitable schema in place. Looking at your buckets I can see that the key name schema is in alphabetical order which reduces a possibility of creating a partition request for the S3 bucket resulting in performance issues.

If you find, after putting in place a bucket structure as recommended in our documentation that you are still seeing 5XX errors, please do contact AWS Support so that they can engage the internal team and request a partition of the bucket. Before they can apply this, they would need some details from you when you raise the case with AWS support.
Bucket Region:
Bucket Name:
API Breakup:  GET:______TPS, PUT:______TPS, LIST:______TPS, Multipart PUT (Upload Part):______TPS
Key-space design: bucketname/https://forums.aws.amazon.com/https://forums.aws.amazon.com//<region>/<customer>/<file>.jpg
Algorithm/method used to generate key-space:
Are keys evenly distributed within the key-space:
Will this bucket be used 'Multi-Part' Uploads:
Will this bucket be used for 'Cross Region Replication'? If so, as the source or destination?

Please note that these limits are per partition, so if all the objects in the bucket are in the same partition it will be a limit for the bucket as well."
Amazon Athena	"Cost of Create Table As
Hi,

I am currently using AWS Athena as a data provisioning tool for data science projects. That is, I am using CTAS-statements to store some query results as parquet files and then subsequently process them on an EC2 Instance.

So far, I have calculated the cost of these CTAS statements based on a statement made in the CTAS-announcement: ""CTAS statements are charged based on bytes scanned in the Select phase, similar to how Athena  charges  for Select queries."" (https://forums.aws.amazon.com/ann.jspa?annID=6220). E.g. Athena tells me it scanned 20GB for a query --> (5$ per TB scanned)--> My employer is charged 0.1$.

Now a colleague of mine told me that the actual cost is way higher, as this 5$ per TB only applies to Select statements. His claim is that as soon as you use CTAS, AWS Glue gets involved and we also have to pay for Glue-ETL jobs (for which a cluster would be provisioned).

Is he right and the cost for the example is (much) higher? Or is the 0.1$ the accurate cost?  

Any help is highly appreciated!

Best regards,
RoetGer"
Amazon Athena	"Re: Cost of Create Table As
Your understanding is correct (your coworkers is not). Glue isn't involved in this process at all (short of the Glue metadata catalog to store the new table metadata). You pay the Athena rates for the amount of data scanned by the select. Likely this is because the CTAS is handled all by Athena (using Presto as this is a feature of Presto) but I'm speculating there. Glue ETL is based on Spark afaik/not used in this case. Detailed billing should call this out though even from the ones you tested for instance."
Amazon Athena	"Re: Cost of Create Table As
Thank you very much, rruppmgp!"
Amazon Athena	"Re: Cost of Create Table As
When using the Athena CREATE TABLE AS SELECT (CTAS) feature, you are only charged for data scanned, as described in the Athena detail pages (1), pricing (2), and documentation (3).

Athena does not use Glue ETL Jobs when converting data formats using CTAS. Therefore, there are no automatic charges for Glue ETL as part of running CTAS queries in Athena. In your example, the only charge would be $0.10 for a CTAS query that scanned 20GB.
1 https://aws.amazon.com/about-aws/whats-new/2018/10/athena_ctas_support/ 
2 https://aws.amazon.com/athena/pricing/ 
3 https://docs.aws.amazon.com/athena/latest/ug/ctas.html

Edited by: JM-AWS on Jan 17, 2019 11:29 AM"
Amazon Athena	"Re: Cost of Create Table As
I believe this this answer is not the complete cost accounting.

1. + (tiny) S3 request costs for GET and PUT.
2. + (tiny) S3 carrying cost for the storage of the output table 
3. + (tiny) Glue Data Catalog cost for adding a metadata object (the output table).
4. + (tiny) Glue Data Catalog carrying cost for storing an additional metadata object.
5. + (variable) cross region data transfer cost if source S3 objects and target S3 objects are in different regions.

Did I miss anything else?

-Kurt"
Amazon Athena	"How to inner join two tables from different databases?
Hi everyone,

I am struggling to perform a join query that select information from two tables located in two different databases. 

Below is example query:
SELECT * FROM ""database1"".""table1"" a inner join ""database2"".""table2"" b ON a.field = b.field limit 10;

Can you please help?

I appreciate your time."
Amazon Athena	"Re: How to inner join two tables from different databases?
Hi there. 
What problem do you face at?

I just recently test join two tables from different databases and it works perfectly

Below is my example code which similar to your code:

SELECT * FROM part2test.id_city_test,testtid.id_industry_test
WHERE part2test.id_city_test.nid=testtid.id_industry_test.nid
LIMIT 10

If anything else just ask.
Regards
Thang"
Amazon Athena	"Bug? Grok Serde ignores the name of matches, and relies upon order only
I expected GrokSerDe to use the names of fields I provided, like Grok specifies, and like the GrokDebugger does: https://grokdebug.herokuapp.com/

However, that does not appear to be the case. 
CREATE EXTERNAL TABLE `logs`(
  `prefix` STRING, 
  `timestamp` STRING, 
  `host` STRING, 
  `request_id` STRING,
  `message` STRING)
ROW FORMAT SERDE 
  'com.amazonaws.glue.serde.GrokSerDe' 
WITH SERDEPROPERTIES ( 
  'input.format'='%{DATA:foo}%{TIMESTAMP_ISO8601:bar} host %{DATA:baz} \- %{GREEDYDATA:blah}', 
  'input.grokCustomPatterns'='POSTFIX_QUEUEID [0-9A-F]{7,12}') 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  's3://ynab-loggly/loggly/2019/01/17'
#


Whether I specify existing column names or not, the names of my fields (foo, bar, baz) do not matter when filling in the columns. Instead, Athena respects the order only. This makes it impossible to parse log files in which the order of fields might change from one line to the next, because I can't specify said order."
Amazon Athena	"Re: Bug? Grok Serde ignores the name of matches, and relies upon order only
Athena grok does not check the column names mapping instead , it uses index of the pattern, first pattern to first column ."
Amazon Athena	"Athena console/jdbc connection sporadically throwing parent builder is null
I'm currently trying to integrate our product with Athena, but fairly frequently, the request fails with the following error:

java.sql.SQLException: [Simba][AthenaJDBC](100071) An error has been thrown from the AWS Athena client. GENERIC_INTERNAL_ERROR: parent builder is null


My requests are fairly simple at the moment:
select * from <table> where dt = '<partition>';


Re-requesting will usually work, but that's not an experience we want to have for our end users.

The same error often occurs from the Athena console as well so I know it's not the way I'm creating the connection. 

Any insight on what causes that error and how to get around it?

Added screenshot of error through Athena Console

Edited by: elavigne15 on Jan 18, 2019 11:16 AM"
Amazon Athena	"duplicate with join 2 table
Hi everyone, 
I have a problem when join two table. Both of them have format 2 columns is 'nid'; 'tid'
When I join them I have some unexpected data 

the first table I have 
nid | tid
4    |   0
5    |   5

the second table I have
nid  | tid
4    |  7
5    |  5
6    |  5

The result
nid | tid
0    |  4  ( THIS IS THE ONE UNEXPECTED)
4    |  0
4    | 7
5   |  5
5   |  5
6   |  5

My code is
SELECT * FROM
(SELECT 
  id_city_test.nid,
  id_city_test.tid
FROM 
   id_city_test) 
AS a

UNION

SELECT * FROM
(SELECT 
  id_industry_test.nid ,
  id_industry_test.tid
FROM 
   id_industry_test)
AS b

Can someone help me. 
Thanks a lot
Thang"
Amazon Athena	"Re: duplicate with join 2 table
Magically the problem is solved itself with the previous code this time so I close the topic"
Amazon Athena	"Quicksight support for Median and Standard Deviation
Hi,
We are trying to build a dashboard where we group dataset by given dimensions and trying to calculate median and standard deviation.

Is this supported by AWS Quicksight ?

Thanks"
Amazon Athena	"Re: Quicksight support for Median and Standard Deviation
I have the same question. can someone please reply to this?"
Amazon Athena	"UNION ALL on same table
My expectation is that independent how table C looks like, the following table should always contain the same numbers of 0s and 1s, but it doesnt???

(
		SELECT
		0 AS target
		FROM C
)
UNION ALL
(
		SELECT
		1 AS target
		FROM C
)"
Amazon Athena	"Re: UNION ALL on same table
Just realized that if is is generated as an alias, like

C AS ( ... )

Then if the generation process of C is not 100% restricted, the problem above might occur"
Amazon Athena	"Re: UNION ALL on same table
Do you have data actively writing to the underlying S3 location during query time? Also, keep in mind the results of the two queries won't necessarily be in order e.g. the 1's could appear before the 0's. This has been a ""gotcha"" for people that are used to single threaded query engines where the order usually is maintained as written in the SQL (but that's not part of the SQL spec without using an explicit order by)."
Amazon Athena	"Athena usability in console with large number of tables
The Athena web UI is rather unresponsive when there are a large number of tables in a database.  We have over 5600 tables (not sure of the exact number, the console only seems to go up to 5600).  When performing any sort of action in the Athena console, the entire console seems to freeze while it appears to be refreshing the table list on the left (even when not doing anything that would change the table list).  It would be great if this could be improved."
Amazon Athena	"Inefficient group by on partition field
Summary: ""SELECT pr FROM mytable GROUP BY pr;"" should be super-fast when partitioned by pr, but it is super-slow.

I was/am getting query timeouts on a group by query on .csv.gz files totalling 55GB,.

So I decided to partition my data by putting each .csv.gz file into a sub-folders: /pr=a/, /pr=b/, etc. and added the `pr` field to my group by clause. This should have parallelized the query over the partitions, with no read or other dependencies in between partitions.

Unfortunately that did not help and I reduced the problem to the following ultra-simple case that shows that Athena is ignoring a trivially available partitioning optimization in its group by.

Here's the ultra-simple query: ""SELECT pr FROM mytable GROUP BY pr;"" where my data is in multiple files, each in ""/pr=a/"", ""/pr=a/"" ... folders. This query should be executable without reading any data at all, and merely knowing the list of partitions. (There is no aggregation or anything else happening.)

Yet, on issuing the above query Athena proceeds to start reading GB after GB of data and eventually times out.

Given the 30-minute timeout, this suboptimality is a show-stopper for the initial processing of raw (compressed) data.

I wish this could be fixed speedily as it is truly low-hanging fruit that would benefit a lot of customers.

Edited by: developer1 on Dec 16, 2018 12:25 AM"
Amazon Athena	"Re: Inefficient group by on partition field
Parition is to filter the data before reading , if you are not using in where condition then you will hit performance 

Example, 
 pr=a,pr=b , if you have where pr=a and table is partition with pr then athena reads from pr=a.  check below link for how group by and other operator work in athena. if you donot have many unique value in group by column then it likely processed by one worker although athena is distributed system (called data skew). you can request for limit increase as it can be increased . speed depends on many factor, not only size of the data but also, content , operator used in  the query

https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/"
Amazon Athena	"Re: Inefficient group by on partition field
Note: Athena does not list the partition and group by (it read the content and group the partition column)"
Amazon Athena	"java.sql.SQLException: [Simba][AthenaJDBC](100122)
Reference:
https://s3.amazonaws.com/athena-downloads/drivers/JDBC/SimbaAthenaJDBC_2.0.6/docs/Simba+Athena+JDBC+Driver+Install+and+Configuration+Guide.pdf

Version
com.amazonaws:aws-java-sdk-athena:1.11.461
com.amazonaws.athena.jdbc:AthenaJDBC42:2.0.6

Code (statement.executeQuery throws error)

public String submitAthenaQuery(String queryString) {
 
     Statement statement = null;
     ResultSet resultSet = null;
     Connection connection = null;
     StringBuilder results = new StringBuilder("""");
     try {
         System.out.println(""Connecting to Athena..."");
         connection = AthenaConnectionFactory.getAthenaConnection();
         statement = connection.createStatement();
         resultSet = statement.executeQuery(""SELECT * FROM db_name.table_name limit 10"");
 
//            while(resultSet.next()) {
//                String name = resultSet.getString(""tab_name"");
//                //Display values.
//                System.out.println(""Name: "" + name);
//            }
     } catch (Exception e) {
         e.printStackTrace();
     } finally {
         DbUtils.closeQuietly(resultSet);
         DbUtils.closeQuietly(statement);
         DbUtils.closeQuietly(connection);
     }
     return results.toString();
 }


Stacktrace

2018-12-18 16:59:26.961java.sql.SQLException: [Simba][AthenaJDBC](100122) An error has occurred. Details: Exception during column initialization: com.simba.athena.support.exceptions.GeneralException: AWS_CLIENT_ERR.
	at com.simba.athena.athena.dataengine.AJStreamResultSet.<init>(Unknown Source)
	at com.simba.athena.athena.dataengine.AJQueryExecutor.execute(Unknown Source)
 	at com.simba.athena.jdbc.common.SStatement.executeNoParams(Unknown Source)
DEBUG	at com.simba.athena.jdbc.common.SStatement.executeNoParams(Unknown Source)
 51107	at com.simba.athena.jdbc.common.SStatement.executeQuery(Unknown Source)
 ---	at com.trp.makara.service.AthenaServiceImpl.submitAthenaQuery(AthenaServiceImpl.java:53)
 [  XNIO-2 task-1]	at com.trp.makara.service.AthenaServiceImpl$$FastClassBySpringCGLIB$$7aec7e2d.invoke(<generated>)
 	at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
c.trp.makara.aop.logging.LoggingAspect   	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:746)
:	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
 Exit: com.trp.makara.service.AthenaServiceImpl.submitAthenaQuery() with result = 
	at org.springframework.aop.aspectj.MethodInvocationProceedingJoinPoint.proceed(MethodInvocationProceedingJoinPoint.java:88)
	at com.trp.makara.aop.logging.LoggingAspect.logAround(LoggingAspect.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:644)
	at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethod(AbstractAspectJAdvice.java:633)
	at org.springframework.aop.aspectj.AspectJAroundAdvice.invoke(AspectJAroundAdvice.java:70)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
	at org.springframework.aop.aspectj.AspectJAfterThrowingAdvice.invoke(AspectJAfterThrowingAdvice.java:62)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
	at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:688)
	at com.trp.makara.service.AthenaServiceImpl$$EnhancerBySpringCGLIB$$9a7fbfae.submitAthenaQuery(<generated>)
	at com.trp.makara.rest.AthenaResource.submitAthenaQuery(AthenaResource.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:209)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:136)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:102)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:891)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:797)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:991)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:925)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:974)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:866)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:851)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:74)
	at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:129)
	at com.codahale.metrics.servlet.AbstractInstrumentedFilter.doFilter(AbstractInstrumentedFilter.java:111)
	at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
	at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
	at org.springframework.web.filter.CorsFilter.doFilterInternal(CorsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
	at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
	at org.springframework.boot.actuate.web.trace.servlet.HttpTraceFilter.doFilterInternal(HttpTraceFilter.java:90)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
	at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:320)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:119)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
	at com.trp.makara.security.jwt.JWTFilter.doFilter(JWTFilter.java:38)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
	at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
	at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:66)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
	at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:215)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:178)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:357)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:270)
	at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
	at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
	at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:109)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
	at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
	at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.filterAndRecordMetrics(WebMvcMetricsFilter.java:155)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.filterAndRecordMetrics(WebMvcMetricsFilter.java:123)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:108)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
	at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
	at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
	at io.undertow.servlet.handlers.FilterHandler.handleRequest(FilterHandler.java:84)
	at io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62)
	at io.undertow.servlet.handlers.ServletChain$1.handleRequest(ServletChain.java:64)
	at io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36)
	at io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:132)
	at io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57)
	at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
	at io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46)
	at io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64)
	at io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60)
	at io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77)
	at io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43)
	at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
	at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
	at io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:292)
	at io.undertow.servlet.handlers.ServletInitialHandler.access$100(ServletInitialHandler.java:81)
	at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:138)
	at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:135)
	at io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:48)
	at io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43)
	at io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:272)
	at io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:81)
	at io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:104)
	at io.undertow.server.Connectors.executeRootHandler(Connectors.java:336)
	at io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:830)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Caused by: com.simba.athena.support.exceptions.GeneralException: [Simba][AthenaJDBC](100122) An error has occurred. Details: Exception during column initialization: com.simba.athena.support.exceptions.GeneralException: AWS_CLIENT_ERR.
	... 138 more


Edited by: himalay on Dec 19, 2018 7:52 AM"
Amazon Athena	"Re: java.sql.SQLException: [Simba][AthenaJDBC](100122)
It looks like jar conflict so either use JDBC or sdk but not both."
Amazon Athena	"Athena unable to read KMS encrypted parquet document
When running a basic 'preview' query against a parquet formatted athena table, i'm receiving the following error:
HIVE_CANNOT_OPEN_SPLIT: Error opening Hive split s3://__s3_bucket_parth/file.parquet (offset=0, length=7329): can not read class parquet.format.FileMetaData: Required field 'version' was not found in serialized data! Struct: FileMetaData(version:0, schema:null, num_rows:0, row_groups:null)

Query Id: 74ba9fef-a7a8-4f53-adca-a70fa9acbc0d

I've been able to run successful queries against the same file in an unecrypted format. When switching over to S3 SSE with KMS and updating the DDL with 'has_encrypted_data'='true', I receive the above error. 

I've also tried tweaking the s3 meta tags on upload such as 'unencrypted-content-length' but have not had success with that either.

Edited by: JustinSoliz on Jan 1, 2019 5:04 PM

Edited by: JustinSoliz on Jan 1, 2019 5:49 PM"
Amazon Athena	"Re: Athena unable to read KMS encrypted parquet document
Hi Justin,
A few suggestions:
1. For KMS encrypted data, you do not need to indicate to Athena that a dataset is encrypted in Amazon S3, that is, the statement  'has_encrypted_data'='true' is not needed in your case.
2. Please see this documentation:
   - Permissions to Encrypted Data in Amazon S3  https://docs.aws.amazon.com/athena/latest/ug/encryption.html#permissions-for-encrypting-and-decrypting-data 
   - Permissions to Encrypted Metadata in AWS Glue Data Catalog  https://docs.aws.amazon.com/athena/latest/ug/encryption.html#glue-encryption 

Let us know if this works!"
Amazon Athena	"Re: Athena unable to read KMS encrypted parquet document
Thanks for the reply. I've removed the 'isEncrypted' flag on table creation but still see an identical error.
Query Id: cc6595c7-4494-4cda-b899-35d7abaf66a6"
Amazon Athena	"Re: Athena unable to read KMS encrypted parquet document
Did you verify the permissions, as indicated in the docs? The error might be related to the fact that you don't have permissions for your Athena user or role either to the AWS Glue metadata or to the tables themselves."
Amazon Athena	"Re: Athena unable to read KMS encrypted parquet document
Per the documentation section that appears relevant to this issue, it seems like it's only necessary to provision iam perms to the user executing the query which in this case is setup. 
AWS KMS. If you use AWS KMS for encryption, Athena users must be allowed to perform particular AWS KMS actions in addition to Athena and Amazon S3 permissions. You allow these actions by editing the key policy for the AWS KMS customer managed keys (CMKs) that are used to encrypt data in Amazon S3. The easiest way to do this is to use the IAM console to add key users to the appropriate AWS KMS key policies. 


We are not using Glue for any part of this so have not setup any iam perms specific to that. In this case, are BOTH iam user and glue policies required to allow athena queries to run against encrypted s3, even in the case where we are not using glue?"
Amazon Athena	"Re: Athena unable to read KMS encrypted parquet document
Athena uses the Glue Data Catalog. Please see this topic regarding permissions to encrypted data in Glue Data Catalog:
https://docs.aws.amazon.com/athena/latest/ug/access-encrypted-data-glue-data-catalog.html"
Amazon Athena	"Re: Athena unable to read KMS encrypted parquet document
Thanks for the reply. Regarding the iam policy in the link you've provided, it's unclear to me which IAM role or service role to attach the policy to? 
The IAM user executing the athena queries already has access to the perms for 'kms:Decrypt' etc."
Amazon Athena	"Preview of results before main result set?
I am attempting to use Athena as the data source for a BI application my company is developing but I am hitting a problem.  I have the ODBC drivers correctly installed, and for small datasets everything works fine and I can import data. I'm hitting issues with larger datasets due to the way Athena provides results.

Basically the way the application ingests data from SQL is

1. You provide a SQL statement that shapes the dataset you want to read in
2. That query is run, and the first page of results that comes back (much like the first 500 that comes back in an IDE) is used to identify field names / types for the data ingestion.
3. The whole query is run to ingest the data.

RDBMS like PostGres (or Aurora MySQL which I've also used for this) happily give back that first page of results before running the entire query if you choose to export.  Athena wants to run the whole query (and I'm talking about a 100GB result set coming back) before giving any data, and my application times out waiting.

Is there a way to have Athena replicate the sort of behaviour seen when querying a SQL database from a SQL client/IDE?  I know the underlying tech is quite different, so I'm not sure this is possible which means if I want to use Athena I need by dev folks to change the app behaviour to first request a LIMIT 500 dataset and then the whole thing?

I have tried changing the UseResultsetStreaming and RowsToFetchPerBlock options, but it doesn't change the behaviour. Any thoughts much appreciated.

Martin"
Amazon Athena	"Re: Preview of results before main result set?
I think generally this can work for Postgres and MySQL because the query is single threaded, so the query executes, you get back a cursor, then the client has a fetch size and it fetches X rows which forces X rows to get computed by the database and so on (I think it does this lazily then based on the client actually fetching). Athena/Presto on the other hand the queries are distributed/parallelized so all the worker nodes are computing their own subsection of the query in parallel (called splits in Presto). Therefore, that kind of control isn't really possible.

If you own the client in this app it would probably be best to do as you mentioned where it issues the query twice, once with limit 500 and the second time without a limit. As a side note have you tested bringing back that much data/does it meet your performance expectations - sounds like you're saying you're ingesting 100GB if the limit wasn't applied for instance? The Presto client protocol isn't really designed to pull back large data sets (to the client), here's an issue discussing this for instance - https://github.com/prestodb/presto/issues/11908 . Usually the suggestion is to use CREATE TABLE AS SELECT with Presto/Athena will then be able to do the write portion of the results on worker nodes. Streaming results back to the client forces Presto to send all that data through the single master node - Athena may be doing something differently here though, I know writing all the results to S3 for instance is an Athena specific thing rather than Presto. It depends what you're ingesting data into though (if CREATE TABLE AS SELECT is an option)."
Amazon Athena	"Re: Preview of results before main result set?
This is super useful information thanks.  I have come to a similar conclusion that Athena really isn't intended for large data extracts like I'm trying to do, but I'm limited in that the application I'm reading into currently only supports ODBC reads from a database (or Excel, which clearly won't work for 100GB  ).  I'll probably try and load the data into RDS as a next step, that should work even if it's potentially slower than running queries against Athena."
Amazon Athena	"Re: Preview of results before main result set?
The recommendation is always to limit the results, to improve performance. Please see these references:
1. https://docs.aws.amazon.com/athena/latest/ug/ctas-examples.html 

2.  https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/  > the Bonus Tip: Only include the columns you need. Quoting from it:

""When running your queries, limit the final SELECT statement to only the columns that you need instead of selecting all columns. Trimming the number of columns reduces the amount of data that needs to be processed through the entire query execution pipeline. This especially helps when you are querying tables that have large numbers of columns that are string-based, and when you perform multiple joins or aggregations.

As for the ODBC parameters that you mention, they affect whether the results are streamed or sent in pages, but do not affect WHICH results are returned. HTH!

Edited by: JM-AWS on Jan 10, 2019 7:28 PM"
Amazon Athena	"Athena unable to read CSE-KMS
Having trouble getting Athena to read a csv that was encrypted client side, with KMS managed keys.
I used aws-encryption-cli to encrypt the data, and s3 cp to copy it to the bucket. I set has_encrypted_data = true....but the athena query still produces encrypted data. Any ideas what I may be doing wrong"
Amazon Athena	"Re: Athena unable to read CSE-KMS
Please see if your permissions are set correctly and in general, check out this doc: https://docs.aws.amazon.com/athena/latest/ug/encryption.html#permissions-for-encrypting-and-decrypting-data"
Amazon Athena	"Using the manifest file to create external table for querying S3 Inventory
I am trying to use a manifest file (as described in https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_EXTERNAL_TABLE.html) to create an Athena table.  My query looks like this:

CREATE EXTERNAL TABLE IF NOT EXISTS testtable 
                     (col1 string, 
                     col2 string ) 
     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' 
     WITH SERDEPROPERTIES ('separatorChar'='|') 
     LOCATION 's3://mybucket/manifest_event/' 
     TBLPROPERTIES ('skip.header.line.count'='1');

""manifest_event"" is a manifest file with the appropriate JSON.

I get this error message on the Athena console. Any ideas?

FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Got exception: java.io.IOException Can't make directory for path 's3://kris2test/manifest_event' since it is a file.)

Edited by: r00lingfool on Jan 4, 2019 7:41 AM

Edited by: JM-AWS on Jan 10, 2019 1:02 PM"
Amazon Athena	"Re: using manifest file to create external table
Note the link you have there is for Redshift Spectrum, which while similar from an external standpoint is not the same stack as Athena (which is based on Presto). So there may be somethings that work in one and not the other. However, I have used manifests in Athena but I'm not sure CSV in particular here's what I have for Parquet files driven by a manifest:

CREATE EXTERNAL TABLE <table>
(<columns>)
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat' 
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
LOCATION s3://mybucket/manifest_folder

Where manifest_folder then has a symlink.txt file in it listing out all the file URIs. I think the location can specify the full path to the manifest file but I haven't tried that.

Side note but I've seen performance issues in Athena when using manifest files especially as the number of files increase. I think this code path isn't really optimized in Athena - I was trying to debug it in Presto to figure out what it spends time on. One thing I noticed when debugging it in Presto directly was that for each file in the manifest it has to call S3 get metadata on the file. So, usually Presto looks like this during split planning:

Call S3 List x number of partitions (or just once if no partitions are used)

With a manifest file it looks like this:

Call S3 Get on manifest file
For each file in the manifest call S3 get metadata

So the planning phase time sort of blows up because of this (I think, didn't get to revisit this more yet). The weird thing though is when using EMR Presto I haven't really noticed the same performance overhead.

Anyway, just a suggestion to test out the performance of using a manifest vs not. I've used a manifest basically when I wanted to control eventual consistency issues e.g. there were some workloads where I needed to delete a bunch of the files under a table in S3 and on occasion this can result in Athena failing on subsequent queries because without a manifest Athena/Presto is using S3 List to figure out what files are involved. S3 List is eventually consistent so it can return files that have since been deleted so S3 List will happen but then S3 Get will fail trying to read the file that has been deleted (these are new UUID based keys so S3 Get is consistent). You'll receive an error indicating ""HIVE_CANNOT_OPEN_SPLIT"" at that point. The manifest works around this because only S3 Get is involved but to make it consistent after every right you need to generate a new manifest file (e.g. new UUID based key or something) and then update the Hive/Glue metadata via ALTER TABLE SET LOCATION to point to the new manifest. There's actually an interesting project from Netflix called Iceberg that changes how table metadata is stored so that S3 List doesn't have to be used - https://github.com/apache/incubator-iceberg . The approach is similar to manifest in a way but has a host of other features. There's a pull request out there to get this included in Presto so hopefully someday it will come to Athena!

If your workload is append only without many deletes then I wouldn't use a manifest for now (just specify the location as the root folder and let Athena use S3 List to discover the files)."
Amazon Athena	"Re: using manifest file to create external table
Thank you so much for this reply. You have provided a great deal of useful information. Can you share the structure of your symlink.txt file? 

I will keep searching for whether a manifest feature is supported with Athena or not, and if not, when it might be incorporated."
Amazon Athena	"Re: using manifest file to create external table
The symlink.txt file is just a list of the object URIs separated by new line so:

s3://<bucket_name>/<path>/<to>/<object>

e.g.:

s3://mybucket/my_table/file_1.parquet
s3://mybucket/my_table/file_2.parquet

If you want to see a working example S3 Inventory uses a manifest file when you have it set to record information, see here - https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-inventory.html

Specifically this section - https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-inventory.html#storage-inventory-athena-query"
Amazon Athena	"Re: using manifest file to create external table
Again, thank you very much for the guidance. We don't know for sure that the Redshift and Athena implementations are different, but both of us suspect that to be the case. For now I have enough information to move forward, so I'm going to mark this as answered. Again, thank you for your detailed insight!"
Amazon Athena	"Re: using manifest file to create external table
Adding tags to this thread, to help locate it."
Amazon Athena	"Cannot access Athena table from Tableau, PyCharm cannot see table columns
I have two seemingly unrelated issues that I suspect have the same root cause.

So I have a number of Athena tables that I have created - these have been created through CloudFormation / AWS Glue to create the table schemas etc.  The underlying format of the tables varies, some tables are csv, some are compressed csv and some are Parquet.  All of these tables are usable in queries through the Athena console, and I am also able to run queries from within PyCharm using the Athena JDBC drivers. 

My two problems are
1. PyCharm shows me the tables, but when I try and look at the columns in each table I get an error 
AWS Athena
			failed to retrieve columns for mdb_transactions.
https://forums.aws.amazon.com/ https://forums.aws.amazon.com/https://forums.aws.amazon.com/(11380) Null pointer exception.
2. If I connect to Athena using Tableau, then I can run queries if I specify the SQL, but if I try and drag in a table I get the error
com.tableausoftware.jdbc.TableauJDBCException: Error in readMetadata(AwsDataCatalog, 2mdb_transactions, 3mdb_transactions) https://forums.aws.amazon.com/https://forums.aws.amazon.com/(11380) Null pointer exception.
There was a Java error.
The table ""<Table Name>"" does not exist.

However - I also have some tables that I have created using a CREATE TABLE AS SELECT from within Athena. All of these work perfectly with Tableau and PyCharm. This leads me to think there must be some sort of setting I have missed when creating my original tables from my S3 data. But I don't know what this might be? From looking at the console it seems like there are no missing settings when I compare the original tables to the created ones.

Are there any obvious gotchas I should be looking for?"
Amazon Athena	"Using CTAS to make a Redshift Spetrum Compatible Table
I am trying to create a table using CTAS and then query that table in Redshift Spectrum.

If I do a simple CREATE TABLE blah2 AS SELECT * FROM blah;

And then try to query it in Spectrum, I get this error:

https://forums.aws.amazon.com/(500310) Invalid operation: ""blah2"" is an external view. External views are currently not supported. ;

If I go in to Glue and manually create a table pointing at the output of the CTAS, it works.  Even if I edit the generated definition from the CTAS to look just like the manually created definition, I still get the external view error.

Is there any way to CTAS in Athena and immediately query it in Redshift?"
Amazon Athena	"Re: Using CTAS to make a Redshift Spetrum Compatible Table
Hi Sona,
All tables created with CTAS are created as external. This is evident by the fact that you either specify an ‘external_location’, or rely on Athena to populate a default ‘external_location’. It is not clear, however, why RedShift cannot query those tables."
Amazon Athena	"Re: Using CTAS to make a Redshift Spetrum Compatible Table
Thanks for getting back to me.

Yes, they are external, and that is fine, but they connect queried from Redshift Spectrum because it thinks they are an external view (they are not).  This can be easily reproduced by running a CTAS in Athena and trying to query the table from Redshift.

Is this a bug?

Edited by: Sona K. on Jan 3, 2019 2:33 PM"
Amazon Athena	"Re: Using CTAS to make a Redshift Spetrum Compatible Table
I suggest contacting support to verify this. Thanks for letting us know."
Amazon Athena	"Question with first_value and last_value
Hello AWS,

I'm trying to diagnose a problem with a few queries we have, I'm trying to to make sure we are using first_value and last_value correctly.  We could just be doing this wrong, however:

Here's an example query: 

SELECT last_value(id) OVER 
(
  PARTITION BY name
  ORDER BY id
) FROM (
    VALUES
        (1, 'a'),
        (2, 'a'),
        (3, 'a')
) AS t (id, name)

I'd expect this to return 3, but it returns all values: 

Results
 	_col0
1	1
2	2
3	3

Can you guys either confirm this is an issue or let us know what we are doing wrong?  Thanks!"
Amazon Athena	"Re: Question with first_value and last_value
last_value() does not look beyond the current window.The default framing clause is RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
 which restricts the window to the current row.

You need to either use first_value() :
SELECT name,first_value(id) 
over(partition by name order by id desc) as lv
FROM (
VALUES
(1, 'a'),
(2, 'a'),
(3, 'a'),
 (4,'b')
) AS t (id, name)


Or change the framing clause of the last_value() function to between unbounded preceding and unbounded following


SELECT name,last_value(id) 
over(partition by name order by id rows between unbounded preceding and unbounded following) as lv
FROM (
VALUES
(1, 'a'),
(2, 'a'),
(3, 'a'),
 (4,'b')
) AS t (id, name)


Both of the above queries will return the desired resultset:
 	name	lv
	a	3
	a	3
	a	3
	b	4

More info : https://www.postgresql.org/docs/9.1/sql-expressions.html#SYNTAX-WINDOW-FUNCTIONS"
Amazon Athena	"Re: Question with first_value and last_value
As of 1/3/2018, Athena supports aggregate and window functions from Presto version 0.172. If we look in their documentation: https://prestodb.io/docs/0.172/functions/window.html, we see these definitions:

first_value(x) → https://forums.aws.amazon.com/    Returns the first value of the window frame.
last_value(x) → https://forums.aws.amazon.com/   Returns the last value of the window frame.

Where the ""window frame"" is defined as follows:
The window frame is a sliding window of rows to be processed by the function for a given row. If the frame is not specified, the window frame defaults to RANGE UNBOUNDED PRECEDING. This is the same as RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW. This frame contains all rows from the start of the partition up to the last peer of the current row.
Hope this helps, together with the examples provided in the previous reply."
Amazon Athena	"Alter table to add columns
Hi,

Does Athena support adding additional columns to existing table(without dropping and recreating table)?"
Amazon Athena	"Re: Alter table to add columns
Amazon Athena explicitly does not support ""ALTER TABLE ADD COLUMN"" . https://docs.aws.amazon.com/athena/latest/ug/unsupported-ddl.html 

However I believe that columns can be added by using the Glue API
1) https://docs.aws.amazon.com/glue/latest/webapi/API_GetTable.html
2) altering Table.StorageDescriptor.Columns in the returned table description and then passing that to
3) https://docs.aws.amazon.com/glue/latest/webapi/API_UpdateTable.html 

My inability to find anyone on GitHub that has actually done this leaves me very concerned regarding this approach however.  I came here looking to firm up my understanding, and instead found this unanswered question."
Amazon Athena	"Re: Alter table to add columns
Hi, 
ALTER TABLE ADD COLUMNS is not supported, you are correct. It is listed here as a limitation: https://docs.aws.amazon.com/athena/latest/ug/unsupported-ddl.html
Regarding adding columns, you can do this in some cases. Please see this documentation:

1. Adding Columns at the Beginning or Middle of the Table: https://docs.aws.amazon.com/athena/latest/ug/types-of-updates.html#updates-add-columns-beginning-middle-of-table

2. Adding Columns at the End of the Table: https://docs.aws.amazon.com/athena/latest/ug/types-of-updates.html#updates-add-columns-end-of-table

Hope this helps!

Edited by: JM-AWS on Jan 3, 2019 2:00 PM

Edited by: JM-AWS on Jan 3, 2019 2:01 PM"
Amazon Athena	"Athena with firehose and glue HIVE_METASTORE_ERROR
Hi,

I have created a delivery stream on firehose which saves data on S3 in a parquet format. I have also created a glue database and table linked to the delivery stream.

The whole environment has been created by using a CloudFormation template and the SDK for actually defining the ""ExtendedS3DestinationUpdate > DataFormatConversionConfiguration"" configuration since it doesn't seem to be supported just yet by CloudFormation.

When I go into the web console in Athena for executing a query I can only see the database but not the table in the list. Anyways when I try to execute a simple ""SELECT *"" query I have the following error:

HIVE_METASTORE_ERROR: com.facebook.presto.spi.PrestoException: Required Table SerDe information is not populated. (Service: null; Status Code: 0; Error Code: null; Request ID: null)

This query ran against the ""stg_kinesis_firehose_requests_db_parquet"" database, unless qualified by the query. Please post the error message on our forum or contact customer support with Query Id: 50d88ce1-a109-4322-bb1f-de86540008f7."
Amazon Athena	"Re: Athena with firehose and glue HIVE_METASTORE_ERROR
Please check that you have the appropriate permissions for your user/role in Athena: https://docs.aws.amazon.com/athena/latest/ug/access.html"
Amazon Athena	"Multiply the rows with dates
Hi everyone. I'm new with aws athena. I have some trouble when using it. Is there any way to I can get my expected output from this input?

INPUT (csv file has following input)
DATE (yyyymmdd) | ip | FINISHED ( 1:finished; 0: not finish)
2017-01-01 | 111 | 1
2017-01-01 | 222 | 1
2017-01-02 | 333 | 1
2017-01-03 | 444 | 1

EXPECTED OUTPUT
DATE (yyyymmdd) | ip | FINISHED ( 1:finished; 0: not finish)
2017-01-01 | 111 | 1
2017-01-01 | 222 | 1
2017-01-01 | 333 | 0
2017-01-01 | 444 | 0
2017-01-02 | 111 | 1
2017-01-02 | 222 | 1
2017-01-02 | 333 | 1
2017-01-02 | 444 | 0
2017-01-03 | 111 | 1
2017-01-03 | 222 | 1
2017-01-03 | 333 | 1
2017-01-03 | 444 | 1
Can someone show me the way to do it?
Your help is very much appreciated.
Regards, 

Thang

Edited by: phithang711 on Nov 20, 2018 1:58 PM"
Amazon Athena	"Re: Multiply the rows with dates
Hello Thang,
Could you please explain in more detail what type of query you are trying to perform?
Have you been able to create a table in Athena?
 Thank you for the details."
Amazon Athena	"Re: Multiply the rows with dates
Hi JM-AWS,
I knew the other way to perform my data in order to use easily. So that right now I don't need to duplicate it anymore. By the way, thanks a lot for your reply"
Amazon Athena	"CTAS create/delete and drop partition
Hi,

I have created table using CTAS as follows:-

CREATE TABLE   mdms_poc.T1d
WITH (
     format = 'TEXTFILE', 
     external_location = 's3://hadoop-datalake-user-uploads/mdms-poc-mdms-raw/mdms/TARGET/T1d',
     partitioned_by = ARRAY 
	 ) 
AS 
SELECT 
	row_number() over (order by dim_vehicle_incident_sk) + M.M SK2,
	*,
	dim_vehicle_incident_sk  wibble
FROM 
	mdms_poc.TARGET_FACT_VEHICLE_INCIDENT  I,
	(
		select 
			max(dim_vehicle_incident_sk) M 
		from 
			mdms_poc.TARGET_FACT_VEHICLE_INCIDENT
	) M
LIMIT 10;

Which works fine, however when I delete the table the S3 folder still exists. is there anyway of removing the S3 bucket during the delete?

Also I tried to remove the partition by calling:-

ALTER TABLE T1d DROP PARTITION (wibble = 0); and I get an error as shown below

Your query has the following error(s):

FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Table name cannot be null

This query ran against the ""mdms_poc"" database, unless qualified by the query. Please post the error message on our forum or contact customer support with Query Id: dfb5470b-6224-499f-b87b-edc71dc25bf9.

Any help very much appreciated

Thanks

Pete Gadsby"
Amazon Athena	"Re: CTAS create/delete and drop partition
Hello,

I see that you have created a table using CTAS feature and you need to know of we can delete the S3 folder when we delete the table.

Athena is a server-less service that runs queries on the data referenced in a particular Amazon S3 location. When Athena deletes the table, it deletes the metadata (the schema) from the Catalog and does not delete the actual content referenced in an Amazon S3 bucket. The table will not be visible under the AWS Glue console as well because Athena uses AWS Glue Data Catalog.

To delete a particular S3 folder/bucket, use these AWS CLI commands:
https://docs.aws.amazon.com/AmazonS3/latest/dev/delete-or-empty-bucket.html

Regarding the next statement that you have triggered to drop the partition and you are facing the below error:
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Table name cannot be null

Checking on the link below for the DROP PARTITION statement, the syntax that you are using seems good but the CTAS statement includes the partition_by attribute as ARRAY which is empty. Can you please check on the below link having the example to create a CTAS with partition: (please try to include the partition column in the CTAS statement and try again)
CREATE TABLE ctas_csv_partitioned WITH ( format = 'TEXTFILE', external_location = 's3://my_athena_results/ctas_csv_partitioned/', partitioned_by = ARRAY) AS SELECT name1, address1, comment1, key1 FROM tables1;

Link: https://docs.aws.amazon.com/athena/latest/ug/ctas-examples.html

If you are still facing the same issue you can definitely open a support case with the AWS Support team.

I hope above information would be helpful.

Thanks 

Edited by: JM-AWS on Jan 2, 2019 11:09 AM"
Amazon Athena	"Can't drop Athena table
Hello,
I have a table in AWS Athena called: `qos-row-id`

I have tried to delete the table I have created. 

DROP TABLE qos-row-id;

I recevive the following message every time I try to.

line 1:15: mismatched input '-' expecting {<eof>, '.'} (service: amazonathena; status code: 400; error code: invalidrequestexception; request id: acd8a81c-8d3c-4add-9308-303980629029)

Edited by: nickanme on Dec 7, 2018 3:22 AM"
Amazon Athena	"Re: Can't drop Athena table
Hi, 

I understand that you are getting an error while trying to drop a table in Athena when the table name has '-' special character. 
Please use the table name between backtics in the DROP command :
DROP TABLE `qos-row-id`; 

I hope it helps. If you still have any concerns, please let us know.

Related Links

[1] https://docs.aws.amazon.com/athena/latest/ug/tables-databases-columns-names.html"
Amazon Athena	"Can Athena read arrayed struct type from parquet?
Hello,

I got following error when run a simple SELECT query.

Your query has the following error(s):
 
HIVE_CANNOT_OPEN_SPLIT: Error opening Hive split s3://*PARQUET_PATH*/2017071312.parquet (offset=0, length=5439688): Expected LIST column 'error_column' to only have one field, but has 2 fields
 
This query ran against the ""DATABASE"" database, unless qualified by the query. Please post the error message on our forum or contact customer support with Query Id: 53a345cb-6705-4318-b6a2-38e0d4376baa.


The error_column column is ARRAY type which element type is struct.
The table was created by following query.
CREATE EXTERNAL TABLE IF NOT EXISTS DATABASE.testtable (
        `test1` STRING,
        `error_column` ARRAY < STRUCT< `item1`: STRING, `item2`: STRING > >,
        `test2` STRING
) 
 
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
LOCATION 's3://PARQUET_PATH'
TBLPROPERTIES ('has_encrypted_data'='false')


When I check a schema of this parquet file by Spark, it looks like
 |-- error_coulmn: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- item1: string (nullable = true)
 |    |    |-- item2: string (nullable = true)

It looks like Athena cannot read arrayed struct.. or I made it wrong.
How can I read these parquet files?"
Amazon Athena	"Re: Can Athena read arrayed struct type from parquet?
Having the exact issue.
Was the issue ever addressed?"
Amazon Athena	"SQL-99 Support
Does anyone know when Athena will support SQL-99?

Specifically, I would like to know when Athena will support recursively defined ""Common Table Expressions"" like this one:

WITH temp (n, fact) AS 
(SELECT 0, 1 -- Initial Subquery
  UNION ALL 
 SELECT n+1, (n+1)*fact FROM temp -- Recursive Subquery 
        WHERE n < 9)
SELECT * FROM temp;

Thanks."
Amazon Athena	"Re: SQL-99 Support
IMO, since Athena depends on Presto for SQL level compliance, this is more a question for the Presto community.  I suggest that instead you may want to post your question here: https://groups.google.com/forum/#!forum/presto-users

-Kurt"
Amazon Athena	"Re: SQL-99 Support
Thanks!

It took me a while but I finally got around to acting upon your suggestion.

Thanks again for your help with this.

Gord"
Amazon Athena	"Is snappy compression supported for CSV files?
I'm getting mixed signals from the docs as to whether snappy compression is supported for CSV files. I feel like some combination of settings on the Glue table have gotten this to work in the past for me, but I really can't figure it out. Or maybe I'm crazy and it never worked...

I can get queries to work fine on gzip and uncompressed CSV files, anyone have tips on how to configure queries on *.csv.snappy files?"
Amazon Athena	"Re: Is snappy compression supported for CSV files?
Hello Marty,
You wrote: I can get queries to work fine on gzip and uncompressed CSV files, anyone have tips on how to configure queries on *.csv.snappy files?

It is s not possible to Snappy-compress text files. You can further specify different compression only for Parquet and O. Text files are recognized as GZIP-compressed only if they have a .gzip extension.  You cannot use any syntax in Athena to tell it to compress a text file in Snappy. Documentation will make this point clearer as well, thank you for bringing it to our attention."
Amazon Athena	"Re: Is snappy compression supported for CSV files?
I ran into the same problem and had to come here to find a clear answer since the documentation was ambiguous. Also when I first tried .snappy extension on my compressed CSV, Athena failed silently claiming there was no data rather than giving an error message that it could not parse the file.

It would be trivial to have Athena detect the .snappy extension and give an error message.

Of course, ideally it should support snappy. As a real-life example of the pain caused by lack of this feature, consider my situation. I am downloading huge ~6GB (after compression) chunks/partitions of a very large CSV data set. If I load these partitions into Athena unmodified then the query times out because it takes over 30-minutes per partition. SO I need to unzip, split into smaller chunks, then compress again and then load the smalller partitions.

The time bottleneck in this entire process is the `gunzip | split | gzip` which is ~10x slower than loading unmodified large partitions. Using `gzip --fast` speeds up the process and `snappy` would speed up the bottleneck even more -- 50% faster than `gzip --fast`.

So please support snappy for compressed CSV/TSV/text partitions.

Edited by: developer1 on Dec 17, 2018 9:13 PM"
