label	description
Amazon EKS	"EKS  - User/Role creation of cluster
Hello,

Currently is there a way to setup an EKS cluster and then give yourself permissions to the master in the Cluster's RBAC configuration? Without being the IAM user that created the cluster initially? 

Is only 1 IAM user (the creator of the cluster) able to grant additional AWS user permission to the cluster at launch?

Example: An EKS cluster is created using the console management through Federated AD access (thus there is no CLI access for this user, nor a way to set this up) how can I access my cluster then? if not recreating it entirely using a different user with CLI access.  

Thanks!

Edited by: Volter on Jun 22, 2018 1:25 PM"
Amazon EKS	"Re: EKS  - User/Role creation of cluster
I have same problem with a different use case. I would like to have a CloudFormation template that creates Kubernetes Control Plane via EKS and configure worker nodes to connect with EKS without any intervention outside cloudformation. The problem is that I can't authorize workers to work with EKS because I have to apply de authorization config map using the IAM User/Role that creates the cloudformation stack.

Is there a way to do that using cloudformation resources? I have think in create EKS cluster from EC2 workers instances instead of using cloudformation resource, or using a custom resource via aws lambda but much more complex than using the EKS cloudformation resource.

Any ideas?

Thank you so much."
Amazon EKS	"Re: EKS  - User/Role creation of cluster
I have the same problem. Any progress with this?"
Amazon EKS	"Deleted EKS Cluster but worker node keeps appearing in EC2
I am testing k8s and accomplished deleting my EKS Cluster without properly deleting my k8s resources. I see in EC2 that my worker node is alive. I terminate it and something keeps reviving it.  How do I end this madness? 

Edited by: blorenz on Feb 16, 2019 5:24 PM"
Amazon EKS	"Re: Deleted EKS Cluster but worker node keeps appearing in EC2
Hi
The getting started guide for EKS deploys the Worker Nodes within an Auto Scaling Group. From what you mention, it seems that you may just need to delete this Auto Scaling Group from the EC2 console.

I'd also recommend, if you created the cluster via CloudFormation, that you also delete the stacks there.

You can see how to do the above in the docs here:
https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-process-shutdown.html#as-shutdown-lbs-delete-asg-cli
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-delete-stack.html"
Amazon EKS	"Re: Deleted EKS Cluster but worker node keeps appearing in EC2
A LaunchConfiguration and an AutoScaling Group need to be deleted to prevent from relaunching EC2 instances."
Amazon EKS	"Automatically creating CloudWatch alarms for ELB or ALB ingress in EKS
Hello everyone!

When creating an AWS ELB (https://kubernetes.io/docs/concepts/services-networking/#internal-load-balancer) or an AWS ALB (https://aws.amazon.com/de/blogs/opensource/kubernetes-ingress-aws-alb-ingress-controller/) ingress for a service in Kubernetes using EKS, how can I automatically create and attach CloudWatch alarms to them?

Is there a recommended method, ideally in a declarative way or as a Kubernetes operator?"
Amazon EKS	"EKS worker node error - cni config uninitialized
New worker nodes added to an existing cluster via ASG are not being configured correctly. EKS sees the new worker node in a not ready state.  Secondary IPs are not being assigned to the node.  This is not impacting the nodes currently connected to the cluster, only new nodes. Any ideas?

Troubleshooting steps

Confirmed worker node connects to the cluster successfully via SSH to the worker node and telnet to cluster
Ran sudo bash /opt/cni/bin/aws-cni-support.sh to collect CNI logs but /opt/cni/ does not exist ‚Äúbash: /opt/cni/bin/aws-cni-support.sh: No such file or directory‚Äù
Saved file from ‚Äú curl https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/scripts/aws-cni-support.sh  | sudo bash‚Äù  and get the following error message: 

‚Äú  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to localhost port 61678: Connection refused‚Äù


worker node shows below error message in /var/log/message

Feb 13 14:17:45 ip-10-0-1-88 kubelet: E0213 14:17:45.532029    4338 kubelet.go:2110] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized

amazon-k8s-cni:1.0.0
Kubernetes version: 1.11"
Amazon EKS	"Re: EKS worker node error - cni config uninitialized
Am currently experiencing this too, was trying to spin up a new worker group and am seeing ""network plugin is not ready"" and the nodes never become Ready."
Amazon EKS	"Re: EKS worker node error - cni config uninitialized
@niamhg in my case it was easy, turns out due to the fact I was using an aws_iam_policy_attachment in Terraform I had inadvertently removed the AmazonEC2ContainerRegistryReadOnly policy from my EKS workers, once added back everything was happy again."
Amazon EKS	"Re: EKS worker node error - cni config uninitialized
Hey Stefan,

Thanks for your reply.. I just checked and the below policies are all attached to the workernode IAM role

AmazonEKSWorkerNodePolicy
AmazonEC2ContainerRegistryReadOnly
AmazonEKS_CNI_Policy

Thanks,
Niamh"
Amazon EKS	"New Amazon EKS-optimized AMI patched for CVE-2019-5736 Docker version!
EU (Ireland) (eu-west-1) ami-0b469c0fef0445d29 - The ami ships docker 17.06 version which hasn't got the CVE fix. The docker version 18.09.2 has the fix https://docs.docker.com/engine/release-notes/#18092. This may effects the ami's in other regions too.

This issue is probably due to this commit https://github.com/awslabs/amazon-eks-ami/commit/5cc7f41462ec5bc291250a790464fef7e5cbb35b#diff-dd3b29961ce3f6ae39aa400fcc19c612

Thanks"
Amazon EKS	"Re: New Amazon EKS-optimized AMI patched for CVE-2019-5736 Docker version!
Please ignore my alarm, the ami's do have the fix, see here - https://twitter.com/ilianaweller/status/1095550999261982720"
Amazon EKS	"Re: New Amazon EKS-optimized AMI patched for CVE-2019-5736 Docker version!
Source: https://nvd.nist.gov/vuln/detail/CVE-2019-5736
""... runc through 1.0-rc6, as used in Docker before 18.09.2 and other products, allows attackers to overwrite the host runc binary""

ECS cluster host shows 
Client:
Version:           18.06.1-ce
API version:       1.38
Go version:        go1.10.3
Git commit:        e68fc7a215d7133c34aa18e3b72b4a21fd0c6136
Built:             Mon Jan 28 20:25:39 2019
OS/Arch:           linux/amd64
Experimental:      false

runc version 1.0.0-rc5+dev
commit: 69663f0bd4b60df09991c08812a60108003fa340
spec: 1.0.0

-I would like AWS to post a mechanism for PROVING that the expected version isn't vulnerable based on a public changelog

Edited by: operationW on Feb 13, 2019 11:25 AM"
Amazon EKS	"Cluster IP show different IP
Hello everyone,

I've deployed the EKS cluster which uses VPC 172.x.x.x. But when I run command kubectl get svc I receive cluster-IP which is attached to different ip range. I assume that cluster-IP should be in the same IP range as VPC. Or I'm wrong? 

 NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes    ClusterIP   10.100.0.1   <none>        443/TCP   1d 


Edited by: liquidbob on Feb 13, 2019 9:45 AM"
Amazon EKS	"Creating namespaces in EKS
Hi,  

I've just created an account in EKS and I'd like to know if it is possible to create, inside a cluster, different namespaces or projects. Our objective is having different ""projects"" inside the created cluster for each environment (development, QA, etc.)

Is it possible to achieve this through the user interface of EKS? I have not been able to find the option to do this. 

Regards."
Amazon EKS	"kubernetes dynamic pv with aws is in pending
I am creating redis-cluster on kube with aws-gp2 persistent volume. I was using  redis-cluster.yml 
 Link: https://raw.githubusercontent.com/sanderploegsma/redis-cluster/master/redis-cluster.yml 

I have created Storage Class according to this  https://kubernetes.io/docs/concepts/storage/storage-classes/ , for dynamic persistence volume creation.

This is my StorageClass definition

    kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: aws-gp2
    provisioner: kubernetes.io/aws-ebs
    parameters:
      type: gp2
      zones: us-west-2a, us-west-2b, us-west-2c
      fsType: ext4
    reclaimPolicy: Retain
    allowVolumeExpansion: true


When I try to create cluster volume creation stuck at pending state, after checking logs found this

$ kubectl -n staging describe pvc data-redis-cluster-0
Name:          data-redis-cluster-0
Namespace:     staging
StorageClass:
Status:        Pending
Volume:
Labels:        app=redis-cluster
Annotations:   <none>
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:
Access Modes:
Events:
  Type    Reason         Age                From                         Message
  ----    ------         ----               ----                         -------
  Normal  FailedBinding  13s (x11 over 2m)  persistentvolume-controller  no persistent volumes available for this claim and no storage class is set


$ kubectl -n staging get events
LAST SEEN   FIRST SEEN   COUNT     NAME                                    KIND                    SUBOBJECT   TYPE      REASON             SOURCE                        MESSAGE
10s         10s          1         redis-cluster.15816c6dc1d6c03a          StatefulSet                         Normal    SuccessfulCreate   statefulset-controller        create Claim data-redis-cluster-0 Pod redis-cluster-0 in StatefulSet redis-cluster success
10s         10s          1         redis-cluster.15816c6dc2226fe0          StatefulSet                         Normal    SuccessfulCreate   statefulset-controller        create Pod redis-cluster-0 in StatefulSet redis-cluster successful
8s          10s          3         data-redis-cluster-0.15816c6dc1dfd0cb   PersistentVolumeClaim               Normal    FailedBinding      persistentvolume-controller   no persistent volumes available for this claim and no storage class is set
3s          10s          5         redis-cluster-0.15816c6dc229258d        Pod                                 Warning   FailedScheduling   default-scheduler             pod has unbound PersistentVolumeClaims (repeated 4 times)


someone point out what is wrong here ?

stackoverflow : https://stackoverflow.com/questions/54579644/kubernetes-dynamic-pv-with-aws-is-in-pending"
Amazon EKS	"Re: kubernetes dynamic pv with aws is in pending
As the message indicates no storage class exists.
no persistent volumes available for this claim and no storage class is set

What is Pod configuration for storage class?"
Amazon EKS	"Re: kubernetes dynamic pv with aws is in pending
dvohra wrote:
As the message indicates no storage class exists.
no persistent volumes available for this claim and no storage class is set

What is Pod configuration for storage class?

I am using StatefulSet and this is the volumeClaimTemplates from https://raw.githubusercontent.com/sanderploegsma/redis-cluster/master/redis-cluster.yml
volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        name: redis-cluster
    spec:
      accessModes: [ ""ReadWriteOnce"" ]
      resources:
        requests:
          storage: 100Mi"
Amazon EKS	"Re: kubernetes dynamic pv with aws is in pending
after adding storageClassName: aws-gp2 to volumeClaimTemplates, everything resolved


      volumeClaimTemplates:
      - metadata:
          namespace: staging
          name: data
          labels:
            name: redis-cluster
        spec:
          accessModes: [ ""ReadWriteOnce"" ]
          storageClassName: aws-gp2
          resources:
            requests:
              storage: 100Mi"
Amazon EKS	"UnknownHostException from container in Kubernetes when accessing Aurora DB
Hi, I have created a kubernetes cluster using EKS and deployed there my application with 3 replications. My application connects to an instance of aurora DB accessible by a public URL. For some reason this morning (first use of the application) there was a UnknownHostException in my pod logs as below:

2019-01-30 08:34:47.352  WARN 5 --- [onnection adder] unknown.jul.logger                       : IOException occurred while connecting to my-database-aurora-psql.cc3ft0tcxorz.eu-north-1.rds.amazonaws.com:5999
¬†
java.net.UnknownHostException: my-database-aurora-psql.cc3ft0tcxorz.eu-north-1.rds.amazonaws.com
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184) ~[na:1.8.0_181]
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[na:1.8.0_181]
	at java.net.Socket.connect(Socket.java:589) ~[na:1.8.0_181]
	at org.postgresql.core.PGStream.<init>(PGStream.java:69) ~[postgresql-42.2.1.jar!/:42.2.1]
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:158) ~[postgresql-42.2.1.jar!/:42.2.1]
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) [postgresql-42.2.1.jar!/:42.2.1]
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195) [postgresql-42.2.1.jar!/:42.2.1]
	at org.postgresql.Driver.makeConnection(Driver.java:452) [postgresql-42.2.1.jar!/:42.2.1]
	at org.postgresql.Driver.connect(Driver.java:254) [postgresql-42.2.1.jar!/:42.2.1]
	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:117) [HikariCP-2.7.8.jar!/:na]
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:365) [HikariCP-2.7.8.jar!/:na]
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:194) [HikariCP-2.7.8.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:460) [HikariCP-2.7.8.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool.access$100(HikariPool.java:71) [HikariCP-2.7.8.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:697) [HikariCP-2.7.8.jar!/:na]
	at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:683) [HikariCP-2.7.8.jar!/:na]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_181]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_181]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181]


I have another application using this database and didn't have any problem accessing this DB Url. Also I can see from the console that the DB was never down. 
Aurora DB is running in the same AWS Zone as the EKS Kubernetes cluster. Does this have anything to do with some internal network problem? Is EKS using internal routing in this case? I am thinking that maybe some internal route didn't work since the other application which is not running in this cluster (not at all in AWS) didn't have this issue."
Amazon EKS	"Re: UnknownHostException from container in Kubernetes when accessing Aurora DB
Is the URL for primary replica? A failover could have made the url different. Is Multi-AZ used?"
Amazon EKS	"Re: UnknownHostException from container in Kubernetes when accessing Aurora DB
This is the public URL of the aurora DB. I don't think it has multiple URL for different replicas so I am not really getting what you mean. Multi AZ is not used from cluster nor from aurora DB. URL is still same, as I explained the other application that doesn't run in AWS worked fine."
Amazon EKS	"Re: UnknownHostException from container in Kubernetes when accessing Aurora DB
For some reason this morning (first use of the application) there was a UnknownHostException in my pod logs as below:

First use of the application ever or first use for the day? Verify that the Security Group Inbound rules for Aurora are configured to allow access from all IP (0.0.0.0/0)."
Amazon EKS	"Re: UnknownHostException from container in Kubernetes when accessing Aurora DB
Thanks for trying to help. It allows only SSH traffic to 0.0.0.0/0. Do you think that AWS was trying to call the DB internally since these are in the same region?"
Amazon EKS	"Re: UnknownHostException from container in Kubernetes when accessing Aurora DB
Does the Aurora DB not expose a Public DNS as is supposed to?"
Amazon EKS	"Re: UnknownHostException from container in Kubernetes when accessing Aurora DB
Yes, it does"
Amazon EKS	"Re: UnknownHostException from container in Kubernetes when accessing Aurora DB
IOException occurred while connecting to my-database-aurora-psql.cc3ft0tcxorz.eu-north-1.rds.amazonaws.com:5999

The default port is 3306 for MySQL-compatible and 5432 for PostgreSQL-compatible. Why is 5999 used?"
Amazon EKS	"Re: UnknownHostException from container in Kubernetes when accessing Aurora DB
I set it to be like this. Does this have anything to do with my problem?"
Amazon EKS	"Re: UnknownHostException from container in Kubernetes when accessing Aurora DB
Shouldn't but update port  to 5432, and add the requisite Inbound rules."
Amazon EKS	"Too many ELBs: RulesPerSecurityGroupLimitExceeded
Hi everyone,

I'm deploying applications in Kubernetes and each one needs 2 ELBs. So, Kubernetes is adding the ELB security group to the main security group of the worker nodes. In this way, I'm reaching the limit of 50 rules per group.

I think it is possible to configure /etc/kubernetes/cloud.conf to disable this behavior:
---
global DisableSecurityGroupIngress=True 
ElbSecurityGroup=sg-deny-everything

---

Then, we add a ""shared"" security group to the service specs:
---
loadBalancer: 
type: Public 
additionalSecurityGroups: 

sg-shared

---

It is discussed here: https://github.com/kubernetes/kubernetes/issues/26670.

How I can do this configuration in EKS?. I don't have access to the cloud.conf file.

Thanks.

Edited by: jlinares on Feb 5, 2019 11:28 AM"
Amazon EKS	"Re: Too many ELBs: RulesPerSecurityGroupLimitExceeded
To increase the RulesPerSecurityGroupLimit limit need to contact AWS Support as explained at  https://aws.amazon.com/premiumsupport/knowledge-center/increase-security-group-rule-limit/

Edited by: dvohra on Feb 5, 2019 1:15 PM"
Amazon EKS	"Re: Too many ELBs: RulesPerSecurityGroupLimitExceeded
Yes, that is possible, but we will run 100+ apps. So, what I want is to change that behavior of Kubernetes.

Is it possible?.

Thanks."
Amazon EKS	"Re: Too many ELBs: RulesPerSecurityGroupLimitExceeded
The RulesPerSecurityGroupLimit is a VPC issue and not an EKS issue."
Amazon EKS	"Is it possible to use EKS with a VPC VPN?
turned out that I ran into a cisco/managed vpn limitation which was not related to EKS."
Amazon EKS	"kubectl error: ""dial tcp 172.17.11.97:10250: connect: no route to host""
Hi everyone,

Here's the situation.
We have 3 clusters: ""dev"", ""staging"", ""production"".
All 3 deployed with the same terraform script by our CI. All 3 using the v1.11 of k8s.
Our clusters are deployed with a public VPC and a private VPC.

We're working since more than 1 month with our ""dev"" cluster. Pushing apps inside, etc.

Now, we're in the step to push these apps in our ""staging"" env to validate that everything is ok before going into production with the ""production"" cluster, ofc.

First, to clean everything and start on the saner possible bases. We killed the ""staging"" cluster and asked to our CI to recreate it.

Then we began to encounter strange, random behaviors from this newly created cluster, which is exactly the same by construction than the ""dev"" cluster.

We deploy our apps thanks to Helm and most of the time we end up with the following error from Helm when we just launch the  `helm version` command:
Client: &version.Version{SemVer:""v2.12.3"", GitCommit:""eecf22f77df5f65c823aacd2dbd30ae6c65f186e"", GitTreeState:""clean""}
Error: forwarding ports: error upgrading connection: error dialing backend: dial tcp 172.17.13.252:10250: connect: no route to host
ERROR: Job failed: exit code 1


If we relaunch the script that deploys our apps 10 times for example, there's a chance that one of the launch will finally work. ü§î

So, this is our first problem we're trying to solve with you here.

The second problem, which is surely related to the first one, is that now that some of our apps are deployed, we can to use the kubectl tool to verify that everything is ok.

So for example, we'll `kubectl get all`, which works fine.
Then, we'll want to inspect a specific pod for example: `kubectl describe pod/pod-name`, which also works fine.
Finally, we'll want to inspect logs of that specific pod: `kubectl logs pod/pod-name`. And here we have the following error:
‚ûú  kubectl logs pod/pod-name
Error from server: Get https://172.17.11.97:10250/containerLogs/.../: dial tcp 172.17.11.97:10250: connect: no route to host


Do you have an idea where the problem could come from ?

Thanks,
Jules

Edited by: jivanic on Feb 7, 2019 6:44 AM

Edited by: jivanic on Feb 7, 2019 6:45 AM

Edited by: jivanic on Feb 7, 2019 6:46 AM

Edited by: jivanic on Feb 7, 2019 6:46 AM

Edited by: jivanic on Feb 7, 2019 6:47 AM

Edited by: jivanic on Feb 7, 2019 6:48 AM

Edited by: jivanic on Feb 7, 2019 6:51 AM"
Amazon EKS	"Re: kubectl error: ""dial tcp 172.17.11.97:10250: connect: no route to host""
Once again, I'll answer myself to my question.

The problem was that there's a amazingly error prone limitation in AWS EKS: you cannot use the `172.17.x.x` IP addresses.

More infos:
https://github.com/aws/amazon-vpc-cni-k8s/issues/137
https://stackoverflow.com/questions/53034064/eks-unable-to-pull-logs-from-pods"
Amazon EKS	"EKS node failure caused by EC2 health check failed. : i-0be8a770
Hello Support.

Just few hours ago, One of our production cluster's node has been terminated because some reason EC2 health check failed for that node.

My autoscaling rule printed failed cause like down below.

Description:DescriptionTerminating EC2 instance: i-0be8a770 (...)
Cause:CauseAt 2019-02-02T06:54:19Z an instance was taken out of service in response to a EC2 instance status checks failure.


So according to this docs - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html#types-of-instance-status-checks , I understood that ""Instance status check failure"" means something went wrong while checking misconfiguration of init steps, networking, kernal or exhausted of memory, or even a failed to check host physical machine's status. 

First 3 things (init steps, networking, kernel) doesn't match with my case because we were using this instance more than a month. also I checked our metric data of that node at that moment, and it was okay. not even close with 100%. 

So, I think this is caused by physical machine failure. Can you guys from AWS check what exactly happened to my instance and let me know? I really want to figure out exact reason why this thing failed. 

My instance id starts with i-0be8a770 and it has been terminated at 06:54 UTC.

Need your help. Thanks."
Amazon EKS	"Re: EKS node failure caused by EC2 health check failed. : i-0be8a770
A known issue, please refer https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-instance-how-terminated/"
Amazon EKS	"Re: EKS node failure caused by EC2 health check failed. : i-0be8a770
Hey. Thanks for answering my question.

but unfortunately, I've already read that docs. but I still didn't get clear reason why status check failure happened on my node. 

Only thing that I confirmed is, my autoscaling group terminated one of node caused by status check failed. also I can see both 'Instance status check' and 'System status check' failed at the same time on my cloudwatch dashboard. so I believe that this thing maybe happened by host machine failure on AWS datacenter. Just want to know what really happened."
Amazon EKS	"Re: EKS node failure caused by EC2 health check failed. : i-0be8a770
Do CloudWatch and CloudTrail provide any additional detail? If not, the reason is as indicated in link."
Amazon EKS	"Re: EKS node failure caused by EC2 health check failed. : i-0be8a770
Hey. so I can check ""StatusCheckFailed_system"", ""StatusCheckedFailed_Instance"", and ""StatusCheckFailed"" all marked as 1 at that moment when my instance get terminated.

I already figured out that my instance is terminated by ASG cause of health check failure. and the thing i am trying to know is ""why"" that health check didn't go well.

The thing I want to make clear is, Does ""StatusCheckFailed_system"" means a physical host machine failure? If so, the reason why my instance terminated is cause of host machine issue?

I attached a screenshot of my cloudwatch chart. Please also check it.

Thanks again."
Amazon EKS	"Re: EKS node failure caused by EC2 health check failed. : i-0be8a770
Seems like more information than provided in the link as to why the node instance failed is not made available to a user. Best to ask Amazon Support."
Amazon EKS	"Re: EKS node failure caused by EC2 health check failed. : i-0be8a770
Yeah. asking to support would be the best but I can't create technical support ticket cause I'm currently on basic plan. I just need a confirmation from AWS so I created a forum question instead.

Anyway, thanks for your question again. It was helpful."
Amazon EKS	"CoreDNS can't resolve on public subnet in VPC with public and private subne
I completed the AWS EKS using their setup steps.

AWS EKS ver 1.11, coredns

With the VPC I create two public and two private subnets according to their docs here: https://docs.aws.amazon.com/eks/latest/userguide/create-public-private-vpc.html

Nodes deployed to a private subnet are labeled private and nodes deployed to a public subnet are labeled public.

When I deploy a busybox pod to each nodeSelector (public/private) the public container cannot resolve dns while the private can.

nslookup: can't resolve 'kubernetes.default'


If I ssh onto the public subnet node itself I am able to ping hostnames (ie google.com) successfully.

Any thoughts?

# kubectl exec -it busybox-private -- nslookup kubernetes.default
¬†
Server:    172.20.0.10
Address 1: 172.20.0.10 ip-172-20-0-10.ec2.internal
¬†
Name:      kubernetes.default
Address 1: 172.20.0.1 ip-172-20-0-1.ec2.internal


# kubectl exec -it busybox-public -- nslookup kubernetes.default
Server:    172.20.0.10
Address 1: 172.20.0.10
¬†
nslookup: can't resolve 'kubernetes.default'
command terminated with exit code 1


# kubectl -n=kube-system get all
NAME                           READY     STATUS    RESTARTS   AGE
pod/aws-node-46626             1/1       Running   0          3h
pod/aws-node-52rqw             1/1       Running   1          3h
pod/aws-node-j7n8l             1/1       Running   0          3h
pod/aws-node-k7kbr             1/1       Running   0          3h
pod/aws-node-tr8x7             1/1       Running   0          3h
pod/coredns-7bcbfc4774-5ssnx   1/1       Running   0          20h
pod/coredns-7bcbfc4774-vxrgs   1/1       Running   0          20h
pod/kube-proxy-2c7gj           1/1       Running   0          3h
pod/kube-proxy-5qr9h           1/1       Running   0          3h
pod/kube-proxy-6r96f           1/1       Running   0          3h
pod/kube-proxy-9tqxt           1/1       Running   0          3h
pod/kube-proxy-bhkzx           1/1       Running   0          3h
¬†
NAME               TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE
service/kube-dns   ClusterIP   172.20.0.10   <none>        53/UDP,53/TCP   20h
¬†
NAME                        DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/aws-node     5         5         5         5            5           <none>          20h
daemonset.apps/kube-proxy   5         5         5         5            5           <none>          20h
¬†
NAME                      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns   2         2         2            2           20h
¬†
NAME                                 DESIRED   CURRENT   READY     AGE
replicaset.apps/coredns-7bcbfc4774   2         2         2         20h


Going through ""Debugging DNS Resolution""
https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/

Odd that AWS has their coredns pods still labelled kube-dns

# kubectl get pods --namespace=kube-system -l k8s-app=kubedns
No resources found.
¬†
# kubectl get pods --namespace=kube-system -l k8s-app=kube-dns
NAME                       READY     STATUS    RESTARTS   AGE
coredns-7bcbfc4774-5ssnx   1/1       Running   0          20h
coredns-7bcbfc4774-vxrgs   1/1       Running   0          20h
¬†
# for p in $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name); do kubectl logs --namespace=kube-system $p; done
2019/01/31 15:23:36 [INFO] CoreDNS-1.1.3
2019/01/31 15:23:36 [INFO] linux/amd64, go1.10.5, d47c9319
.:53
CoreDNS-1.1.3
linux/amd64, go1.10.5, d47c9319
2019/01/31 15:23:36 [INFO] CoreDNS-1.1.3
2019/01/31 15:23:36 [INFO] linux/amd64, go1.10.5, d47c9319
.:53
CoreDNS-1.1.3
linux/amd64, go1.10.5, d47c9319"
Amazon EKS	"Re: CoreDNS can't resolve on public subnet in VPC with public and private subne
The busybox images should be <= 1.28.4"
Amazon EKS	"Re: CoreDNS can't resolve on public subnet in VPC with public and private subne
dvohra wrote:
The busybox images should be <= 1.28.4

Thanks. I was on busybox:1.28.4."
Amazon EKS	"Re: CoreDNS can't resolve on public subnet in VPC with public and private subne
Looking at the worker node security groups is where I think I found the issue.

The AWS EKS kube-dns endpoints and pods were on the private subnet.  

I have two CloudFormation stacks....one for autoscaling nodes in the private subnets and one for autoscaling nodes in the public subnets.

They didn't have a common security group so the pods running in the public nodes weren't able to access the kube-dns pods running on the private nodes.

Once I update the worker node security groups to allow cross communication the dns started working.

Pls post if anyone sees any unintended consequences.  Thx!"
Amazon EKS	"Configuring the horizontal-pod-autoscaler-sync-period flag
How do I change horizontal-pod-autoscaler-sync-period from the default 30s value?"
Amazon EKS	"Re: Configuring the horizontal-pod-autoscaler-sync-period flag
The Horizontal Pod Autoscaler is implemented as a control loop, with a period controlled by the controller manager‚Äôs --horizontal-pod-autoscaler-sync-period flag (with a default value of 30 seconds). On EKS the --horizontal-pod-autoscaler-sync-period flag cannot be set by user. Ask Amazon Support if provision to set --horizontal-pod-autoscaler-sync-period flag could be made."
Amazon EKS	"data exposed on ALL EKS CLUSTERS of any customer?
Regarding the kubernetes api on EKS:

The domain name of my cluster endpoint resolves on two ips.

host xxxxxxxxxxxxxxx.yl4.eu-west-1.eks.amazonaws.com
xxxxxxxxxxxxxxx.yl4.eu-west-1.eks.amazonaws.com has address x.x.x.x
xxxxxxxxxxxxxxx.yl4.eu-west-1.eks.amazonaws.com has address x.x.y.y

Our clusters are deployed on private subnets and security groups in place allowing access only from our office.

This problems exists on ALL our eks clusters. And ALL of them are deployed on private subnets

We need to mitigate this and get some answers.

Is the aws eks api exposed to the world and absolutely out of our control?
Any way of limiting the access to the api to particular IPs?
Any way of disabling the anonymous users to request data to the api?

I'm very disapointed with this issue

Edited by: adminrgawsmain on Dec 14, 2018 7:18 AM

Edited by: adminrgawsmain on Dec 14, 2018 8:13 AM"
Amazon EKS	"Re: data exposed on ALL EKS CLUSTERS of any customer?
It is by design. ""We recommend a network architecture that uses private subnets for your worker nodes, and public subnets for Kubernetes to create public load balancers within.""

https://docs.aws.amazon.com/eks/latest/userguide/create-public-private-vpc.html"
Amazon EKS	"Kubectl get logs timeout
Error attaching, falling back to logs: error dialing backend: dial tcp 10.0.75.172:10250: getsockopt: connection timed out

kubetl get logs started to throw getsockopt: connection timed out, i have already checked SG both from master and workers.

Any clue on how to debug that? kubectl proxy does not work either..."
Amazon EKS	"Re: Kubectl get logs timeout
The command is not Kubectl get logs. The command is Kubectl logs."
Amazon EKS	"Error updating EKS Cluster
I have an EKS cluster that was configured to multiple AZ (at least 6 subnets and 3 AZ), after some time we realized that we could change the ASG to limit the AZ available to the EC2 instances, as some of those subnets were without any instances running, we choose to delete some of them but now we have a big problem, we are stuck trying to update the cluster with this error.

InvalidRequestException
The subnet ID 'subnet-04eeceaee863ea238' does not exist (Service: AmazonEC2; Status Code: 400; Error Code: InvalidSubnetID.NotFound; Request ID: 0426237f-2981-4fe6-b5fa-0f2af86067ec)

Any clues on how i can resolve this issue?

Edited by: rafaelbmoraes on Jan 4, 2019 11:12 AM"
Amazon EKS	"Re: Error updating EKS Cluster
If no applications are yet deployed it is better to create a new cluster."
Amazon EKS	"Can't SSH to EKS Service
I created the EKS cluster and tried to deployment my ""image"" which create other EC2 instance.
Then expose it by creating service has type ""LoadBalancer"".
I can access the k8s container (EC2 instance) by kubectl  command. But I want to ssh'in as normal EC2 instance.

ubuntu@ip-xxx:~$ kubectl get svc
NAME         TYPE           CLUSTER-IP       EXTERNAL-IP                                                                    PORT(S)        AGE
kubernetes   ClusterIP      10.100.0.1       <none>                                                                         443/TCP        1h
ubuntu       LoadBalancer   10.100.248.111   a4cac49af15ad11e9a745066a6b95b5f-1846317482.ap-southeast-1.elb.amazonaws.com   22:32279/TCP   1h

I can ping k8s service from my local but _can't ssh _

ssh -i my_key ubuntu@a4cac49af15ad11e9a745066a6b95b5f-1846317482.ap-southeast-1.elb.amazonaws.com -vvv
Warning: Identity file my_key not accessible: No such file or directory.
OpenSSH_7.1p2, OpenSSL 1.0.1g 7 Apr 2014
debug1: Reading configuration data /etc/ssh_config
debug2: ssh_connect: needpriv 0
debug1: Connecting to a4cac49af15ad11e9a745066a6b95b5f-1846317482.ap-southeast-1.elb.amazonaws.com http://54.169.21.126 port 22.
debug1: Connection established.
debug1: key_load_public: No such file or directory
ssh_exchange_identification: Connection closed by remote host

Does anyone know how can ssh'in it ?"
Amazon EKS	"Re: Can't SSH to EKS Service
The service itself is not meant to be accessed with ssh. The worker nodes may be accessed with ssh."
Amazon EKS	"How to change the default number of pods per node?
Hi,

I'm learning EKS and I've set up a cluster with a t3.small node following https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html. Somehow I'm not able to scale my services to run more than 4 pods, Can someone please tell me why?

Some info here:
`kubectl describe nodes` shows that the number of allocatable pods is set to 8:
Allocatable:
 cpu:                2
 ephemeral-storage:  19316009748
 hugepages-1Gi:      0
 hugepages-2Mi:      0
 memory:             1902056Ki
 pods:               8

And here is the allocated resources:
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ------------  ----------  ---------------  -------------
  310m (15%)    0 (0%)      140Mi (7%)       340Mi (18%)

I've found that besides CPU and Memory, it is also subject to the number of Elastic Network Interface and thus IP addresses. According to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI, t3.small supports up to 3 network interfaces and each with up to 4 IPs (so a total of 12 IPs). The instance currently has 2 network instances and 8 private IPs (2 primary and 6 secondary). Minus the 4 kube-system containers (assuming they all need an IP), it should be able to provide 8 IPs for user pods; however, the fifth one always has a status of ""Pending"" and here below is the event:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  1m (x632 over 1h)  default-scheduler  0/1 nodes are available: 1 Insufficient pods.

Can someone please help? Thanks."
Amazon EKS	"Re: How to change the default number of pods per node?
Pods per node is based on scheduling policy. By default spread scheduling is used which schedules equal number of pods per node, keeping in consideration that some nodes would have more pods scheduled if the number of pods is not a multiple of the number of nodes. Custom scheduling could be used to allocate more pods to a specific node."
Amazon EKS	"Tagging EKS Cluster
I need to tag the costs associated with the EKS Cluster, specifically this line that is appearing in my bill:

Amazon Elastic Container Service for Kubernetes CreateOperation $5.60
AmazonEKS Cluster usage for HA in EU-West-128 Hours $5.60

I've opted into the new ARN as per this article: https://aws.amazon.com/about-aws/whats-new/2018/11/amazon-ecs-and-aws-fargate-now-allow-resources-tagging-/

But I still cannot see a way of allocating this cost to a tag. Has anyone managed to solve this problem?"
Amazon EKS	"Re: Tagging EKS Cluster
The link is for ECS and Fargate and not EKS."
Amazon EKS	"EKS, how can I increase the network interfaces (IPs) per worker?
I have a question regarding AWS EKS. After creating the cluster, it will deploy automatically the workers/nodes. Unfortunately, depending on the worker/node EC2 instance type, it will give you a number of IPs for the PODS.

How can I increase the range of IPs in order to have more PODs for each worker?

Thanks!"
Amazon EKS	"Re: EKS, how can I increase the network interfaces (IPs) per worker?
As EKS is configured to use the VPC networking model (implemented by https://github.com/aws/amazon-vpc-cni-k8s), you are restricted by the number of network interfaces and IP addresses supported by your particular instance type - see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI"
Amazon EKS	"Re: EKS, how can I increase the network interfaces (IPs) per worker?
Effectively, that is the problem! However, I am trying to find a solution in order to break this limitation."
Amazon EKS	"Re: EKS, how can I increase the network interfaces (IPs) per worker?
You can not, just spin up more nodes or larger ones, its somtimes better to have 2 large vs 1 xlarge as the ENI counts are more.

I thought Calico would do it but ive read it still implements AWS CNI limitations."
Amazon EKS	"Re: EKS, how can I increase the network interfaces (IPs) per worker?
Thank you for your answer about Calico, I have never heard about it. Do you have any document/tutorial in order to learn Calico and break this limitation?"
Amazon EKS	"Re: EKS, how can I increase the network interfaces (IPs) per worker?
Refer https://docs.aws.amazon.com/eks/latest/userguide/calico.html"
Amazon EKS	"Request for clarification on roles/user accounts during setup tutorial
Hello!

I've been following the EKS setup  https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html  and all seems to have gone successfully up until the point of connecting to the server with kubectl. 

When running: kubectl get svc
 no matter what I do I seem to get:
error: the server doesn't have a resource type ""svc""


I understand that there requirements on the IAM account being used on the instance calling kubectl. I've read variously that the authenticated identity during kubectl config generation must be either a) the IAM user that created the EKS cluster or b) a configured profile in aws config, where the role used to build the eks instance (e.g. eksServiceRole) has been delegated to my user IAM. 

Is either of these assumptions correct? I seem to successfully be getting tokens in both of these scenarios, but neither seems to be the right thing for connecting to the cluster. 

Thank you so much for your help; I've spent many hours trying to figure this out."
Amazon EKS	"Re: Request for clarification on roles/user accounts during setup tutorial
Use command

kubectl get service


Edited by: dvohra on Jan 29, 2019 5:50 PM"
Amazon EKS	"Re: Request for clarification on roles/user accounts during setup tutorial
I ran into this with IAM and just found it easier to provision the cluster using the AWS CLI, https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html
# aws eks create-cluster --name <cluster-name> --role-arn <role-arn> --resources-vpc-config subnetIds=<subnet-ids>,securityGroupIds=<sg-ids>

# aws eks update-kubeconfig --name <cluster-name>


Edited by: ahec on Jan 31, 2019 12:05 PM"
Amazon EKS	"Service LoadBalancer Preserve DNS
How can you preserve DNS when dealing with an EKS Service?  If a Service is deleted I don't want to have to propagate a change through DNS.

I found that that the Kubernetes ""loadBalancerIP"" spec does not appear to work.

Next I am exploring running two load balancers so that we can manage the external and manage the Kubernetes Service ELB in IP based target group.

Any thoughts on a this path or a better one?  Should we be thinking about this differently?

Thx! bkc"
Amazon EKS	"Re: Service LoadBalancer Preserve DNS
As a first config, I've decided to use the aws-alb-ingress-controller pointing to our nginx-ingress-controller.

https://aws.amazon.com/blogs/opensource/kubernetes-ingress-aws-alb-ingress-controller/

https://github.com/helm/charts/tree/master/stable/nginx-ingress"
Amazon EKS	"AWS EKS giving x509 error after upgrade to v1.11.5
Hi,

I recently upgraded my EKS cluster to v1.11.5 from v.1.10.3. I am unable to view logs or exec into any pods as I constantly am given
x509: cannot validate certificate for <WORKER\_IP> because it doesn't contain any IP SANs

Any other command on the cluster works fine.

For the upgrade, I clicked ""Upgrade cluster"" button on the web console, and modified cloudformation template for workers to use latest AWS provided ami.

This issue I am experiencing is almost identical to this other forum post here AWS EKS is giving x509 error after upgrade to kubernetes v1.11.5.
I do not want to be posting a duplicate question, however, I have received no response on the other message and have found no other resources to solve this problem online. This is a critical issue for me.

If anyone has a solution or can point me in a good direction that would be incredibly helpful and greatly appreciated.

Thank you

Edited by: marksansomeRR on Jan 31, 2019 6:04 AM"
Amazon EKS	"Authenticate EKS cluster with Java SDK
Hello, 

 I'm able to create an EKS cluster with the worker nodes using the Java SDK, but I'm having difficulty in authenticating with the cluster using the SDK. Right now, what I have to do is create the cluster, open up a command line prompt:

\home\XXXXX] aws-iam-authenticator token -i {clusterName}

retrieve the token name, plug that into the KubernetesClient instance on Java using the following configuration:

 new DefaultKubernetesClient(new ConfigBuilder().withMasterUrl({clusterEndPoint})
        .withOauthToken({retrievedToken}).withTrustCerts(true).build());


If I try to authenticate the Java KubernetesClient with the CA cert data that I get back from the cluster object directly, any request to the cluster using the K8s library results in the cluster assuming that it's coming from an anonymous source.  In order for me to get over this issue, I have to run (again on the command line):

\home\XXXX] aws eks-update kubeconfig --name {clusterName}

which generates the kubeconfig file under my .kube directory, and under the .kube/cache/discovery/ folder, places a new folder with the cluster properties on my local machine. That's again a manual step. In addition, I also have to create a clusterRoleBinding for the clusterRole system:admin - that I am then assumed to be which is : arn:aws:eks:us-east-1:{accountId}:cluster/{clusterName} to use the KubernetesClient library to perform CRUD operations on the cluster. 

The K8s library I'm using is Fabric8: https://github.com/fabric8io/kubernetes-client

Is there a way for me to get the K8s token any way from the SDK (like an aws-iam-authenticator implementation) instead of having to follow all the manual workarounds?

Thanks"
Amazon EKS	"Re: Authenticate EKS cluster with Java SDK
Hello, we have the same issue. Could someone describe a flow/way how to do it, without using a docker image containing  aws-iam-authenticator? How can we generate a token?"
Amazon EKS	"AWS EKS is giving x509 error after upgrade to kubernetes v1.11.5.
Hello there.

I just upgraded my EKS cluster to v1.11.5 from v.1.10.3, but it's giving me 
x509: cannot validate certificate for <WORKER\_IP> because it doesn't contain any IP SANs
 
error when i try to get logs or helm ls on it. Other commands like kubectl get nodes are working fine.

For the upgrade, I clicked ""Upgrade cluster"" button on the web console, and modified cloudformation template for workers to use latest AWS provided ami (ami-0a9006fb385703b54). read this guide from AWS, including this.

My cluster was completely unusable so I rollbacked my worker nodes with old ami (ami-0c7a4976cb6fafd3a) for now, and the error is gone.

I'm really not sure that what that i missed. Anyone experiencing similar issue with me? or Is this some kind of bug on that Image?

Thanks. and need your help badly.

Edited by: kycfeel on Dec 20, 2018 9:12 PM

Edited by: kycfeel on Dec 20, 2018 9:13 PM"
Amazon EKS	"Re: AWS EKS is giving x509 error after upgrade to kubernetes v1.11.5.
Hello,

Thank you for contacting us through AWS Forums. 

One of our engineers will reach you via outbound case as we require more info like logs and other details. 

Please let us know if you have any additional concerns."
Amazon EKS	"Re: AWS EKS is giving x509 error after upgrade to kubernetes v1.11.5.
Hey there.

Yes. I got a contact from AWS engineer last week. He asked me to reproduce the error and try to collect it's logs with provided script. 

but I was pretty busy last week, so I didn't work on that... until today.

Now I successfully reproduced that error & collected logs with script. but seems my case is closed cause of ""no activity"" and there's no way to contact AWS engineer again.

I can't find any case re-open option on my support center page. Probably cause i'm on basic support plan. Can you reopen the case on your side for me? or let me know if there's any other way to contact the engineer again.

My case-id : 5648596731

Thanks a lot.

Edited by: kycfeel on Jan 2, 2019 8:56 PM"
Amazon EKS	"Re: AWS EKS is giving x509 error after upgrade to kubernetes v1.11.5.
Hi,
Was this issue ever resolved? I am experiencing almost the exact same issue and am having trouble finding much information on a solution. If you could please let me know what worked for you that would be greatly appreciated.
Thanks"
Amazon EKS	"Reserve static IP and use it in a service as LoadBalancerIP
Hello,

Sorry if this question have been asked by someone, but I couldn't find a resource about how can I create/use static IP's in EKS for LoadBalancers.

To be clear what I mean (with a GKE example): https://cloud.google.com/kubernetes-engine/docs/tutorials/configuring-domain-name-static-ip

But in short:

$ gcloud compute addresses create helloweb-ip --region us-central1

(it will create a static IP, and print out the created static IP address, so I can use it.)

And in my service.yaml I can use that IP for LoadBalancerIP.

apiVersion: v1
kind: Service
metadata:
  name: helloweb
  labels:
    app: hello
spec:
  selector:
    app: hello
    tier: web
  ports:
  - port: 80
    targetPort: 8080
  type: LoadBalancer
  loadBalancerIP: ""YOUR.IP.RESERVED.STATIC.IP.ADDRESS.HERE""


So I dont have to modify DNS if my service gets deleted, I can always reuse that IP.
Is it possible in EKS?

Thanks in advance, if anyone can help.

Edited by: khris on Jan 5, 2019 2:04 AM

Edited by: khris on Jan 5, 2019 2:05 AM"
Amazon EKS	"Re: Reserve static IP and use it in a service as LoadBalancerIP
Did you have any success with this?  I'm also looking for a way to keep DNS static even if Service is deleted.  Thx!"
Amazon EKS	"ELB creation: could not find any suitable subnets for creating the ELB
Hello,

We're trying to deploy a Nginx ingress and here the error we have:

  Type     Reason                      Age               From                Message
  ----     ------                      ----              ----                -------
  Normal   EnsuringLoadBalancer        3s (x4 over 38s)  service-controller  Ensuring load balancer
  Warning  CreatingLoadBalancerFailed  2s (x4 over 38s)  service-controller  Error creating load balancer (will retry): failed to ensure load balancer for service api-ingress-nginx: could not find any suitable subnets for creating the ELB


Does someone have the same error ? 
How can we debug this ? 

It seems that the first creation of the ingress in a fresh new cluster works but when we try to redeploy it, we have this error."
Amazon EKS	"Re: ELB creation: could not find any suitable subnets for creating the ELB
Hey,

Here's the solution https://github.com/kubernetes/kubernetes/issues/29298#issuecomment-274400650"
Amazon EKS	"Re: ELB creation: could not find any suitable subnets for creating the ELB
You have not tagged up your subnets correctly, check the AWS docs."
Amazon EKS	"How to debug pod failure on EKS?
Hello there.

I'm currently using EKS v1.11.5 (control plane only) and just noticed that one of my pod failed a week ago.

Kubernetes automatically created a new pod right away, so there was no downtime, but still want to debug what really happened.

I tried to get logs from the failed pod but it only returned me container ""<deployment-name>"" in pod ""<pod-name>"" is not available


Is there any way to track down this problem deeply? Like kubernetes audit-logging from control plane? 

Thanks.

+ Maybe something left on syslog of worker nodes, but I blocked ssh connection to nodes for security reasons. so it's not possible for now."
Amazon EKS	"Re: How to debug pod failure on EKS?
As long as you have not removed the pod you could do kubectl describe pod ....

It should indicate why the pod failed, could be due to OOM or other resource failure etc."
Amazon EKS	"Will EKS be supported in GovCloud?
Is there a plan to support EKS in either GovCloud regions?

Thank you."
Amazon EKS	"Re: Will EKS be supported in GovCloud?
I understand that you would like to know if EKS will be supported in GovCloud. 

Firstly, we thank you for your interest in our service.

Region expansion is definitely a top priority for our EKS service team. They are currently evaluating customer requests to figure out the order of rolling out to specific regions. 

Unfortunately, we can't share any ETA on this. However, as soon as it gets released, it should be publicly announced in either one of the following links:




AWS blogs: https://aws.amazon.com/blogs/aws/
Whats New: http://aws.amazon.com/new/




For better tracking of features and regions being worked upon along with timelines, kindly refer the following link:
https://github.com/aws/containers-roadmap/projects/1"
Amazon EKS	"Re: Will EKS be supported in GovCloud?
They are currently evaluating customer requests to figure out the order of rolling out to specific regions. 

I am a current customer. How do I register a customer request for EKS in GovCloud?"
Amazon EKS	"how can we customise EKS service IP range like pod ip can be bind to vpc
IN EKS, the pod can assign vpc's IP, but is there a way to specify kubernetes service ip? 

for me I've create an EKS cluster and deploy work node in my vpc

10.0.0.0/16 and I can get my pod with correct ip

kubectl get pods -o wide
NAME                                  READY     STATUS    RESTARTS   AGE       IP           NODE
hello-world-d8b8b7db6-hlfr9           1/1       Running   0          2d        10.0.3.228   ip-10-0-3-25.ec2.internal
hello-world-d8b8b7db6-r9sjl           1/1       Running   0          2d        10.0.3.197   ip-10-0-3-182.ec2.internal
helloworld-service-58d96776c4-hk6lq   2/2       Running   0          1m        10.0.3.118   ip-10-0-3-182.ec2.internal
helloworld-service-58d96776c4-l4b44   2/2       Running   0          1m        10.0.3.241   ip-10-0-3-25.ec2.internal
redis-master-57hvn                    1/1       Running   0          2d        10.0.3.229   ip-10-0-3-11.ec2.internal


but when I create kubenetes service, it's the ip range is not in my vpv
kubectl get svc --all-namespaces
NAMESPACE     NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE
default       kubernetes             ClusterIP   172.20.0.1       <none>        443/TCP           3d
kube-system   heapster               ClusterIP   172.20.218.223   <none>        80/TCP            2d
kube-system   kube-dns               ClusterIP   172.20.0.10      <none>        53/UDP,53/TCP     3d
kube-system   kubernetes-dashboard   ClusterIP   172.20.43.194    <none>        443/TCP           2d
kube-system   monitoring-influxdb    ClusterIP   172.20.123.199   <none>        8086/TCP          2d
stage         helloworld-service     ClusterIP   172.20.215.100   <none>        8080/TCP,80/TCP   54m
stage         redis-master           ClusterIP   172.20.103.166   <none>        6379/TCP          3d
stage         redis-slave            ClusterIP   172.20.159.49    <none>        6379/TCP          3d


is there way to make sure the cluster ip in my vpc range, e.g 10.0.60.0/24, I will make sure the service ip and pod ip range has no overlap

Edited by: email2liyang on Jul 22, 2018 6:56 PM"
Amazon EKS	"Re: how can we customise EKS service IP range like pod ip can be bind to vpc
There is not a way. The AWS CNI assigns addresses to pods using ENIs in your VPC address range. Service addresses in kubernetes get assigned from 172.20.0.0/16 (or 10.100.0.0/16). If EKS allowed --pod-cidr (and probably a couple others) to be configured on the controlplane, this should possible, as would bringing your own CNI like flannel. Your pods should still be able to reach your services in this case though.

It would also be great if AWS seemed to care about EKS and made an official response.

Edited by: wylie on Jul 24, 2018 9:30 AM"
Amazon EKS	"Re: how can we customise EKS service IP range like pod ip can be bind to vpc
Hello!

EKS actually picks a cluster IP range (either 10.100 or 172.20) depending on the range of your VPC that you tell us to use. We pick a range that doesn't overlap with your VPC CIDR as to avoid routing conflicts. The cluster IP range is internal to your cluster, but the IPs assigned to your pods actually come from the VPC IPAM via our CNI plugin. 

Let me know if this isn't clear."
Amazon EKS	"Re: how can we customise EKS service IP range like pod ip can be bind to vpc
I have the same question. The limits for the PODs are here: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html"
Amazon EKS	"(RE) AWS EKS is giving x509 error after upgrade to kubernetes v1.11.5.
Hi. sorry to create question with same issue again.
This is a dup of  https://forums.aws.amazon.com/thread.jspa?threadID=295599&tstart=0 .

The reason why I created this is I want to contact with AWS engineer again with this issue.

AWS recently closed my case cause of ""no activities"", but I am on basic plan so I can't re-open my technical support case again.

My Issue is not solved. Can you people please open my case again or create a new case for me?

My last case id : 5648596731

Thanks. You can remove this post if it's violating a forum rule."
Amazon EKS	"additional VPC CIDR blocks - How to attach an annotation on node boot?
Hello, 

Related to Announcement: Amazon EKS now supports additional VPC CIDR blocks (https://forums.aws.amazon.com/ann.jspa?annID=6264). 

Look like is necessary attach a annotation to the node in order to use a different CIDR block. There is a way to do this on node boot? Or another ""automatic"" tool? 

Or the only option is manually add the annotation every time a node startups up? 

Thanks,"
Amazon EKS	"Automatic region/zone labeling for a PersistentVolume 1.11
From this documentation (https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#persistentvolumelabel), it looks like the PersistentVolumeLabel admission controller is now disabled (by default) starting with 1.11.  However, we can't figure out how to enable the new way to achieve the same thing.  Specifically, I'm referring to these two labels for a PersistentVolume:

failure-domain.beta.kubernetes.io/region
failure-domain.beta.kubernetes.io/zone


The documentation (https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#failure-domain-beta-kubernetes-io-region) explains how these labels will ensure that PODs requiring a volume in a specific region/zone will get scheduled in the appropriate region/zone.

In 1.10, if I manually create an EBS volume, and then create a PV with awsElasticBlockStore.volumeID matching that new EBS volume id and inspect the PV, I can see that those 2 labels are added to the PersistentVolume.  I suspect this is because the PersistentVolumeLabel admission controller is ENABLED.

In 1.11, those labels are NOT added.  I suspect this is because the PersistentVolumeLabel admission controller is DISABLED.

As a side note, if I manually add those labels to the PV in 1.11, the scheduler does ensure that PODs requesting a volume in a particular region/zone are scheduled to a node in the appropriate region/zone.  However, without adding those labels, scheduling does not -- sometimes PODs are scheduled in the correct region/zone, and sometimes they are not.

Also as a side note, if I create a PersistentVolumeClaim to a dynamically provisioned (EBS) PersistentVolume, the labels ARE added in 1.11.  Therefore, it looks like this is limited only to manually created (EBS) PersistentVolumes in 1.11.

The documentation seems to indicate that this automatic PV labeling responsibility now falls to the cloud controller manager and the use of an initializer, which is described here (https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/)

However, the documentation references a few requirements of the masters which we simply cannot verify because there is no access to the masters themselves.  Additionally, even our workers which are supposed to launch kubelet with --cloud-provider=external
 are using --cloud-provider=aws


Has anyone else run into this and been able to figure out how to make this work?"
Amazon EKS	"Does Heptio Authenticator be deployed automatically when creating EKS?
I have done the below steps.
Created an EKS Cluster
Installed aws-iam-authenticator client binary
Execute ""aws eks update-kubeconfig --name <cluster_name>""
Execute ""kubectl get svc""

I am able to view the services available in my cluster. When I see ~/.kube/config file it is using an external command called ""aws-iam-authenticator"".

My understanding is that ""aws-iam-authenticator"" uses my ~/.aws/credentials and retrieves the token from AWS(aws-iam-authenticator token -i cluster-1) and uses that token for ""kubectl get svc"" command. Is my understanding correct?

If my understanding correct, where does heptio comes into picture in this flow? Does Heptio Authenticator be deployed automatically when creating the EKS Cluster?

Edited by: karthikeayan on Dec 18, 2018 12:21 AM"
Amazon EKS	"Re: Does Heptio Authenticator be deployed automatically when creating EKS?
You'll find some interesting details in the readme on https://github.com/kubernetes-sigs/aws-iam-authenticator. As far as I understand, this is what's been previously referred to as the Heptio Authenticator (heptio-authenticator-aws), now called aws-iam-authenticator. You are using the client on your machine and there is also a server component running out of the box in your EKS cluster. The readme has a nice explanation of how it works."
Amazon EKS	"Worker Nodes keep going to ""Not Ready"" status, what can I do about this?
First of all I'd like to find out 1. why is this happening? and 2. how can I restart the nodes once this happens?

When I run kubectl describe node <node-name>

Conditions:
  Type             Status    LastHeartbeatTime                 LastTransitionTime                Reason                    Message
  ----             ------    -----------------                 ------------------                ------                    -------
  OutOfDisk        Unknown   Fri, 30 Nov 2018 15:16:57 +0000   Fri, 30 Nov 2018 15:17:38 +0000   NodeStatusUnknown         Kubelet stopped posting node status.
  MemoryPressure   Unknown   Fri, 30 Nov 2018 15:16:57 +0000   Fri, 30 Nov 2018 15:17:38 +0000   NodeStatusUnknown         Kubelet stopped posting node status.
  DiskPressure     Unknown   Fri, 30 Nov 2018 15:16:57 +0000   Fri, 30 Nov 2018 15:17:38 +0000   NodeStatusUnknown         Kubelet stopped posting node status.
  PIDPressure      False     Fri, 30 Nov 2018 15:16:57 +0000   Fri, 30 Nov 2018 15:09:56 +0000   KubeletHasSufficientPID   kubelet has sufficient PID available
  Ready            Unknown   Fri, 30 Nov 2018 15:16:57 +0000   Fri, 30 Nov 2018 15:17:38 +0000   NodeStatusUnknown         Kubelet stopped posting node status.


I'm running two EKS clusters with the relevant VPCs in Ireland and also one in Oregon. Everytime I switch k8s context and run kubectl get nodes, the nodes are all suddenly Not Ready. I then cannot kubectl exec or look at any kubectl logs once this has happened. The period between node creation and the nodes turning to Not Ready varies, sometimes 40 minutes, sometimes 21 hours.
Once this has happened I have to delete the Cloud Formations and peering connections and EKS clusters and start all over again!!!

Any ideas? Going slowly insane here..."
Amazon EKS	"Re: Worker Nodes keep going to ""Not Ready"" status, what can I do about this?
Update:
It seems very arbitrary, the times when the nodes go Not Ready. Not related to switching context at all."
Amazon EKS	"Re: Worker Nodes keep going to ""Not Ready"" status, what can I do about this?
Ok. It seems that explicitly associating the subnets breaks the routing. API is still able to be reached."
Amazon EKS	"Re: Worker Nodes keep going to ""Not Ready"" status, what can I do about this?
Ok, turns out kubelet would do this when subnet associations were made explicit. 
So somewhere along the way routing becomes incorrect. But, once these associations are removed kubelet does not report Ready statuses afterwards. At least we have found the reason as to why kubelet would be unreachable."
Amazon EKS	"Re: Worker Nodes keep going to ""Not Ready"" status, what can I do about this?
Hi,
I'm having the same problem - what do you mean by explicit subnet association?"
Amazon EKS	"Re: Worker Nodes keep going to ""Not Ready"" status, what can I do about this?
Same problem here as well.  I have specified 2 subnets for my node autoscaling group but I assume thats not what you meant.  Any hints?"
Amazon EKS	"Re: Worker Nodes keep going to ""Not Ready"" status, what can I do about this?
What do you mean by explicit subnet associations ? 

within EKS we need to specify the subnets for the EKS worker nodes , or are you suggesting we remove that ?"
Amazon EKS	"Cannot finish tutorial...
I was following https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html tutorial and in the step number 8 nothing happens (i had been waiting the dns propagation for 6 hours, no luck):

""Step 8. After your external IP address is available, point a web browser to that address at port 3000 to view your guest book. For example, http://a7a95c2b9e69711e7b1a3022fdcfdf2e-1985673473.us-west-2.elb.amazonaws.com:3000

Note
It may take several minutes for DNS to propagate and for your guest book to show up.""

In my console i have (change my url for security):
kubectl get services -o wide
NAME           TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)          AGE       SELECTOR
guestbook      LoadBalancer   10.100.79.107    xxxxxxxxxxx-xxxxxxxxx.us-east-2.elb.amazonaws.com   3000:31949/TCP   6h        app=guestbook
kubernetes     ClusterIP      10.100.0.1       <none>                                                                   443/TCP          9h        <none>
redis-master   ClusterIP      10.100.186.135   <none>                                                                   6379/TCP         6h        app=redis,role=master
redis-slave    ClusterIP      10.100.156.226   <none>                                                                   6379/TCP         6h        app=redis,role=slave

But when i put the Guestbook example i always obtain problem loading the page (i tried in the office network and home network with no port restrictions).

Even make a ping 

Request timeout for icmp_seq 5290
Request timeout for icmp_seq 5291
Request timeout for icmp_seq 5292
Request timeout for icmp_seq 5293
Request timeout for icmp_seq 5294
Request timeout for icmp_seq 5295

 heeeelp"
Amazon EKS	"Does 1 EKS cluster support multiple worker node groups?
For example, I want to have different worker groups (each uses one different ec2 instance type) for different workloads. Is it supported within 1 EKS cluster?

I see the official documentation only mentions a 2nd worker node ASG for the worker node upgrade scenario

Edited by: yuanlinios on Dec 19, 2018 6:46 AM"
Amazon EKS	"Re: Does 1 EKS cluster support multiple worker node groups?
EKS isn't really aware of nodegroups - you can have as many as you'd like but you will have to work a bit for it
You can use this project https://github.com/nanit/eks_cli which allows you to have multiple nodegroups easily"
Amazon EKS	"Re: Does 1 EKS cluster support multiple worker node groups?
Glad to know that. Thank you for the information!"
Amazon EKS	"ebs pv availability consideration in EKS
Hello

I am considering the availability of ebs PV in EKS. It looks ebs pv is bound to an ebs volume ID. If this ebs volume is lost, and I restore its snapshot to a new volume, how can I make it usable (replace the volume behind the original pv) in EKS?

It looks I have to manually create the pv and pvc, bind the pvc to the new pv, and update the deployment to use the new pvc. It really sucks... and becomes worse for stateful set pvc templates

Any suggestion or insight is appreciated"
Amazon EKS	"Problem logging into cluster with kubectl
Hi,

I followed the user guide and i'm having problems at this point [1]

This is the content of my eksconfig:

apiVersion: v1
clusters:

cluster:

    server: ""https://xxxx.us-east-1.eks.amazonaws.com""
    certificate-authority-data: ""xxxxxxxxx=""
  name: kubernetes
contexts:

context:

    cluster: kubernetes
    user: aws
  name: aws
current-context: aws
kind: Config
preferences: {}
users:

name: aws

  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      command: heptio-authenticator-aws
      args:
        - ""token""
        - ""-i""
        - ""terraform-eks-demo""
        # - ""-r""
        # - ""<role-arn>""

When i run  heptio-authenticator-aws token -i terraform-eks-demo i can get a token.

The url and certificate-authority-data are correct (i tried with and without quotes, just in case and fails on both situations)

When i run a kubectl command i get this error:


kubectl get nodes

error: You must be logged in to the server (Unauthorized)

the cluster details: 

aws eks list-clusters

{
    ""clusters"": [
        ""terraform-eks-demo""
    ]
}

region: us-east-1

Any help here?

Cheers
[1] https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html"
Amazon EKS	"Re: Problem logging into cluster with kubectl
This can be helpful https://medium.com/pablo-perez/common-errors-when-setting-up-eks-for-the-first-time-a43cbf989a2e

Basically, you just need to verify that your IAM user ACCESS KEY is the same. In my setup I had MFA keys, so that just did not work.
I created temporary IAM user that did not require MFA - created a cluster, now trying to add other roles/users to the RBAC."
Amazon EKS	"Re: Problem logging into cluster with kubectl
Oh God!

What if i'm not using iam users?

I have multiple accounts and i log into aws using okta with SAML, so the users are virtual and they assume a role.

u_u"
Amazon EKS	"Re: Problem logging into cluster with kubectl
I have not tried it yet, but I assume after you will get working EKS cluster with just one temporary IAM (think about it as a super-root account) - you will be able to map IAM Roles to the RBAC roles using the aws-auth configmap https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html, that will let your users from SAML to get access to EKS.
Again, I have not tried it yet. I would suggest talking to AWS Support to confirm."
Amazon EKS	"Re: Problem logging into cluster with kubectl
I had a similar issue on Windows 10. I found out that the docker had a kubectl.exe under ...\resources\bin and it was first in my path order, so it did not see the correct kubectl.exe that I had downloaded from AWS (per their Getting Started instructions)

Basically I had to make sure of following:

Have the latest of kubectl and the version from AWS
Have the latest AWS CLI (to get EKS support)
If using IAM: make sure sts:AssumeRole is assigned via an inline JSON policy on the IAM Role Trusted Relationship to the IAM User role you are using (I needed this to get heptio-authenticator-aws to create a token)
configure your credentials for AWS CLI"
Amazon EKS	"Re: Problem logging into cluster with kubectl
Maybe you created the cluster with the root account. Create another AMI user with the privileges you need and try creating the cluster from the CLI with it. As today, remember to not include any subnet on us-east-1b."
Amazon EKS	"Re: Problem logging into cluster with kubectl
ok"
Amazon EKS	"Time frame for Kubernetes 1.11 support?
Kubernetes 1.11 was released 27 June. Does anybody know the likely time frame for this to be supported in EKS?"
Amazon EKS	"Re: Time frame for Kubernetes 1.11 support?
At this rate, v1.8 is probably next"
Amazon EKS	"Re: Time frame for Kubernetes 1.11 support?
Hi, this is a high priority item for us. We are working on providing both 1.11 for new clusters and functionality for performing upgrades for existing clusters."
Amazon EKS	"Re: Time frame for Kubernetes 1.11 support?
Hi Brandon,

Is there a time estimation on supporting 1.11? The time table might impact tech decisions we take that's why it is important for us.

Thanks"
Amazon EKS	"Re: Time frame for Kubernetes 1.11 support?
Kubernetes 1.11.5 is now available it seems.
https://aws.amazon.com/blogs/compute/making-cluster-updates-easy-with-amazon-eks/

https://docs.aws.amazon.com/eks/latest/userguide/platform-versions.html

Edited by: ricardobrancokodakone on Dec 13, 2018 1:53 AM"
Amazon EKS	"EKS availability in ap-northeast-1(Tokyo)
Hi!

Is there any news when EKS will be available in the AWS ap-northeast-1 (Tokyo) region?"
Amazon EKS	"Re: EKS availability in ap-northeast-1(Tokyo)
Hello,

Thank you for your interest in EKS service. Availability of EKS in the AWS ap-northeast-1 is currently a feature request. I have added +1 on your behalf. As of now, there is no ETA available for the feature roll out.
https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/

Cheers,
Nitheesha"
Amazon EKS	"MFA enforcement for kubectl IAM users
Hello,
I just realized something pretty bad:
=> All our IAM users have a policy which is supposed to prevent actions without MFA:
        {
            ""Sid"": ""DenyNotMfa"",
            ""Effect"": ""Deny"",
            ""Action"": ""*"",
            ""Resource"": ""*"",
            ""Condition"": {
                ""BoolIfExists"": {
                    ""aws:MultiFactorAuthPresent"": false
                }
            }
        }


=> In effect it forces them to sts.get_session_token to get temp credentials to do any action

So on our EKS cluster i did kubectl edit configmap aws-auth
 and duly added:
  mapUsers: |
    - userarn: arn:aws:iam::xxxxxxxxxxxxx:user/bob
      username: bob
      groups:
        - system:masters

It works  great, bob is cluster-admin.. But without mfa!!
Basically the aws-iam-authenticator is just getting an sts token, and the token is enough for EKS to grant unlimited access to the cluster, and it is possible to get the token without MFA code.

Is it possible to modify the map so that there is a verification that MFA is enforced for the provided token?
What  strategy do you suggest for this?

Edited by: wngaws on Dec 3, 2018 10:04 AM

Edited by: wngaws on Dec 3, 2018 10:05 AM"
Amazon EKS	"Re: MFA enforcement for kubectl IAM users
Hi there,

IAM MFA support in kubectl is currently a feature request open with our EKS team. I have added +1 on your behalf.

Cheers,
Nitheesha"
Amazon EKS	"kubelet unauthorized communication with EKS api server
Hi,

I have a few nodes with the exact same IAM roles
some of them successfully register on the cluster and some can't

I tried to debug it and it seems to be some kind of an authorization issue with kubelet on these nodes
In /var/log/messages I see this message repeating:
kubelet: E1212 18:47:39.487089    3845 kubelet_node_status.go:106] Unable to register node ""ip-172-21-207-71.us-west-2.compute.internal"" with API server: Unauthorized

I ssh into the cluster and run /usr/bin/aws-iam-authenticator token -i <my-cluster>
I take the token from the returned JSON and run
curl -k https://XXXXXXXXXXXXX.sk1.us-west-2.eks.amazonaws.com/api/v1/nodes -H ""Authorization: Bearer $TOKEN""
on the nodes that successfully register into the cluster it returns a valid answer and on the others I get a Unauthorized response

The nodes have the exact same IAM role
What else can I look at to debug this?"
Amazon EKS	"Fargate integration?
Various sources mentioned before that EKS integration with Fargate is expected to be available in 2018, for example:
https://aws.amazon.com/blogs/aws/aws-fargate/ 
But Fargate home page removed such timeline even though it was there before. 
Seems AWS is weighting in more business reason than tech convenient for end user here, trying to sell ECS harder ? 

Anyone know the current status of EKS Fargate support?

Thanks!"
Amazon EKS	"aws-node Failed to communicate with K8S Server.
I created a cluster on AWS EKS a few days ago, and it has been running fine for 3-4 days until today.

When deploying a few new containers today, they entered in `ContainerCreating` state and were stuck, all of them failing with the same error message, for example:

```
  Warning  FailedCreatePodSandBox  2m3s (x772 over 17m)  kubelet, ip-10-0-79-115.ec2.internal  Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin cni failed to set up pod ""grafana-df9bfd765-lp4kf_monitoring"" network: rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: connection error: desc = ""transport: Error while dialing dial tcp 127.0.0.1:50051: connect: connection refused""
```

Checking all pods on all namespaces, I saw that a few `aws-node` pods are in `CrashLoopBackOff` state, returning the following error:

```
Failed to communicate with K8S Server. Please check instance security groups or http proxy setting%
```

Trying to terminate and reboot the machines containing the failed pods doesn't help - the pods continue to fail on the new machines that the Autoscaling Group starts to replace the terminated ones.

Any ideas how to debug this?"
Amazon EKS	"Re: aws-node Failed to communicate with K8S Server.
The problem turned out to be incorrect source group in a security group. Sorry for the noise."
Amazon EKS	"Resize PVC
Any ideas on how to resize a PVC in EKS?"
Amazon EKS	"Any news from AWS on CVE-2018-1002105 ?
It looks bad but I can't find any response from Amazon. Remote unauthenticated root access is not something I want on my EKS clusters. Console still only shows version 1.10 available, can't tell what minor release.

My current clusters are all:
Server Version: version.Info{Major:""1"", Minor:""10+"", GitVersion:""v1.10.3-eks"", GitCommit:""58c199a59046dbf0a13a387d3491a39213be53df"", GitTreeState:""clean"", BuildDate:""2018-09-21T21:00:04Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}

Still no managed upgrades either.

https://www.zdnet.com/article/kubernetes-first-major-security-hole-discovered/"
Amazon EKS	"Re: Any news from AWS on CVE-2018-1002105 ?
Hi,

We are also interested in hearing AWS' official word on the vulnerability. From what I can tell, the anonymous privilege escalation vulnerability banks on two features:


anonymous API access to kube-apiserver
existence of API aggregation support


AWS EKS uses custom patches on top of Kubernetes 1.10.3 (https://docs.aws.amazon.com/eks/latest/userguide/platform-versions.html), and it looks like the latest version enabled the aggregation layer. However, I can't tell if anonymous access is turned on. Other kubernetes management projects like kops appear to disable the anonymous access (https://github.com/kubernetes/kops/issues/6151), so I am wondering if EKS also does that as well.

Thanks!

Edited by: Yori on Dec 4, 2018 7:10 AM"
Amazon EKS	"Re: Any news from AWS on CVE-2018-1002105 ?
This wouldn't be nearly as bad if we could control security groups for accessing the API server (like we can for ELBs), but until this is fixed there's nothing we can do. I'm sweating bullets, here."
Amazon EKS	"Re: Any news from AWS on CVE-2018-1002105 ?
The lack of communication from Amazon on this is extremely frustrating. 

According to someone on reddit, this was posted in the AWS Developer Community Slack by a Containers team member:

Good morning folks.

Apologies for the delay. As of noon today, all new clusters will be running 1.10.11, which contains the patch for the aforementioned CVE. (edited) Over the next 24 hours, patching for all clusters will be complete. There is a mitigation step available via security group modification, as well. The security group modification is to revoke outbound access from the control plane security group to the worker nodes. This will obviously break exec, logs, and proxy.

Source: https://www.reddit.com/r/kubernetes/comments/a2tklj/kubernetes_first_major_security_hole_discovered/eb33h60/?utm_content=permalink&utm_medium=front&utm_source=reddit&utm_name=kubernetes"
Amazon EKS	"Re: Any news from AWS on CVE-2018-1002105 ?
Thanks for the info, MikeO22, I'm hoping this is real."
Amazon EKS	"Re: Any news from AWS on CVE-2018-1002105 ?
i caught this version change in our current eks instance

Server Version: version.Info{Major:""1"", Minor:""10+"", GitVersion:""v1.10.11-eks"", GitCommit:""6bf27214b7e3e1e47dce27dcbd73ee1b27adadd0"", GitTreeState:""clean"", BuildDate:""2018-12-04T13:33:10Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}

it seems likely that this is a hotfix for the cve although i cannot confirm that this is so.  hopefully there will be an official notification going out soon."
Amazon EKS	"Re: Any news from AWS on CVE-2018-1002105 ?
An announcement a couple of days ago indicates all EKS instances have been patched: https://aws.amazon.com/security/security-bulletins/AWS-2018-020/

December 5, 2018 12:00 PM PST
AWS is aware of a recent security issue within Kubernetes, assigned CVE identifier CVE-2018-1002105. Amazon Elastic Container Service for Kubernetes (EKS) manages the Kubernetes control plane on behalf of customers. We have completed patching of the Amazon EKS fleet. All Amazon EKS Kubernetes clusters are now running a version of Kubernetes that is not affected by this issue. No customer action is required to receive these updates."
Amazon EKS	"failed to save outputs: Access Denied
Hi folks, I was able to follow instructions in AWS documentaiton to generate an EKS cluster inside a VPC and create t2.medium node cluster.  

I am able to run hello world just fine through argo:
argo submit --watch https://raw.githubusercontent.com/argoproj/argo/master/examples/hello-world.yaml

However, when I run the artifact passing one, it fails with access denied.  I am guessing it is trying to connect to S3 and can't?  Any help in debugging would be great!

Command to run artifact passing: 
argo submit --watch https://raw.githubusercontent.com/argoproj/argo/master/examples/artifact-passing.yaml

Output:
STEP                       PODNAME                            DURATION  MESSAGE
 ‚úñ artifact-passing-swxfr                                               child 'artifact-passing-swxfr-3316651130' failed
 ‚îî---‚ö† generate-artifact   artifact-passing-swxfr-3316651130  2s        failed to save outputs: Access Denied"
