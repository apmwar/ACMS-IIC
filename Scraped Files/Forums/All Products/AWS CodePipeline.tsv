label	description
AWS CodePipeline	"CodePipeline setup an ECR event in CloudWatch but event is in CloudTrail
I am setting up a really simple CodePipeline.

I have an ECR docker registry, I have ECS with Fargate as the cluster, and I have a CodePipeline. 

What I want to do is when I put a new image with the tag ""deploy"" on my docker registry. I want to trigger the CodePipeline to start the deployment. Right now I have to manually go to the CodePipeline page and click on ""Release Change"". I don't want to do that. I was researching this a bit more and I saw there are specific CloudWatch events that CodePipeline is looking for to trigger the build. 

I took a look at my CloudWatch events are and sure enough CodePipeline had defined one. Please see the rule below. 

The problem is I never see this event occurring in CloudWatch it seems the ""putImage"" event is only in CloudTrail. I don't want to integrate with CloudTrail if I can avoid it since it will make this more complex then it needs to be. 

Rules > codepipeline-thx-deploy-556745-rule
{
  ""source"": [
    ""aws.ecr""
  ],
  ""detail"": {
    ""eventName"": [
      ""PutImage""
    ],
    ""requestParameters"": {
      ""repositoryName"": [
        ""thx""
      ],
      ""imageTag"": [
        ""deploy""
      ]
    }
  }
}"
AWS CodePipeline	"Re: CodePipeline setup an ECR event in CloudWatch but event is in CloudTrail
By default CloudTrail should log ""management events"" via CloudTrail event history without any extra configuration: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events.html

So, your CloudWatch event rule will use CloudTrail data but you shouldn't need to configure anything in CloudTrail to make it work. In other words, it will use CloudTrail but shouldn't add any extra complexity."
AWS CodePipeline	"Re: CodePipeline setup an ECR event in CloudWatch but event is in CloudTrail
Hi Tim,
Thanks for replying. 

Here is what I am doing:
1. I am pushing a docker image to ECS
2. I am going to CloudWatch > Events > Rules > click on ""Show metrics for rule""
I don't see anything. The rule is active. 
Would have expected the build to be triggered but it not being triggered. 

When I go to CloudTrail I see this:
2018-11-29, 04:42:16 PM
	
ThxDockerRepo
	
PutImage
	
	
 
AWS access key
 
<Removed>
 
AWS region
 
us-east-1
 
Error code
 
Event ID
 
0fea429d-0e37-42d7-8d68-d495464f5f46
 
Event name
 
PutImage
 
Event source
 
ecr.amazonaws.com
 
Event time
 
2018-11-29, 04:42:16 PM
 
Read only
 
false
 
Request ID
 
a403b5b3-f41f-11e8-9b02-2bf0c2c2ecf0
 
Source IP address
 
AWS Internal
 
User name
 
ThxDockerRepo


Because nothing shows up in CloudWatch I think that is the reason behind why it won't automatically trigger the build.

I don't know exactly how this works but could this be because CloudWatch is looking for an source value of:
aws.ecr
And CloudTrail has an event source value of: 
ecr.amazonaws.com

I don't know if this is a bug I could be doing something wrong, but I was expecting the CloudWatch metric to pick up the putImage. 

Also, someone may want to update the document here: 
https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html
If ECR is suppose to be support.

Edited by: PankilSmile on Nov 29, 2018 2:10 PM
Message quota reached trying to edit this message instead."
AWS CodePipeline	"Re: CodePipeline setup an ECR event in CloudWatch but event is in CloudTrail
Hello, 

Thank you for your valuable feedback on the AWS CodePipeline User Guide.

Thank you for your input regarding needed updates for ECR source change detection. This has now been corrected. This change detection methods table has been updated for ECR: https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html#change-detection-methods

Thank you again for pointing out this issue, and sorry for any confusion while the information was missing."
AWS CodePipeline	"Re: CodePipeline setup an ECR event in CloudWatch but event is in CloudTrail
No problem."
AWS CodePipeline	"I can't use aws pipeline (Frankfurt) with aws deployment (Stockholm)
Hi All,
I'm working as a dev-op engineer in Stockholm based company and now we are in process of migrating our servers from Frankfurt Region to Stockholm Region so far we are going good.
During the process I learn we don't have code-commit,code-pipeline and code-build in Stockholm Region but Codedeploy available, so I thought it's no problem because we can use already setupped Pipeline in Frankfurt Region with  Stockholm Region code deploy. But from the console, I can't select the Stockholm Region. I can see all of the other regions can someone help me with this?

Thanks!"
AWS CodePipeline	"Re: I can't use aws pipeline (Frankfurt) with aws deployment (Stockholm)
Hi, CodePipeilne is not available in Stockholm yet, therefore, cross-region action in Stockholm not available neither.
We understand that it is important to support pipeline in Stockholm and we are working on it.

Edited by: jayzaws on Feb 27, 2019 11:52 AM"
AWS CodePipeline	"Github MonoRepo as source
We use the monorepo approach for storing our source in github. 
Is it currently possible to have CodePipeline trigger only on a commit to a particular subfolder.
This is something that is currently possible with TeamCity by setting a filter on the Source Repository but I've seen no example of this with CodePipeline."
AWS CodePipeline	"Re: Github MonoRepo as source
Hello,

Unfortunately, CodePipeline doesn't support this as of now. Thanks for bringing this to our attention, I've conveyed this feature request to the service team, however, we don't have any ETA for this feature.

As a workaround you can get a list of files changed in commit using GitHub API [1], [2] or command line (if you are using webhooks [3] and git pull method):
git diff-tree --no-commit-id --name-only -r  <commit_id>


Then you can check which folders have changed and implement a custom logic (in CodeBuild or Lambda actions) based on this.

Also, consider using submodules for your project [4] and have custom pipelines for all required sub-repositories.

Best,
Ruslan.

[1] https://developer.github.com/v3/git/commits/#get-a-commit
[2] https://developer.github.com/v3/repos/contents/#get-contents
[3] https://aws.amazon.com/blogs/devops/integrating-git-with-aws-codepipeline/
[4] https://git-scm.com/book/en/v2/Git-Tools-Submodules"
AWS CodePipeline	"Re: Github MonoRepo as source
+1 on this being a valueable feature."
AWS CodePipeline	"Re: Github MonoRepo as source
+1 for mono repo support

We have code for Lambda functions and front end apps (hosted in S3) all in one repo. Ideally a commit would trigger a rebuild and deploy of all the lambda functions and front end apps"
AWS CodePipeline	"Re: Github MonoRepo as source
+1 for mono repo support"
AWS CodePipeline	"Re: Github MonoRepo as source
+1"
AWS CodePipeline	"Re: Github MonoRepo as source
Monorepo will be critical for our team as well. We will use lambda events as a workaround in the meantime. Is there anywhere we can track progress for this feature request?"
AWS CodePipeline	"Re: Github MonoRepo as source
+1 The need for monorepo support is strong."
AWS CodePipeline	"Re: Github MonoRepo as source
How is it that you suggest the use of submodules when you lack fundamental support for it?

See https://forums.aws.amazon.com/thread.jspa?threadID=248267"
AWS CodePipeline	"Re: Github MonoRepo as source
+1 monorepo support is needed"
AWS CodePipeline	"CodePipeline ECS blue/green deployment action fails with an InternalError
I have been trying to build a CodePipeline pipeline that would build and push a Docker image to ECR, and deploy it using CodeDeploy and ECS blue/green deployment in a single pipeline – as opposed to splitting building and deployment to two separate pipelines, where the latter is triggered from ECR source. After some trial and error it seemed that things were starting to work out, but then the deployment action started failing with just

Action execution failed
InternalError. Error reference code: f1618295-2d5b-4cf2-8306-de4df828639c.

The pipeline consists of 4 stages:

1. Source: a CodeCommit repository
2. Build: compiles a JAR that is available as an artifact
3. Image: builds a Docker image that contains the JAR artifact and pushes the image to ECR – produces imageDetail.json containing just the image URI as an artifact
4. Deploy: the ECS blue/green CodeDeploy deployment action

The JAR is built just fine and the image pushed to ECR, and the CodeDeploy deployment works, if initiated by updating the ECS service manually. I have tried removing and rebuilding the pipeline and deployment app/group. I also built a separate pipeline, with an ECR source in case my own imageDetail.json was flawed, but still the deployment action dies of an internal error.

My question is: is there a way to debug such internal errors, other than start from scratch and hope?

The source contains the following for use with the deployment:

appspec.yaml:
version: 0.0
Resources:
  - TargetService:
      Type: AWS::ECS::Service
      Properties:
        TaskDefinition: <TASK_DEFINITION>
        LoadBalancerInfo:
          ContainerName: ""DemoContainer""
          ContainerPort: 8080


taskdef.json:
{
  ""executionRoleArn"": ""arn:aws:iam::...:role/ecsTaskRole"",
  ""containerDefinitions"": [
    {
      ""name"": ""DemoContainer"",
      ""image"": ""<IMAGE>"",
      ""essential"": true,
      ""portMappings"": [
        {
          ""hostPort"": 0,
          ""protocol"": ""tcp"",
          ""containerPort"": 8080
        }
      ],
      ""logConfiguration"": {
        ""logDriver"": ""awslogs"",
        ""options"": {
          ""awslogs-group"": ""/ecs/DemoTask"",
          ""awslogs-region"": ""eu-west-1"",
          ""awslogs-sream-prefix"": ""ecs""
        }
      }
    }
  ],
  ""requiresCompatibilities"": [
    ""EC2""
  ],
  ""cpu"": ""256"",
  ""memory"": ""512"",
  ""family"": ""DemoTask""
}


The image details are produced using:

version: 0.2
 
phases:
 pre_build:
   commands:
     - echo Logging in to Amazon ECR...
     - $(aws ecr get-login --no-include-email --region eu-west-1)
     - ...
     - REPOSITORY_URI=...
     - IMAGE_TAG=build-$(echo $CODEBUILD_BUILD_ID | awk -F"":"" '{print $2}')
 build:
   commands:
     - echo Build started on `date`
     - echo Building the Docker image...
     - docker build -t test:latest .
     - docker tag test:latest $REPOSITORY_URI:latest
     - docker tag test:latest $REPOSITORY_URI:$IMAGE_TAG
     - printf '{""ImageURI"":""%s""}' $REPOSITORY_URI:$IMAGE_TAG > imageDetail.json
 post_build:
   commands:
     - echo Build completed on `date`
     - echo Pushing the Docker image...
     - docker push $REPOSITORY_URI:latest
     - docker push $REPOSITORY_URI:$IMAGE_TAG
artifacts:
     files: imageDetail.json


Edited by: IljaE on Dec 10, 2018 11:47 PM

Edited by: IljaE on Dec 12, 2018 12:18 AM

Edited by: IljaE on Dec 14, 2018 3:45 AM"
AWS CodePipeline	"Re: CodePipeline ECS blue/green deployment action fails with an InternalError
Found the culprit: the awslogs-stream-prefix log configuration option seems incompatible with the EC2 ECS cluster I've running; the container instances are old. After removing said option the deployment action succeeds. Seems I've some more reading to do about the options and their usage.

Registering a task definition and running a deployment based on it manually provided better error messages that lead to this conclusion."
AWS CodePipeline	"Re: CodePipeline ECS blue/green deployment action fails with an InternalError
hi,

i came across a similar issue today. my ecs service is configured with bluegreen deployment model and i can successfully trigger the new service deployment with an updated version of task definition related to my ecs service. however, when i config the deploy phase with ecs for my pipeline, i came across the issue similar to you:
Action execution failed:
InternalError. Error reference code: a0572548-f3bb-4e9e-b9cd-7ef4d08d9389.

any suggestion on this or where i can find the debug log info for this issue?"
AWS CodePipeline	"Re: CodePipeline ECS blue/green deployment action fails with an InternalError
@xiaoyzhu I'm having the same issue as you. When I update the fargate task, it deploys fine, but when CodePipeline tries to deploy using ECS Blue/Green, I get InternalError. Error reference code: 0d3cd84c-5650-417a-b9c9-fb44adbed969.


Any ideas of what that means or how to diagnose?"
AWS CodePipeline	"Re: CodePipeline ECS blue/green deployment action fails with an InternalError
I have some additional information that helped resolve this for me. I hope it helps someone else searching for this issue.

I needed to update my buildspec.yaml file with the following details:

version: 0.2
### Defines the steps used by AWS CodeBuild
 
phases:
  pre_build:
    commands:
      - echo Build started on `date`
      - echo Logging in to Amazon ECR...
      - aws --version
      - $(aws ecr get-login --region $AWS_DEFAULT_REGION --no-include-email)
      - COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)
      - IMAGE_TAG=${COMMIT_HASH:=latest}
  build:
    commands:
      - echo Building the Docker image...
      - docker build -t $REPOSITORY_URI:latest -t $REPOSITORY_URI:$IMAGE_TAG -f Dockerfile .
  post_build:
    commands:
      - echo Pushing the Docker images to container registry...
      - docker push $REPOSITORY_URI:latest
      - docker push $REPOSITORY_URI:$IMAGE_TAG
      - echo Writing image definitions file...
      - printf '{""ImageURI"":""%s""}' $REPOSITORY_URI_WEB:$IMAGE_TAG > imageDetail.json
      - echo Build completed on `date`
artifacts:
  files:
    - appspec.yaml
    - taskdef.json
  secondary-artifacts:
    DefinitionArtifact:
      files:
        - appspec.yaml
        - taskdef.json
    ImageArtifact:
      files:
        - imageDetail.json


Note the ""secondary-artifacts"" portion, where I define the ImageArtifact (build image definition) separately from the task/app definition files.

Then, when defining the CodeBuild stage in CodePipeline, make sure to specify DefinitionArtifact
 and ImageArtifact
 as Output artifacts for the stage.

The taskdef.json file should have ""image"": ""<IMAGE1_NAME>"",
 specified in the container definition. This placeholder will be replaced by CodeDeploy ECS Blue/Green when it is part of a CodePipeline.

For what it's worth, my appspec.yaml file looks like:
version: 0.0
 
Resources:
  - TargetService:
      Type: AWS::ECS::Service
      Properties:
        TaskDefinition: <TASK_DEFINITION>
        LoadBalancerInfo:
          ContainerName: ""web""
          ContainerPort: 80
        PlatformVersion: ""LATEST""


Again, the <TASK_DEFINITION>
 placeholder is replaced by CodePipeline. 

Finally, I used the following configuration for CodeDeploy as part of my CodePipeline deployment stage:
Action Provider:
  Amazon ECS (Blue/Green)
Input Artifacts:
  DefinitionArtifact
  ImageArtifact
Amazon ECS Task Definition:
  DefinitionArtifact: taskdef.json
Amazon CodeDeploy AppSpec file:
  DefinitionArtifact: appspec.yaml
Dynamically update task definition image:
  ImageArtifact
  IMAGE1_NAME"
AWS CodePipeline	"Re: CodePipeline ECS blue/green deployment action fails with an InternalError
Apologies, there was a slight bug in the buildspec.yaml above. This is what it should be:

version: 0.2
### Defines the steps used by AWS CodeBuild
 
phases:
  pre_build:
    commands:
      - echo Build started on `date`
      - echo Logging in to Amazon ECR...
      - aws --version
      - $(aws ecr get-login --region $AWS_DEFAULT_REGION --no-include-email)
      - COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)
      - IMAGE_TAG=${COMMIT_HASH:=latest}
  build:
    commands:
      - echo Building the Docker image...
      - docker build -t $REPOSITORY_URI:latest -t $REPOSITORY_URI:$IMAGE_TAG -f Dockerfile .
  post_build:
    commands:
      - echo Pushing the Docker images to container registry...
      - docker push $REPOSITORY_URI:latest
      - docker push $REPOSITORY_URI:$IMAGE_TAG
      - echo Writing image definitions file...
      - printf '{""ImageURI"":""%s""}' $REPOSITORY_URI:$IMAGE_TAG > imageDetail.json
      - echo Build completed on `date`
artifacts:
  files:
    - appspec.yaml
    - taskdef.json
  secondary-artifacts:
    DefinitionArtifact:
      files:
        - appspec.yaml
        - taskdef.json
    ImageArtifact:
      files:
        - imageDetail.json"
AWS CodePipeline	"Not setting executable bit/permissions from git on script files
I've got a project in Github that I deploy to Elastic Beanstalk via ""eb deploy"" and it properly sets the executable bit on the script files when it deploys them to the server.

When I deploy the same repo to the same Elastic Beanstalk servers with CodePipeline, the executable bit is NOT set on the files. They are all 644 rather than 755.

Anyone else have this issue?

Seems like I shouldn't have to write some post-deploy script to chmod +x fix this. It should be respecting the file permissions stored in git."
AWS CodePipeline	"Re: Not setting executable bit/permissions from git on script files
Hi Chris,

Currently our recommendation is to use .ebextensions to set file permissions as you suggested. This will ensure that the permissions are set as you expect regardless of the container environment.

I've made a note of your use-case and we'll update this thread when we're able to provide better support for this in the future.


Tim."
AWS CodePipeline	"Re: Not setting executable bit/permissions from git on script files
I am running into what sounds like the same problem, in a slightly different form not involving Elastic Beanstalk.

I have a CodePipeline that uses a github repo in a Source step, followed by a CodeBuild step. Within the repo are several scripts with their executable bits set (stored with permissions 0755 in github). The executable bits are critical since the scripts need to get called as part of the build+unittest process.

When the CodeBuild step runs within CodePipeline, the executable bit is lost. If I directly download from S3 the zip file that the Source step created, I can see that the executable bit is already gone in that zip file (i.e. the executable bit is not getting lost because of anything that the CodeBuild step is/isn't doing).

If I run the CodeBuild job on its own (outside of CodePipeline), then CodeBuild fetches the source directly from github (rather than being fed a zip file from the CodePipeline/Source step), and in this case the executable bit is correctly preserved.

Am I missing something in how to keep the executable bits properly preserved, or is there a workaround? Or any progress/ETA toward a fix?

Thanks,
Rob"
AWS CodePipeline	"Re: Not setting executable bit/permissions from git on script files
Hi RobPeters, 

our current recommendation is to replace ""script"" invocation by ""<interpreter> script"", ie: if you have a shell script named my-script.sh, you may be able to pass it as an argument directly to your shell, replace that with ""/bin/bash my-script.sh""."
AWS CodePipeline	"Re: Not setting executable bit/permissions from git on script files
Thanks for the reply. Yes that is what I'm doing for now - using ""bash myscript.sh"" to call it from buildspec.yml. However if myscript.sh also calls out to other script/executables that are involved in build or unit tests, which in turn call others etc., then having to change all those references to explicitly say ""<interpreter> ..."" is quite an invasive and unmaintainable workaround.

Do you have any information about whether there is work in progress to fix this problem in CodePipeline?

Best,
Rob"
AWS CodePipeline	"Re: Not setting executable bit/permissions from git on script files
Agreed - this is a totally unmaintainable suggestion.  It looks like they are unzipping to a filesystem with noexec and then copying the source or just not copying the source and preserving permissions.  It's unrealistic to go through bash scripts that call bash scripts and implement that ugly hack.

More to the point, why do we have two different behaviors?  CodeBuild vs CodePipeline?

Edited by: rlauer6@comcast.net on Mar 15, 2017 11:58 AM"
AWS CodePipeline	"Re: Not setting executable bit/permissions from git on script files
+1 on RobPeters' comment -- I just ran into this on CodeBuild and it's a problem that the Execution bit isn't preserved in the cloned copy of my repo.  Why remove the execution bit?
(BTW I can chmod +x the files so it's not because /tmp is noexec or something like that -- the permissions simply aren't being preserved).  Would really appreciate a fix for this or something better than a ""chmod -R +x ."" because I can't predict which files are supposed to be executable.

Edited by: goodwinb99 on May 5, 2017 12:51 PM"
AWS CodePipeline	"Re: Not setting executable bit/permissions from git on script files
I have the same issue.

Unfortunately, none of the solutions suggested here are appropriate for me because the files I care about are being referenced in a Docker image build.

This effectively results in a complete rebuild each time my CodePipeline -> CodeBuild jobs execute, as the file permissions in existing image cache are now inconsistent with what's on the file system, rendering image cache pretty much useless.

Does anyone at AWS actually care about this? This thread is over two years old and the only suggested solutions are work arounds for specific problems and a casual ""It's still your problem"" post on the Troubleshooting page: https://docs.aws.amazon.com/codepipeline/latest/userguide/troubleshooting.html#troubleshooting-file-permissions"
AWS CodePipeline	"Re: Not setting executable bit/permissions from git on script files
Similar issue here. I'm quite surprised to find this behavior.

Creating a CodePipeline that builds a docker image from GitHub source, but the executable bits for the executables inside the repo are lost.
As reported by others, when running CodeBuild directly (without CodePipeline) it pulls the code directly from GitHub and this is not an issue.

Anyone from AWS: It's unacceptable that the files from the git repo are altered, is this on your roadmap to fix? There is currently no (sensible) work-around for this.

Edited by: rtuin on Feb 25, 2019 11:37 PM"
AWS CodePipeline	"Set environment variable on CodeBuild Action
Hi,

In my pipeline, one of my actions is to call a pre-defined CodeBuild build to compile Java source code. The configuration for the CodeBuild build allows me to set environment variables to control the way in which the build operates.

When running the CodeBuild build manually, I can set the environment variables before starting the build. How do I achieve the same thing from my CodePipeline? As far as I can see there is no way to define the values of environment variables set on CodeBuild actions.

cheers,

Rob"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
Hi,

When creating the CodeBuild project from CodePipeline you can expand the ""Advanced"" panel and it should give you the option to specify environment variables.

Alternatively you can create a build project from the CodeBuild console with environment variables and use it from CodePipeline.

 - Tim."
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
Would it be possible to set them for each action in CloudFormation templates? Currently, I can specify environmental variables in CodeBuild project resource, but there's no place to override them for a CodeBuild action in CodePipeline resource.

Thank you"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
Thanks for the feedback!

Currently you cannot override environment variables on a per-action basis in CodePipeline, you'd need to have separate build projects.

At the time of writing there is no per-project cost for AWS CodeBuild, so while it will require some additional setup it won't cost you anything extra.

I've created a feature request for supporting overriding CodeBuild environment variables in a per-action basis.

 - Tim."
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
Thanks, really appreciated!"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1 on the feature request, copy/paste isn't great"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1  ... would really like to see this feature added.   

At present, if I want to set up a CI pipeline consistent with the GitFlow branching model http://nvie.com/posts/a-successful-git-branching-model/ , I'm forced to:
1) create a pipeline to pickup commits to develop branch
2) create a codebuild project deploy to the dev environment
3) create a pipeline to pickup commits to a release branch
4) create a codebuild project deploy to the QA environment

This is just a basic CI pipeline of course...  
Ideally a single code pipeline could be triggered by commits to a number of branch regex patterns that I specify e.g. develop, release-* then the branch name could be passed as an environment variable to a single CodeBuild project.   From there the buildspec.yml could pick it up and decide where to deploy the code based on the branch being built."
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
Hi Tim,

some news about the feature request ?

A workaround might have been to use the action ""name"" of the pipeline launching the CodeBuild.
Example : 
Action ""BuildType1"" > CodeBuildProject
Action ""BuildType2"" > CodeBuildProject
If CodeBuildProject knows that the action name is BuildType1, it could define some environment variables for the type 1, or other values for the type 2.
Unfortunately, CodeBuild only knows the pipeline that triggered it (CODEBUILD_INITIATOR) but neither the action nor the stage.

Thx

Edited by: julamand on Nov 29, 2017 7:27 AM"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1 for feature request. 

Any update on this?"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1, would find this feature very helpful"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1"
AWS CodePipeline	"Re: Set environment variable on CodeBuild Action
+1 - this would be really helpful so as not to have to maintain a separate CodeBuild and CodePipeline project for every environment of an app.

Edited by: MattSavino on Nov 7, 2018 2:56 PM"
AWS CodePipeline	"How to deploy CICD dev/test/prod S3 web site across accounts?
We have been wrestling with automated deployment of web site content to S3 buckets in development, test and production environments. Most of the examples I have been able to find online relate to deploying Lambda functions, CloudFormation stacks or EC2 content. There are some specific problems with deploying to S3 which we have come across. This must be a solved problem, so what is the best/correct way?

We have 2 accounts in scope, let's call them x06 (Development) and x14 (Test & Prod) We are using CodePipeline running in the x06 account to build our web sites. Deployment of the compiled sites within the x06 account as the dev environment is not a problem. We are using the new (Jan 2019) CodePipeline ""Amazon S3"" Action to deploy. After the timeout problem was solved https://forums.aws.amazon.com/thread.jspa?threadID=296949 this has worked well for all of our current projects.

Deployment to test (and eventually production) is another matter. Ideally we would take the CodePipeline build artifact and unzip it into the destination. This has been our experience:

1. ""Amazon S3"" action. Needs all the right permissions of course, and that isn't part of our problem. This can deploy the objects to the S3 bucket in the x14 account, but the objects are all owned by the x06 root account. There is of course no option to use --acl bucket-owner-full-control here, but that probably won't make a difference (see 2). For some reason the bucket policy does not allow our CloudFront Origin Access Identity to get at the objects under these circumstances, so the website shows ""Access Denied"". If we could run a function to set ownership of all object to the bucket owner, I suspect this route might work and would probably count as ""best"" solution to me at least.

2. Use a CodeBuild project to deploy with a yml file. Here we can of course use --acl bucket-owner-full-control but the result is the same as 1. Objects are owned by the x06 root account and the websites show ""access denied"" to all objects. This option is of course non-ideal due the the extra time taken to spin up the container to run the project. I'm not sure how easy it would be to use alternate credentials within the yml file. Obviously we'd prefer not to hard-code these in our scripts.

3. Use Jenkins. As a couple of our web sites are written in Flare, they must be compiled in a Windows Environment, so we use Jenkins to handle this with x06 credentials. That works fine, more or less. Trying to use Jenkins for the deployment did not work for us as we gave the Jenkins job x14 credentials to avoid the S3 object ownership problem. From the x14 account, Jenkins is unable to interact properly with CodePipeline in the x06 account. This may work using x06 credentials and then hard-coding x14 credentials within the script, for the s3 cp or possibly storing them in Jenkins's secrets area, but we've not tried that yet. Using Jenkins for this feels like a ""sledgehammer to crack a nut"".

4. Use ""Invoke Lambda"". Even running within the x06 account, and using x14 credentials for the S3 PutObject calls, this nearly worked. The objects are deployed to the correct place and are owned by the x14 root account ... but the process for extracting files from the Input Artifact zip does not preserve the ContentType metadata and all objects arrive with a binary content-type. I can't recall the exact value, but the website doesn't work of course. In theory we could write our own large switch to apply the correct metadata from the ""file extension"", but that just feels like the wrong way to go. Also the Lambda function takes a long time to run, 3 sec for our tiny site, probably a minute or two for the larger ones. This doesn't seem to be a good approach.

We're not looking for alternative ideas, we can think of a few more ourselves! We're looking for someone who is already doing this to tell us ""this way works for us"".

Thanks for reading this far

Regards

Mark"
AWS CodePipeline	"Re: How to deploy CICD dev/test/prod S3 web site across accounts?
Hello!

I'm one of the CodePipeline developers who wrote the S3 Deploy action type.

Since you're trying to deploy to S3, I would prefer to focus on your first solution, to try to sort out the kinks in the S3 Deploy action type, instead of having you develop your own thing which does roughly the same thing, except with a lot more effort from your side (our goal is to make it easy to use, of course). I personally use the S3 Deploy action type to deploy a React application so I am quite proud of it and would love to make sure it works for you too.

I'm interested in how you've set up your bucket so that only CloudFront can access it - could you show me what kind of bucket policy you're using? When I've used this feature, I've just set the bucket policy wide open, so I haven't had any permission problems, but your solution sounds better. Do you use this same bucket policy in your beta accounts? Does it work when the object is owned by the same account as the bucket? This information will help me reproduce your pipeline better so I can work on a solution.

I've heard other people ask to be able to specify object ACL templates in their S3 Deploy action, and it makes sense as a feature. I've made a note of these requests.

Thanks for using CodePipeline!

Matthew"
AWS CodePipeline	"Re: How to deploy CICD dev/test/prod S3 web site across accounts?
I did a bit more digging and found some interesting things.

Using a static S3 hosting bucket I have with a permissive bucket policy, I created an object which is not owned by the bucket owner. I discovered that trying to access this object using s3 static hosting gave me a 403 Forbidden error.

I then updated the object ACL on the object I just uploaded to ""bucket-owner-read"" but still I got the error.

Then I found this link: https://aws.amazon.com/premiumsupport/knowledge-center/s3-website-cloudfront-error-403/

It says ""The AWS account that owns the bucket must also own the object."" - so it looks like it might be a requirement that the object be owned by the bucket owner.

Now, this is actually achievable using the S3 Deploy action, using action roles.

I don't think the action role feature is documented anywhere (I have let the docs team know) but I can just summarize it here.

In your action declaration, you can specify a role that the pipeline will assume to execute your action. This role can be in your prod accounts (x14) and needs the right permissions to access your artifact bucket in your dev account, and deploy to your bucket. Additionally, your pipeline role needs AssumeRole permissions on your action role.

Update your action declaration like this:

{
  ""inputArtifacts"": [
    { ""name"": ""s3artifact"" }
  ],
  ""outputArtifacts"": [],
  ""name"": ""ActionName"",
  ""region"": ""Region"",  //optional
  ""actionTypeId"": {
    ""category"": ""Deploy"",
    ""owner"": ""AWS"",
    ""version"": ""1"",
    ""provider"": ""S3""
  },
  ""configuration"": {
    ""Extract"": ""true"",
    ""BucketName"": ""prod-bucket-name""
  },
  ""roleArn"": ""arn:aws:iam::x14:role/S3DeployRole"" // <-- new field
}


Could you try this out and see whether it works for you?"
AWS CodePipeline	"Re: How to deploy CICD dev/test/prod S3 web site across accounts?
Hello Matthew,

Thanks for taking the time on this. And thank you for the CodePipeline S3 deploy action. It is working nicely within our dev account. 

I agree, getting it to work across accounts would be the best answer to our problem. I have tried using the role_arn (I'm using Terraform and it recognises the field OK which was unexpected!). I had to amend the prod account deployment role to permit the CodePipeline service to use it before I could configure it. I'm now getting an internal error from the action. Sadly I can't see where to look for any further cause of the error. I've tried adjusting the bucket policy, to no avail. I suspect some permission problem somewhere...

You also asked about our S3 production & test bucket policies. They are like this (after the adjustments noted above):
{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Sid"": ""Allow CloudFront and cross account access to bucket objects"",
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""AWS"": [
                    ""arn:aws:iam::<<DevAccountNo>>:root"",
                    ""arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity <<ID>>"",
                    ""arn:aws:iam::<<ProdAccountNo>>:root"",
                    ""arn:aws:iam::<<ProdAccountNo>>:role/<<DeployRole>>""
                ]
            },
            ""Action"": ""s3:GetObject"",
            ""Resource"": ""arn:aws:s3:::<<ProdDeployBucket>>/*""
        },
        {
            ""Sid"": ""Allow CloudFront and cross account access to push bucket objects"",
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""AWS"": [
                    ""arn:aws:iam::<<DevAccountNo>>:<<CodepipelineRole>>"",
                    ""arn:aws:iam::<<DevAccountNo>>:root"",
                    ""arn:aws:iam::<<ProdAccountNo>>:role/<<DeployRole>>""
                ]
            },
            ""Action"": [
                ""s3:PutObject"",
                ""s3:DeleteObject""
            ],
            ""Resource"": ""arn:aws:s3:::<<ProdDeployBucket>>/*""
        },
        {
            ""Sid"": ""Allow CloudFront and cross account access to bucket"",
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""AWS"": [
                    ""arn:aws:iam::<<DevAccountNo>>:root"",
                    ""arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity <<ID>>"",
                    ""arn:aws:iam::<<ProdAccountNo>>:root"",
                    ""arn:aws:iam::<<ProdAccountNo>>:role/<<DeployRole>>""
                ]
            },
            ""Action"": ""s3:ListBucket"",
            ""Resource"": ""arn:aws:s3:::<<ProdDeployBucket>>""
        }
    ]
}"
AWS CodePipeline	"""Unexpected"" internal error preventing code build deploy to S3 via pipeline
I have a simple pipeline which imports code from github, runs an npm command on the code (via CodeBuild) and attempts to upload to my S3 bucket.

When I remove the middle step, the deploy stage runs fine using just the unbuilt code from github. Yet when introducing codebuild and attempt to deploy the successfully built code, the deploy times out and fails.

The error message just says ""InternalError. Error reference code...""

When I look at the build in CodeBuild, it has built successfully but when I try to view the artifacts I just get the message ""Error An unexpected error occurred.""

Edited by: adamgrant on Feb 22, 2019 5:13 PM

Edited by: adamgrant on Feb 22, 2019 5:14 PM"
AWS CodePipeline	"Slow Source stage from GitHub
We have an 8.6 MB GitHub repo. Our CodePipeline detects changes in the branch we are monitoring right away, but the source stage takes 4.5 minutes to complete. This seems excessive. Is there anything we can do to speed this up?"
AWS CodePipeline	"Re: Slow Source stage from GitHub
Hello!

I'm sorry that your experience using the GitHub source action has been so frustrating.

4.5 minutes for an 8MB download seems very unusual! I looked at our metrics and only our most exceptional cases take that long - usually customers that have extremely large artifacts (this isn't one).

What region do you experience this problem in? Is it all the time or only occasionally?

Edited by: mattsainsaws on Feb 22, 2019 2:31 PM"
AWS CodePipeline	"AWS Codepipeline deploy to S3 bucket
I have a problem when deploying to s3 using Codepipeline. I have two accounts in use:
Account A has codebuild and codepipeline
Account B has target S3 bucket.

With bucket policy in Account B i can give access to Account A to write objects to the target S3 bucket. However, only Account A have access to the objects, not Account B.
I am using CloudFront to access the files, which works fine if I upload them from Account B. My bucket policy looks like this:
{
    ""Version"": ""2008-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""AWS"": ""arn:aws:iam::cloudfront:user/CLOUDFRONT:OAI""
            },
            ""Action"": ""s3:GetObject"",
            ""Resource"": ""arn:aws:s3:::S3_BUCKET/*""
        },
        {
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""AWS"": ""arn:aws:iam::ACCOUNT_A:root""
            },
            ""Action"": [
                ""s3:PutObject"",
                ""s3:GetObject"",
                ""s3:ListBucket""
            ],
            ""Resource"": [
                ""arn:aws:s3:::S3_BUCKET/*"",
                ""arn:aws:s3:::S3_BUCKET""
            ]
        }
    ]
}


You could fix this by giving all objects separately ACL bucket_owner_full_control. But you cannot give it to bucket upon creation. Is there a way around this? Thanks!"
AWS CodePipeline	"Re: AWS Codepipeline deploy to S3 bucket
Update:
Figured out that using Bucket Policies is not the way to go here (correct me if I am wrong). I tried using roles instead, but for some reason Codepipeline does not want to assume the role. I can assume the role, and put objects in the chosen bucket. But when running the pipeline I only get this error:
{
                    ""actionName"": ""Deploy"", 
                    ""entityUrl"": ""https://s3.console.aws.amazon.com/s3/buckets/S3_BUCKET_NAME/?region=us-east-1&tab=overview"", 
                    ""latestExecution"": {
                        ""status"": ""Failed"", 
                        ""errorDetails"": {
                            ""message"": ""InternalError. Error reference code: 30db0ceb-ed42-4bdb-8132-8e41dfb22737."", 
                            ""code"": ""JobFailed""
                        }, 
                        ""lastStatusChange"": 1548754180.151, 
                        ""summary"": ""Deployment Failed. Retrying""
                    }
                }


I assume the role with Codepipeline using this:
- Effect: Allow
            Action: sts:AssumeRole
            Resource:
            - !Sub arn:aws:iam::${AccountId}:role/S3_BUCKET_ROLE


And this is my role (that is being assumed by Codepipeline):
Type: AWS::IAM::Role
    Properties:
      RoleName: S3_BUCKET_ROLE
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
        - Effect: Allow
          Principal:
            AWS: !Ref OrgAccountId
          Action: sts:AssumeRole
      Policies:
      - PolicyName: Access-Front-bucket
        PolicyDocument:
          Version: 2012-10-17
          Statement:
          - Effect: Allow
            Action:
            - s3:PutObject
            - s3:PutObjectAcl
            - s3:GetObject
            - s3:ListBucket
            Resource:
            - !Join ['',['arn:aws:s3:::',!Ref FrontS3Bucket, '/*']]
            - !Join ['',['arn:aws:s3:::',!Ref FrontS3Bucket]]
          - Effect: Allow
            Action:
            - iam:PassRole
            Resource: '*'
          - Effect: Allow
            Action: kms:*
            Resource: !Ref CMKArn"
AWS CodePipeline	"Re: AWS Codepipeline deploy to S3 bucket
I was able to fix this myself. I had forgot to give access to the role to access Artifact bucket."
AWS CodePipeline	"Re: AWS Codepipeline deploy to S3 bucket
Hello!

Thanks for posting, and sorry for taking so long to respond.

I took a look at the InternalError you received, and you were right, it was a permissions issue. Internally, we try to call s3.GetObjectMetadata on the input artifact, and the role didn't have permissions for that.

I'm sorry you had to deal with such a vague error message, and I'll work on fixing that error so it surfaces the error we got from S3 so that next time it's easier to debug 

Matthew"
AWS CodePipeline	"Re: AWS Codepipeline deploy to S3 bucket
Hello again!

Just to let you know, based on this forum post, we've fixed the vast majority of InternalError messages resulting from configuration errors. Now, it should be easier to figure out what's going wrong with the S3 Deploy action.

Thanks for helping us improve the product"
AWS CodePipeline	"Feature request: GIT_PREVIOUS_COMMIT equivalent in CodeBuild
Jenkins' Git plugin exposes GIT_PREVIOUS_COMMIT and GIT_COMMIT as environment variables. CODEBUILD_RESOLVED_SOURCE_VERSION is CodeBuild's equivalent to GIT_COMMIT, but there's no way to determine the last commit that was built.

Introducing CODEBUILD_RESOLVED_PREVIOUS_SOURCE_VERSION (or similar) would allow CodePipeline stages to skip (fail) when nothing relevant changed that requires a rebuild and redeploy. (This would also benefit from including git metadata in the Source artifact, something I get around by re-cloning the repo and then checking out the target source version in a subdirectory.)

Rationale: if a repository contains a Dockerfile and a README, changes to the README (only) needn't trigger redeploys. This could be checked using something similar to grep -q Dockerfile <<< $(git diff --name-only $CODEBUILD_RESOLVED_PREVIOUS_SOURCE_VERSION $CODEBUILD_RESOLVED_SOURCE_VERSION)
, failing (by returning 1) if the Dockerfile wasn't touched."
AWS CodePipeline	"CodePipeline + git submodules
Hello,
I'm trying to setup building and deployment for an application. I've successfully bootstrapped a CodeBuild build, which run successfully (if I run it manually). But when I tried to trigger the build using the CodePipeline it fails with the following: https://forums.aws.amazon.com/ 2017/01/31 21:07:51 fatal: Not a git repository (or any of the parent directories): .git
All I'm trying to do is to initialize all submodules using: git submodule update --init --recursive
So, seems like CodePipeline somehow make the sources to be not a git repository for CodeBuild, which is weird, because I'd expect CodePipeline just to trigger the build without affecting it anyhow.

P.S. for now I just clone the dependencies using git clone, but it's a little too hacky."
AWS CodePipeline	"Re: CodePipeline + git submodules
Hi Bleshik, 
unfortunately the .git directory is not present in the source package CodePipeline sends to CodeBuild, this is the reason your source tree is not recognized as a git repository. 

I've made a note of your use-case and we'll update this thread when we're able to provide better support for this in the future.

Felipe."
AWS CodePipeline	"Re: CodePipeline + git submodules
Thanks! Note, that almost all services integrated with git (DockerHub, TravisCI, etc.) just pull all submodules by default."
AWS CodePipeline	"Re: CodePipeline + git submodules
+1 Please add the .git folder in the source packaged sent to CodeBuild."
AWS CodePipeline	"Re: CodePipeline + git submodules
+1"
AWS CodePipeline	"Re: CodePipeline + git submodules
+ 1

It would also be useful to also still have the credentials, so I can pull down my common build logic as part of the project (since I have a shared build process for multiple projects)"
AWS CodePipeline	"Re: CodePipeline + git submodules
+1

Really could use this.  Now I am going to have to trigger the build another way as I need the submodules."
AWS CodePipeline	"Re: CodePipeline + git submodules
+1 

This is the only thing preventing us to use Code Pipeline."
AWS CodePipeline	"Re: CodePipeline + git submodules
We have run into this just now. The Source package created by the Source action in our Pipeline does not do a git clone --recursive. So, none of our submodules are available during the Build action."
AWS CodePipeline	"Re: CodePipeline + git submodules
+1"
AWS CodePipeline	"Re: CodePipeline + git submodules
+1K Please add the .git folder in the source packaged sent to CodeBuild. We have a lot of git repos using submodules."
AWS CodePipeline	"Re: CodePipeline + git submodules
+1.  Having the .git allows other nice things, like reading tags for versioning your builds.  As it is, I had to write a lambda that does that for me.  It looks like I may have to write a lambda to pull down the submodule as well."
AWS CodePipeline	"Re: CodePipeline + git submodules
+1"
AWS CodePipeline	"Re: CodePipeline + git submodules
+1"
AWS CodePipeline	"Re: CodePipeline + git submodules
+1

This issue blocks our artefact versioning strategy."
AWS CodePipeline	"Re: CodePipeline + git submodules
+1"
AWS CodePipeline	"Re: CodePipeline + git submodules
+1"
AWS CodePipeline	"Re: CodePipeline + git submodules
+1"
AWS CodePipeline	"Re: CodePipeline + git submodules
+1"
AWS CodePipeline	"Re: CodePipeline + git submodules
yet another +1."
AWS CodePipeline	"Re: CodePipeline + git submodules
+1 for me too; because of this issue it's hard to integrate with third party services like codecov.io."
AWS CodePipeline	"Re: CodePipeline + git submodules
While I'd love for CodePipeline to copy over the .git folder as well, I've rearranged my workflow to compensate for it. My workflow looks like this now:

CodeBuild -> CodePipeline S3 Source -> CodePipeline CodeDeploy Deploy

CodeBuild
Initiate -> build with .git metadata -> uploads artifact to S3

CodePipeline
Source: S3 artifact from CodeBuild
Staging: normal CodeDeploy stage

While it's not ideal, and I have to monitor the entire workflow in 2 places (CodeBuild & CodePipeline), it gets the job done."
AWS CodePipeline	"Re: CodePipeline + git submodules
+1. Such a fundamental feature that's missing, git metadata is incredibly important for updating other things."
AWS CodePipeline	"Re: CodePipeline + git submodules
Yeah, same here. We have the same use case and we need to get some git metadata. It would be really cool if you consider adding this CodePipeline."
AWS CodePipeline	"Re: CodePipeline + git submodules
+1, dead in the water without submodule support"
AWS CodePipeline	"[Bug] - RunOrder is not respected
Hi AWS,

I believe I have found a bug in regards to the runOrder
 parameter for CodePipeline actions not being respected if they are defined out of order in the template.

Example of it not respecting runOrder definitions;

        - Name: StageName
          Actions:
            - Name: ""Task2""
              InputArtifacts:
                - Name: GitBundle
              ActionTypeId:
                Category: Build
                Owner: AWS
                Version: 1
                Provider: CodeBuild
              Configuration:
                ProjectName: !Ref Task2Project
              RunOrder: 2
            - Name: ""Task1""
              InputArtifacts:
                - Name: GitBundle
              ActionTypeId:
                Category: Build
                Owner: AWS
                Version: 1
                Provider: CodeBuild
              Configuration:
                ProjectName: !Ref Task1Project
              RunOrder: 1


In the above actions example, both actions will execute in parallel as it appears while the runOrder
 parameter is respected visually in the CodePipeline Console, if the higher runOrder action is defined BEFORE the lower runOrder action CodePipeline will still execute both in parallel.

To rectify this, if your initial template is out of order, you simply need to migrate your action code block to be below any HIGHER runOrder items.

i.e. Your runOrder: 1 Action block must be defined in the CFN before your runOrder: 2 Action block.

CodePipeline will then respect the runOrder and wait for the lower runOrder item to complete before progressing.

This to me appears to be a bug, regardless of what order the action has been defined, the runOrder parameter value should be respected and executed as intended.

Cheers,
Jeremy

Edited by: jbeffektd on Jan 30, 2019 5:09 PM

Edited by: jbeffektd on Jan 30, 2019 5:11 PM"
AWS CodePipeline	"Re: [Bug] - RunOrder is not respected
Hello, so far I've been unable to reproduce this behavior. The RunOrder was respected regardless of ordering within the template in the pipelines I created using stage definitions similar to yours. Will you provide us with the full resource definition with all sensitive information removed?

Thanks,
Luke"
AWS CodePipeline	"Re: [Bug] - RunOrder is not respected
Hi Luke,

Thanks for replying.

That is strange, I definitely observed this at the time of the post. I've moved onto other project work however I will see if I can reproduce it again when I get an opportunity.

A few other things to mention;


The runOrder was amended and pushed via CFN to an existing stack, not a new stack
The stack was run in ap-southeast-2
CodePipeline was currently setup in the below way



GitHub Pull
CB 1
CB 1
CB 2
CB 2


Whether any of the above makes a difference, I'm not sure, I'll try replicate with a fresh deployment and then test with a CFN update and see if it replicates.

Cheers,
Jeremy

Edited by: jbeffektd on Feb 20, 2019 3:10 PM

Here is an extract specific to CP and CB

  AmiPipeline: 
    Type: AWS::CodePipeline::Pipeline
    Properties: 
      RoleArn: !GetAtt CodePipelineRole.Arn
      Stages: 
        - 
          Name: Source
          Actions: 
            - 
              Name: ""Retrieve""
              ActionTypeId:
                Category: Source
                Owner: ThirdParty
                Version: 1
                Provider: GitHub
              Configuration:
                Owner: !Ref GitHubOrg
                Repo: !Ref GitHubRepo
                Branch: !Ref GitHubBranch
                OAuthToken: !Ref GitHubToken
              OutputArtifacts:
                - Name: GitBundle
        - Name: BuildAMI
          Actions:
            - Name: ""XXXXXXXXXXXXXX""
              InputArtifacts:
                - Name: GitBundle
              ActionTypeId:
                Category: Build
                Owner: AWS
                Version: 1
                Provider: CodeBuild
              Configuration:
                ProjectName: !Ref AmiProject1
              RunOrder: 1
            - Name: ""XXXXXXXXXXXXX""
              InputArtifacts:
                - Name: GitBundle
              ActionTypeId:
                Category: Build
                Owner: AWS
                Version: 1
                Provider: CodeBuild
              Configuration:
                ProjectName: !Ref AmiProject2
              RunOrder: 1
            - Name: ""XXXXXXXXXXXXX""
              InputArtifacts:
                - Name: GitBundle
              ActionTypeId:
                Category: Build
                Owner: AWS
                Version: 1
                Provider: CodeBuild
              Configuration:
                ProjectName: !Ref AmiProject3
              RunOrder: 2
            - Name: ""XXXXXXXXXXXXX""
              InputArtifacts:
                - Name: GitBundle
              ActionTypeId:
                Category: Build
                Owner: AWS
                Version: 1
                Provider: CodeBuild
              Configuration:
                ProjectName: !Ref AmiProject4
              RunOrder: 2
      ArtifactStore:
        Type: S3
        Location: !Ref ArtifactBucket
        EncryptionKey:
          Id: !Ref CodePipelineKey
          Type: KMS
 
  AmiProject1:
    Type: AWS::CodeBuild::Project
    Properties:
      Artifacts:
        Type: ""CODEPIPELINE""
      Source:
        Type: ""CODEPIPELINE""
        BuildSpec: buildspec/buildspec.yml
      Environment:
        ComputeType: ""BUILD_GENERAL1_MEDIUM""
        Image: !Ref DockerImageId
        Type: ""LINUX_CONTAINER""
        EnvironmentVariables:
          - Name: GITHUB_BRANCH
            Value: !Ref GitHubBranch
          - Name: NETWORK_STACK
            Value: !Ref NetworkStack
          - Name: REPORTS_BUCKET
            Value: !Ref ArtifactBucket
          - Name: AWS_REGION
            Value: !Ref ""AWS::Region""
      ServiceRole: !GetAtt CodeBuildServiceRole.Arn
      EncryptionKey: !Ref CodePipelineKey
      TimeoutInMinutes: 120
      VpcConfig:
        VpcId: 
          Fn::ImportValue: !Sub ""${NetworkStack}-VPCID""
        Subnets:
          - Fn::ImportValue: !Sub ""${NetworkStack}-SubnetPrivateA""
          - Fn::ImportValue: !Sub ""${NetworkStack}-SubnetPrivateB""
        SecurityGroupIds:
          - !Ref PipelineSecurityGroup
 
  AmiProject2:
    Type: AWS::CodeBuild::Project
    Properties:
      Artifacts:
        Type: ""CODEPIPELINE""
      Source:
        Type: ""CODEPIPELINE""
        BuildSpec: buildspec/buildspec.yml
      Environment:
        ComputeType: ""BUILD_GENERAL1_MEDIUM""
        Image: !Ref DockerImageId
        Type: ""LINUX_CONTAINER""
        EnvironmentVariables:
          - Name: GITHUB_BRANCH
            Value: !Ref GitHubBranch
          - Name: NETWORK_STACK
            Value: !Ref NetworkStack
          - Name: REPORTS_BUCKET
            Value: !Ref ArtifactBucket
          - Name: AWS_REGION
            Value: !Ref ""AWS::Region""
      ServiceRole: !GetAtt CodeBuildServiceRole.Arn
      EncryptionKey: !Ref CodePipelineKey
      TimeoutInMinutes: 120
      VpcConfig:
        VpcId: 
          Fn::ImportValue: !Sub ""${NetworkStack}-VPCID""
        Subnets:
          - Fn::ImportValue: !Sub ""${NetworkStack}-SubnetPrivateA""
          - Fn::ImportValue: !Sub ""${NetworkStack}-SubnetPrivateB""
        SecurityGroupIds:
          - !Ref PipelineSecurityGroup
 
  AmiProject3:
    Type: AWS::CodeBuild::Project
    Properties:
      Artifacts:
        Type: ""CODEPIPELINE""
      Source:
        Type: ""CODEPIPELINE""
        BuildSpec: buildspec/buildspec.yml
      Environment:
        ComputeType: ""BUILD_GENERAL1_MEDIUM""
        Image: !Ref DockerImageId
        Type: ""LINUX_CONTAINER""
        EnvironmentVariables:
          - Name: GITHUB_BRANCH
            Value: !Ref GitHubBranch
          - Name: NETWORK_STACK
            Value: !Ref NetworkStack
          - Name: REPORTS_BUCKET
            Value: !Ref ArtifactBucket
          - Name: AWS_REGION
            Value: !Ref ""AWS::Region""
      ServiceRole: !GetAtt CodeBuildServiceRole.Arn
      EncryptionKey: !Ref CodePipelineKey
      TimeoutInMinutes: 120
      VpcConfig:
        VpcId: 
          Fn::ImportValue: !Sub ""${NetworkStack}-VPCID""
        Subnets:
          - Fn::ImportValue: !Sub ""${NetworkStack}-SubnetPrivateA""
          - Fn::ImportValue: !Sub ""${NetworkStack}-SubnetPrivateB""
        SecurityGroupIds:
          - !Ref PipelineSecurityGroup
 
  AmiProject4:
    Type: AWS::CodeBuild::Project
    Properties:
      Artifacts:
        Type: ""CODEPIPELINE""
      Source:
        Type: ""CODEPIPELINE""
        BuildSpec: buildspec/buildspec.yml
      Environment:
        ComputeType: ""BUILD_GENERAL1_MEDIUM""
        Image: !Ref DockerImageId
        Type: ""LINUX_CONTAINER""
        EnvironmentVariables:
          - Name: GITHUB_BRANCH
            Value: !Ref GitHubBranch
          - Name: NETWORK_STACK
            Value: !Ref NetworkStack
          - Name: REPORTS_BUCKET
            Value: !Ref ArtifactBucket
          - Name: AWS_REGION
            Value: !Ref ""AWS::Region""
      ServiceRole: !GetAtt CodeBuildServiceRole.Arn
      EncryptionKey: !Ref CodePipelineKey
      TimeoutInMinutes: 120
      VpcConfig:
        VpcId: 
          Fn::ImportValue: !Sub ""${NetworkStack}-VPCID""
        Subnets:
          - Fn::ImportValue: !Sub ""${NetworkStack}-SubnetPrivateA""
          - Fn::ImportValue: !Sub ""${NetworkStack}-SubnetPrivateB""
        SecurityGroupIds:
          - !Ref PipelineSecurityGroup


Edited by: jbeffektd on Feb 20, 2019 3:17 PM"
AWS CodePipeline	"Code Pipeline
So we have at least 60 projects and if we wanted to do CI/CD would we create service roles for EACH one of those projects or just one service role for the CI/CD to use to create and make stuff?

Every example seems to have a single project and not a use case for a larger company."
AWS CodePipeline	"Re: Code Pipeline
Hello!

If all the pipelines are in the same account, then you can re-use the same pipeline role for all of them, no problem!

Matthew

Edit: You can also re-use the artifact bucket as well 

Edited by: mattsainsaws on Feb 19, 2019 2:44 PM"
AWS CodePipeline	"Re: Code Pipeline
The security folks would like us NOT to reuse roles and have separation of security for each project.  Just wonder is that really what dev shops are doing?"
AWS CodePipeline	"File [imagedefinitions.json] does not exist in artifact [testart]
I have a Jenkins build stage with AWS CodePipeline plugin. It uploads the artifact to s3. (which is a file named imagedefinitions.json).

The next stage is a CloudFormation CreateChangeSet, which consumes the artifact created by Jenkins. But I get an error saying ""File (imagedefinitions.json) does not exist in artifact (testart)"".

Any help is appreciated 

Edited by: dauti on Feb 4, 2019 12:16 PM

Edited by: dauti on Feb 4, 2019 12:18 PM"
AWS CodePipeline	"Re: File [imagedefinitions.json] does not exist in artifact [testart]
Hello!

Sorry for asking obvious questions, but have you looked at the artifact that the Cloudformation action is consuming and seeing if the imagedefinitions.json file is actually there?

If you have trouble doing that, replace the Cloudformation action with an S3 deploy action and then download the zip file it uploads.

Matthew"
AWS CodePipeline	"AWS CodePipeline Deploying to S3: Not able to set read public access
I am using a codepipeline containing three stages:
1) Source: from GitHub
2) Build: create-react-app static web page.
3) Deploy: Using the recently announcment deploying to an S3 bucket configured to host a static website with acl = public and public read bucket policy.

Everything works fine unless it is necessary to make all files public after the deployment stage.

From the recent announcement of publishing to S3  https://forums.aws.amazon.com/ann.jspa?annID=6515  this shall be possible: 
""You can now use CodePipeline to deploy files, such as static website content or artifacts from your build process, to Amazon S3.""

I have also tried the tutorial example directly using only 2 stages (Source in GitHub, Deploy to S3) with same result.  https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-s3deploy.html#tutorials-s3deploy-acc 

The artifacts from source and build stages are zipped and files are extracted before deployment by selecting the ""Extract file before deploy"" option.

What is recommend way to ensure that the files after deployment is public accessible using this approach?"
AWS CodePipeline	"Re: AWS CodePipeline Deploying to S3: Not able to set read public access
I'd like to add to this post and hopefully get this filed as a feature request.
It would be great to be able to set default metadata fields through the CodePipeline Deploy Stage to S3
That way it would be possible to set a Cache-Control header for all files uploaded to S3 through CodePipeline.
In my use case I serve the files through CloudFront and since that doesn't add cache-control headers (unless one uses the additional overhead of an Lambda@Edge function), I can't really use the otherwise very convenient S3 Deploy stage."
AWS CodePipeline	"Re: AWS CodePipeline Deploying to S3: Not able to set read public access
Hello!

I'm happy to hear you're using the S3 deploy action with React, that's the exact use case I was thinking of while working on this feature.

For serving these objects from the bucket, I recommend using S3 static hosting: https://docs.aws.amazon.com/AmazonS3/latest/dev/HostingWebsiteOnS3Setup.html

In that tutorial, they set a bucket policy that makes the entire bucket public. So I'd set up the pipeline so that only the assets you want to be public are put in that bucket, then serve them via Cloudfront as you're doing.

By the way, you might not be able to set cache control headers through CodePipeline, but you can use Cloudfront to do this without using Lambda@Edge, by creating a new ""Behavior"" in the Cloudfront distribution, specifying a path to the object(s) you want a different cache on. Then, Cloudfront will only fetch the object from S3 given your cache parameters. I've attached two screenshots on how I did this with my own react website hosted on S3. In the second screenshot, you can see that for the index.html file, I've set a cache time of 60 seconds.

By the way, you might want to invest in a cache busting strategy with React + long cache TTLs on Cloudfront to create a really high availability react app - I just did this and it worked really nicely!

Thanks for sharing your experiences using our product!"
AWS CodePipeline	"Best way to deploy microservices using codepipeline
Hello, I have a single repository in codecommit holding 5 microservices(separate projects in solution). I build the AWS code pipeline Taking codecommit as source and then build(using Jenkins build) the image and push to ECR. Later the oodedeploy pull the image and deploy using revised task defination.

The problem here is if developer do changes in any one of the service, the pipeline builds the image and deploying to all services. I am not sure how to identify the commit happens to specific service.
Is there any way to identify this at time of Build ?"
AWS CodePipeline	"Re: Best way to deploy microservices using codepipeline
Hello!

Sorry for taking so long to reply.

I don't think there's a reasonable way to do this. Even if you could figure it out in a CodeBuild step, how would you stop the pipeline? Mark the build as failed? That would make it hard to figure out if the build actually failed or it just failed to stop the pipeline.

If you're using event based pipelines (and you should be) you might be able to use something in the Cloudwatch event (https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EventTypes.html#codecommit_event_type) used by your Cloudwatch Event rule to cause your pipeline to start. For example, you could put each service on a different branch, or have a git tag for each service. More information is here: https://docs.aws.amazon.com/codepipeline/latest/userguide/triggering.html

However, I would recommend putting each micro-service in a different repository. That way you won't have to solve this problem. I think you might be doing this because you have some shared libraries used across multiple micro-services. I would recommend putting those in their own repositories too, and combining the micro-service package with the library packages using CodePipeline and CodeBuild.

So for example, in micro-service A's pipeline, you'd have a source action for micro-service-a-repo, and a source action for shared-library-1-repo, and have them both as a input artifacts for CodeBuild step that would build them into a single output artifact.

Let me know if you have any more questions!

Matthew"
AWS CodePipeline	"CodePipeline yaml template - problems with account number with leading zero
Hi,

I am experiencing a very strange problem in the template file for my pipeline. When I specify a configuration in a deploy stage, I use a parameter to specify the account number in the RoleArn - which works fine. However, when I do the exact same thing to specify the account number in the RoleArn for the action itself, the account number seems to get converted into a number and fails - because of the leading zero. Everything works fine for account numbers without a leading zero.

Does anyone know why one works fine while the other fails? I have included part of the code below and shown which RoleArn works and which one fails. Then the error I get is at the bottom

---
AWSTemplateFormatVersion: 2010-09-09
 
Parameters:
  AccountNumber:
    Type: String
    Default: ""012345678912""
 
Resources:
  CodeBuildProject:
    ...
 
  Pipeline
    Type: AWS::CodePipeline::Pipeline
    Properties:
      ...
      Stages:
      ...
      - Name: Deploy
        Actions:
          - Name: Create-ChangeSet
            InputArtifacts:
              - Name: BuildArtifact
            ActionTypeId:
              Category: Deploy
              Owner: AWS
              Version: ""1""
              Provider: CloudFormation
            Configuration:
              ActionMode: CHANGE_SET_REPLACE
              ChangeSetName: service-name-changeset
              # Works:
              RoleArn: !Sub arn:aws:iam::${AccountNumber}:role/DeployerRole
              Capabilities: CAPABILITY_IAM
              StackName: service-name-stack
              TemplatePath: BuildArtifact::output-service-name.yaml
            RunOrder: 1
            # Fails:
            RoleArn: !Sub arn:aws:iam::${AccountNumber}:role/CloudFormationRole
...


This is the error I get:

Value 'arn:aws:iam::1.2345678912E10:role/CloudFormationRole' at 'pipeline.stages.3.member.actions.1.member.roleArn' failed to satisfy constraint: Member must satisfy regular expression pattern: arn:aws(-[\w]+)*:iam::[0-9]{12}:role/.*;"
AWS CodePipeline	"Re: CodePipeline yaml template - problems with account number with leading zero
I found the issue quite quickly after posting. Unfortunately, every time I tried to post a reply, I got the error ""Your message quota has been reached. Please try again later. "" So now I'm posting after the weekend...

The problem was that the first time I used the template, I did not have quotes around the AccountNumber variable. When I introduced quotes, the problem should have been fixed, but the old value remained in the pipeline. By simply renaming the variable, I was able to kick the pipeline into understanding that the variable has changed - which finally fixed the problem.

It also turns out that the problem was the same with both RoleArn values; it's just that the RoleArn inside the Configuration block seems to have less validation and was happy to be defined with an invalid account number."
AWS CodePipeline	"How to get execution ID?
Hello,

We have a numer of custom actions in a number of stages in a pipeline. What is the easiest way for a custom action to retrieve the pipeline execution ID?  Our custom actions are written in Go. From checking through the API reference it appears that I need to:

1. Call PollForJobs() to get the pipeline name and stage name from the job details. 
2. I then need to call GetPipelineState() and sort through the stages to find my stage and get the execution ID.

Is that correct? 

-nathan

edited: removed unnecessary details and questions

Edited by: nmar on Nov 22, 2016 4:06 PM

Edited by: nmar on Nov 22, 2016 4:10 PM"
AWS CodePipeline	"Re: How to get execution ID?
Unfortunately there's not an easier way that I can think of right now.

Are you able to share more information about how you intend to use the execution ID? Typically we don't see the execution ID as being that useful inside jobs seeing as it's mainly an internal reference for calling our own APIs.

Is your use case to retrieve the Git commit information from the GetPipelineExecution API?"
AWS CodePipeline	"Re: How to get execution ID?
Re: Use Case

This is just a different take on pipeline history request. In our case, we may have multiple pipelines using the same custom actions. If we want to get the logs of a pipeline execution, we need some kind of key in the custom action logs to differentiate between pipeline A execution 432 vs. pipeline B execution 223.

My thinking was that the pipeline execution ID might something to track this. Let me know if that's not the case. Perhaps my questions should have been ""what is the pipeline execution ID and what is it used for""?

Thanks."
AWS CodePipeline	"Re: How to get execution ID?
I see, thanks for the feedback.

Perhaps my questions should have been ""what is the pipeline execution ID and what is it used for""?

Generally we just see it as a reference which is only relevant when calling related APIs of ours, but I see how it would be useful to your use case.

It's not ideal, but in the meantime perhaps you could combine the fields from the actionTypeId field in the job details with a timestamp to build an ID."
AWS CodePipeline	"Re: How to get execution ID?
I wanted to let you know that we recently launched Pipeline Execution History. As part of this release, you can view the execution ID in the console. 

The AWS CodePipeline console will now provide you with change details associated with each execution. You will also be able to track the status, start time, end time, and duration of every execution. To get started, visit the “View Pipeline History” link in the CodePipeline console to get started. You can also view the list of executions through the CodePipeline API and CLI.

Click here to learn more: http://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-view.html"
AWS CodePipeline	"Re: How to get execution ID?
I've come up with a one-liner that you can run within a CodeBuild job triggered by CodePipeline. It will return the current CodePipeline execution ID.

This approach uses the built-in CODEBUILD_BUILD_ID environment variable (CodeBuild Environment Variables: https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html) to look for the current CodeBuild Project in the Stages of a given Pipeline. When it finds that build ID it will return the current PipelineExecutionId for the Pipeline Stage that contains the CodeBuild Project.

aws codepipeline get-pipeline-state --region us-west-2 --name your-pipeline-here --query 'stageStates[?actionStates[?latestExecution.externalExecutionId==`'${CODEBUILD_BUILD_ID}'`]].latestExecution.pipelineExecutionId' --output text


GitHub Link: https://gist.github.com/rupertbg/1fbdec3dfa746e0d5eee69234d995689

Note: You will need to give your build project get-pipeline-state IAM rights

Edited by: rupertbg on Feb 17, 2019 2:56 PM

Edited by: rupertbg on Feb 17, 2019 2:57 PM

Edited by: rupertbg on Feb 17, 2019 5:29 PM"
AWS CodePipeline	"code pipeline and release webhook
I have two aws Code Pipeline, one with push and one with release webhook.
I have verified that they exist in github, one for push events and one for release events.
I can see that the events are sent from github to aws webhook and get status 200 back.
If i check aws webhooks with aws codepipeline list-webhooks I can see that targetPipeline and targetAction are correct as are the filters.
For push:
""filters"": [
 {
  ""jsonPath"": ""$.ref"",
  ""matchEquals"": ""refs/heads/master""
  }]
For release:
""filters"": [
 {
  ""jsonPath"": ""$.action"",
  ""matchEquals"": ""published""
  }]
But what happens is that on push both pipelines run and nothing happens when I create release.
Any ideas?"
AWS CodePipeline	"Re: code pipeline and release webhook
Hi, did you manually change the configuration of the webhook created by codepipeline to make it respond to release event? cause webhooks created by codepipeline supports only push events by default.

Coud you try deleting the release webhook and try push the code, see if the two pipeline still runs?"
AWS CodePipeline	"Git Webhooks with AWS services template doesn't work
I'm following the instruction here:https://aws-quickstart.s3.amazonaws.com/quickstart-git2s3/doc/git-to-amazon-s3-using-webhooks.pdf  in order to automate deployment of static site from GitHub to S3.
However, when I started the template (with the right region), I have these errors:
CREATE_FAILED	AWS::KMS::Key	KMSKey	Resource creation cancelled
17:45:43 UTC-0500	
CREATE_FAILED	AWS::CloudFormation::CustomResource	CopyZips	Failed to create resource. See the details in CloudWatch Log Stream: 2019/01/29/[$LATEST]...


I check CloudWatch Log and get this:
[ERROR]	2019-01-29T22:45:41.776Z	...	Exception: An error occurred (NoSuchBucket) when calling the CopyObject operation: The specified bucket does not exist


It looks like the S3 bucket that contains the template or the zip file doesn't exist anymore. Can you help?
Thank you.

Edited by: awsuser29 on Jan 29, 2019 5:54 PM"
AWS CodePipeline	"Re: Git Webhooks with AWS services template doesn't work
Thanks for pointing this out, we are trying to fix the template.
And the guide you are using is old, there is AWS webhooks available, you might want to take advantage of that.
These links might be helpful:
https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-webhooks.html
https://aws.amazon.com/blogs/devops/using-custom-source-actions-in-aws-codepipeline-for-increased-visibility-for-third-party-source-control/"
AWS CodePipeline	"Action execution failed InternalError
Hi,
I'm running through the CodePipeline Tutorial. Create a Pipeline with an Amazon ECR Source and ECS-to-CodeDeploy Deployment (in the eu-central-1 region)
Source and Deploy stages.  
The Deploy stage fails. 
Tried changing the configurations here and there.

A couple of the error codes:

Action execution failed
InternalError. Error reference code: 8d667efd-851a-4797-b5e8-91dc269da31b.

Action execution failed
InternalError. Error reference code: 0416a552-758d-435f-a35c-15ec904eba88.

Of course, there may be something which was mis-configured.  

How could I debug further? Is there log output?

Thanks,
Sam"
AWS CodePipeline	"Re: Action execution failed InternalError
Solved. Very easy. I just had to continue to the step ""add an Amazon ECR source action to your pipeline"".  Add the ECR source. Maybe in this case ""Action execution failed InternalError"" could instead say ""Expecting a source input in the format of a Docker image such as an ECR source. The pipeline input source was not in that format."""
AWS CodePipeline	"Re: Action execution failed InternalError
Hello! Sorry for the delay in looking at this post. 

Thanks for your comments, I agree this is a pretty bad experience because it doesn't give you any information to help fix the issue. I will pass along your comments to the team responsible for this action and ask them to make the error message more useful.

I hope you're enjoying using the ECS action in spite of this problem!

Matthew"
AWS CodePipeline	"CodePipeline CloudFormation Deploy - Disable Rollbacks
We are using CodePipeline to automatically deploy new CloudFormation stacks when code is updated.

When things go wrong, it is useful to disable rollbacks in CloudFormation, so that stacks aren't immediately deleted and it's possible to troubleshoot.

If using the AWS CLI, you can just use the --disable-rollback parameter.

How do you disable rollbacks in CodePipeline when deploying a CloudFormation stack?

Thanks

Tom"
AWS CodePipeline	"Re: CodePipeline CloudFormation Deploy - Disable Rollbacks
Hi! Is there any update on this? It's a real deal breaker for us and means we have to deploy CloudFormation stacks from the command line.

The annoyance with that is that the command line has a different format for the params.json file than CodePipeline does --- a real pain!"
AWS CodePipeline	"Re: CodePipeline CloudFormation Deploy - Disable Rollbacks
Thanks for your feedback. We have conveyed your feature request to the service team in order to prioritize this feature development. Regrettably, we do not have ETA for these features yet, however, as soon as it gets released, it should be publicly announced in either one of the following links or in this forum: 

AWS blogs: http://blogs.amazon.com/ 
What’s New: http://aws.amazon.com/new/ 
AWS Release Notes: https://aws.amazon.com/releasenotes/"
AWS CodePipeline	"Re: CodePipeline CloudFormation Deploy - Disable Rollbacks
Thanks awsdarryn - any word on this? Being unable to disable rollbacks is making this pretty rough for development pipelines. Would be awesome if that option could be added."
AWS CodePipeline	"Re: CodePipeline CloudFormation Deploy - Disable Rollbacks
We'd really like this, too. In theory, it's no big deal to bring up stacks using the command line. But in practice, because they both use a different and incompatible way of specifying parameters, it becomes a bit of a nightmare..."
AWS CodePipeline	"Re: CodePipeline CloudFormation Deploy - Disable Rollbacks
Just in case it helps anybody who's looking for this... Here's a quick command I've written to convert from the codepipeline-style parameters file, to the command-line style of parameters:

cat codepipeline-params.json | jq '[.Parameters | to_entries[] | {ParameterKey: .key, ParameterValue: .value}]' > cli-params.json


... This should make it a bit less painful to create stacks from the command line (And you can then specify --disable-rollback)

Edited by: tomkerswill-conquest-testing on Nov 27, 2018 5:17 AM

Formatting code. Edited by: tomkerswill-conquest-testing on Nov 27, 2018 5:18 AM"
AWS CodePipeline	"Re: CodePipeline CloudFormation Deploy - Disable Rollbacks
Hi Team,

Was searching for this feature but couldn't find and stumbled upon this thread.
Is this feature available now .?"
AWS CodePipeline	"Re: CodePipeline CloudFormation Deploy - Disable Rollbacks
I'm pretty sure it still isn't available, sadly    ... Would have thought should be easy fix, but sadly not. 

I guess workaround would be to use an AWS cli command within codebuild to deploy the stack during that part (rather than using deploy step)."
AWS CodePipeline	"CodePipeline ECS deploy update task definition
Hi there,

We are using CodePipeline to deploy an ECS service. Everything is working great with normal deployments.
We are running into issues when it comes time to make an update to the task definition for the service.
If you update the task definition, without changing the services task-def to the latest revision, the CodePipeline deploy reverts the task definition back to the previous state.
It looks as if the CodePipeline deploy makes a copy of the service task-def itself, instead of the latest available task definition revision.

We have the task definition JSON file in the repository and would like to update the task definition based on this file every time we deploy, however CodePipeline deploy is reverting it back making this impossible.

Is there a solution to this issue, or a way to work around it that can be automated as part of the build/deploy?

Thanks

P.S. here is a link to someone with the same/similar issue: https://stackoverflow.com/questions/54450005/how-do-i-control-the-memory-and-cpu-footprint-of-an-ecs-deployment-when-using-co"
AWS CodePipeline	"buildspec.yml completes in CodeBuild but fails in CodePipeline
My buildspec.yml is actually quite simple: 

version: 0.1
phases:
  build:
    commands:
      - git archive --format=zip HEAD > application.zip
artifacts:
  files:
    - application.zip


When I run a build standalone in CodeBuild, there are no problems. But when it's part of CodePipeline, the build fails and I get this:

[Container] 2017/03/18 09:19:33 Entering phase BUILD
[Container] 2017/03/18 09:19:33 Running command git archive --format=zip HEAD > application.zip
[Container] 2017/03/18 09:19:33 Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM n
[Container] 2017/03/18 09:19:33 Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
[Container] 2017/03/18 09:19:33 Command did not exit successfully git archive --format=zip HEAD > application.zip exit status 128
[Container] 2017/03/18 09:19:33 Phase complete: BUILD Success: false
[Container] 2017/03/18 09:19:33 Phase context status code: COMMAND_EXECUTION_ERROR Message: Error while executing command: git archive --format=zip HEAD > application.zip. Reason: exit status 128


Please advise."
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
Hi,

CodePipeline downloads the source as a zip from the source provider rather than doing a Git clone, which means the .git folder won't be retained and git commands like the one you're running won't work.

Can you share more details about your use-case? It seems like your goal here is to produce a zip archive of your repository, which you could also achieve using something like the zip command.

If you have other use-cases for retaining the .git folder I'd be interested in hearing more about them as this is a feature request we've heard from several customers.

 - Tim."
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
In our use case, we're trying to post slack notifications when certain build events are triggered. In these slack notifications, we want to put things like the git author, subject, and shortcode."
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
In our case, we determine the Docker version tag of the image based on the current state workspace. It is essential that we get the .git directory too.  

See https://github.com/mvanholsteijn/docker-makefile for the functionality."
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
We have the same issue. We use CodeCommit, CodeBuild, & CodePipeline for our CI/CD flow.

Our build is maven with Java 1.8. Our use case is that we brand our artifact with the build details from GIT (branch name & revision). All our pages display this and the environment so that screen captures that the help desk get from users include that information.

From a support side, it lets us get to exactly the code base & configuration where the user is experiencing their issue.

Maven does have a facility where I can check out the code (embedded GIT url in SCM section - mvn scm:checkout). I played with that some, but had no luck. It got me the .git directory, but then failed with other problems.

CodeBuild does things one way, and you guys do it a different way. So far, I have been able to trick CodeBuild into doing the right thing. But then those builds break under CodePipeline.

One of the main themes of CI/CD is that you have one way of building. Currently, I have to have 3 - 1 for local developers, 1 for CodeBuild, and 1 for CodePipeline. 

Debugging the CodeBuild & CodePipeline builds has been an exercise that takes me back 30 years - add more printf's, rerun it, and review the logs. My local build is no longer a reflection of what my CI build will be, so it is of no use in those contexts.

The best doc's I have seen for how CodeBuild & CodePipeline do their code checkouts has been forum messages. That really needs to be in the documentation.

I have dealt with shallow checkouts in the with Cruise Control & Jenkins, so I had an idea what to look for.

So far, I have been super happy with how well your CI/CD stack has worked. It really surprised me to run into this.

I realize you have multiple VCS's that you support and that you have a reason for not copying the repo intact (hidden directories and all). It would be nice to know your reasoning, and where you are headed on it.

My next experiment for a work around is to use the build date. Nothing like the branch/version, but it will get me closer.

Thanks for all the hard work on what you have delivered so far.
Bob"
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
Our use-case requirement for accessing the .git contents, is for running tools like GitVersion and GitReleaseNotes in our builds - these tools help us automatically determine the next appropriate version number for a build, and can also automatically build releases notes from data within the .git folder. 

However, we cannot do any of this without having the .git contents pulled in from github. I understand that code build does work this way now, whilst code pipeline does not.

So this would be extremely useful for us.

Cheers

Edited by: StewS on Jan 31, 2018 3:18 AM"
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
How frustrating is this! There should be a little checkbox that does a checkout instead of a zip download... I mean how hard would this be exactly???"
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
Do any of the env variables help?

CODEBUILD_RESOLVED_SOURCE_VERSION

How is the IMAGE_TAG (based on source version) passed from the build step to the deploy step in the following example?

https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-cd-pipeline.html"
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
Agree. We need the .git directory. As most people here are commenting, we want to automatically increase the version number. It seems like a ""standard"" thing for CI/CD."
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
We've been using a workaround for the .git folder; cloning the repo (again) in the build step.  Our source is on CodeCommit, and the builder has the required IAM credentials to pull/push so we're able to tag based on build exit status.  

This is used for NodeJS projects, so the repo url is in the package.json.  It's an ugly fix, but it works."
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
Our use case is pulling in submodules in the Build stage."
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
Our use-case is to update the version number during a build.

Any ETA or indication of if this will ever be a feature?"
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
I just ran across this issue, and it's super frustrating. CodeBuild and CodePipeline should operate (especially for base functions) in the same way.

Our use case for including the .git directory (doing a full pull, not just zip downloads) is for git tags. Downstream, we'd like to tag our containers based on these. This seems like a no-brainer. Give the users the ability to select zip vs. checkout so we can control what we want.

CodePipeline has a number of nice functions, and I love the ephemeral nature, but we're using it in place of Jenkins and many other developers know that these baseline functions are available in those pipelines. Makes for a difficult discussion.

The thing I fear is you've heard from ""several customers"" over a year and half ago, and the functionality remains the same. Can we get some guidance before we start pushing workarounds?"
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
This lack of Git metadata when trying to build using CodePipeline is frustrating us as well. 

The complete build process for almost any project I've worked with in recent years involves querying the Git metadata (which is stored in the .git subdirectory), for the same purposes other people have already mentioned in this thread: to extract the tags, author name, date, generate a canonical version identifier based on this Git metadata and both incorporate this information into the built product, use it for notifications (Slack, email, whatever) and display it on CI/test/build/release informational dash board systems.

I realize not every project uses Git, but most do, and for many/most of those, ""the source code minus the Git information"" is not the entire source code. The Git metadata is often effectively part of the source code. Constant values within the built application are frequently based on this information.

So this issue makes coming to CodePipeline from any other system (even AWS CodeBuild!) frustrating.

The only workaround to  I have found is to actually clone the project again as part of the build process, but this is a clumsy workaround.

The CODEBUILD_RESOLVED_SOURCE_VERSION environment variable contains the SHA of the specific commit being built. If you use the CodePipeline API to get the pipeline state (e.g. aws codepipeline get-pipeline-state --name ""my-project-pipeline"" ) you can obtain a big JSON object which will somewhere contain a field like:
""entityUrl"": ""https://github.com/my-company/my-project/tree/foo-branch""
 and so you have a way to know the branch being built (at least, if you are using GitHub). 

With those two things, you can git clone the source and then use `git` or other tools that read the Git metadata to generate the values your build needs. 

That works, but it adds minutes, network overhead, and credentials-management headache to the process. (Credentials management for this redundant git clone operation is particularly annoying, since the details are likely to be different for each different projects.)

I would encourage AWS to implement a solution for this, by using the project's revision control system (e.g. Git) to fetch the source code, rather than using some other means to fetch the source code without revision control information. 

I believe this is what the vast majority of users would expect, when fetching a project from GitHub, and is how every other CI/build-automation system I have ever seen works (including CodeBuild).

Cheers."
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
It appears that some meta information is already exchanged between CodePipeline stages by the use of environment variables. (example: commit hash is transmitted via CODEBUILD_RESOLVED_SOURCE_VERSION environment variable).

It would be nice if CodeCommit action within Source stage could append to environment variables new set of key/value pairs generated by GitVersion - this could address a big chunk of concerns here."
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
Any progress on this issue? This is a vital part for our build process."
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
Any update regarding this issue ?"
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
+1 for .git folder"
AWS CodePipeline	"Re: buildspec.yml completes in CodeBuild but fails in CodePipeline
+1
We need this as well. Very frustrating that this issue is open since March 2017."
AWS CodePipeline	"Cloudformation stack tagging
How can I set tagging for the stack created in ""Create or update a stack"" action mode?
We are deploying services consisting of multiple cf templates with service specific pipelines. Now I want the stacks (and the resources) to have identical billing related tagging set preferably by a yaml/json file.
How about adding an option to set stack tagging file?

Edited by: psnm on Apr 6, 2017 10:55 AM"
AWS CodePipeline	"Re: Cloudformation stack tagging
Hi, CodePipeline does not support CloudFormation stack tagging at this moment. However, this is a feature that would benefit many customers and it is on our roadmap."
AWS CodePipeline	"Re: Cloudformation stack tagging
so....been a little under 2 years...how's that roadmap looking?"
AWS CodePipeline	"CodePipeline incorrectly creating wrapper zip around CodeBuild output
I have a CodeBuild project and a CodeDeploy application.  When I run these via the console my application deploys successfully.  The output of my CodeBuild project is a zip file.  The ""Artifacts packaging"" of the CodeBuild project is set to ""None"".  I've verified the result of the CodeBuild is the expected zip file.

With a CodePipeline comprised of the previously stated projects I'm unable to deploy the application successfully because CodePipeline creates a container zip around my build output zip.  The failure I receive from CodeDeploy is ""No such file or directory @ rb_sysopen - <path>/deployment-archive/appspec.yml"".  I've verified this by downloading the artifact from S3.  If I extract the CodePipeline zip the contents are the result of the CodeBuild build.  Once the second zip is extracted the contents are correct and it contains my appspec.yml as well as my other build artifacts.

This sounds similar to the issue reported in https://forums.aws.amazon.com/thread.jspa?threadID=247398&tstart=0.

Any help would be greatly appreciated.  Thanks,

bradr"
AWS CodePipeline	"Re: CodePipeline incorrectly creating wrapper zip around CodeBuild output
Hi Bradley, I'm from the CodeBuild team.  You are correct: CodePipeline-triggered builds will always use the 'zip' packaging, overriding the packaging type selected in your CodeBuild project.  Can you describe your use case for creating the zip yourself?  We're actively working on features to make the zip packaging type cover more use cases, so I'd like to make we'll cover yours as well."
AWS CodePipeline	"Re: CodePipeline incorrectly creating wrapper zip around CodeBuild output
Hi Clare.  Thank you for the response.  I was using CodeBuild + CodeDeploy before I thought I'd try CodePipeline.  I create a zip as the output of CodeBuild because it's the input expectation of CodeDeploy.  In the CodeDeploy console we select the ""revision location"" which is described in the help as: ""Type or copy and paste the location of your application file bundle in Amazon S3. Use the format: s3://bucket-name/folder/object.tar.""

In general I'd assume a zip would the output of a build for the following reasons:

1. transfer efficiency
2. verification of the build output of a single file is much simpler than for a directory tree
3. build signing
4. if I have issues with CodePipeline I'd like to fall back on CodeBuild + CodeDeploy

Edited by: bradleyjamesr on Jan 25, 2017 11:22 AM"
AWS CodePipeline	"Re: CodePipeline incorrectly creating wrapper zip around CodeBuild output
Thanks Bradley.  It makes total sense that you want to be able to do CodeBuild -> CodeDeploy directly as well as deploy via CodePipeline.  Have you tried the 'zip' packaging type in CodeBuild? CodeBuild can create the zip for you (instead of you having to do the zip in your build commands) before uploading to S3.  Then your build has exactly the same behavior when the build is triggered via a pipeline or directly in CodeBuild.

Here's a sample:
http://docs.aws.amazon.com/codebuild/latest/userguide/sample-codedeploy.html

Do let us know if the 'zip' packaging behavior doesn't cover what you need for bundling your application for CodeDeploy."
AWS CodePipeline	"Re: CodePipeline incorrectly creating wrapper zip around CodeBuild output
I have tried allowing CodeBuild to zip the results without luck.  I'll do my best to explain...

I create a build folder in my project that contains my build artifacts. 

build/bin
build/lib
build/node_modules
build/appspec.yml


In my buildspec.yml I have:

artifacts:
  files:
    - build/**/*


With the zip packaging type it creates a zip with a top level build folder which I don't want.  I want the top level of the zip to be the bin, lib, and node_modules folders.  I tried using discard-paths
 but that removes all paths, not just the build folder.

I don't want to have to model my build artifacts in the buildspec.yml as it's already in my build script.  The script is simpler to maintain and having to remember to update the buildspec would be error prone."
AWS CodePipeline	"Re: CodePipeline incorrectly creating wrapper zip around CodeBuild output
Hi Bradley, we recently released a new config for the buildspec that should help with this scenario: ""base-directory"".
Check out the documentation on this page: https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html
Hopefully this should help you achieve the zip file layout you need."
AWS CodePipeline	"Re: CodePipeline incorrectly creating wrapper zip around CodeBuild output
+1 for the initial feature request.

Codepipeline must not enforce zip packaging. I build my zip artifact for elastic beanstalk, by using maven-assembly-plugin with a set of advanced include/exclude rules. It's way too difficult to reproduce the build artifact with CodeBuild zip packaging."
AWS CodePipeline	"Re: CodePipeline incorrectly creating wrapper zip around CodeBuild output
+1 For some sort of documentation. (In BOLD!)

Just got stuck for hours on this! Had no idea it creates a wrapper zip. Thought the zip file had not been extracted."
AWS CodePipeline	"Re: CodePipeline incorrectly creating wrapper zip around CodeBuild output
I have a similar problem.
My build process (sbt native packager) generates a SINGLE zip file.
I need that it gets automatically unzipped by codepipeline because it contains buildspec.yml (required by codebuild) and Dockerfile needed to publish the docker image.

Edited by: MatteCarra on Oct 10, 2017 1:12 PM"
AWS CodePipeline	"Re: CodePipeline incorrectly creating wrapper zip around CodeBuild output
I have the same problem. I use `CodePipeline + CodeBuild + Deploy to S3` and try to use `buildspec.yml` file to grab all files and subfolders inside my `public` folder and put in the root folder of an S3 bucket so my `index.html` file will be in the root of the bucket but I cannot do this with both:
files:
        - build-output/**/*
      base-directory: build-output

My build keep failing.
Any suggestion?"
AWS CodePipeline	"Cannot access deploy artifacts in a cross-account pipeline
I have a cross-account pipeline running in an account CI deploying resources via CloudFormation in another account DEV.
After deploying I save the artifact outputs as a JSON file and want to access it in another pipeline action via CodeBuild. 
CodeBuild fails in the phase DOWNLOAD_SOURCE with the following messsage:

CLIENT_ERROR: AccessDenied: Access Denied status code: 403, request id: 123456789, host id: xxxxx/yyyy/zzzz/xxxx= for primary source and source version arn:aws:s3:::my-bucket/my-pipeline/DeployArti/XcUNqOP

The problem is likely that the CloudFormation, when executed in a different account, encrypt the artifacts with a different key than the pipeline itself.

Is it possible to give the CloudFormation an explicit KMS key to encrypt the artifacts with, or any other way how to access those artifacts back in the pipeline?

Here is my code snippet (pipeline deployed in the CI account):
  CodePipeline:
    Type: AWS::CodePipeline::Pipeline
    Properties:
      Name: ""my-pipeline""
      RoleArn: !GetAtt CodePipelineRole.Arn
      Stages:
      - Name: Source
        ...
      - Name: StagingDev
        Actions:
        - Name: create-stack-in-DEV-account
          InputArtifacts:
          - Name: SourceArtifact
          OutputArtifacts:
          - Name: DeployArtifact
          ActionTypeId:
            Category: Deploy
            Owner: AWS
            Version: ""1""
            Provider: CloudFormation
          Configuration:
            StackName: ""my-dev-stack""
            ChangeSetName: !Sub ""${ProjectName}-changeset""
            ActionMode: CREATE_UPDATE
            Capabilities: CAPABILITY_NAMED_IAM
           # this is the artifact I want to access from the action 
           # whitin this CI account pipeline
            OutputFileName: ""my-DEV-output.json""   
            TemplatePath: !Sub ""SourceArtifact::stack/my-stack.yml""
            RoleArn: !Sub ""arn:aws:iam::${DevAccountId}:role/dev-cloudformation-role""
          RoleArn: !Sub ""arn:aws:iam::${DevAccountId}:role/dev-cross-account-role""
          RunOrder: 1
        - Name: process-DEV-outputs
          InputArtifacts:
          - Name: DeployArtifact
          ActionTypeId:
            Category: Build
            Owner: AWS
            Version: ""1""
            Provider: CodeBuild
          Configuration:
            ProjectName: !Ref MyCodeBuild
          RunOrder: 2
      ArtifactStore:
        Type: S3
        Location: !Ref S3ArtifactBucket
        EncryptionKey:
          Id: !GetAtt KMSKey.Arn
          Type: KMS"
AWS CodePipeline	"Re: Cannot access deploy artifacts in a cross-account pipeline
Hi, Did you give CodeBuild the s3 permission of pipeline artifact bucket?"
AWS CodePipeline	"Re: Cannot access deploy artifacts in a cross-account pipeline
I'm having a similar problem and I don't know how to solve it in a good way.

My pipeline runs in Account-A and deploys to Account-B. There are three roles involved.

CodePipelineAccountA: Main pipeline role that assumes CodePipelineAccountB.
CodePipelineAccountB: Is assumed by CodePipelineAccountA and does iam:PassRole on CloudFormationAccountB to CloudFormation.
CloudFormationAccountB: creates the resources in all CloudFormation stacks that are deployed from the pipeline.


One action in one stage is an Invoke Lambda action that is performed in Account-A. This action tries to access one output artifact from a previous step but can't read the files. Using my admin user in Account-A I have the same problem, I can't access the output artifacts (I can list them but not read them). This seems to be because when these output artifacts are uploaded to S3 they are owned by Account-B.

Is there any way to force the roles in Account-B that uploads output artifacts to S3 to specify the ""bucket-owner-full-access"" property?

I'"
AWS CodePipeline	"CodePipeline deploy stage fails to upload to S3 exceeded 2 min time limit
Hello,

We have a CodePipeline deploy stage that deploys our build artefact to S3.

The stage keeps failing with the following error:

Action execution failed
The action failed to complete. The action exceeded the time limit for this action type: 2.0 minutes. This could be caused by a temporary network issue. Wait a few minutes, and then choose ""Release change"" to restart this change through the pipeline.

We are trying to deploy a large number of objects  (<10k) that are not large in size (total size < 500MB).

Can anyone suggest how we get this limit increased or even resolve it?

Thanks,

Ben"
AWS CodePipeline	"Re: CodePipeline deploy stage fails to upload to S3 exceeded 2 min time limit
Hi, you could either 

1. reduce the number of objects 
2. ask codepipeline to increase the limit for your account (provide your information use case to them and they will solve it for you)

Best wishes!"
AWS CodePipeline	"Re: CodePipeline deploy stage fails to upload to S3 exceeded 2 min time limit
Hi,

Unfortunately, I cannot reduce the number of objects as we need them all.

I've tried opening a support case for a limit increase on CodePipeline but there is no option for increasing the time limit when copying objects to S3.

Any ideas?

Thanks,

Ben"
AWS CodePipeline	"Re: CodePipeline deploy stage fails to upload to S3 exceeded 2 min time limit
We have identified this problem and the new timeout limit of 10 minutes will be released in a few days.

Which region does your pipeline locates?

Will get back to you when the new limit become effective.

If the problem still persists after the new limit available, please let us know."
AWS CodePipeline	"Re: CodePipeline deploy stage fails to upload to S3 exceeded 2 min time limit
Hi,

That's great news, thank you!

My pipeline resides in eu-central-1 (Frankfurt).

Thanks,

Ben"
AWS CodePipeline	"Re: CodePipeline deploy stage fails to upload to S3 exceeded 2 min time limit
Welcome, the now limit is available in eu-central-1 now.

Please try.

Best wishes!"
AWS CodePipeline	"Re: CodePipeline deploy stage fails to upload to S3 exceeded 2 min time limit
Hi,

(I'm a colleague of Ben's)

That seems to have worked for now. We have a need to copy a larger set of files (count and size), and I don't know for sure how much headroom we now have. But certainly I can confirm that the change has worked!

Kind regards

Mark"
AWS CodePipeline	"Re: CodePipeline deploy stage fails to upload to S3 exceeded 2 min time limit
I am having the same issue in a us-east-1 pipeline. Do you know when will this be rolled out there too?"
AWS CodePipeline	"CodePipeline built the wrong SHA
Hi,

We are using CodePipeline hooked to a GitHub repo. The Pipeline is linked to a branch and is using webhooks.

Yesterday we merged a PR to this branch, which triggered a build successfully. We realised we didn't want that deploy to happen so we manually cancelled the pipeline by aborting one of the build steps.

40 minutes later we merged another PR to the branch, and for some unknown reason the Pipeline reran with the old SHA rather than the new one, ending up in code being deployed that is not what's on that branch HEAD.

We have looked at the payloads the GitHub Webhook sent and they look ok to us:
First hook:
{
  ""ref"": ""refs/heads/master"",
  ""before"": ""ba32e93045264d67e53d18b93c4d072838a1b4e2"",
  ""after"": ""04fa7adb00b8d661982dd43f459e8e1181269fb2""
}

Second hook:
{
  ""ref"": ""refs/heads/master"",
  ""before"": ""04fa7adb00b8d661982dd43f459e8e1181269fb2"",
  ""after"": ""4e768669278dae7d5b5acac47ea847f984bd75b0""
}

We would have expected the second hook to trigger a Pipeline run with the SHA 4e768669278dae7d5b5acac47ea847f984bd75b0, however the SHA that ended up deploying was 04fa7adb00b8d661982dd43f459e8e1181269fb2. Are we misunderstanding how CodePipeline should work?

Pasting the execution IDs just in case they can aid debugging if someone from AWS is having a look.
Execution ID for the first hook: 02194076-fa74-4565-8dc9-1e09fc95a899
Execution ID for the second hook: 614b6bec-5926-4045-8416-89d3b28f4168"
AWS CodePipeline	"Action execution failed in codedeploy stage of codepipeline.
Hello, I've setup an ECS cluster with the service and task definition with the image present in ECR. My Codepipeline has stage of Source(with revised taskdefination) & codedeploy. THe pipeline fails at codedeploy and I'm getting the following error.

The error says
""Action execution failed
InternalError. Error reference code: c01e114a-be75-4aad-9208-acd650555eba.""

I am not sure where to check the details of this error. Appreciate any help on this.

Edited by: Satish-wiki on Jan 19, 2019 11:50 AM"
AWS CodePipeline	"Re: Action execution failed in codedeploy stage of codepipeline.
Could you find any useful logs in the ECS or ECR? 
If not please contact customer support and provide your information for further assistance."
AWS CodePipeline	"Could not create IAM role
When I am creating pipeline at the last step i am getting ""Could not create IAM role"", even though, i have IAM role created.

Any insight much appreciated.
Thanks,
Rama"
AWS CodePipeline	"Re: Could not create IAM role
I'm getting the same error. I have right IAM permissions to create a role. Even if I create new service role or use the created one, same error is appearing at last stage.
Let me know exact permissions that will need for my account."
AWS CodePipeline	"Re: Could not create IAM role
Greetings! It sounds like you want to use an existing IAM Role, and not create a new IAM Role, as you create a new Pipeline. 

If you are creating a new pipeline, then in Step 5 ('Service Role') you can click inside the 'Role name' input field, and the AWS Console will automatically pre-populate a number of eligible IAM Roles available to be used as the Code Pipeline Service Role. If you need to create a service role, please be sure that the new role has at least all of the  permissions provided in the Default AWS CodePipeline Service Role Policy [1].

In order to create a new compliant IAM Role, the IAM User/Group/Role being used must ""Allow"" the ""iam:CreateRole""[2] action. 

Does this help?

[1] http://docs.aws.amazon.com/codepipeline/latest/userguide/iam-identity-based-access-control.html#view-default-service-role-policy
[2] http://docs.aws.amazon.com/IAM/latest/APIReference/API_CreateRole.html"
AWS CodePipeline	"Re: Could not create IAM role
No. It's not working with below two approaches.
1) On step 5(ServiceRole), AWS pre populates existing role. I select it and go to next step. After clicking 'create pipeline', I'm getting same error.

2) On Step 5(ServiceRole), Even if I create a new role and select it, in next step getting same error while creating pipeline. However new role is getting created successfully.

Pipeline creation works with other accounts having full access. Some IAM permissions issue for my account. Let us know exact required IAM permissions for AWS Codepipeline."
AWS CodePipeline	"Re: Could not create IAM role
Greetings @avj! I should have clarified that my reply was directed towards the issue brought by @rampraxim. My apologies.

I can't be sure that the cause of these two issues are identical, yet.

It sounds that, in your case, there may be different IAM User credentials used to create the new Pipeline and the bespoke IAM Role, if you are receiving the same error when creating a Pipeline. To help further, I need to fully understand what is occurring with your pipeline creation. To start:

1) Can you please clarify how you are creating the IAM role outside of CodePipeline? 
2) Which user ID are you using to create the new Pipeline? 
3) Can you please paste the exact text error you are seeing, as well as the name and region of the pipeline you are trying to create?"
AWS CodePipeline	"Re: Could not create IAM role
We are experiencing the same problem. When we are trying to create a pipeline, in Step 5 we create a new role with the permissions that are automatically populated and everything seems to work fine but in Step 6, when we try to create the pipeline, an error arises and the pipeline is not created.

The error message says: Error. Could not create IAM role. However, the role is created and listed in IAM.

It has to be said that this error happens when we try to create the pipeline with an IAM user with PowerUserAccess permissions besides other custom single IAM permissions to work with Code Commit, Code Build, Code Deploy and Code Pipeline. The IAM user is 
IAM User: cn-pelaez
Account: 4467-7178-8112
However, if we try to create the pipeline with the root user, the pipeline is successfully created, what makes us think that it is somehow related to the permissions granted to the IAM user."
AWS CodePipeline	"Re: Could not create IAM role
We were unable to reproduce your issue experienced using the same permissions as the user cn-pelaez has. This may be due to adding all required permissions with the policy IAMCustomReadAccess:

{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Sid"": ""VisualEditor0"",
            ""Effect"": ""Allow"",
            ""Action"": [
                ""iam:AddRoleToInstanceProfile"",
                ""iam:CreatePolicy"",
                ""iam:CreatePolicyVersion"",
                ""iam:GenerateCredentialReport"",
                ""iam:CreateInstanceProfile"",
                ""iam:Get*"",
                ""iam:List*"",
                ""iam:DeleteRolePolicy"",
                ""iam:GenerateServiceLastAccessedDetails"",
                ""iam:PutUserPolicy"",
                ""iam:CreateRole"",
                ""iam:AttachRolePolicy"",
                ""iam:PutRolePolicy""
            ],
            ""Resource"": ""*""
        }
    ]
}

Can you confirm that you can create a Pipeline with a new Role now?"
AWS CodePipeline	"Re: Could not create IAM role
I have the same issues. I believe this is a bug of code pipeline which should be fixed.

Here is my situation:
I have two user roles. 
uLisaAdmin role can create roles, but has no permission to create CodePipeline and most other services; uLisaPowerUser role has permission to create CodePipeline and most of other services but has not permission to create roles.

if I create CodePipeline using uLisaPowerUser, no matter I pick an pre-created service role created by uLisaAdmin, or try to create a new role, I see ""cannot create role"" at the last step; Besides, each time when I create CodePipeline,  no matter I pick an pre-created service role or try to create a new role, I see that a new role is created in IAM role, the name of it shows: cwe-role-<region>-< CodePipeline name>, and trusted entity shows: event. e.g.: cwe-role-us-east-1-TestCodePipeline

if I create CodePipeline using uLisaAdmin, of Course uLisaAdmin does not have permission to create CodePipeline, but if I temporarily attach PowerUser permission to  uLisaAdmin, I can now create CodePipeline. and even I pick the pre-created service role, I saw a new role cwe-role-<region>-< CodePipeline name> is created.

Obviously, when I create a new CodePipeline, AWS tries to create a new role cwe-role-<region>-< CodePipeline name> as this user role, since user_role=uLisaPowerUser does not have permission to create roles. it fails. this is a bug, should be fixed!!!

Besides, when I create the code pipeline service role using uLisaAdmin, I was asked ""Choose the service that will use this role"" and I do not see codepipeline. so I have to choose ""code deploy"", attach policy of code deploy, and later on manually change ""CodeDeploy"" to ""CodePipeline"" in the policy json file. this is also a bug, should be fixed!!!"
AWS CodePipeline	"Re: Could not create IAM role
I'm pretty sure this is a bug.

I first tried to create a pipeline with the name ""Recommender Service"" and continued to get the IAM error.

I changed the name to ""Recommender"" and I was able to create the pipeline.

¯\_(ツ)_/¯"
AWS CodePipeline	"Re: Could not create IAM role
Had the same issue. Simply changing the name of the pipeline did the job for me too."
AWS CodePipeline	"Re: Could not create IAM role
I was facing the same issue, however after testing with multiple accounts and creating/recreating pipelines, I think I found the cause of issue.

If you create a pipeline with Cloudwatch events as an option to automatically start the Pipeline (you pick this option during source step) then code pipeline creates Cloud watch event and rule along with corresponding Role and Policy. 

I am not able to find a way to manually create that Cloudwatch service role and assign during the code pipeline as it happens in the background and there is no option to customise this step.

This is the step which results in ""Could not create IAM role "" error (If user creating pipeline does not have the permission to create IAM roles).

If you choose AWS Codecommit periodic checks as an option to automate the pipeline, then you will not face this issue."
AWS CodePipeline	"codepipeline.putJobSuccessResult() Hanging when Invoking a Lambda from VPC
After much trial and tribulation, I have determined that the code below (nodejs8.1 runtime), which is a basic CodePipeline setup with no additional code, works fine when one invokes a Lambda normally, but if one tries to invoke the Lambda from within a VPC, the `codepipeline.putJobSuccessResult()` hangs and the Lambda times out.  Any code that comes before the call runs fine, but it just won't run `codepipeline.putJobSuccessResult()` and give a proper return value back to CodePipeline despite both codepipeline and the Lambda having a role that has all kinds of policies and trust relationships and the VPC has lots of endpoints as well as a NAT Gateway and Internet Gateway.  This results in CodePipeline continually retrying until the timeout period (~15 minutes).

Note also that prior to adding the Lambda to CodePipeline and adding an endpoint that I was running the Lambda manually and successfully utilizing a static IP through a NAT => Internet Gateway (https://medium.com/@matthewleak/aws-lambda-functions-with-a-static-ip-89a3ada0b471) and again, even within the CodePipeline, the Lambda runs fine until it utilizes the AWS SDK `aws.CodePipeline.putJobSuccessResult()`/`aws.CodePipeline.putJobFailureResult()` functions; all other code is successfully executed.

In theory, to reproduce, one need only take the code below and create a Lambda, set up a VPC as described in the article above, set up a basic CodePipeline and invoke the Lambda as part of the pipeline.  The first run through should work fine.  Then assign the Lambda to the VPC and subnets, then run the pipeline again and see if it doesn't hang when it tries to putJobSuccessResult.

The hanging behavior implies it is a networking issue, but if CodePipeline has an endpoint to the VPC and successfully is able to invoke the Lambda, why would the Lambda not be able to talk back to CodePipeline to putJobSuccessResult/putJobFailureResult?  My guess is that either I'm missing something in terms of the VPC or CodePipeline isn't working correctly and/or utilizing its endpoint correctly -- but I'd like to figure it out.

It's looking more and more to me like an endpoint issue.  I found this thread while digging: https://forums.aws.amazon.com/thread.jspa?threadID=293780 – If I enabled the endpoint in the VPC, have the internet gateway, the route table points to the internet gateway, and the subnet uses the internet routable route table, and there's no security group or ACL denying traffic it is supposed to work.

    // Working Base response code for CodePipeline

    'use strict';
    const aws = require('aws-sdk');
    const codepipeline = new aws.CodePipeline();

    let environment = 'dev';
    let callback;
    let context = {
        invokeid: ''
    }

    exports.handler = async (event, context, callback) => {
        context = context;
        callback = callback;
        console.log('Inside deploy-website Lambda');
        if (!('CodePipeline.job' in event)) {
            return Promise.resolve();
        }
        // Retrieve the Job ID from the Lambda action
        let jobId;
        if (event) {
            jobId = event.id;

            // Retrieve the value of UserParameters from the Lambda action configuration in AWS CodePipeline, in this case the environment
            // to deploy to from this function
            environment = event.data.actionConfiguration.configuration.UserParameters || environment;
        }

        console.log(`Envrionment: ${environment}`);


        console.log('Copy Successful');
        console.log('Entering Results');
        return await putJobSuccess('Copy Successful', jobId);
    }

    // Notify AWS CodePipeline of a successful job
    async function putJobSuccess(message, jobId) {
        console.log(`Post Job Success For JobID: ${jobId}`);
        const params = {
            jobId: jobId
        };
        console.log(`Job Success Params: ${JSON.stringify(params)}`);
        await codepipeline.putJobSuccessResult(params).promise();
        console.log('Job Success: Successfully reported hook results');
        return callback(null, 'Job Success: Successfully reported hook results');
    }"
AWS CodePipeline	"Re: codepipeline.putJobSuccessResult() Hanging when Invoking a Lambda from VPC
It turns out it was indeed a networking issue.  It seems that the VPC route tables are what got me.  When you create a route table, it has you choose a name and a VPC to associate it to.  What I forgot to do was go to the subnets and associate them to the proper route table under the Route Table tab and/or I didn't choose the right one on one of them because when you choose a route table to associate it to, it doesn't show the logical name, just the route table ID, which makes it more prone to error.  So, while it was definitely a ""Newb"" mistake, I think there is something left to be desired in terms of user experience in associating route tables."
AWS CodePipeline	"AWS CodePipeline error: Cross-account pass role is not allowed
I am trying to create an AWS CodePipeline that deploys the production code to a separate account. The code consists of a lambda function which is setup using a sam template and cloudformation. I have it currently deploying to the same account without error. I added another stage that has a manual approval action and after approval it should deploy to the other account. It fails with the following error:

Cross-account pass role is not allowed (Service: AmazonCloudFormation; Status Code: 403; Error Code: AccessDenied; Request ID: d880bdd7-fe3f-11e7-8a8c-7dcffeae19ae)

I have a role in the production account that has a trust relationship back to the dev account that has the pipeline. I gave the pipeline role and the production role administrator policies just to make sure it was not a policy issue. I edited the pipeline using the technique in this https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-create-cross-account.html. I am following the walkthrough loosely since they are setting their scenario up just slightly different from what I am doing.

The deploy section in my pipeline looks like:
    {
       ""name"": ""my-stack"",
       ""actionTypeId"": {
           ""category"": ""Deploy"",
           ""owner"": ""AWS"",
           ""provider"": ""CloudFormation"",
           ""version"": ""1""
       },
       ""runOrder"": 2,
       ""configuration"": {
           ""ActionMode"": ""CHANGE_SET_REPLACE"",
               ""Capabilities"": ""CAPABILITY_IAM"",
           ""ChangeSetName"": ""ProductionChangeSet"",
           ""RoleArn"": ""arn:aws:iam::000000000000:role/role-to-assume"",
           ""StackName"": ""MyProductionStack"",
           ""TemplatePath"": ""BuildArtifact::NewSamTemplate.yaml""
       },
       ""outputArtifacts"": [],
       ""inputArtifacts"": [
           {
               ""name"": ""BuildArtifact""
          }
       ]
    }

I am able to assume into the role in the production account using the console. I am not sure how passrole is different but from everything I have read it requires the same assume role trust relationship.

How can I configure IAM for cross account pipelines?

Cross post from https://stackoverflow.com/questions/48362572/aws-codepipeline-error-cross-account-pass-role-is-not-allowed."
AWS CodePipeline	"Re: AWS CodePipeline error: Cross-account pass role is not allowed
Hello,

In order to deploy CloudFormation template into another account you need to use three roles:


CodePipeline ServiceRole in account A that should allow ""sts:AssumeRole"" on a role in account B [1]
The role in Account B that allows access from account A [2]
CloudFormation service role in account B to deploy itself (can be combined with the role above, then AssumeRolePolicyDocument should have two principals: Account A and service cloudformation.amazonaws.com).


The example of cross-account deploy action configuration can be found here [3]. Remember that you also need to use custom KMS key and S3 bucket with policies allow cross-account access as described in our documentation. For more details and full description of the code examples I used above please refer to our blog post [4].

Hope it helps. 

References:
[1] https://github.com/awslabs/aws-refarch-cross-account-pipeline/blob/master/ToolsAcct/code-pipeline.yaml#L198-L202
[2] https://github.com/awslabs/aws-refarch-cross-account-pipeline/blob/master/TestAccount/toolsacct-codepipeline-cloudformation-deployer.yaml#L27-L35
[3] https://github.com/awslabs/aws-refarch-cross-account-pipeline/blob/master/ToolsAcct/code-pipeline.yaml#L252-L279
[4] https://aws.amazon.com/blogs/devops/aws-building-a-secure-cross-account-continuous-delivery-pipeline/"
AWS CodePipeline	"Re: AWS CodePipeline error: Cross-account pass role is not allowed
The ""RoleArn"" inside the action configuration is for CFN to assume to access cross account B.
There is another ""roleArn"" out side action configuration (same level as region) needs to be set. This role for account A the access B, which is missing in your action."
AWS CodePipeline	"Can pipelines be placed under SCM?
Using source control to store a pipeline definition enables code review/iteration and audit trail on the pipeline itself. Jenkins supports Pipeline as Code. Is this feature available in CodePipeline?"
AWS CodePipeline	"Re: Can pipelines be placed under SCM?
Do you mean configure Jenkins with codepipeline? This might help: https://wiki.jenkins.io/display/JENKINS/AWS+CodePipeline+Plugin

And if you want to store pipeilne definition of codepipeline, you might want to store cloudformation template in source control, and deploy to cloudformation: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline.html"
AWS CodePipeline	"Codepipeline deploy stage fails with internal error
Hi, 
I have created a new pipeline that uses a github repository as source, builds a docker container and pushes it to AWS ECR. The deploy stage is configured to ECS.
The build runs successfully, but the deploy stage fails immediately with:

Action execution failed
InternalError.

I am using the default cluster, with a free tier EC2 machine that runs ecs-optimized linux. I have created a service and a task definition. 
In the ECS console, I'm getting these logs:

service default-service-2 has reached a steady state.
(service default-service-2, taskSet ecs-svc/9223370489128272172) updated state to STEADY_STATE.
(service default-service-2, taskSet ecs-svc/9223370489128272172) registered 1 targets in target-group tg-defaul-default-service-2-1
(service default-service-2, taskSet ecs-svc/9223370489128272172) has started 1 tasks: task df0b88a1-e19e-432b-9629-7795e707d71b.

And SSHing to the EC2 machine it looks like the docker container is pulled, and is actually running (appears in 'docker container ls' with appropriate datetime)

logs from EC2 machine:

2019-01-17T14:58:21Z [INFO] Saving state! module=""statemanager""
2019-01-17T14:58:21Z [INFO] Managed task [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: unable to create task state change event []: create task state change event api: status not recognized by ECS: NONE
2019-01-17T14:58:21Z [INFO] Managed task [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: waiting for any previous stops to complete. Sequence number: 13
2019-01-17T14:58:21Z [INFO] Managed task [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: wait over; ready to move towards status: RUNNING
2019-01-17T14:58:21Z [INFO] Task engine [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: pulling container tf-orayya-frontend concurrently
2019-01-17T14:58:21Z [INFO] Task engine [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: Recording timestamp for starting image pulltime: 2019-01-17 14:58:21.34925625 +0000 UTC m=+401.454788231
2019-01-17T14:58:22Z [INFO] Adding image name- 291180007533.dkr.ecr.eu-west-1.amazonaws.com/orayya-frontend:latest to Image state- sha256:cf08b69396dc50e961805cbc1fd457745e71a7bd0ecca2533d0b6a1f4d062d40
2019-01-17T14:58:22Z [INFO] Updating container reference tf-orayya-frontend in Image State - sha256:cf08b69396dc50e961805cbc1fd457745e71a7bd0ecca2533d0b6a1f4d062d40
2019-01-17T14:58:22Z [INFO] Saving state! module=""statemanager""
2019-01-17T14:58:22Z [INFO] Task engine [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: Finished pulling container 291180007533.dkr.ecr.eu-west-1.amazonaws.com/orayya-frontend:latest in 652.598942ms
2019-01-17T14:58:22Z [INFO] Task engine [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: creating container: tf-orayya-frontend
2019-01-17T14:58:22Z [INFO] Task engine [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: created container name mapping for task:  tf-orayya-frontend -> ecs-tf-orayya-frontend-9-tf-orayya-frontend-d0a5a8ffb5fad7b51500
2019-01-17T14:58:22Z [INFO] Saving state! module=""statemanager""
2019-01-17T14:58:22Z [INFO] Task engine [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: created docker container for task: tf-orayya-frontend -> 9162641afb7fb69f0553f6b2d6664e23afc8a998bebe9b6a20e4df6146d8be2d
2019-01-17T14:58:22Z [INFO] Task engine [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: created docker container for task: tf-orayya-frontend -> 9162641afb7fb69f0553f6b2d6664e23afc8a998bebe9b6a20e4df6146d8be2d, took 184.962805ms
2019-01-17T14:58:22Z [INFO] Managed task [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: unable to create task state change event []: create task state change event api: status not recognized by ECS: CREATED
2019-01-17T14:58:22Z [INFO] Task engine [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: starting container: tf-orayya-frontend
2019-01-17T14:58:22Z [INFO] Managed task [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: sending container change event [tf-orayya-frontend]: arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b tf-orayya-frontend -> RUNNING, Ports [{80 80 0.0.0.0 0}], Known Sent: NONE
2019-01-17T14:58:22Z [INFO] Managed task [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: sent container change event [tf-orayya-frontend]: arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b tf-orayya-frontend -> RUNNING, Ports [{80 80 0.0.0.0 0}], Known Sent: NONE
2019-01-17T14:58:22Z [INFO] Managed task [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: sending task change event [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b -> RUNNING, Known Sent: NONE, PullStartedAt: 2019-01-17 14:58:21.34925625 +0000 UTC m=+401.454788231, PullStoppedAt: 2019-01-17 14:58:22.001884571 +0000 UTC m=+402.107416530, ExecutionStoppedAt: 0001-01-01 00:00:00 +0000 UTC]
2019-01-17T14:58:22Z [INFO] TaskHandler: batching container event: arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b tf-orayya-frontend -> RUNNING, Ports [{80 80 0.0.0.0 0}], Known Sent: NONE
2019-01-17T14:58:22Z [INFO] TaskHandler: Adding event: TaskChange: [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b -> RUNNING, Known Sent: NONE, PullStartedAt: 2019-01-17 14:58:21.34925625 +0000 UTC m=+401.454788231, PullStoppedAt: 2019-01-17 14:58:22.001884571 +0000 UTC m=+402.107416530, ExecutionStoppedAt: 0001-01-01 00:00:00 +0000 UTC, arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b tf-orayya-frontend -> RUNNING, Ports [{80 80 0.0.0.0 0}], Known Sent: NONE] sent: false
2019-01-17T14:58:22Z [INFO] TaskHandler: Sending task change: TaskChange: [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b -> RUNNING, Known Sent: NONE, PullStartedAt: 2019-01-17 14:58:21.34925625 +0000 UTC m=+401.454788231, PullStoppedAt: 2019-01-17 14:58:22.001884571 +0000 UTC m=+402.107416530, ExecutionStoppedAt: 0001-01-01 00:00:00 +0000 UTC, arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b tf-orayya-frontend -> RUNNING, Ports [{80 80 0.0.0.0 0}], Known Sent: NONE] sent: false
2019-01-17T14:58:22Z [INFO] Managed task [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: sent task change event [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b -> RUNNING, Known Sent: NONE, PullStartedAt: 2019-01-17 14:58:21.34925625 +0000 UTC m=+401.454788231, PullStoppedAt: 2019-01-17 14:58:22.001884571 +0000 UTC m=+402.107416530, ExecutionStoppedAt: 0001-01-01 00:00:00 +0000 UTC]
2019-01-17T14:58:22Z [INFO] Managed task [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: task at steady state: RUNNING
2019-01-17T14:58:22Z [INFO] Task engine [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: started docker container for task: tf-orayya-frontend -> 9162641afb7fb69f0553f6b2d6664e23afc8a998bebe9b6a20e4df6146d8be2d, took 336.334808ms
2019-01-17T14:58:22Z [INFO] Managed task [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: redundant container state change. tf-orayya-frontend to RUNNING, but already RUNNING
2019-01-17T14:58:22Z [INFO] Managed task [arn:aws:ecs:eu-west-1:291180007533:task/df0b88a1-e19e-432b-9629-7795e707d71b]: task at steady state: RUNNING
2019-01-17T14:58:31Z [INFO] Saving state! module=""statemanager"" 


My buildspec file:

version: 0.2
 
env:
  parameter-store:
    AWS_ACCESS_KEY_ID: ""access""
    AWS_SECRET_ACCESS_KEY: ""secret""
phases:
  pre_build:
    commands:
      - echo Logging in to Amazon ECR..
      - $(aws ecr get-login --no-include-email --region eu-west-1)
  build:
    commands:
      - echo Build started on `date`
      - echo Building the Docker image....         
      - docker build -t orayya-frontend .
      - docker tag orayya-frontend:latest 291180007533.dkr.ecr.eu-west-1.amazonaws.com/orayya-frontend:latest      
  post_build:
    commands:
      - echo Build completed on `date`
      - echo Pushing the Docker image...
      - docker push 291180007533.dkr.ecr.eu-west-1.amazonaws.com/orayya-frontend:latest
      - printf '[{""name"":""tf-orayya-frontend"",""imageUri"":""291180007533.dkr.ecr.eu-west-1.amazonaws.com/orayya-frontend:latest""}]' > imagedefinitions.json
artifacts:
  files: imagedefinitions.json 


Any ideas what is going on?

Thanks in advance

Edited by: orayya on Jan 18, 2019 1:37 AM"
AWS CodePipeline	"copy build output into another build directory in codePipeline
Hello.
I just configured a pretty decent pipeline in codePipeline and I'm happy with it.

a Github commit (*Source*) will trigger a Build (run +php composer+) and generate the 
artifact that will be deployed to Elastic Beanstalk (*Staging*). everything is working perfectly fine.

I want to go one step further and change it so during the build process I want to build a secondary application (run npm start to generate some CSS and JS) and save the output into the /public directory of the first application and then deploy everything to Elastic Beanstalk. I'm not sure how to do this. can you help me?"
AWS CodePipeline	"Re: copy build output into another build directory in codePipeline
The simplest solution would be to extend your CodeBuild buildspec.yml to first run the ""other"" build commands (npm start), then run the pip composer command, then copy the outputs to /public.

You can run multiple commands in your buildspec by adding extra items to the ""commands"" list in the YAML: https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html"
AWS CodePipeline	"Re: copy build output into another build directory in codePipeline
thanks Tim for your help. that looks good
the only problem is that the dependency is in a different github repo"
AWS CodePipeline	"Deploying lambda from a subdirectory in the project directory tree
Hi,

I have a SAM Node service project I kicked off using Codestar. I have a pipeline configured and when I push to the repo all works well.

I am trying now to organise the project codes in a particular way:
/docs/...
/src/...
template.yml
buildspec.yml
.gitignore

/src is where the lambdas are stored and that's the code to be deployed onto Amazon Lambda.
/docs folder will contain various files re the projects and I don't want these files loaded as part of the lambda.

I added ""CodeUri"" in each lambda in template.yml and set it to ""./src"".

No matter what I tried I couldn't get it to work - CodeBuild always fails the build.

I noticed that node_modules wasn't created inside src, and the functions were deployed without it, causes various missing module errors. I tried to move the package.json into /src and add ""cd ./src"" before running ""npm install"" (the install phase), but that failed as well.

I am sure I am not the first one who requires this and CodeUri is all about supporting this scenario.

What am I missing?

Thanks!"
AWS CodePipeline	"Re: Deploying lambda from a subdirectory in the project directory tree
Bump"
AWS CodePipeline	"Re: Deploying lambda from a subdirectory in the project directory tree
No matter what I tried I couldn't get it to work - CodeBuild always fails the build.

What error message does CodeBuild fail with? Are you able to replicate the issue running the same SAM commands locally?

It might be worth asking over on the CloudFormation forums as SAM is closely related to CloudFormation:
https://forums.aws.amazon.com/forum.jspa?forumID=92&start=0"
AWS CodePipeline	"Suppress rebuilding when source commit hasn't changed
Is there a way to get CodePipeline to not perform a build action when the source artifacts haven't changed? I have a pipeline that has two source actions, that pull from repositories A and B, creating the source artifacts A_src and B_src respectivley. Then in the build step there are two action groups, one that builds A (which depends only on A_src) and creates the build artifact A_build, and another that builds B (which depends on B_src and A_build) and creates B_build. However, it seems that whenever I make a commit to B, A gets rebuilt, even though A_src has not changed; or at least still refers to the same commit, it appears that when the commit to B happens CodePipeline creates a new source version arn for both A and B. This is annoying for my particular case at least because A has a very long build process (about an hour) but should change only quite rarely, whereas B is the subject of most of the development and builds very quickly. 

I suppose it might be possible to extract A into a separate pipeline that builds a docker container that is used as the build environment for B, and then have some kind of trigger to kick off a new build of B when A finishes, but it would be much nicer to be able to do this in the single pipeline."
AWS CodePipeline	"Re: Suppress rebuilding when source commit hasn't changed
CodePipeline will always re-execute all the actions in your pipeline each time the pipeline runs. This ensures everything is in a correct and consistent state. For example, some builds pull in external resources (eg. dependencies from a dependency management system) and it's not always true that the build won't change because the source hasn't changed.

I suppose it might be possible to extract A into a separate pipeline that builds a docker container that is used as the build environment for B, and then have some kind of trigger to kick off a new build of B when A finishes, but it would be much nicer to be able to do this in the single pipeline.

If the extra built time is enough of an issue for you then this is probably the best option. There may also be ways to make your build faster, such as using a larger CodeBuild instance."
AWS CodePipeline	"Unit / JUnit tests on AWS pipeline
Hi,

I have been using Jenkins for building and deploying my application.
On the Jenkins pipeline I have been using gradle to do the build and run the units tests, and plugin on Jenkins for checking the results of tests. 

As I migrate the pipeline to AWS, what would be most best solution to handle the units tests results on code-pipeline?
Currently I save the test results on S3 and check them with Lambda (also I need to take care that lambda wont accidentally read old unit-tests result file.

Thank you in advance!

Edited by: tatuser on Jan 9, 2019 10:43 PM"
AWS CodePipeline	"Re: Unit / JUnit tests on AWS pipeline
I'm assuming you're using AWS CodeBuild to actually run the tests, and you're trying to fail the action if the tests fail without needing your Lambda function.

The simplest solution is to ensure your unit test command returns a non-0 exit code when they they fail. This should ensure that the CodeBuild build fails, which will in turn fail the CodeBuild action in CodePipeline."
AWS CodePipeline	"Deploy from GitHub to S3
Hello,

I would like to watch my GitHub repository and after commits deploy to S3. It's a static website that I'm hosting with S3.

I've been looking at CodePipeline, CodeDeploy and a lot of AWS documentation, but it seems that all these tools can only deploy from S3 to something like EC2 or Elastic Beanstalk.

Is there no easy way to deploy something to S3?

Currently I have to go the workaround with Travis CI, but that won't be an option once my free tier expires."
AWS CodePipeline	"Re: Deploy from GitHub to S3
Hi Dominik,

I assume you'd want to unzip the repository artifact from GitHub into an S3 bucket so you could access it as a website (as oppose to just copying the zip to a bucket)?

We don't currently have this functionality, however I'll make a note of your interest in it.

As an alternative you should be able to achieve this via our Lambda integration.

We have an example function which interacts with a zipped artifact here: http://docs.aws.amazon.com/codepipeline/latest/userguide/how-to-lambda-integration.html#how-to-lambda-more-samples

You'd need to extend that to upload the zip contents to an S3 bucket rather than CloudFormation.


Tim."
AWS CodePipeline	"Re: Deploy from GitHub to S3
I see, but would that really work? I've just looked at the CodePipeline interface and noticed that in ""Step 4: Beta"" it requires me to either choose CodeDeploy or Elastic Beanstalk. So I don't see how I can use the Lambda function only.

Your reply has made me have an idea though. I saw that there's a SNS integration in GitHub. Couldn't I just activate that Lambda function with SNS and trigger the deploy with a modified version of the function you suggested? It's not the simple solution I was looking for, but is it possible?"
AWS CodePipeline	"Re: Deploy from GitHub to S3
Hi Dominik,

Yes, you should be able to have a pipeline with just a source and Lambda action. Our quick start wizard currently requires that you have a deploy step, but you can remove that once the pipeline has been created and add a Lambda invoke action instead. Alternatively you can create the pipeline directly via the CLI and skip the wizard.

Couldn't I just activate that Lambda function with SNS and trigger the deploy with a modified version of the function you suggested?

That solution may work too, it depends on whether any of the other CodePipeline functionality is useful to you. For example, integration with other testing and build products, the ability to visually track deployment progress and to manually trigger a deployment via ""Release change"".


Tim"
AWS CodePipeline	"Re: Deploy from GitHub to S3
Since it's possible to run a static website from an S3 bucket it seems like a valid deploy target and should be supported without needing to write a Labmba function to deploy"
AWS CodePipeline	"Re: Deploy from GitHub to S3
I need this feature as well. I have a CodeBuild project that will write my files to S3 but if I invoke that same function from CodePipeline it doesn't create the artifacts. 

The workaround I'm going to end up using is setting CodePipeline to invoke a lambda function that just calls my CodeBuild job. Really wonky setup but it should work."
AWS CodePipeline	"Re: Deploy from GitHub to S3
I agree - it would be very useful to be able to deploy to S3 from CodePipeline, without having to go through Lambda."
AWS CodePipeline	"Re: Deploy from GitHub to S3
+1"
AWS CodePipeline	"Re: Deploy from GitHub to S3
Hi,

If you are using CodeBuild. An uncomplicated workaround would be to upload the S3 files as part of your build process. 

My use case is that I'm deploying an angular application. My buildspec.yml contains:
build:
    commands:
      - echo Build started on `date`
      - npm install -g @angular/cli
      - npm install
      - ng build
      - aws s3 cp dist s3://your_bucket/ --recursive


Hopefully that helps. 

Regards.
JJ"
AWS CodePipeline	"Re: Deploy from GitHub to S3
I think the Quick Start Guide in https://aws.amazon.com/about-aws/whats-new/2017/09/connect-your-git-repository-to-amazon-s3-and-aws-services-using-webhooks-and-new-quick-start/ resolves your problem. The following steps worked for me:


Create a new CloudFormation stack
Use the https://s3.amazonaws.com/quickstart-reference/git2s3/latest/templates/git2s3.template template for your stack
Enter a (new!) S3 bucket to deploy to
Note down the resulting details in the Cloudformation stack Output tab (GitPullWebHookApi and PublicSSHKey)
Create a webhook in your GitHub repository; remember to change Content type to application/json and to enter a secret
Add a deploy key in your GitHub repository; use the PublicSSHKey from your Cloudformation stack
Update the Cloudformation stack API secret with the value you entered in GitHub
Make a new commit in GitHub and see it propagated to your S3 bucket"
AWS CodePipeline	"Re: Deploy from GitHub to S3
That would work. But it’s not part of a code pipeline solution. What would you do if you wanted to add steps before the code is deployed to S3? Let’s say testing. Seems like a simple feature request to have codedeploy have a target be S3 as well."
AWS CodePipeline	"Re: Deploy from GitHub to S3
I also wish codepipeline could easily deploy to S3. I would like to deploy websites to S3 the same way that I deploy API code to EC2 machines. I keep coming back, hoping that it has been added, but no luck so far. I guess I should give in and figure out a solution with lambda..."
AWS CodePipeline	"Re: Deploy from GitHub to S3
I'm also looking for a easy way to deploy my static website to S3."
AWS CodePipeline	"Re: Deploy from GitHub to S3
I am also trying to deploy a website on S3 bucket. I have created the buckets and for frontend i am using node.js8.11 version in code build. I am able to succefully build the project and upload my output artifacts in the S3 bucket root folder itself. I can see index.html file in the S3 bucket root folder also. But when i try to load bucket endpoint url, i am getting the below error:

400 Bad Request
Code: InvalidRequest
Message: The object was stored using a form of Server Side Encryption. The correct parameters must be provided to retrieve the object.
RequestId: 7D1044E8F70659C4
HostId: v6Rw4gYw6c1ZpTdfoNYlkka8jQpcvzJt3ai7TRIGpoCyt4u9CYjHwS7siJyXayLwxmUS3vy6ve0=
An Error Occurred While Attempting to Retrieve a Custom Error Document
Code: NoSuchKey
Message: The specified key does not exist.
Key: error.html

Can anyone help me to resolve this issue, i understand that the codebuild is encrypting every output artifacts with aws kms key. In this case how can get the index.html page correctly. What bucket policy should i use?"
AWS CodePipeline	"Re: Deploy from GitHub to S3
This just gives me a zip file in my S3 bucket.

We are deploying our image files on CloudFront and just need the files unpacked in our S3 bucket. There's no code.

1. Can I have it just unpack into our bucket directly or...

2. Do I need to have another option that will unpack the zip file into yet another bucket?"
AWS CodePipeline	"Re: Deploy from GitHub to S3
I have the same needs.

Effectively, for the build process of my front-end application, I package a CloudFormation script, create a ChangeSet with the packaged script, and then execute that ChangeSet.  The script creates Outputs (Exports) for an S3 bucket name and a CloudFront distribution ID and URL.

In the next step, I build my application - passing in the CloudFront URL.  Once it's built, I simply copy the relevant files to the S3 bucket created in the CloudFormation step.

Getting this into CodePipeline has been a very frustrating experience: Source (push) -> CodeBuild (CloudFormation package) -> CodeDeploy (CloudFormation) -> CodeBuild (application compilation, testing, copy to S3 bucket)

It would probably be more straightforward to do everything with CLI commands in the buildspec for a single CodeBuild step.

Things are actually a little bit more complicated than this.  The front-end depends on a backend in ApiGateway to work.  The front-end application needs the URL of the ApiGateway to know where to send requests; the ApiGateway needs the URL where the front-end application is hosted so that it can have the correct Cross-Origin-Resource-Sharing (CORS) setting.  So chicken-and-egg situation.

Luckily, this will be solved once I can get DNS records created ahead of time.  If I have SSM parameters for API: api.example.com and APP: app.example.com, then I won't need to wait until Amazon can give me the unpredictible URLs for the ApiGateway and CloudFront distribution."
AWS CodePipeline	"GitHub Enterprise and Webhooks
According to this link: https://help.github.com/articles/about-webhooks/
Only GitHub users with admin privileges can register webhooks. This is a problem for our CI user, which is not an admin for our GitHub Enterprise account. It means we cannot set up a CodePipeline to trigger on a webhook from our GitHub repository without the attached user being an admin. I'd rather not a CI user be the admin.

CodeBuild offers a way to specify a webhook (endpoint, secret token) you create in your own GitHub account. Is there a way to do this with CodePipeline? Or are the options ""write a custom Lambda"" or ""use polling?"""
AWS CodePipeline	"Re: GitHub Enterprise and Webhooks
Please note CodePipeline is currently unable to fetch source code from GitHub Enterprise.

That said, it is possible to create a webhook on CodePipeline and manually register it with GitHUb using github console.

1. Create the webhook using PutWebhook API: https://docs.aws.amazon.com/codepipeline/latest/APIReference/API_PutWebhook.html, your request should look something like:

aws --profile felipeal codepipeline put-webhook --cli-input-json file:///tmp/webhook.json

and webhook.json contents should look like:

{
  ""webhook"": {
      ""name"": ""<webhook-name>"",
      ""targetPipeline"": ""<pipeline-name>"",
      ""targetAction"": ""<source-action-name>"",
      ""filters"": [
      {
        ""jsonPath"": ""$.ref"",
        ""matchEquals"": ""refs/heads/{Branch}""
      }
    ],
    ""authentication"": ""GITHUB_HMAC"",
    ""authenticationConfiguration"": {
      ""SecretToken"": ""<secret-token>""
    }
  }
}


2. use the secret-token you pass on the request and the URL returned by the API to register the webhook on github console"
AWS CodePipeline	"Re: GitHub Enterprise and Webhooks
Thanks, I will have to try this out when I am able to get CodeBuild to properly output an artifact."
AWS CodePipeline	"Code pipeline not working with Git LFS
It seems to me as if Git LFS files are being ignored, is that so ?"
AWS CodePipeline	"Re: Code pipeline not working with Git LFS
Unfortunately, Git LFS files are not supported at the moment. Only the text file which includes a link to it is included in the artifact, but not the file itself."
AWS CodePipeline	"Re: Code pipeline not working with Git LFS
Is there any plan to include this in the future ?"
AWS CodePipeline	"Re: Code pipeline not working with Git LFS
This sounds like a feature that would benefit many customers and I've made a note of your interest on our internal feature request."
AWS CodePipeline	"Re: Code pipeline not working with Git LFS
+1, would (still) really like to see git lfs support."
AWS CodePipeline	"Re: Code pipeline not working with Git LFS
It would be really helpful to have support to git LFS in AWS code pipeline indeed.
Such a great feature from git, and is supported by a lot of CI systems. It is a pity it is not supported here yet."
AWS CodePipeline	"Cannot use immutable identifier (sha256 image URI) in imagedefinitions.json
Docker has 2 ways to specify a image, tag and digest.

ECS task definition works well both, then I can use digest image URI, however imagedefinitions.json in codepipeline dinied digest format image URI.

Message by codepipeline console.
ConfigurationError
The image URI contains invalid characters.


This is failed imagedefinitions.json.
[
  {
    ""name"": ""this-is-container-name"",
    ""imageUri"": ""1234567890.dkr.ecr.ap-northeast-1.amazonaws.com/our-ecr-repository-name@sha256:9af9edad625a2c68412abc396ec1178ed51a8d8a538bca2f6d221d6160dbe13b""
  }
]


If I rewrite by tag, works perfectly.
[
  {
    ""name"": ""this-is-container-name"",
    ""imageUri"": ""1234567890.dkr.ecr.ap-northeast-1.amazonaws.com/our-ecr-repository-name:latest""
  }
]


I guess this is imagedefinitions.json validation bug of codepipeline, isn't it?"
AWS CodePipeline	"Re: Cannot use immutable identifier (sha256 image URI) in imagedefinitions.json
Thanks for bringing that to our attention, we'll investigate the issue and post an update to this thread."
AWS CodePipeline	"Re: Cannot use immutable identifier (sha256 image URI) in imagedefinitions.json
We identified the problem and we're working on the fix"
AWS CodePipeline	"Re: Cannot use immutable identifier (sha256 image URI) in imagedefinitions.json
The issue should be fixed now, please try again and let me know if you have problems.

Thank you again for bringing this to my attention."
AWS CodePipeline	"False Positive Pipeline Failures, Again
We originally posted about seeing issue on 10/23/2018 here:
https://forums.aws.amazon.com/thread.jspa?threadID=293762

It was reported that this was due to a bug that had been resolved.  However, we've started to see this behavior again, where a CodePipeline execution will report as failed, but drilling into the CodeBuild execution shows a successful outcome.  

In this case, the CodePipeline execution ID was 62c2ebec-38c6-4a7c-80e1-83ce41d8cfa9, and the CodeBuild execution ID was primary-spree_core:4bde06a6-88b9-4fd7-9a4c-bf588e0f2e44. 

Please advise."
AWS CodePipeline	"Re: False Positive Pipeline Failures, Again
Hello primarymark,

Sorry you're still seeing this issue.

I've confirmed that the fix for the bug that was causing this issue to show up more regularly has indeed been deployed. I checked the last three weeks and only see a failure being reported this one time in your account. Let me know if I'm wrong here and you are seeing this more frequently than my research suggests.

As for the root cause, we are still investigating and hope to have some detail for you soon. Thanks for bringing this to our attention!

Luke"
AWS CodePipeline	"Re: False Positive Pipeline Failures, Again
Hi Luke,
I can confirm that this time was the only reported case we've seen in the last few weeks. I will update if any of our users report the issue again.

-Mark"
AWS CodePipeline	"Cannot get artifact encryption using customer-managed KMS key to work
Hi,

This query is related to a 'child' account in AWS organizations that I am using.

I am having trouble getting KMS encryption on codepipeline artifacts working when using my own key.

I have created a key and added permissions to it to allow my codepipeline role to encrypt/decrypt.

I have given my codepipeline role permission to use the KMS key.

I have configured my codepipeline properly to enable KMS encryption using my key ID on artifacts.

Yet when I try to run the pipeline, I get his error:

Insufficient permissions
The provided role does not have permissions to perform this action. Underlying error: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 9CD9441D6F6288AF; S3 Extended Request ID: uytHPZmvE64+eZuZuQO/VkLi0oC/1krYRTWgrwpjgcd+rEaDiK+ViQeRVxZewo7JpZcc/XYJ6kU=)

Can anyone from AWS support help?"
AWS CodePipeline	"Re: Cannot get artifact encryption using customer-managed KMS key to work
I figured it out - I need to give the codepipeline role kms:GenerateDataKey and kms:GenerateDataKeyWithoutPlaintext on the role."
AWS CodePipeline	"CodeBuild falsely reports success in uploading output artifacts
I have a CodeBuild activity inside of a CodePipeline that is operating on an input artifact (generated by a source activity retrieving code from GitHub). The build activities run successfully, except for the part where it needs to upload an output artifact for CodeDeploy to operate on. In the CodeBuild logs, the phase details indicate a successful upload but there is no output artifact generated. CodeDeploy then fails as a result.

The source is writing to the same bucket that the output artifact is supposed to be placed into, so the service role for the CodePipeline definitely has write access to the bucket. The bucket being used is the default that is generated by CodePipeline, so it should be passed in to CodeBuild since CodeBuild is using CodePipeline as a source. I don't know if it's the same issue as  https://forums.aws.amazon.com/thread.jspa?threadID=293898&tstart=0 . CodePipeline execution ID is 826d82e6-a588-407c-a523-a413ca845570. CodeBuild execution ID is 0655cf6e-3a02-4199-b48b-2ac1d15f7b54. I re-created the CodePipeline in case it was just that particular configuration that was in error, but the same problem happens."
AWS CodePipeline	"Re: CodeBuild falsely reports success in uploading output artifacts
I experience a very similar issue: CodeBuild should upload an artifact to a S3 bucket, and it actually does so when I start CodeBuild directly; however, when triggered through CodePipeline, it still reports success but no artifact is uploaded to S3.

I tried attaching AmazonS3FullAccess to the CodePipeline IAM role but that doesn't help."
AWS CodePipeline	"Re: CodeBuild falsely reports success in uploading output artifacts
Update: since I'm deploying to Beanstalk, I can see on the Application Versions page that an artifact is uploaded to Beanstalk - although nothing changes in the S3 bucket.

Not sure if it's important: the artifact should be a testapp1.war file, but for artifacts pushed through CodePipeline the Application Versions page shows as Source: ""TestApp1/BuildArtif/woV4T72"" which is a zip file that contains target/testapp1.war instead.

Edited by: FreeWill on Dec 10, 2018 5:26 PM"
AWS CodePipeline	"Re: CodeBuild falsely reports success in uploading output artifacts
Ok, I think I get it now, after reading some more docs like this: https://docs.aws.amazon.com/codebuild/latest/APIReference/API_ProjectArtifacts.html

=> When using CodeBuild from within CodePipeline, all the S3 related settings are ignored, so it's perfectly normal that nothing appears in the S3 bucket.

Makes sense, it's just confusing.

Now I still have to find out how to get my .war to tomcat without it being zipped. Or rather have it unzipped by Beanstalk...?

Edited by: FreeWill on Dec 11, 2018 3:56 AM"
AWS CodePipeline	"Re: CodeBuild falsely reports success in uploading output artifacts
""When using CodeBuild from within CodePipeline, all the S3 related settings are ignored, so it's perfectly normal that nothing appears in the S3 bucket.""

I believe this refers to the S3 bucket that CodeBuild is defaulted to. CodePipeline should be overriding those settings. There should still be an output artifact so that CodeDeploy can pick it up. The error message I get is ""unable to access artifact"" in the deploy stage. The role attached to the pipeline Allows s3:*, which is the role created from CodePipeline has. It's not a permissions issue as far as I can tell."
AWS CodePipeline	"Access Denied on DOWNLOAD_SOURCE
Hi,

I'm trying to create a pipeline to trigger a build with CodeBuild on committing to CodeCommit.
I created a build on CodeBuild. I can start this manually - this works just fine. When it is triggered by my pipeline, it fails in phase DOWNLOAD_SOURCE with the message Access Denied. 
It looks like a missing permission. So - I started with an ""yes to all""-Pipeline and Build. Now I've three roles:

One Role with ""trusted entitiy"" codepipeline. The attached policy shows CancelUploadArchive, GetBranch, GetCommit, GetUploadArchiveStatus, UploadArchive for CodeCommit on all ressources. For CodeBuild it shows BatchGetBuilds, StartBuild on all ressource.
Another role for CodeBuild. This policy is allowed for all actions on all codecommit-ressources.  
A third role for AWS service events, which may codepipeline:StartPipelineExecution on my pipeline.

So: my pipeline gets triggered and the first step ""Source"" passes. On ""Build"" I get the error from CodeBuild. I don't really understand which role requires another permission: the one for CodeBuild or for CodePipeline? Actually both should be allowed.

Cheers, th1l6f"
AWS CodePipeline	"Re: Access Denied on DOWNLOAD_SOURCE
I ran into this issue yesterday and the docs are not very helpful. What I found is that CodePipeline downloads your source code from Github/CodeCommit (in the Source stage) and uploads it to S3 as a zip. So whatever build step you use (in your case, CodeBuild) needs to have access to the bucket that CodePipeline created via the role you assigned to CodeBuild.

In addition (this is actually the main issue I ran into), for some reason using an existing CodeBuild project in CodePipeline does NOT work. The CodeBuild will run fine when started manually and has no problem downloading the source from Github/CodeCommit/whatever, but when CodeBuild is triggered from CodePipeline it is passed an artifacts parameter and does not use the source set in the CodeBuild project. You can see this if you look in the CodeBuild logs - it's trying to get the source from S3.

The way I solved this was by creating the CodeBuild project when creating the CodePipeline. In ""Step 3: Build"" select ""AWS CodeBuild"" and then ""Create a new build project"". That will allow it to pull in the correct artifacts. After you do that, check out the new CodeBuild project. You will see that the ""Current source"" is ""AWS CodePipeline"", which is NOT even an option when creating a CodeBuild project from scratch.

HTH

Dave"
AWS CodePipeline	"Re: Access Denied on DOWNLOAD_SOURCE
Hi Dave,
thank you very much for letting us know about your solution! It works fine for me.

Just one more question about your answer: you said, you got some hints in the CodeBuild logs. I didn't see anything there, just the phase and ""access denied"". Were did you get the hint about S3?

Thanks you! Cheers, th1l6f"
AWS CodePipeline	"Re: Access Denied on DOWNLOAD_SOURCE
Hello,

This generally happens when you have a CodeBuild project already and you integrate it to a CodePipeline pipeline. When you integrate a Codebuild project with CodePipeline, the project will retrieve it's source from the CodePipeline Source output. Source output will be stored in the artifact store location, which is an S3 bucket, either a default bucket created by CodePipeline or one you specify upon pipeline creation. The default is usually something like, codepipeline-us-east-1-accnumber (where us-east-1 would be replaced with pipeline region and accnumber with your account number).

So, you will need to provide permissions to the CodeBuild Service role to access the CodePipline bucket in S3. The role will require permissions to put S3 objects in the bucket, as well as get objects.

The Codebuild role would need permissions such as below sample operations:

s3:PutObject
s3:GetObject
s3:GetObjectVersion

Setting up service roles for CodeBuild: https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role


Ajish"
AWS CodePipeline	"Re: Access Denied on DOWNLOAD_SOURCE
Everything works. But when I select my VPC in CodeBuild settings I can't get the code. There is an error CLIENT_ERROR: RequestError: send request failed caused by: Get https://codepipeline-us-east-1-xxxxxxx.s3.amazonaws.com/xxxxx-dev/SourceArti/xxxxx: dial tcp 52.216.97.51:443: i/o timeout for primary source and source version arn:aws:s3:::codepipeline-us-east-1-xxxxxxx/xxxxx-dev/SourceArti/xxxxx

If I remove VPC in Enrironment section of the CodeBuild project - it works. How to run CodeBuild inside of VPC?"
AWS CodePipeline	"CodePipeline artifact store to S3 has truncated the folder name
Hi,

I had codepipeline with a long name such as ""test-develop-long-name-of-pipeline""
Then when PipeLine has executed the folder under S3 bucket was truncated to ""test-develop-long-na""

The spec of pipeline name has limited to 100 characters. 
Why the folder name under S3 bucket allowed only 21 characters?

Regards,
Thanh"
AWS CodePipeline	"Re: CodePipeline artifact store to S3 has truncated the folder name
CodePipeline will behave correctly regardless of the truncation, so you don't need to be concerned if multiple pipelines share the same prefix in S3.

It's truncated to ensure the full S3 path does not exceed policy size limits when CodePipeline generates temporary credentials for job workers (eg. Jenkins plugin, custom actions or Lambda functions).

If you need to determine the pipeline name for a given job you can call the  https://docs.aws.amazon.com/codepipeline/latest/APIReference/API_GetJobDetails.html  with the job ID which will return the pipeline name under the pipeline context field."
AWS CodePipeline	"Re: CodePipeline artifact store to S3 has truncated the folder name
Thank you for your clarification."
AWS CodePipeline	"Re: CodePipeline artifact store to S3 has truncated the folder name
Thanks for clarification. Can someone please add this to the CodePipeline documentation? I just wasted a couple hours trying to troubleshoot this issue in my terraform templates before I came across this forum posting.

In my case, I was trying to manage the IAM policy which governs access to the artifact bucket. My build job was failing because the IAM policy had the full path prefix for the artifact store, but CodePipeline had truncated it to 20 characters, so the policy didn't match on the path."
AWS CodePipeline	"ECR Source with image in another account/region
I'm trying to run a pipeline that should pull the image info from another account & region. Account A can already access repositories in account B and just using a task definition with the correct ARN works when deploying a service directly.

What I want to do is have the pipeline, running in region us-west-1 in account A, fetch info from a repository in region us-east-1 in account B. Currently, the pipeline action configuration seems to support the parameters RepositoryName and ImageTag (although this last one is not shown in the docs in the pipeline ref:  https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html#action-requirements )

I've tried modifying the pipeline via command line and while aws won't complain when I change RepositoryName to an ARN, it will fail when running the pipeline with the following error:

ConfigurationError
Repository name or image tag is invalid. Underlying error: Invalid parameter at 'repositoryName' failed to satisfy constraint: 'must satisfy regular expression '(?:[a-z0-9]+(?:[._-][a-z0-9]+)*/)*[a-z0-9]+(?:[._-][a-z0-9]+)*'' (Service: AmazonECR; Status Code: 400; Error Code: InvalidParameterException; Request ID: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx)


Is there a way to achieve this directly with CodePipeline in its current state? My alternative for now is to avoid all of this and trigger a new deployment by calling codedeploy:CreateDeployment manually with a custom appspec.yml file. But this is just recreating what CodePipeline could be doing for me, so I'd like to use it if possible.

Thanks"
AWS CodePipeline	"Re: ECR Source with image in another account/region
Hi,

Currently the ECR source action doesn't support cross region or cross account use case. Thanks for your feature request and we will look into that."
