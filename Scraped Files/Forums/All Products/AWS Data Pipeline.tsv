label	description
AWS Data Pipeline	"ShellCommandActivity timedout before reaching terminateAfter
Based of our requirements we need to pull 3rd party API to retrieve data and as number of calls per second and amount of data per call limited, i do it in cycle inside python script. My estimation job should run for about 3 hours, but it's getting TIMEDOUT in about 1,5 hours. Setting on Ec2Resource 'terminateAfter' set to 6 hours. 

PipelineId df-0294889QJXIOPTDILRQ

Thanks,
Sergei"
AWS Data Pipeline	"Re: ShellCommandActivity timedout before reaching terminateAfter
My recent reruns shows that it can TIMEDOUT in less than 45 minutes and unfortunately Stdout log not collected in full, i assume just some buffer got coppied to S3.

That's beg the question - what are triggers and controls TIMEDOUT for ShellCommandActivity.

Recreated pipeline id df-046239115F6OM7BE8JE5

Sergei"
AWS Data Pipeline	"Re: ShellCommandActivity timedout before reaching terminateAfter
Hi Team,

I am also facing this issue:

My EC2 ""Terminate After"" set to 2 hours.
But my Shell Command Activity Failed with Status ""TIMEDOUT"" in 20-30 mins only.

Also as per the documentation: https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-status.html 
TIMEDOUT
The resource exceeded the terminateAfter threshold and was stopped by AWS Data Pipeline. After the resource reaches this status, AWS Data Pipeline ignores the actionOnResourceFailure, retryDelay, and retryTimeout values for that resource. This status applies only to resources.

Then why it is creating problem in shell command Activity.

Please help me to resolve this issue

PIPELINE ID: df-05597142MC6XESXMKWFH"
AWS Data Pipeline	"Datapipline : from rds to s3 (csv with headers)
Hi, 

Currently I have created a data pipeline for data transfer from rds to s3. It works fine i.e. it transfers the data in the csv file. But the headers are repeated 3 times and they are not the first record. They appear in between and at different row positions. 

What i require is that the header,  to be the first row and rest of the 2 rows should not exist. Is there a property which handles this?

Thanks
Prachin Soparkar"
AWS Data Pipeline	"Error migrating data from Dynamo DB to S3
I need to export data from Dyanmo DB to S3, however always encounter this error

errorMsg :	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:322)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:198)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1341)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1338)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1338)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:575)
	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:570)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:570)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:561)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:870)
	at org.apache.hadoop.dynamodb.tools.DynamoDbExport.run(DynamoDbExport.java:79)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.dynamodb.tools.DynamoDbExport.main(DynamoDbExport.java:30)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:234)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:148)

How do I debug this, or proceed. Any help will be appreciated"
AWS Data Pipeline	"Re: Error migrating data from Dynamo DB to S3
Hello,

Thank you for reaching out to us. 

Can you please provide the Data Pipeline ID in order to investigate the exact cause and provide a resolution.

Alternatively, you would consider opening a support case for prompt response.

Thanks,
Snehal"
AWS Data Pipeline	"Re: Error migrating data from Dynamo DB to S3
Thank you,

It turns out that tables with on demand capacity are not yet supported by Data Pipeline.
So to resolve it, I got to manually assign the read throughput"
AWS Data Pipeline	"IAM role instead of IAM user
Hi,
  Is it possible to user the IAM role of the EC2 instance running TaskRunner instead of entering an IAM user access key and secret?

Thanks"
AWS Data Pipeline	"Re: IAM role instead of IAM user
Hi,

Yes, it is possible to make use of EC2 instance role instead of configurations i.e. IAM user access key and secret key.

Steps -

Launch a fresh instance with the correct role - ""DataPipelineDefaultResourceRole"", and then downloaded the Taskrunner jar.

$  java -jar TaskRunner-1.0.jar --workerGroup=wg-xxxxx --region=eu-west-1 & disown

The IAM role on the instance should be: ""DataPipelineDefaultResourceRole""

Sample is here:
 https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html 

Please let me know if this is working on your side. 

I hope above helps. If you are facing and further issue, you can definitely contact AWS Support for more in-depth investigation."
AWS Data Pipeline	"Record status of Pipeline and individual activity
I am looking for a way to record the status of the pipeline in a DB table. Assuming this is a very common use case.
Is there any way where I can record 
1. status and time of completion of the complete pipeline.
2. status and time of completion of selected individual activities.
3. the ID of individual runs/execution.

The only way I found was using SQLActivity that is dependent on an individual activity but even there I cannot access the status of the parent/node.

I am using a jdbc connection to connect to a remote SQLServer. And the pipeline is for coping S3 files into the SQLServer DB."
AWS Data Pipeline	"Unable to validate an instance profile with the role DataPipelineDefault
Hi,
I am facing a weird issue while trying to set up a DataPipeline via Cloudformation.

The Cloudformation yaml file is used to create the two needed Roles ( DataPipelineDefaultRole and DataPipelineDefaultResourceRole ) and its DataPipeline as described in the AWS doc : https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-datapipeline-pipeline.html

I am using exactly that example including the creation of two Roles by strictly following this AWS tutorial : https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-iam-roles.html

To make it short:

If I create the two roles via AWS Web Console and then run the CloudFormation process, everything works as expected (datapipeline and all needed resources are properly created).

But if I try to include the creation of the roles into the CloudFormation file and skip the Web Console, then I get the below error :
Pipeline Definition failed to validate because of following Errors: [{ObjectId = 'EmrClusterForBackup', errors = [Unable to validate an instance profile with the role name'DataPipelineDefaultResourceRole'.Please create an EC2 instance profile with the same name as your resource role]}] and Warnings: [{ObjectId = 'Default', warnings = ['pipelineLogUri'is missing. It is recommended to set this value on Default object for better troubleshooting.]}]


So, I have spent hours today trying to debug this issue and can guarantee that the generated Roles are identical either using the Web Console or the CloudFormation definition. I have extracted their json definition via iam get-role command in both cases and they are indeed the same.

Can someone help out here ?
Best,
M.

Edited by: tundraspar on Feb 6, 2019 1:22 PM"
AWS Data Pipeline	"Re: Unable to validate an instance profile with the role DataPipelineDefault
just compared the Roles details and noticed the one created via CF automation has an extra line (Sid:) which is empty anyway:

Role generated via Web Console
{
    ""Role"": {
        ""RoleName"": ""DataPipelineDefaultRole"",
        ""CreateDate"": ""2019-02-06T17:22:13Z"",
        ""RoleId"": ""AROAI2B7HMTSEAUOGJOK4"",
        ""Path"": ""/"",
        ""Arn"": ""arn:aws:iam::429416768433:role/DataPipelineDefaultRole"",
        ""AssumeRolePolicyDocument"": {
            ""Version"": ""2012-10-17""
            ""Statement"": [
                {
                    ""Effect"": ""Allow"",
                    ""Principal"": {
                        ""Service"": [
                            ""elasticmapreduce.amazonaws.com"",
                            ""datapipeline.amazonaws.com""
                        ]
                    },
                    ""Action"": ""sts:AssumeRole""
                }
            ],
        },
    }
}


Role generated via CF
{
    ""Role"": {
        ""RoleName"": ""DataPipelineDefaultRole"",
        ""CreateDate"": ""2019-02-06T17:46:25Z"",
        ""RoleId"": ""AROAJGHEOSAQTO6DWRNWY"",
        ""Path"": ""/"",
        ""Arn"": ""arn:aws:iam::429416768433:role/DataPipelineDefaultRole"",
        ""AssumeRolePolicyDocument"": {
            ""Version"": ""2012-10-17"",
            ""Statement"": [
                {
                    ""Effect"": ""Allow"",
                    ""Principal"": {
                        ""Service"": [
                            ""datapipeline.amazonaws.com"",
                            ""elasticmapreduce.amazonaws.com""
                        ]
                    },
                    ""Action"": ""sts:AssumeRole"",
                    ""Sid"": """"
                }
            ]
        }
    }
}


Can it interfere somehow ?"
AWS Data Pipeline	"Re: Unable to validate an instance profile with the role DataPipelineDefault
After having spent hours on this, I found out that there is a need to create the instanceProfile (manually or via CM if you use any automation tools like ansible, terraform or chef).

The AWS documentation was a bit misleading as the Emr cluster definition field specifies to provide the resourceRole whereas the instanceProfile previously created was meant to be set there.

Here is my terraform procedure :
data ""aws_iam_policy_document"" ""ec2_assume_role"" {
  statement {
    effect = ""Allow""
    principals {
      type        = ""Service""
      identifiers = [""ec2.amazonaws.com"",""datapipeline.amazonaws.com"",""elasticmapreduce.amazonaws.com""]
    }
    actions = [""sts:AssumeRole""]
  }
}
 
resource ""aws_iam_role"" ""emr_ec2_instance_profile"" {
  name               = ""MyInstanceProfile""
  assume_role_policy = ""${data.aws_iam_policy_document.ec2_assume_role.json}""
}
 
resource ""aws_iam_role_policy_attachment"" ""emr_ec2_instance_profile1"" {
  role       = ""${aws_iam_role.emr_ec2_instance_profile.name}""
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceforEC2Role""
}
 
resource ""aws_iam_role_policy_attachment"" ""emr_ec2_instance_profile2"" {
  role       = ""${aws_iam_role.emr_ec2_instance_profile.name}""
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforDataPipelineRole""
}
 
resource ""aws_iam_instance_profile"" ""emr_ec2_instance_profile"" {
  name = ""${aws_iam_role.emr_ec2_instance_profile.name}""
  role = ""${aws_iam_role.emr_ec2_instance_profile.name}""
}
 

In short :

create the Policy Document
a IAM Role
Two Policies attachment MapReduce and DataPipeline (perhaps the first one not needed though)
The most important => attach them together with the instanceProfile


Let me know if you need more help or get stuck
Hope it helps!
Best"
AWS Data Pipeline	"AWS Data Pipeline future?
Out of curiosity, for any AWS people monitoring this forum, what are the plans, if any, for further evolving the AWS Data Pipeline service. I assume since there has been very little activity over the last several years that there are not further plans to iterate or improve this service?

For instance, are there any plans to add CloudWatch events and monitoring for pipeline specific activities and events? Any plans for integrating with other future/current data stores (e.g. Aurora Serverless via SQL Activity) and EC2 generations?"
AWS Data Pipeline	"Aurora Serverless Integration
Are there any plans to add Aurora Serverless integration with the AWS Data Pipeline? 

Right now, it does not appear to be supported for the ""SQLActivity"" and ""RdsDatabase"" for example."
AWS Data Pipeline	"Re: Aurora Serverless Integration
Could you post the exception/error detail ?
 you can try jdbcdatabase instead of rdsinstance 

https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html"
AWS Data Pipeline	"Re: Aurora Serverless Integration
I needed to use the ""LOAD"" command and Aurora Serverless does not support LOAD DATA FROM S3. I used the ShellActivity to stage the data and ""LOAD DATA FROM INFILE"" as a workaround."
AWS Data Pipeline	"Data pipeline could not find my RDS amazon aurora(mysql) instance
Object:RdsMySqlTableCreateActivity
ERROR: RDs instance with id wsbdbaws not found in region ap-south-1

RDS instance is in ap-south-1 region and data pipeline in ap-southeast-2

Here is my pipeline object:

{
  ""objects"": [
    {
      ""output"": {
        ""ref"": ""DestinationRDSTable""
      },
      ""input"": {
        ""ref"": ""S3InputDataLocation""
      },
      ""dependsOn"": {
        ""ref"": ""RdsMySqlTableCreateActivity""
      },
      ""name"": ""DataLoadActivity"",
      ""id"": ""DataLoadActivity"",
      ""runsOn"": {
        ""ref"": ""Ec2Instance""
      },
      ""type"": ""CopyActivity""
    },
    {
      ""failureAndRerunMode"": ""CASCADE"",
      ""resourceRole"": ""DataPipelineDefaultResourceRole"",
      ""role"": ""DataPipelineDefaultRole"",
      ""pipelineLogUri"": ""s3://path-of-log-file/"",
      ""scheduleType"": ""ONDEMAND"",
      ""name"": ""Default"",
      ""id"": ""Default""
    },
    {
      ""instanceType"": ""t1.micro"",
      ""name"": ""Ec2Instance"",
      ""actionOnTaskFailure"": ""terminate"",
      ""securityGroups"": ""#{myEc2RdsSecurityGrps}"",
      ""id"": ""Ec2Instance"",
      ""type"": ""Ec2Resource"",
      ""terminateAfter"": ""2 Hours""
    },
    {
      ""database"": {
        ""ref"": ""rds_mysql""
      },
      ""name"": ""RdsMySqlTableCreateActivity"",
      ""runsOn"": {
        ""ref"": ""Ec2Instance""
      },
      ""id"": ""RdsMySqlTableCreateActivity"",
      ""type"": ""SqlActivity"",
      ""script"": ""#{myRDSCreateTableSql}""
    },
    {
      ""name"": ""DataFormat1"",
      ""id"": ""DataFormat1"",
      ""type"": ""CSV""
    },
    {
      ""*password"": ""#{*myRDSPassword}"",
      ""name"": ""rds_mysql"",
      ""jdbcProperties"": ""allowMultiQueries=true"",
      ""id"": ""rds_mysql"",
      ""type"": ""RdsDatabase"",
      ""region"": ""ap-south-1"",
      ""rdsInstanceId"": ""#{myRDSInstanceId}"",
      ""username"": ""#{myRDSUsername}""
    },
    {
      ""database"": {
        ""ref"": ""rds_mysql""
      },
      ""name"": ""DestinationRDSTable"",
      ""insertQuery"": ""#{myRDSTableInsertSql}"",
      ""id"": ""DestinationRDSTable"",
      ""type"": ""SqlDataNode"",
      ""table"": ""#{myRDSTableName}"",
      ""selectQuery"": ""select * from #{table}""
    },
    {
      ""directoryPath"": ""#{myInputS3Loc}"",
      ""dataFormat"": {
        ""ref"": ""DataFormat1""
      },
      ""name"": ""S3InputDataLocation"",
      ""id"": ""S3InputDataLocation"",
      ""type"": ""S3DataNode""
    }
  ],
  ""parameters"": [
    {
      ""description"": ""RDS MySQL password"",
      ""id"": ""*myRDSPassword"",
      ""type"": ""String""
    },
    {
      ""watermark"": ""security group name"",
      ""helpText"": ""The names of one or more EC2 security groups that have access to the RDS MySQL cluster."",
      ""description"": ""RDS MySQL security group(s)"",
      ""isArray"": ""true"",
      ""optional"": ""true"",
      ""id"": ""myEc2RdsSecurityGrps"",
      ""type"": ""String""
    },
    {
      ""description"": ""RDS MySQL username"",
      ""id"": ""myRDSUsername"",
      ""type"": ""String""
    },
    {
      ""description"": ""Input S3 file path"",
      ""id"": ""myInputS3Loc"",
      ""type"": ""AWS::S3::ObjectKey""
    },
    {
      ""helpText"": ""The SQL statement to insert data into the RDS MySQL table."",
      ""watermark"": ""INSERT INTO #{table} (col1, col2, col3) VALUES(?, ?, ?) ;"",
      ""description"": ""Insert SQL query"",
      ""id"": ""myRDSTableInsertSql"",
      ""type"": ""String""
    },
    {
      ""helpText"": ""The name of an existing table or a new table that will be created based on the create table SQL query parameter below."",
      ""description"": ""RDS MySQL table name"",
      ""id"": ""myRDSTableName"",
      ""type"": ""String""
    },
    {
      ""watermark"": ""CREATE TABLE pet IF NOT EXISTS (name VARCHAR(20), owner VARCHAR(20), species VARCHAR(20), gender CHAR(1), birth DATE, death DATE);"",
      ""helpText"": ""The idempotent SQL statement to create the RDS MySQL table if it does not already exist."",
      ""description"": ""Create table SQL query"",
      ""optional"": ""true"",
      ""id"": ""myRDSCreateTableSql"",
      ""type"": ""String""
    },
    {
      ""watermark"": ""DB Instance"",
      ""description"": ""RDS Instance ID"",
      ""id"": ""myRDSInstanceId"",
      ""type"": ""String""
    }
  ],
  ""values"": {
    ""myRDSInstanceId"": ""wsbdbaws"",
    ""myRDSUsername"": ""test"",
    ""myRDSTableInsertSql"": ""INSERT INTO #{table} (mobile, name, pincode) VALUES(?,?,?)"",
    ""*myRDSPassword"": ""testpw"",
    ""myEc2RdsSecurityGrps"": [
      ""rds-launch-wizard"",
      ""default""
    ],
    ""myRDSCreateTableSql"": ""CREATE TABLE `test_data` (\n  `id` INT(20) NOT NULL AUTO_INCREMENT,\n  `mobile` VARCHAR(10) NULL,\n  `name` VARCHAR(45) NULL,\n  `pincode` VARCHAR(6) NULL,\n  PRIMARY KEY (`id`));"",
    ""myInputS3Loc"": ""s3://test/csv/"",
    ""myRDSTableName"": ""test""
  }
}


Can anyone help me to fix it?"
AWS Data Pipeline	"Getting ""Instance type 'm3.xlarge' is not supported"" error
I am trying to take DynamoDB table dump (db located in ap-south-1) using a data pipeline in us-east-1, that spawns an EMR in ap-south-1.

I get the following error:
Unable to create resource for @EmrClusterForBackup_2019-01-15T07:36:34 due to: Instance type 'm3.xlarge' is not supported. (Service: AmazonElasticMapReduce; Status Code: 400; Error Code: ValidationException;

I get the above error irrespective of the choice of instance type i.e. irrespective of the instance type I select for Master and Core instances, I get the above error. I have tried m4.large, m4.xlarge, m5.xlarge, c4.large, c5.xlarge, each of which are available in ap-south-1

I can provide request id in DM, if that helps.

Edited by: siberiancrane on Jan 14, 2019 11:55 PM"
AWS Data Pipeline	"Re: Getting ""Instance type 'm3.xlarge' is not supported"" error
Set resizeClusterBeforeRunning=false in EMR activity so that dp uses same instance what you configure in definition otherwise, it would try to use m3.xlarge. 

https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html"
AWS Data Pipeline	"AWS Data Pipeline stuck in ""WAITING FOR RUNNER""
Hi,

all my AWS Data Pipelines i create to export data from a dynamodb table to a .csv in an s3 bucket are stuck in the ""WAITING FOR RUNNER"" state. I am following the tutorial here: https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb.html

I also checked the values ""Runs on resource"" which is: ""EmrClusterForBackup"" and ""workerGroup"" which is ""df-0274180EXA6BQJAA9HV_@EmrClusterForBackup_2019-01-13T10:21:37""

can you tell me if these values are correct?"
AWS Data Pipeline	"Re: AWS Data Pipeline stuck in ""WAITING FOR RUNNER""
Common issue would be ,
1.  check the IAM access of emr ec2 role and make sure that it has datapipeline:*
2.  make sure that internet connectivity is available from emr/ec2  either nat or igw

you can add ec2 key in emr cluster and log in to emr master  (ssh) then check task runner log

tail -100f /mnt/taskrunner/output/logs/tasrunner*.log

https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html#dp-verify-task-runner"
AWS Data Pipeline	"Re: AWS Data Pipeline stuck in ""WAITING FOR RUNNER""
Hi Shivan, 
thank you for your answer.

I think with EMR EC2 Role you mean the DataPipelineDefaultResourceRole? The one i assign durin the creation of the pipeline to the field ""EC2 instance role""?
If so, i checked. It has datapipeline full access.

If i activate my datapipeline, then switch to the EMR console, and check the respective cluster, it says ""Install Task Runner cancelled"".

Do you have an idea what the issue could be?

Best regards

Edited by: OlliHC on Jan 14, 2019 7:08 AM

Edited by: OlliHC on Jan 14, 2019 7:14 AM"
AWS Data Pipeline	"Re: AWS Data Pipeline stuck in ""WAITING FOR RUNNER""
check your pipeline terminateaftertimeout parameter and increase 2-3 times normal run +15 minutes for cluster launch 

Example,
 Job usual run time is 10 minutes then 30 +15 =45 minutes would be best value for terminateafter"
AWS Data Pipeline	"AWS Data Pipeline failing due to unknow error
Few of my Data Pipelines are failing randomly. I found the following error in Data Pipeline console under EC2 resource -  Unable to create resource for @Ec2Resource0_2018-12-19T09:45:00 due to: internal error

Can someone help me on this. I have more than 10 pipelines running without any issue for the past six months. Regarding EC2 limits, I don't think we are hitting any.

Edited by: Eldhose on Dec 19, 2018 12:02 PM"
AWS Data Pipeline	"Re: AWS Data Pipeline failing due to unknow error
It could be any limit , sometimes, AZ run out of instance which you are requesting . You can analyze the cloudtrail logs to find out the runinstance request from datapipeline if it is intermittent issue"
AWS Data Pipeline	"DataPipeline RedshiftCopyActivity: ""Unable to establish connection to jdbc""
I have an issue with runny RedshiftCopyActivity, to load data from S3 to my Redshift cluster(Redshift/S3/DP are all in the same aws account). I got it to work in our Beta account, but not able to do this in Prod.

When creating Ec2Resource in DP, I used DataPipelineDefaultResourceRole and DataPipelineDefaultRole roles.

The error I receiving is:

19 Dec 2018 04:28:14,773 https://forums.aws.amazon.com/ (TaskRunnerService-resource:df-07957651VTJTTKG0GPL3_@ResourceId_LVKHP_2018-12-19T00:48:11-0)  amazonaws.datapipeline.database.ConnectionFactory: Unable to establish connection to jdbc:postgresql://vse-rs-dw.c1234us-east-1.redshift.amazonaws.com:8192/vsersdw Connection refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.

I found some topics on sage, stack overflow that discussed same problem, and I found out that I need to launch my ec2 instance in the same VPC I use for Redshift. I followed this wiki (https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-resources-vpc.html)

I created new VPC security group and added it to Redshift cluster (aws-datapipeline-ec2-s3-redshift).
Then, I created another VPC security group for Ec2 instance in the same VPC as my Redshift (called it aws-dp-ec2-group). For this group, I didn’t specified any Inbound rules, outbound rules were there by default.

In Redshift security group (aws-datapipeline-ec2-s3-redshift), I added Inbound rule to accept All Traffic for all Ports from ec2 security group.

Then, when launching Data pipeline, I specified securityGroupIds: “aws-dp-ec2-group”(my ec2 security group) and subnetId: subnet-12345 (this is one of 4 subnet ids that I found in VPC, that I used for both Redshift and EC2 security group).

After that, I still receiving same error. Not sure if the problem is in IAM permissions or in VPC, since in my devo account, with devo Redshift cluster everything worked just fine. Please help me understand what did I wrong.


Thanks,
Natalia

Edited by: natkul on Dec 19, 2018 8:48 AM"
AWS Data Pipeline	"Re: DataPipeline RedshiftCopyActivity: ""Unable to establish connection to jdbc""
Steps looks good to me, it could be DNS issue. Could you run ping vse-rs-dw.c1234us-east-1.redshift.amazonaws.com with in VPC where rs exists and make sure that it returns private ip instead of public ip ?

If you are getting public ip with in vpc then you have to create a rs snapshot , create a new rs from snapshot then terminate the old one finally, rename to new rs name to old name (caveat, you need to stop traffic to rs until this process completes)

Edited by: Shivan on Jan 14, 2019 12:21 AM"
AWS Data Pipeline	"Canceled Task Exception thrown after running for 5 days
Hello Datapipeline Team,

I have been trying to run an AWS datapipeline that calls a bash process that calls several long running python and java processes from a shell command activity. Each time the shell command activity runs, a reportProgress error is thrown in the Task Runner logs after exactly 5 days, and the task is cancelled. This problem persisted even after I set the attemptTimeout and LateAfterTimeout fields to longer than 5 days. The Task Runner log message, datapipeline json definition, and error message are shown below:

TASK RUNNER LOG MESSAGE:

01 Dec 2018 18:55:05,693 https://forums.aws.amazon.com/ (HeartBeatService-df-01341812NWJEQ1FAYI1K-@ShellCommandActivityId_UdTMC_2018-11-26T18:54:03_Attempt=1) amazonaws.datapipeline.taskrunner.HeartBeatService: HeartBeatService DataPipeline reportProgress error thrown and workCancelleddf-01341812NWJEQ1FAYI1K-@ShellCommandActivityId_UdTMC_2018-11-26T18:54:03_Attempt=1 
amazonaws.datapipeline.taskrunner.CanceledTaskException: DataPipeline service requested this work be canceled.
 at amazonaws.datapipeline.taskrunner.DataPipelineProgressReporter.reportProgress*(DataPipelineProgressReporter.java:31) 
at amazonaws.datapipeline.taskrunner.DataPipelineProgressReporter.reportProgress(DataPipelineProgressReporter.java:24) 
at amazonaws.datapipeline.taskrunner.HeartBeatService.run(HeartBeatService.java:85) 
at java.lang.Thread.run(Thread.java:701) 
01 Dec 2018 18:55:05,697 https://forums.aws.amazon.com/ (HeartBeatService-df-01341812NWJEQ1FAYI1K-@ShellCommandActivityId_UdTMC_2018-11-26T18:54:03_Attempt=1) private.com.amazonaws.services.datapipeline.runner.ProcessTreeKiller: Exec: /bin/kill -s TERM 30428 
01 Dec 2018 18:55:05,700 https://forums.aws.amazon.com/ (HeartBeatService-df-01341812NWJEQ1FAYI1K-@ShellCommandActivityId_UdTMC_2018-11-26T18:54:03_Attempt=1) private.com.amazonaws.services.datapipeline.runner.ProcessTreeKiller: kill exit status: 0 
01 Dec 2018 18:55:05,701 https://forums.aws.amazon.com/ (HeartBeatService-df-01341812NWJEQ1FAYI1K-@ShellCommandActivityId_UdTMC_2018-11-26T18:54:03_Attempt=1) private.com.amazonaws.services.datapipeline.runner.ProcessTreeKiller: Exec: /bin/kill -s TERM 30430 
01 Dec 2018 18:55:05,705 https://forums.aws.amazon.com/ (HeartBeatService-df-01341812NWJEQ1FAYI1K-@ShellCommandActivityId_UdTMC_2018-11-26T18:54:03_Attempt=1) private.com.amazonaws.services.datapipeline.runner.ProcessTreeKiller: kill exit status: 0 
01 Dec 2018 18:55:05,705 https://forums.aws.amazon.com/ (HeartBeatService-df-01341812NWJEQ1FAYI1K-@ShellCommandActivityId_UdTMC_2018-11-26T18:54:03_Attempt=1) private.com.amazonaws.services.datapipeline.runner.ProcessTreeKiller: Exec: /bin/kill -s TERM 30432 
01 Dec 2018 18:55:05,710 https://forums.aws.amazon.com/ (HeartBeatService-df-01341812NWJEQ1FAYI1K-@ShellCommandActivityId_UdTMC_2018-11-26T18:54:03_Attempt=1) private.com.amazonaws.services.datapipeline.runner.ProcessTreeKiller: kill exit status: 0 
01 Dec 2018 18:55:05,710 https://forums.aws.amazon.com/ (HeartBeatService-df-01341812NWJEQ1FAYI1K-@ShellCommandActivityId_UdTMC_2018-11-26T18:54:03_Attempt=1) private.com.amazonaws.services.datapipeline.runner.ProcessTreeKiller: Exec: /bin/kill -s TERM 30433 
01 Dec 2018 18:55:05,715 https://forums.aws.amazon.com/ (HeartBeatService-df-01341812NWJEQ1FAYI1K-@ShellCommandActivityId_UdTMC_2018-11-26T18:54:03_Attempt=1) private.com.amazonaws.services.datapipeline.runner.ProcessTreeKiller: kill exit status: 0 
01 Dec 2018 18:55:05,715 https://forums.aws.amazon.com/ (HeartBeatService-df-01341812NWJEQ1FAYI1K-@ShellCommandActivityId_UdTMC_2018-11-26T18:54:03_Attempt=1) private.com.amazonaws.services.datapipeline.runner.ProcessTreeKiller: Exec: /bin/kill -s TERM 30434 
01 Dec 2018 18:55:05,719 https://forums.aws.amazon.com/ (HeartBeatService-df-01341812NWJEQ1FAYI1K-@ShellCommandActivityId_UdTMC_2018-11-26T18:54:03_Attempt=1) 
private.com.amazonaws.services.datapipeline.runner.ProcessTreeKiller: kill exit status: 0 
01 Dec 2018 18:55:05,722 https://forums.aws.amazon.com/ (TaskRunnerService-wg-10000-2) amazonaws.datapipeline.connector.staging.StageFromS3Connector: Script returned with exit status 143 
01 Dec 2018 18:55:05,723 https://forums.aws.amazon.com/ (TaskRunnerService-wg-10000-2) amazonaws.datapipeline.taskrunner.LogMessageUtil: Returning tail errorMsg : 
01 Dec 2018 18:55:06,726 https://forums.aws.amazon.com/ (TaskRunnerService-wg-10000-2) amazonaws.datapipeline.taskrunner.HeartBeatService: Finished waiting for heartbeat thread @DefaultShellCommandActivity1_2018-11-26T18:54:03_Attempt=1 
01 Dec 2018 18:55:06,726 https://forums.aws.amazon.com/ (TaskRunnerService-wg-10000-2) amazonaws.datapipeline.taskrunner.TaskPoller: Work ShellCommandActivity took 7201:0 to complete

PIPELINE JSON DEFINITION

{
  ""objects"": [
    {
      ""failureAndRerunMode"": ""CASCADE"",
      ""resourceRole"": ""DataPipelineDefaultResourceRole"",
      ""role"": ""DataPipelineDefaultRole"",
      ""pipelineLogUri"": ""s3://oobhuntoo1/"",
      ""scheduleType"": ""ONDEMAND"",
      ""name"": ""Default"",
      ""id"": ""Default""
    },
    {
      ""onLateAction"": {
        ""ref"": ""ActionId_V6bq0""
      },
      ""lateAfterTimeout"": ""7 Days"",
      ""name"": ""DefaultShellCommandActivity1"",
      ""id"": ""ShellCommandActivityId_UdTMC"",
      ""workerGroup"": ""wg-10000"",
      ""type"": ""ShellCommandActivity"",
      ""command"": ""python ~/AWS_5day_Test/Python/Layer1.py""
    },
    {
      ""name"": ""DefaultAction1"",
      ""id"": ""ActionId_V6bq0"",
      ""type"": ""Terminate""
    }
  ],
  ""parameters"": []
}"
AWS Data Pipeline	"Re: Canceled Task Exception thrown after running for 5 days
It is a hard limit, you cannot run activity more than 5 days in datapipeline."
AWS Data Pipeline	"Unable to establish connection to database
Hi, I am using Data Pipeline to export a MySQL table into Redshift, but I am having the following error:

amazonaws.datapipeline.database.ConnectionFactory: Unable to establish connection to jdbc:mysql://urlname.us-west-2.rds.amazonaws.com:3306/dbname Could not create connection to database server.

The EC2 instance created is in the same subnet as the RDS, they have the same security group, ACL, everything...

This is the pipeline id: df-06936403OONJPKGY3NWS

Any help will be appreciated,
Thanks"
AWS Data Pipeline	"Re: Unable to establish connection to database
you should open port to security group , if you use same sg then go to sg and click inbound tab .Finally, Click add then select MYSQL/auroa  (port 3306) then add ""sg"" in the destination box."
AWS Data Pipeline	"Re-run aws data pipeline job in cancelled state that failed on a past date
I have configured a data pipeline job that gets triggered every night at 2am. The job reads all files from S3 that were created the previous day and does some processing. I use the scheduledStartTime of the data pipeline job to figure out which files in S3 I have to process.

However, some times this job fails (lets say due to the ec2 limit reached or some other issue). Now my question is I discover this a few days later. I go to the console and this job is in Cancelled state. There is a option to re-run.

Does the re-run take the scheduledStartTime from the past day or the day I re-run it?

Right now, I have an on demand job where I have to hard code the S3 path for the days this job might fall and do my processing.

Is there a better way to re-run past data pipeline jobs where it uses the failed job's scheduledStartTime?"
AWS Data Pipeline	"Re: Re-run aws data pipeline job in cancelled state that failed on a past date
I think it would take past date (scheduledStartTime)"
AWS Data Pipeline	"Pipeline definition into CloudFormation
Is there a way to get a pipeline definition into a CloudFormation template? Or alternatively, a tool that converts a pipeline definition into a CloudFormation template?"
AWS Data Pipeline	"Re: Pipeline definition into CloudFormation
I have not tried personally, you can take a look on this if helps

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-cloudformer.html"
AWS Data Pipeline	"Error in backup process amazonaws.datapipeline.taskrunner
Good afternoon.

The last couple of weeks it is impossible to make backups for some tables from DynamoDB (we use the data pipeline service).

As a result of attempt to make a backup we receive here such an error:

amazonaws.datapipeline.taskrunner.TaskExecutionException: Failed to complete EMR transform. at amazonaws.datapipeline.activity.EmrActivity.runActivity(EmrActivity.java:67) at amazonaws.datapipeline.objects.AbstractActivity.run(AbstractActivity.java:16) at amazonaws.datapipeline.taskrunner.TaskPoller.executeRemoteRunner(TaskPoller.java:136) at amazonaws.datapipeline.taskrunner.TaskPoller.executeTask(TaskPoller.java:105) at amazonaws.datapipeline.taskrunner.TaskPoller$1.run(TaskPoller.java:81) at private.com.amazonaws.services.datapipeline.poller.PollWorker.executeWork(PollWorker.java:76) at private.com.amazonaws.services.datapipeline.poller.PollWorker.run(PollWorker.java:53) at java.lang.Thread.run(Thread.java:745) Caused by: amazonaws.datapipeline.taskrunner.TaskExecutionException: at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132) at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:460) at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:343) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282) at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562) at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557) at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548) at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833) at org.apache.hadoop.dynamodb.tools.DynamoDbExport.run(DynamoDbExport.java:79) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.hadoop.dynamodb.tools.DynamoDbExport.main(DynamoDbExport.java:30) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212) at amazonaws.datapipeline.cluster.EmrUtil.runSteps(EmrUtil.java:286) at amazonaws.datapipeline.activity.EmrActivity.runActivity(EmrActivity.java:63)

Any differences in the tables that we can not copy not found (their size is not the largest in terms of the number of records and the weight of the data). Previously, we were able to make a backup, and now we see an error although nothing has changed in the tables themselves, only the data was added.

To solve the problem tried the following:
1) the tables that we can not copy, we increased the capacity
2) to Improve the instance for backup
3) Recreate the pipeline

Nothing helped us to make a backup. 

Who faced a similar problem, please prompt how it is possible to correct this error?"
AWS Data Pipeline	"Re: Error in backup process amazonaws.datapipeline.taskrunner
it is not actual error, you may need to check emr step logs and application master log"
AWS Data Pipeline	"Datapipeline cancelled but EMR cluster is running
I ran spark-submit task from the DataPipeline. DataPipeline has EMR Cluster on which spark -submit task runs. 
Once cluster is up, I have also done Termination Protection On. I saw that the step is running on cluster but pipeline shows cancel status. 

Cluster Details are:

ID:j-3V5508AZUFQ8N
Creation date:2019-01-04 16:03 (UTC+5:30)
End date:2019-01-04 21:32 (UTC+5:30)
Elapsed time:5 hours, 28 minutes
Auto-terminate:No
Termination protection:
Off
Configuration details
Release label:emr-5.10.0
Hadoop distribution:Amazon 2.7.3
Applications:hive 2.3.1, spark 2.2.0, pig 0.17.0
Log URI:s3://aws-logs-694870696341-us-east-1/df-037917732XIPQHGFKICD/ResourceId_4IVUP/@ResourceId_4IVUP_2019-01-04T10:31:35/@ResourceId_4IVUP_2019-01-04T10:31:35_Attempt=1/
EMRFS consistent view:Disabled
Custom AMI ID:--
Network and hardware
Availability zone:us-east-1d"
AWS Data Pipeline	"Re: Datapipeline cancelled but EMR cluster is running
EMR step does not support to cancel the step if it is running hence dp api would have failed while cancelling step and do not enable terminate protection ""I have also done Termination Protection On"""
AWS Data Pipeline	"DynamoDb backup via Data Pipeline: number of cluster nodes
Hello,
I'm trying to run DynamoDB to S3 export via Data Pipeline (following this tutorial https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part2.html)

I specified to use 3 cluster nodes (see attached file Screenshot_1.png), but after activating the pipeline I see that 4 cluster nodes have been started (see Screenshot_2.png). Can anyone explain why? 

Thanks!"
AWS Data Pipeline	"Re: DynamoDb backup via Data Pipeline: number of cluster nodes
Number of nodes are calculated at runtime based on dynamodb through put. if you want to disable then set false for resizeClusterBeforeRunning	 in the emractivity. 

https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-emractivity.html"
AWS Data Pipeline	"How do I make a AWS bastion Node as a slave into local Jenkins
Hi,
I am trying to configuring a AWS bastion node as a slave into my local Jenkins which I have already configured Amazon EC2 Plugin configuration successfully.

Can anyone reply for this.

Thanks
Subbu"
AWS Data Pipeline	"Creating and attaching EBS Volume to an EC2Resource
I need to unzip a file that is to large for the ec2resource. The way I intend to solve this is to create and attach an EBS volume. 

I have added EC2:CreateVolume og EC2:AttachVolume policies to both DataPipelineDefaultRole and DataPipelineDefaultResourceRole. 

I have also tried setting AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY for an IAM role with the same permissions in the shell, but alas no luck.

I am currently stumped and don't know what to try next. Google isn't helping either. Does anyone here have any ideas?"
AWS Data Pipeline	"Re: Creating and attaching EBS Volume to an EC2Resource
So I'll answer my own question. Turns out the actual issue was specified in the encrypted authorization message. It wasn't CreateVolume itself that caused the problem. It was setting tags. My roles didn't have the ec2:CreateTags permission. Adding that permission solved the problem."
AWS Data Pipeline	"Is it possible for s3datanode to retrieve files after certain timestamp.
Hello, 
I have a requirement where I need to read data from s3 using aws data pipeline.
I checked that s3datanode can read s3 give a directory/filepath with some expression.

But I would like to check if it is possible to read files after certain timestamp in a directory.
Reason being there is external file drop to same s3 directory, and s3datanode reads everything from the directory."
AWS Data Pipeline	"FedRAMP or HIPAA compliance
Does anyone know if Data Pipeline is scheduled for FedRAMP or HIPAA compliance?"
AWS Data Pipeline	"Re: FedRAMP or HIPAA compliance
Heard from our aws rep, currently Data pipeline is not in schedule FedRAMP"
AWS Data Pipeline	"SQLCommand-Truncate table and perform insert into doesn't work occasionally
Hi Team,

In AWS Data pipeline, I have a Sql Command Activity which loads Redshift tables.

I have a table in Redshift with 2 million records. And I am performing Truncate table and load the same with insert into operation.This has been scheduled to run on a periodical basis of 2 hours. 

The issue is, the table is loaded with 2 million records correctly. And over a period of time, the table is with zero records or only 92K records. I have no clue why the same query performs differently.

Based on the below forum response, 
https://forums.aws.amazon.com/thread.jspa?messageID=853486??

I have implemented ANALYZE statement to overcome this issue. But still the issue persists.

Kindly assist me with your experience."
AWS Data Pipeline	"Copy RDS postgresql to S3 causes Out of Memory error (java heap space)
I have been trying for a while creating a pipeline to copy data from RDS Postgresql and send it to s3, but I always get a java heap space error.

The query performs a join and returns a file of about 2.5Gb with more than 2 million records.

As far as I know, the file size should not be a problem since copy activity copies record by record, but for some reason I get OutOfMemoryError.

What I tried:

Different instance types, the largest one I tried is m1.large.
Adding a LIMIT to the query to obtain a smaller amount of data to copy. Doing this the copy works.


As for me it seems to be an issue with the CSV Data Format. It seems to me that is not going record by record, and that's why crashes due memory issues.

Could you give me some light on that please?

Thanks."
AWS Data Pipeline	"Re: Copy RDS postgresql to S3 causes Out of Memory error (java heap space)
Hello,

Do you have stack trace for the OutOfMemoryError? If not, can you please tell me about the datapipeline id that had this issue. I will look into it. Please private message me the details if you would like.

Thanks,
Ankit"
AWS Data Pipeline	"Re: Copy RDS postgresql to S3 causes Out of Memory error (java heap space)
Hello,

sorry for my late reply, just saw your answer. This is my pipeline df-069713921I4XMUYL6A2I, if you could take a look I would appreciate it.

In the meanwhile, I am trying to rewrite the same pipeline with a shellcommand activity instead.

Thanks in advance."
AWS Data Pipeline	"Re: Copy RDS postgresql to S3 causes Out of Memory error (java heap space)
Hello,
For the pipeline id provided I see that you have used m1.large instance type. 

There are few other suggestions mentioned at http://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html

If you provide compressed data files as input and do not indicate this using the compression field on the S3 data nodes, CopyActivity might fail. In this case, CopyActivity does not properly detect the end of record character and the operation fails. Further, CopyActivity supports copying from a directory to another directory and copying a file to a directory, but record-by-record copy occurs when copying a directory to a file. Finally, CopyActivity does not support copying multipart Amazon S3 files.

Please do let us know if any of the above suggestions work for you.

Edited by: Gayatri@AWS on Jul 12, 2016 12:17 PM"
AWS Data Pipeline	"Re: Copy RDS postgresql to S3 causes Out of Memory error (java heap space)
I seem to be having the same issue as described above -- was there ultimately a resolution?


Adding a limit to my query also worked (copy worked on smaller dataset sizes)
Using a larger resource (e.g. m1.large) did not do the trick


Gayatri -- I PM'd you the pipeline id."
AWS Data Pipeline	"Re: Copy RDS postgresql to S3 causes Out of Memory error (java heap space)
I am using a shell command activity by now.

The CopyActivity haven't worked for me. Could you make it work @matt10?"
AWS Data Pipeline	"Re: Copy RDS postgresql to S3 causes Out of Memory error (java heap space)
Was this issue resolved? If so, can you please share the steps you took for solving it. Sorry for asking a question here as I could not find a genuine solution to this issue, neither am I able to post a new thread.

I'm trying to run an extract job from RDS (Postgres via Jdbc connector) to S3. The query consists of a little over 1.7M records and it keeps failing with Java heap space issue. I tried running it on m3.medium and m3.large instances and both resulted in failing. Also, I'm running the latest postgres Jdbc driver and have tried setting the defaultRowFetchSize limit to 1000 and 10 in the Jdbc properties, but it ends up failing. The job had ran successfully for over two months before it started failing since yesterday. I would really appreciate any help in this regard.

Thanks!

Regards,
Ninad"
AWS Data Pipeline	"Re: Copy RDS postgresql to S3 causes Out of Memory error (java heap space)
I did little research on this, it seems you need to set conn.setAutoCommit(false); before running sql query inorder to avoid loading table into memory however, it is not supported in datapipeline 

https://jdbc.postgresql.org/documentation/91/query.html#fetchsize-example

one workaround is , you can write java code and run it on shell command activity"
AWS Data Pipeline	"Data pipeline from RDS to S3 fails with OutOfMemoryError (Java heap space)
I'm trying to run an RDStoS3Copy Activity from Postgres RDS (via Jdbc connector). The query is a simple {select * from table} and consists of a little over 1.7M records. The job ran successfully for well over two months before failing yesterday with an OutOfMemoryError Java heap space. I tried running it on m3.medium and m3.large instances and both resulted in failing. Also, I'm running the latest postgres Jdbc driver and have tried setting the defaultRowFetchSize limit to 1000 and 10 in Jdbc properties, but it ends up failing. 

Below is the errorStackTrace:
java.lang.OutOfMemoryError: Java heap space at java.lang.Class.getDeclaredFields0(Native Method) at java.lang.Class.privateGetDeclaredFields(Class.java:2509) at java.lang.Class.getDeclaredField(Class.java:1959) at java.util.concurrent.atomic.AtomicReferenceFieldUpdater$AtomicReferenceFieldUpdaterImpl$1.run(AtomicReferenceFieldUpdater.java:231) at java.util.concurrent.atomic.AtomicReferenceFieldUpdater$AtomicReferenceFieldUpdaterImpl$1.run(AtomicReferenceFieldUpdater.java:229) at java.security.AccessController.doPrivileged(Native Method) at java.util.concurrent.atomic.AtomicReferenceFieldUpdater$AtomicReferenceFieldUpdaterImpl.<init>(AtomicReferenceFieldUpdater.java:228) at java.util.concurrent.atomic.AtomicReferenceFieldUpdater.newUpdater(AtomicReferenceFieldUpdater.java:105) at java.sql.SQLException.<clinit>(SQLException.java:371) at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1809) at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:508) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:384) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeQuery(AbstractJdbc2Statement.java:269) at amazonaws.datapipeline.connector.SqlInputConnector.open(SqlInputConnector.java:49) at amazonaws.datapipeline.connector.SqlInputConnector.<init>(SqlInputConnector.java:25) at amazonaws.datapipeline.connector.SqlDataNode.getInputConnector(SqlDataNode.java:79) at amazonaws.datapipeline.activity.copy.SingleThreadedCopyActivity.processAll(SingleThreadedCopyActivity.java:47) at amazonaws.datapipeline.activity.copy.SingleThreadedCopyActivity.runActivity(SingleThreadedCopyActivity.java:35) at amazonaws.datapipeline.activity.CopyActivity.runActivity(CopyActivity.java:22) at amazonaws.datapipeline.objects.AbstractActivity.run(AbstractActivity.java:16) at amazonaws.datapipeline.taskrunner.TaskPoller.executeRemoteRunner(TaskPoller.java:136) at amazonaws.datapipeline.taskrunner.TaskPoller.executeTask(TaskPoller.java:105) at amazonaws.datapipeline.taskrunner.TaskPoller$1.run(TaskPoller.java:81) at private.com.amazonaws.services.datapipeline.poller.PollWorker.executeWork(PollWorker.java:76) at private.com.amazonaws.services.datapipeline.poller.PollWorker.run(PollWorker.java:53) at java.lang.Thread.run(Thread.java:748)

I would really appreciate any help in this regard. Thanks!

Regards,
Ninad"
AWS Data Pipeline	"Re: Data pipeline from RDS to S3 fails with OutOfMemoryError (Java heap space)
I did little research on this, it seems you need to set conn.setAutoCommit(false); before running sql query inorder to avoid loading table into memory however, it is not supported in datapipeline 

https://jdbc.postgresql.org/documentation/91/query.html#fetchsize-example

one workaround is , you can write java code and run it on shell command activity"
AWS Data Pipeline	"Heap memory error in data pipeline R
Hi I created a pipe line for copying data from RDS postgres to S3 (I tried both a directory path and file path). The data to be copied is around 200GB and I used the default instance type for ec 2 resource which is m1.small using copy activity.

The pipeline failed due out of memory.
Does the copy activity loads the entire data in memory? If yes then my instance will definitely go out of memory but if the copy activity copies record by record then it should not go out of memory as individual records won't be large.

How can I resolve this? I am open to using something other than aws data pipeline, ultimately my goal is to migrate data from rds->redshift.

Error stack trace and logs

01 Mar 2018 09:46:50,222 https://forums.aws.amazon.com/ (TaskRunnerService-resource:df-090470026ZFLPD6UCCVF_@ResourceId_QMAtz_2018-03-01T09:40:06-0)  amazonaws.datapipeline.logpusher.Uploader: Uploading to s3://answeriq.com/pipeline/df-090470026ZFLPD6UCCVF/CopyActivityId_6Vve1/@CopyActivityId_6Vve1_2018-03-01T09:40:06/@CopyActivityId_6Vve1_2018-03-01T09:40:06_Attempt=1/Activity.log@000000000000000-000000000001635.gzSize:539
01 Mar 2018 09:46:50,371 https://forums.aws.amazon.com/ (TaskRunnerService-resource:df-090470026ZFLPD6UCCVF_@ResourceId_QMAtz_2018-03-01T09:40:06-0)  amazonaws.datapipeline.logpusher.Uploader: Consolidating s3://answeriq.com/pipeline/df-090470026ZFLPD6UCCVF/CopyActivityId_6Vve1/@CopyActivityId_6Vve1_2018-03-01T09:40:06/@CopyActivityId_6Vve1_2018-03-01T09:40:06_Attempt=1/Activity.log@000000000000000-000000000001635.gz
01 Mar 2018 09:46:50,399 https://forums.aws.amazon.com/ (TaskRunnerService-resource:df-090470026ZFLPD6UCCVF_@ResourceId_QMAtz_2018-03-01T09:40:06-0)  amazonaws.datapipeline.logpusher.Uploader: Upload s3://answeriq.com/pipeline/df-090470026ZFLPD6UCCVF/CopyActivityId_6Vve1/@CopyActivityId_6Vve1_2018-03-01T09:40:06/@CopyActivityId_6Vve1_2018-03-01T09:40:06_Attempt=1/Activity.log.gz
01 Mar 2018 09:46:50,427 https://forums.aws.amazon.com/ (TaskRunnerService-resource:df-090470026ZFLPD6UCCVF_@ResourceId_QMAtz_2018-03-01T09:40:06-0)  amazonaws.datapipeline.logpusher.Uploader: Deleting:pipeline/df-090470026ZFLPD6UCCVF/CopyActivityId_6Vve1/@CopyActivityId_6Vve1_2018-03-01T09:40:06/@CopyActivityId_6Vve1_2018-03-01T09:40:06_Attempt=1/Activity.log@000000000000000-000000000001635.gz
01 Mar 2018 09:46:50,479 https://forums.aws.amazon.com/ (TaskRunnerService-resource:df-090470026ZFLPD6UCCVF_@ResourceId_QMAtz_2018-03-01T09:40:06-0)  amazonaws.datapipeline.taskrunner.TaskPoller: Error in remote runner
java.lang.OutOfMemoryError: Java heap space
	at java.util.jar.Manifest$FastInputStream.<init>(Manifest.java:332)
	at java.util.jar.Manifest$FastInputStream.<init>(Manifest.java:327)
	at java.util.jar.Manifest.read(Manifest.java:195)
	at java.util.jar.Manifest.<init>(Manifest.java:69)
	at java.util.jar.JarFile.getManifestFromReference(JarFile.java:186)
	at java.util.jar.JarFile.getManifest(JarFile.java:167)
	at sun.misc.URLClassPath$JarLoader$2.getManifest(URLClassPath.java:816)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:409)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:64)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:354)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:347)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1809)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255)
	at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:508)
	at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:384)
	at org.postgresql.jdbc2.AbstractJdbc2Statement.executeQuery(AbstractJdbc2Statement.java:269)
	at amazonaws.datapipeline.connector.SqlInputConnector.open(SqlInputConnector.java:49)
	at amazonaws.datapipeline.connector.SqlInputConnector.<init>(SqlInputConnector.java:25)
	at amazonaws.datapipeline.connector.SqlDataNode.getInputConnector(SqlDataNode.java:79)
	at amazonaws.datapipeline.activity.copy.SingleThreadedCopyActivity.processAll(SingleThreadedCopyActivity.java:47)
	at amazonaws.datapipeline.activity.copy.SingleThreadedCopyActivity.runActivity(SingleThreadedCopyActivity.java:35)
	at amazonaws.datapipeline.activity.CopyActivity.runActivity(CopyActivity.java:22)
	at amazonaws.datapipeline.objects.AbstractActivity.run(AbstractActivity.java:16)
	at amazonaws.datapipeline.taskrunner.TaskPoller.executeRemoteRunner(TaskPoller.java:136)
	at amazonaws.datapipeline.taskrunner.TaskPoller.executeTask(TaskPoller.java:105)
	at amazonaws.datapipeline.taskrunner.TaskPoller$1.run(TaskPoller.java:81)
	at private.com.amazonaws.services.datapipeline.poller.PollWorker.executeWork(PollWorker.java:76)
	at private.com.amazonaws.services.datapipeline.poller.PollWorker.run(PollWorker.java:53)"
AWS Data Pipeline	"Re: Heap memory error in data pipeline R
Was this issue resolved? If so, can you please share the steps you took for solving it.

I'm trying to run a similar job (copy from RDS to S3) which consists of a little over 1.7M records and it keeps failing with Java heap space issue. I tried running it on m3.medium and m3.large instances and both resulted failing. 

Thanks!

Edited by: ninad1487 on Oct 9, 2018 3:41 PM"
AWS Data Pipeline	"Re: Heap memory error in data pipeline R
Postgres JDBC driver keeps entire table in memory instead of streaming the result hence use latest postgres jar and set defaultRowFetchSize jdbc properties.

https://jdbc.postgresql.org/documentation/head/connect.html

Alternatively, you can try AWS Glue which comes with memory handling and parallel. 


https://docs.aws.amazon.com/glue/latest/dg/run-jdbc-parallel-read-job.html"
AWS Data Pipeline	"Re: Heap memory error in data pipeline R
Hi Shivan,
Thanks a lot for your response. I tried your suggestion and ran the pipeline after setting the defaultRowFetchSize (as 1000 and later as 10), but it is still failing. The pipeline has been running well for over 2 months before it failed yesterday, hence I'm a bit skeptical about introducing a new tool (AWS Glue). Although, I would consider moving to it in case this issue is beyond repair. 

Thanks again for your help!

Regards,
Ninad"
AWS Data Pipeline	"Re: Heap memory error in data pipeline R
I did little research on this, it seems you need to set conn.setAutoCommit(false); before running sql query inorder to avoid loading table into memory however, it is not supported in datapipeline 

https://jdbc.postgresql.org/documentation/91/query.html#fetchsize-example

one workaround is , you can write java code and run it on shell command activity"
AWS Data Pipeline	"ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
RDS to S3 CopyActivity pipeline.
CopyActivity failed.
Attempts 3 of 3.
---
Error copying record
Cause: java.io.IOException: No space left on device
Cause: No space left on device
---
Why would something like this happen? I followed the AWS tutorial at 
http://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html
---
Screenshot attached."
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
Hello,

I've asked engineering to take a look at this.

I'll report back when I know more.

Richard"
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
Hello

I was unable to track down the Pipeline at this end that resulted in this error.

Could you please provide us with the Pipeline ID so we can look into this failure for you?

Best regards,

Le Clue L."
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
hi, i had terminated the pipeline and recreated the pipeline... still facing the same error ""No space left on device"". The new pipeline's ID is: df-064923926XX0YYBTYWS4 . Screenshot attached."
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
Hi there!

I've passed the pipeline ID onto our EDP team for further review, and we'll let you know as soon as we hear back.

Cheers
Iain"
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
Hi there!

How large is the Database in RDS that you are using?

CopyActivity has some limitations on file sizes:
http://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html

CopyActivity also does not yet support Multi-part uploads. It might be more efficient to make use of ShellCommandActivity to run a command/shell script to perform the export using the AWS CLI tools.

Hope that helps!
Iain"
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
The MySQL table is actually pretty huge - 204 GB with over 800 million rows.

I don't think the issue here has to do with CopyActivity.

I was initially using 'SELECT * FROM {table_name}'. When i used 'SELECT * FROM {table_name} LIMIT 20000000' (limit 20 million rows), i was able to export the MySQL successfully to an S3 object. 

The failure occurred whenever the S3 object exceeded 5 GB size which is the S3 limitation. Maybe not a CopyActivity issue. Is it?"
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
Hi syedrakib,

As you have noted, S3 has a limit of 5GB per key when not using the Multipart Upload API. The CopyActivity does not support this feature (as per: http://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-copyactivity.html )

As per my colleagues guidance, if you need to export > 5GB in a single key, you may be able to achieve this using ShellCommandActivity to run a command/shell script to perform the export using the AWS CLI tools.

Regards,

Alastair"
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
Hi Alaistar

I am having the same issue here - I am getting a no space left on device. I just want to confirm what the best approach is to import this data into S3? I am trying to import a table from RDS which has 430898856 records.

Can you advise? 

Regards

Alexandra"
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
Hello,
Please could you share your pipeline id. And what are the configurations that you tried.
Thanks"
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
Hi Alaistar

Very desperate so hoping you can help me! 

The pipeline ID is: df-04190361TMFLKB2SVMFC 

The error I get is ""No Space Left On Device"" : 
java.io.IOException: No space left on device at java.io.FileOutputStream.writeBytes(Native Method) at java.io.FileOutputStream.write(FileOutputStream.java:345) at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126) at java.io.FilterOutputStream.write(FilterOutputStream.java:97) at amazonaws.datapipeline.dataFormat.AbstractRecordWriter.writeRecord(AbstractRecordWriter.java:39)

So I spoke to someone and they said it may be the EC2 instance is running out of space, he recommended creating an AMI which we did and we then specified the image and key value in the ""Resources"" of the pipeline, but then once we ran it, it gave the error ""Stalled"", the copying never even begun. 
""Resource is stalled. Associated tasks not able to make progress. @RDStoS3CopyActivity_2016-07-15T11:44:03_Attempt=1"" (data pipeline: df-04190361TMFLKB2SVMFC)

The Ec2 instance type we are specifying is t1.micro.  

When we created the custom AMI, we launched a new EC2 Instance which was instance and created an AMI off of this. We had to use an instance with paravirtual virtualisation, because we tried to create an EC2 instance without this before and got the following error in the pipeline: 

Unable to create resource for @Ec2Instance_2016-07-13T14:33:27 due to: Non-Windows instances with a virtualization type of 'hvm' are currently not supported for this instance type. (Service: AmazonEC2; Status Code: 400; Error Code: InvalidParameterCombination; Request ID: 098496bd-c7ca-4911-ba4f-8c6d616648d1) 

(data pipeline: df-04190361TMFLKB2SVMFC)

If you can advise how we can set this up it would be really helpful because it's a massive bottleneck for us at the moment and can't find a solution

Alexandra"
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
Hi 

Just following up on the above query

Alexandra"
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
Hi, 

In my pipeline I just changed the EC2 Instance type from t1.small to m1.medium and no longer had this issue. 

Alexa"
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
We are facing the same issue.
The support engineer suggested increasing EBS root volume by creating custom AMI, but i still got the same error.

Can someone provide instructions on how to  us this:

`It might be more efficient to make use of ShellCommandActivity to run a command/shell script to perform the export using the AWS CLI tools.`"
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
Datapipeline is single threaded . if table is big then I would recommend to try AWs Glue which is backed by spark engine which runs the SQL in parallel (split the table) and write to s3 in parallel. 

https://docs.aws.amazon.com/glue/latest/dg/run-jdbc-parallel-read-job.html

Edited by: Shivan on Oct 8, 2018 5:13 AM"
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
@Shivan all i need is my table copied to S3. i really don't care what does it, so long as it happens. With Glue i have to code it myself in python or scala."
AWS Data Pipeline	"Re: ERROR Copying RDS to S3: ""java.io.IOException: No space left on device""
@azaytsev  Glue generate basic code for you

Example, 
1.Create a RDS/JDBC connection
2.Run crawler which creates a table in datacatalog similar to your RDS 
3.Create a ETL job (select the datacatalog table and s3 location,format (json,csv,parquet etc..) a

Manual edit,
   if you need partition in s3 output then add partition option (group) for that column so that you can query in Athena like date,country etc and read jdbc optimize 

I think, you do not need to be python expert. there is an option for DPU and metrics to optimize the read
Enable the metrics and change the DPU to 4 or 5 (default is 10)

Partition:
https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-partitions.html
https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html

parallel read in reading:
   additional_options = {""hashfield"": ""month""}

Edited by: Shivan on Oct 8, 2018 12:11 PM"
AWS Data Pipeline	"AWS Data Pipeline Loading CSV from S3 to RDS MySQL
Hi,

I'm new to AWS and Data pipeline, trying to load csv file from s3 to RDS mysql, using ""load S3 data into RDS mysql table"" template, getting error DriverClass not found for database:aurora. I have set jdbc driver just uri property point to driver jar in s3.

Thanks!"
AWS Data Pipeline	"Re: AWS Data Pipeline Loading CSV from S3 to RDS MySQL
use JDBC option instead of RDSdatabase in datapipeline

i.e add jdbc url manually and rerun the job.

https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-jdbcdatabase.html"
AWS Data Pipeline	"Re: AWS Data Pipeline Loading CSV from S3 to RDS MySQL
Is the Data Pipeline the ONLY way to LOAD DATA INFILE into RDS mySQL (from a file on S3)? 

I've been trying this (in RDS mySQL): 
LOAD DATA INFILE 'https://s3.amazonaws.com/bean.com/MCA_tax-exempt_ID.csv' INTO TABLE tax_exempt2  FIELDS TERMINATED BY ',' 

but getting this:
Error Code: 1045. Access denied for user 'root'@'%' (using password: YES)

Thanks."
AWS Data Pipeline	"Re: AWS Data Pipeline Loading CSV from S3 to RDS MySQL
1. you can use copyactivity which copies data from s3 to rds but it is slow compare to load command.
2. use sqlactivity to run load infile command in datapipeline

Edited by: Shivan on Oct 2, 2018 1:02 PM"
AWS Data Pipeline	"Support for Java 8 in shell command activity
Does aws support Java 8 in shell command activity?

Thanks,
Kam"
AWS Data Pipeline	"Re: Support for Java 8 in shell command activity
Hello Kam,

I guess you are referring to running this activity on resource created by the pipeline itself? I tested using an ec2 instance that gets automatically created (runsOn resource) on EDP for shellCommandActivity and following is what currently bundled with the instances.
ec2-user@ip-10-28-56-161 ~$ java -version
java version ""1.6.0_33""
OpenJDK Runtime Environment (IcedTea6 1.13.5) (amazon-67.1.13.5.0.68.amzn1-x86_64)
OpenJDK 64-Bit Server VM (build 23.25-b01, mixed mode)


Unfortunately at this stage, it is difficult to give an ETA for this to be ready in the default Amazon Linux AMIs. However, you can always add an additional shellCommandAcitvity (like a bootstrap) that installs java 8 for you when the instance is started up and run your commands or you can use a custom resource (with task runner installed - http://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-how-task-runner-user-managed.html) that is prepared for java 8 support and define that as a worker group resource for your data pipeline.

Hope this helps.
Cheers,
Amo"
AWS Data Pipeline	"Re: Support for Java 8 in shell command activity
you can create an ami with java 8 and use it datapipeline"
AWS Data Pipeline	"s3 to mySQL RDS csv import fails with double quotes
Good day, 
I am trying to import a csv file from S3 to mySQL RDS using data pipeline. The dataload fails when the csv file has double quotes in it. 
I have tried the following: 
Just importing it. For this one I used data format:
 {
      ""id"": ""DataFormat1"",
      ""name"": ""DataFormat1"",
      ""type"": ""CSV""
}

Error message: ""Error copying record Cause: Parse error, expected record or column separator to follow endQuote at 195"". 195 is the position of the end quote in my csv.
Thereafter I tried to update the data node dataformat by adding a column definition for each column in the csv:
 {
      ""id"": ""DataFormatId_hJr4g"",
      ""name"": ""DefaultDataFormat2"",
      ""column"": [
        ""a string"",
        ""b string"",
        ""c string"",
        ""d string"",
        ""cur string"",
        ""f string"",
        ""g string"",
        ""h string"",
        ""i string"",
        ""j string"",
        ""k string"",
        ""l string""
      ],
      ""type"": ""CSV""
    }

ERROR message: Error copying record Cause: Parse error, expected record or column separator to follow endQuote at 195
Finally, I tried a tsv: 
{
      ""id"": ""DataFormatId_wxyA3"",
      ""escapeChar"": ""\\"",
      ""name"": ""DefaultDataFormat3"",
      ""recordSeparator"": ""\\n"",
      ""columnSeparator"": "","",
      ""type"": ""TSV""
}

Error message: Error copying record Cause: java.sql.SQLException: Parameter index out of range (6 > number of parameters, which is 5). Cause: Parameter index out of range (6 > number of parameters, which is 5).
I would appreciate any help on how to get the csv import from s3 to mySQL RDS to work with double quotes.

Many thanks

More info:
I did manage to get it to work without the double quotes, but there is a requirement to be able to support text importing with commas in, and therefore I cannot get away without the double quotes. Also, I have very limited control over the file that gets dropped in my s3 bucket."
AWS Data Pipeline	"Re: s3 to mySQL RDS csv import fails with double quotes
It seems like the problem is that the csv file's lines cannot end with a double quote then new line. 
I.e. the following is invalid and I get an error: Error copying record Cause: Parse error, expected record or column separator to follow endQuote at 162
""abcde"",""""<LF>


The following is valid:
""abcde"",0<LF>


Where <LF> is a new line, i.e. \n"
AWS Data Pipeline	"Re: s3 to mySQL RDS csv import fails with double quotes
I posted a wrong answer earlier my bad.

Please could you share a sample/test CSV with us so we can dig deeper into the issue of parsing.

Thanks,
Gayatri

Edited by: gayatrideo on Apr 21, 2015 4:04 PM"
AWS Data Pipeline	"Re: s3 to mySQL RDS csv import fails with double quotes
Hi,
Im also having the same problem when copying a csv in S3 to MySQL (RDS). Rather than preprocessing the data to remove the double quotes, is there another workaround for this.
Attached is a sample CSV.
Thanks"
AWS Data Pipeline	"Re: s3 to mySQL RDS csv import fails with double quotes
Am getting a similar error 

Parse error: expected field to be terminated by endOfRecord or endOfField but was endOfFile

Can we get some insight how to get around this?"
AWS Data Pipeline	"Re: s3 to mySQL RDS csv import fails with double quotes
I ran into this issue because field values are wrapped in double quotes. If double quotes are removed, the import works. 

Edited by: phillipKO on Dec 12, 2017 2:36 PM"
AWS Data Pipeline	"Re: s3 to mySQL RDS csv import fails with double quotes
Similar problem: 

Trying to use Pipeline (first time so I don't know all it can do by any means) to get data from a CSV file on S3 into RDS mySQL. 

Using the template for that. (Disappointed that the template doesn't seem to support LOAD DATA INFILE, but only INSERT as a query, but moving on . .. )

My CSV file has BOTH double-quoted fields (because commas in the data) and Windows EOLs (CRLF.)

Here's my error message: 
Java.lang.Thread.run(Thread.java:748) Caused by: java.lang.RuntimeException: Parse error, expected record or column separator to follow endQuote at 659

Suggestions appreciated! 

PS--Especially since this is a one-time bulk load and not a scheduled ETL or other recurring process, it would be nice if I could just do a LOAD DATA INFILE directly from mySQL to load the CSV file on s3, but that seems to be impossible, right?  Can't GRANT the required FILE permission in the RDS mySQL implementation, apparently?"
AWS Data Pipeline	"Re: s3 to mySQL RDS csv import fails with double quotes
Digging further, I find this in the Data Pipeline documentation: 

CopyActivity has specific limitations to its CSV support. When you use an S3DataNode as input for CopyActivity, you can only use a Unix/Linux variant of the CSV data file format for the Amazon S3 input and output fields. The Unix/Linux variant requires the following:

The separator must be the "","" (comma) character.

The records are not quoted.

The default escape character is ASCII value 92 (backslash).

The end of record identifier is ASCII value 10 (or ""\n"").

Windows-based systems typically use a different end-of-record character sequence: a carriage return and line feed together (ASCII value 13 and ASCII value 10). You must accommodate this difference using an additional mechanism, such as a pre-copy script to modify the input data, to ensure that CopyActivity can properly detect the end of a record; otherwise, the CopyActivity fails repeatedly.

So, pretty much end of story I guess."
AWS Data Pipeline	"Re: s3 to mySQL RDS csv import fails with double quotes
use aws glue"
AWS Data Pipeline	"How to ""cut"" columns from very large csv files before Redshift COPY?
I'm trying to pre-process several hundred very large csv files in order to COPY to Redshift only the small number of columns I need from the original files.

I'm new to AWS, but thought I'd use Data Pipeline and the ShellCommandActivity with the ""cut"" command to extract just the columns I need. This does seem to work on a small file, but times out on the file I'm using to test on. It takes about 20 minutes to run on a local box, but I've set the timeout to 60 minutes for this activity in Data Pipeline and it stops ""CANCELED"" after 60 minutes.

The command I'm using (myShellCmd) is: ""unzip -p ${INPUT1_STAGING_DIR}/myfilename_2016-10-01.zip hit_data.tsv | cut -f2,7,9,11,19,26,27,29,284-286,288-293,299-302,311,316,344,359-362,365,366,375,380,395,397,399,401,402,618,656-658,660,669,670,692,696,709,728,740,771,798-801,881-883,929,933,967-979 > ${OUTPUT1_STAGING_DIR}/hit_data_72col.tsv""

Any tips appreciated. Given that I have about 600 of these files, taking hours for each one isn't really an option. Thanks.

Edited by: MarkEvans on Apr 19, 2018 4:43 PM"
AWS Data Pipeline	"Re: How to ""cut"" columns from very large csv files before Redshift COPY?
INPUT1_STAGING_DIR would copy all the files to local directory hence I would recommend to disable the state flag to false in command activity 
Use aws s3 cp to copy specific files to local .

example,
 aws s3 cp s3://<path>  /tmp/<zipfile>"
AWS Data Pipeline	"How to use a specific filename for exporting dynamodb to S3?
Hi,

I want to export my dynamodb to S3 bucket by pipeline. My problem is that filePath is always create dictionary in S3 not file.
For example, my filePath in pipeline's DataNodes is ""s3://mybucket/exportlog/data.txt""
but the result is that I got s3://mybucket/exportlog/data.txt/9ce47217-4f2b-4f5e-a917-98c416167dcb in my S3. 
""data.txt"" is converted to dictionary, I hope it's a file.
How to get a specific filename? 

Thanks."
AWS Data Pipeline	"Re: How to use a specific filename for exporting dynamodb to S3?
Dynamodb export runs on parallel hence it would create multiple files , each one for each worker.
hence you have to run some job/command to merge file or rename if you want to be in specific name or single file ."
AWS Data Pipeline	"Attach an Elastic IP to the EC2 resources in Datapipeline?
We are using Data pipeline to copy files from customer's sftp to our S3 buckets. However customer needs to allow our source IP into theyŕe firewall in order for us to reach SFTP server.

I read we can attach elastic IP to a ec2 resource but is it possible to configure this in Data pipeline so that ec2 instance that pipeline spins up will the IP has outgoing IP address?

Please let me know.

Thanks"
AWS Data Pipeline	"Re: Attach an Elastic IP to the EC2 resources in Datapipeline?
I can think of 2 options 
1. Use NAT gateway in route table  so that all the traffic's would go from single ip 
2. use shellcommand activity to attach EIP to ec2 instance which is being launched in datapipeline.  May be using aws cli 

https://docs.aws.amazon.com/cli/latest/reference/ec2/associate-address.html

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html"
AWS Data Pipeline	"AWS Data Pipeline connection to On Premise Oracle database
Hi,
I am trying to explore data pipeline as an ETL tool to test a couple of functionality.
1) Can data pipeline connect to my on premise oracle database and extract data through SQL query and load it directly to AWS Aurora database ? I don't intend to use S3 as data landing area and want to directly run SQL query on oracle database and load the query output from Oracle to AWS RDS.
2) Does data pipeline support SQL Query for data transformation ? I intend to create some summary tables in RDS and load that through SQL by using intermediary RDS tables.
3) Where does the data pipeline run ? Can i configure it to use my EC2 instance for all computation purpose ?

I apologize if the questions are stupid or immature but this is my first proof of concept on data pipeline."
AWS Data Pipeline	"Re: AWS Data Pipeline connection to On Premise Oracle database
1)  yes, you should be able to as long as you are able to connect from AWS VPC (using vgw) 
2. you can write select query in copyactivity  as long as it is a single select statement
3. It runs on your default VPC, if you want to run on specific VPC then configure the subnet in ec2 resource in datapipeline

However, I think AWS Glue is the best option as it supports parallel read and write where datapipeline is single thread JDBC and also, you have option to write python code for transformation."
AWS Data Pipeline	"Data consistency in Data Pipeline + DynamoDB?
I need to transfer data from one DynamoDB to another. And I wonder how Data Pipeline, that do full copy from one DynamoDB table to another DynamoDB table, will handle records, that was written in first table after pipeline was triggered. Pipeline will be triggered manually. 
Does pipeline work as `scan()` operation? And after it finish job, destination table will have only those data that were in source table before pipeline start working? 
Does pipeline always use `consistent read`?"
AWS Data Pipeline	"Re: Data consistency in Data Pipeline + DynamoDB?
Datapipeline is a ETL tool and it is based on which service your use with it. like hive or map reduce 

both uses the scan with eventual consistent . you can find more detail here

https://github.com/awslabs/emr-dynamodb-connector"
AWS Data Pipeline	"Export from RDS to S3 with double quotes to string in CSV file
Hi,

I´m using Data Pipeline for export a table from RDS.
It´s working, but I need that for string colunm this field must be export between double quotes.
My data has a lot of comma inside them, so, this double quotes would resolve.

I tried ""data format"" CSV and custom, but didn´t work.

Could you help me?"
AWS Data Pipeline	"Re: Export from RDS to S3 with double quotes to string in CSV file
as far as I know it is not supported in datapipeline. if you are keen, you can try aws glue where you can change the underlaying code and more flexibility 

https://aws.amazon.com/blogs/database/how-to-extract-transform-and-load-data-for-analytic-processing-using-aws-glue-part-2/."
AWS Data Pipeline	"Unable to establish connection to jdbc...Communications link failure
Hi Team,

I'm trying to load data from RDS to Redshift, however I'm getting that error in 'RDSToS3CopyActivity'.
Below is my pipeline id 
df-00066971XD9OF54MIYYP.

FYI - I have RDS and Redshift in same region(Singapore) and same VPC, however as datapipeline is not available in that region I had to start that in other region which is 'N.Virginia'. My RDS is not publicly accessible, however I can access that from EC2 of same VPC/region(singapore).

I had tried doing 'VPC peering' thinking, it may fix problem. However, that was failed due to overlapping IPv4 CIDR.

Please suggest a solution for this Data pipeline issue.

Best Regards,
Sumit Sahu"
AWS Data Pipeline	"Re: Unable to establish connection to jdbc...Communications link failure
you can specify the region in ec2  resource in datapipline so that it will launch the ec2 in Singapore. 

https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-manage-region.html

https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html"
AWS Data Pipeline	"Input XML, Output JSON
Is possible to define an AWS Data Pipeline that get a XML file from Amazon S3 and transform it to JSON that it will be used by AWS Kinesis or I need to use lambda?

Thanks.

Edited by: mchuecostfm on Jul 11, 2018 5:53 AM"
AWS Data Pipeline	"Re: Input XML, Output JSON
you can use glue service which is easy to implement than datapipeline (but it is possible in dp if you are good at EMR spark)
https://docs.aws.amazon.com/glue/latest/dg/add-classifier.html"
AWS Data Pipeline	"Launch EMR without Hive or Pig
Hello,
I would like to launch an EMR 5.x cluster using Data Pipeline with Spark only. I specify ""spark"" as the only application, but it continues to bootstrap Hive and Pig. How can I change this behavior to only bootstrap with Spark? I am no longer using Hive or Pig for any operations, and do not want either running on our cluster.

Thank you

Edited by: davemasino on Jul 15, 2018 12:24 PM"
AWS Data Pipeline	"Re: Launch EMR without Hive or Pig
as far as I know , hive and pig are default service which can not be removed."
AWS Data Pipeline	"DynamoDB import from S3 failing
I'm trying to use the DynamoDB import template with Data Pipeline to import a table from files on S3. I'm currently getting this error during import:

Error: java.lang.RuntimeException: com.amazonaws.AmazonServiceException: Supplied AttributeValue is empty, must contain exactly one of the supported datatypes (Service: AmazonDynamoDBv2; Status Code: 400; Error Code: ValidationException; Request ID: DVFM9UPVII4EF9QV5SM4S982TNVV4KQNSO5AEMVJF66Q9ASUAAJG) at org.apache.hadoop.dynamodb.DynamoDBFibonacciRetryer.handleException(DynamoDBFibonacciRetryer.java:107) at org.apache.hadoop.dynamodb.DynamoDBFibonacciRetryer.runWithRetry(DynamoDBFibonacciRetryer.java:83) at org.apache.hadoop.dynamodb.DynamoDBClient.writeBatch(DynamoDBClient.java:220) at org.apache.hadoop.dynamodb.DynamoDBClient.putBatch(DynamoDBClient.java:170) at org.apache.hadoop.dynamodb.write.AbstractDynamoDBRecordWriter.write(AbstractDynamoDBRecordWriter.java:91) at org.apache.hadoop.mapred.MapTask$DirectMapOutputCollector.collect(MapTask.java:844) at org.apache.hadoop.mapred.MapTask$OldOutputCollector.collect(MapTask.java:596) at org.apache.hadoop.dynamodb.tools.ImportMapper.map(ImportMapper.j


Clearly one of my 25000+ input files is not formatted correctly. How do I get the import process to spit out meaningful info like filename or at least request payload in the error msg so that I know what's causing the problem?"
AWS Data Pipeline	"Re: DynamoDB import from S3 failing
I assume, you are importing csv to dynamodb in dp. 

it is using hive service to achieve this hence you need to modify the hive query either filter out empty value or add default value in it 

edit datapipelie and hive activity where you see hive script (which needs to be modified)

based on error, some columns are empty which is not supported in dynamodb."
AWS Data Pipeline	"rds to s3 failing for very large tables
I have an aws rds aurora table with more than 11 million records, i no longer need this data to be in a database, so I'm moving the data to s3 as a csv file. For this I'm trying to use aws data pipelines, I've followed this tutorial https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql-console.html 

I activated the pipeline and it gets stuck on the ""RDStoS3CopyActivity"" stage, then after some minutes the status is changed to cancelled and it stops. I've checked the s3 destionation bucket and I have a csv file, but with just 1,276,367 records. I've checked the logs of the stage and the last line it's something like this:
24 Jul 2018 21:35:17,127 [INFO] (TaskRunnerService-resource:df-033887111D2PZ83ZHWMO_@Ec2Instance_2018-07-24T21:17:31-0) df-033887111D2PZ8324 Jul 2018 21:48:39,797 [INFO] (TaskRunnerService-resource:df-033877111D2PZ83ZHWMO_@Ec2Instance_2018-07-24T21:17:31-0) df-033887111D2PZ83ZHWMO amazonaws.datapipeline.connector.staging.S3Helper: Completed upload of local file /media/ephemeral0/mnt/taskRunner/output/tmp/1fee1353-caf2-40a2-aea4-e0b79b0aa111.csv to s3://backrds//2018-07-24-21-17-31/1fee1353-caf2-40a2-aea4-e0b79b0aa111.csv
24 Jul 2018 21:48:39,798 [INFO] (TaskRunnerService-resource:df-033877111D2PZ83ZHWMO_@Ec2Instance_2018-07-24T21:17:31-0) df-033877111D2PZ83ZHWMO amazonaws.datapipeline.connector.s3.S3OutputConnector: Committed 1276367 records to s3 file backrds//2018-07-24-21-17-31/1fee1353-caf2-40a2-aea4-e0b79b0aa111.csv 


I executed the pipeline twice with the same results, always an incomplete csv file with the same number of rows.

It is something I'm missing so the pipeline can handle such large file?"
AWS Data Pipeline	"Re: rds to s3 failing for very large tables
Dp is single threaded one hence use sqoop in datapipeline 

https://github.com/aws-samples/data-pipeline-samples/tree/master/samples/RDStoRedshiftSqoop

modify according to your requirement

I would recommend to try with glue as it runs in parallel 

https://docs.aws.amazon.com/glue/latest/dg/run-jdbc-parallel-read-job.html"
AWS Data Pipeline	"Template ""Export DDB to S3"" cross-region problem
Hi,

the export of a DDB Table to a S3 bucket in a single region finally works. (https://forums.aws.amazon.com/thread.jspa?threadID=286006&tstart=0). 
However, now I want to export a DyanmoDB Table in eu-central-1 to a S3 bucket in eu-west-1. In a Stackoverflow question(https://stackoverflow.com/questions/47602807/error-while-exporting-data-from-dynamodb-to-s3-using-amazon-data-pipeline?rq=1), it is suggested that the region of the EmrCluster is set wrongly to the DDB's region. However, changing the EmrCluster's region to eu-west-1 resulted in the same exception in the TableBackupActivity: IOTIMP-<TABLENAME> not found (Service: AmazonDynamoDBv2; Status Code: 400; Error Code: ResourceNotFoundException; Request ID: K0Q9SN0KE07KHL5L9TPTDO6AVNVV4KQNSO5AEMVJF66Q9ASUAAJG)
. I double checked the table name. So my guess is that sth regarding the region is the cause.

Full error message:
at org.apache.hadoop.dynamodb.tools.DynamoDbExport.setTableProperties(DynamoDbExport.java:94) at org.apache.hadoop.dynamodb.tools.DynamoDbExport.run(DynamoDbExport.java:75) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.hadoop.dynamodb.tools.DynamoDbExport.main(DynamoDbExport.java:30) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212) Caused by: java.lang.RuntimeException: com.amazonaws.services.dynamodbv2.model.ResourceNotFoundException: Requested resource not found: Table: IOTIMP-tableIOTInMediasP2-1HCC715LX9WE7 not found (Service: AmazonDynamoDBv2; Status Code: 400; Error Code: ResourceNotFoundException; Request ID: K0Q9SN0KE07KHL5L9TPTDO6AVNVV4KQNSO5AEMVJF66Q9ASUAAJG) at org.apache.ha


Error stack:
amazonaws.datapipeline.taskrunner.TaskExecutionException: Failed to complete EMR transform. at amazonaws.datapipeline.activity.EmrActivity.runActivity(EmrActivity.java:67) at amazonaws.datapipeline.objects.AbstractActivity.run(AbstractActivity.java:16) at amazonaws.datapipeline.taskrunner.TaskPoller.executeRemoteRunner(TaskPoller.java:136) at amazonaws.datapipeline.taskrunner.TaskPoller.executeTask(TaskPoller.java:105) at amazonaws.datapipeline.taskrunner.TaskPoller$1.run(TaskPoller.java:81) at private.com.amazonaws.services.datapipeline.poller.PollWorker.executeWork(PollWorker.java:76) at private.com.amazonaws.services.datapipeline.poller.PollWorker.run(PollWorker.java:53) at java.lang.Thread.run(Thread.java:745) Caused by: amazonaws.datapipeline.taskrunner.TaskExecutionException: at org.apache.hadoop.dynamodb.tools.DynamoDbExport.setTableProperties(DynamoDbExport.java:94) at org.apache.hadoop.dynamodb.tools.DynamoDbExport.run(DynamoDbExport.java:75) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.hadoop.dynamodb.tools.DynamoDbExport.main(DynamoDbExport.java:30) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212) Caused by: java.lang.RuntimeException: com.amazonaws.services.dynamodbv2.model.ResourceNotFoundException: Requested resource not found: Table: IOTIMP-tableIOTInMediasP2-1HCC715LX9WE7 not found (Service: AmazonDynamoDBv2; Status Code: 400; Error Code: ResourceNotFoundException; Request ID: K0Q9SN0KE07KHL5L9TPTDO6AVNVV4KQNSO5AEMVJF66Q9ASUAAJG) at org.apache.hadoop.dynamodb.DynamoDBFibonacciRetryer.handleException(DynamoDBFibonacciRetryer.java:107) at org.apache.hadoop.dynamodb.DynamoDBFibonacciRetryer.runWithRetry(DynamoDBFibonacciRetryer.java:83) at org.apache.hadoop.dynamodb.DynamoDBClient.describeTable(DynamoDBClient.java:86) ... 9 more Caused by: com.amazonaws.services.dynamodbv2.model.ResourceNotFoundException: Requested resource not found: Table: IOTIMP-tableIOTInMediasP2-1HCC715LX9WE7 not found (Service: AmazonDynamoDBv2; Status Code: 400; Error Code: ResourceNotFoundException; Request ID: K0Q9SN0KE07KHL5L9TPTDO6AVNVV4KQNSO5AEMVJF66Q9ASUAAJG) at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182) at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770) at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310) at com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient.invoke(AmazonDynamoDBClient.java:1772) at com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient.describeTable(AmazonDynamoDBClient.java:1071) at org.apache.hadoop.dynamodb.DynamoDBClient$1.call(DynamoDBClient.java:90) at org.apache.hadoop.dynamodb.DynamoDBClient$1.call(DynamoDBClient.java:87) at org.apache.hadoop.dynamodb.DynamoDBFibonacciRetryer.runWithRetry(DynamoDBFibonacciRetryer.java:80) ... 10 more at amazonaws.datapipeline.cluster.EmrUtil.runSteps(EmrUtil.java:286) at amazonaws.datapipeline.activity.EmrActivity.runActivity(EmrActivity.java:63) ... 7 more


Any help and tips are greatly appreciated.

Sincerely,
Dominik

Edited by: kuhnke on Aug 9, 2018 3:53 AM"
AWS Data Pipeline	"Re: Template ""Export DDB to S3"" cross-region problem
Launch emr in  eu-west-1 where S3 bucket exist. Add below properties in emr activity step in dp template. 

 -Ddynamodb.region=eu-central-1"
AWS Data Pipeline	"How can I set bid price as on-demand price?
How can I set bid price as on-demand price? Is 0 bid price means as on-demand-price?"
AWS Data Pipeline	"Re: How can I set bid price as on-demand price?
Hello,
How can I set https://forums.aws.amazon.com/ bid price as on-demand price? 
See  https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-spot-instances.html 

Is 0 bid price means as on-demand-price?

Because on-demand instances are used in the Data Pipeline by default, as stated here:  https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-spot-instances.html , setting the SpotBidPrice to zero effectively means you are going to use on-demand instances.
More information, for context, is below:
1. Here is a definition of the SpotBidPrice field, from EC2Resource  https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-ec2resource.html:
*SpotBidPrice* is the maximum amount per hour for your Spot Instance in dollars, which is a decimal value between 0 and 20.00, exclusive. 

2. Here is how Spot Instances and On-Demand Instances are used in the Data Pipeline:
""Pipelines can use Amazon EC2 Spot Instances for the task nodes in their Amazon EMR cluster resources. By default, pipelines use on-demand Amazon EC2 instances. In addition, you can use Spot Instances. Spot Instances let you use spare Amazon EC2 instances and run them. The Spot Instance pricing model complements the on-demand and Reserved Instance pricing models, potentially providing the most cost-effective option for obtaining compute capacity, depending on your application.""

Hope this helps!"
AWS Data Pipeline	"Re: How can I set bid price as on-demand price?
I've json pipeline definition. If not put masterInstanceBidPrice/coreInstanceBidPrice means use on-demand instance type. How can I set on-demand price in json"
AWS Data Pipeline	"Please add Data Pipeline support for the new T3 instance types
The new T3 instance types offer greatly improved network throughput, which will be a great help when processing large amounts of data."
AWS Data Pipeline	"Template ""Export DDB to S3"" seems to be not working
Hi,

I am trying to export a dyonamodb table to a S3 bucket using the template. However, it is not working because the EmrClusterForBackup is exceeding the maximum creation time (Max wait for resource creation exceeded) in the status CREATING stuck (I assume stuck as I waited up to 2 hours). Furthermore, as the component is stuck at CREATING no logs are written. All my resources are in a single region (eu-west-1) for testing purposes. Though, I want to make it work also cross-regionally later on.

I attached the the generated json from the template which I use.

Any help and tips are greatly appreciated.

Sincerely,
Dominik

Edited by: kuhnke on Jul 19, 2018 5:54 AM"
AWS Data Pipeline	"Re: Template ""Export DDB to S3"" seems to be not working
After further testing I found the problem: we did not have sufficient EC2-Instance-Limits in the data pipeline's region eu-west-1."
AWS Data Pipeline	"Re: Template ""Export DDB to S3"" seems to be not working
The EC2-Instance-Limits were not sufficient in the data pipeline's region eu-west-1."
AWS Data Pipeline	"DevSecOps -> AWS Lamba giving module not found error
I am running the AWS DevSecOps project present here: https://aws.amazon.com/blogs/devops/implementing-devsecops-using-aws-codepipeline/

In the ""StaticCodeAnalysis"" stage of the pipeline I am getting AWS Lambda function failed. On checking the log the error is: ""Unable to import module 'cfn_validate_lambda': No module named cfn_validate_lambda"".

I checked that the S3 bucket has the python code Zip and also ensured that the zip file has ""Public"" in the permissions.

Please let me know how to resolve this. Thanks."
AWS Data Pipeline	"ADP Workforce Now integration
Hi,

I'm new to AWS and evaluating if it will solve a client's integration challenge.  I'm not even sure this is the right forum to post this question so sorry if that is the case.

The end goal is to pull data daily from two separate ADP Workforce Now instances, merge the data sets (Employees will overlap between the two instances) and then transfer a subset of the data, also daily, to a 3rd party vendor in a flat-file format (pipe-delimited).  I should note that I don't require payroll-specific history from ADP but do need job, manager, and salary change history.  My main questions are:


Is there an AWS connector for ADP?  If not, appears Boomi or Tray.io may do the trick?  Are there others I should consider as well?  ADP provides a REST API to retrieve data, which Boomi and Tray.io appear to leverage, but is there a simpler solution built into AWS?  I don't see REST as an available data source option but may not be looking in the right place.
Can the flow from ADP to AWS be scheduled to run daily?
Can the flow from AWS to 3rd party be scheduled to run daily?
What AWS service should I use to merge the datasets?  AWS Glue?
Can I apply transformation logic to the merged dataset before sending to 3rd party? Some business logic needs to be applied to the merged data to distinguish effective dates of salary, job, manager, status, org, etc changes as the ADP data schema is somewhat flat.
Does AWS support creating and FTPing flat files to the 3rd party? Their model will only accept pipe-delimited or XML files via SFTP.


There are a myriad of AWS offerings but not sure which best serve the needs above.  Initially, I thought using AWS Glue to merge the two ADP instances but not sure that's really necessary.  Hoping there are some experienced AWS data integration experts out there that can provide a noob some guidance.

Thanks in advance

Edited by: Stratega on Jul 23, 2018 8:08 PM

Edited by: Stratega on Jul 23, 2018 8:11 PM"
AWS Data Pipeline	"M5 instance types
Hi,

These were announced for EMR in May but can't be used with Data Pipeline - would be good to understand when/if they are to be made available.

Thanks
Neil"
AWS Data Pipeline	"Re: M5 instance types
I would also LOVE to use more instance types in the datapipeline product. I would love to use i3.large because our pipeline would benefit from large fast local storage.

It is good that aws publish the instance types that are available to datapipeline: https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-ec2-supported-instance-types.html"
