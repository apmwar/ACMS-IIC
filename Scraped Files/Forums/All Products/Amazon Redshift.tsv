label	description
Amazon Redshift	"Compare dc2.large and dc2.8xlarge
Hello,

I need to choose between cluster with 32 node of type dc2.large or cluster with 2 node of type dc2.8xlarge. So Which is better performance between them? I know the price of 2 node dc2.8xlarge is a little bit more expensive than 32 node of dc2.large. But i'm not sure if dc2.8xlarge has a better performance than dc2.large.  

Thank you."
Amazon Redshift	"Re: Compare dc2.large and dc2.8xlarge
Get the large nodes.

1. You get an 8x type leader node - this is important.
2. Network is slow.  It's much better to have as much data as possible on as few nodes as possible."
Amazon Redshift	"Re: Compare dc2.large and dc2.8xlarge
Additionally, you would be at the max size of your cluster with 32 dc2.large nodes. You wouldn't be able to add any more nodes. With the two dc2.8xlarge you can still add 126 more nodes."
Amazon Redshift	"Re: Compare dc2.large and dc2.8xlarge
Thank you guys, that very helpful."
Amazon Redshift	"A sane way to manage user permissions?
Hey everyone
Some early user-management design choices are coming back to bite me in the butt and I was wondering if anyway has a better way to do things.

Basically here's my situation:
1) We have many users access Redshift to run adhoc queries. They are all members of a group.
2) The group was given read privileges to all tables (including any future tables) in the primary schemas using default privileges.
3) Default privileges require a table owner reference (e.g. give these permissions to all future objects created-by and/or owned-by a specific user). Because of this, we have a single 'admin' user who has the proper default privileges who we use to create new tables, views, and udfs.

This works very well as long as we remember to switch the user to 'admin' before creating any new objects.
The issue I'm having is when a user needs to truncate a table. My specific reason for needing this is due to dimension table ETLs, which completely refresh the table with every load. Truncating a table requires that the current user is either the owner of the table, or a superuser. I'd prefer to not make the ETL user a superuser, so I have a dilemma.

My current solution to this is to just run a 'delete from table' statement to clear the table, but obviously this will have effects on sorting and will require a complete vacuum after every load.
Are there any better ways to manage users in a way that I can automatically give permissions to new objects and allow a user to truncate a table?
One idea I'm leaning toward it setting up a second set of default privileges for the ETL user, then changing the owner of the table to that same user, but I feel like adding more default privileges adds to the complexity of managing the users.

Is there is a better, non-obvious way of getting what I want?

Thanks!"
Amazon Redshift	"Re: A sane way to manage user permissions?
Hi mmasseth,

For exactly all the reasons you specify, we make the ELT/ETL user the schema owner and table owner so that it can exercise those DEFAULT permissions and execute those TRUNCATE statements without being a superuser.  We found that to be a much more automation friendly solution instead of having some other admin user be table and schema owner.

There's a nice side effect to this design in that it forces you to automate all DDL for the table and schema such that no other user should have to do any DDL on it.  Combine that with ""IF NOT EXISTS"" clauses in CREATE and DROP statement automation and you can build an idempotent DDL system.  The down side is that you need a metadata repository/source somewhere outside of the Redshift catalog that includes the Redshift specific bits, like DISTSTYLE, DISTKEY, SORTKEY, and ENCODEing, etc, to build you DDL statements.

I hope that helps or at least validates part of your approach,
-Kurt"
Amazon Redshift	"Re: A sane way to manage user permissions?
I personally wouldn't do a truncate because it will cause some downtime on your table, but you'd still end up with the table owner being the ETL user with my recommended method.

BEGIN;
CREATE new_table ...
INSERT INTO new_table ...
ALTER TABLE table RENAME TO old_table;
ALTER TABLE new_table RENAME TO table;
(optionally recreate any views on the old table to point to new table)
DROP TABLE IF EXISTS old_table CASCADE;
END;"
Amazon Redshift	"ORDER BY clause not working properly on one of Redshift cluster
Running a simple query with ORDER BY clause on one of our Redshift cluster does not seem to be working at all.  

select * from annual_float where order by asof desc;

Results is not sorted by column specified.  Adding "" limit "" does not work either as it still returns all data.   Seems to be happening only on one of our cluster.

Any idea what is going?"
Amazon Redshift	"Re: ORDER BY clause not working properly on one of Redshift cluster
Do you have primary key defined on this table? If so, please try the query after removing the PK. 

Redshift trusts the PK if it's defined and may skip certain checks."
Amazon Redshift	"Re: ORDER BY clause not working properly on one of Redshift cluster
thanks! Query working correctly after dropping the primary key.  Also tried re-creating the table (with primary key defined) and running unload/copy to new table.  Both cases the order by clause is working.

Edited by: rgarc on Aug 30, 2017 1:53 PM"
Amazon Redshift	"Re: ORDER BY clause not working properly on one of Redshift cluster
I was just having the same issue and was able to resolve it using the DENSE_RANK Window Function.
https://docs.aws.amazon.com/redshift/latest/dg/r_WF_DENSE_RANK.html
It seems it has to be in the Select statement to work as intended.

For your example:
select DENSE_RANK() Over(Order by asof desc) orderByColumn,
    *
from annual_float
order by orderByColumn;"
Amazon Redshift	"Re: ORDER BY clause not working properly on one of Redshift cluster
Hi rgarc,

Redshift does not enforce PK or FK constraints as an OLTP row store would.  Instead Redshift treats them more like query optimizer hints similar to what some other RDBMS's would and trusts that something external to Redshift is enforcing the semantics of the constraint. 

Look at the doc here: https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-defining-constraints.html

-Kurt"
Amazon Redshift	"Security of credentials used in COPY
We would like to use a customer's Redshift cluster to load data from our S3 bucket. Currently, the idea is to connect to the customer's cluster and provide our S3 credentials in the COPY command, which raises security concerns, since the customer could potentially gain access to our credentials via logs etc. I've checked the stl_ tables and the logs in the AWS console about this and discovered that the credentials are redacted there, but the question remains:

Is it guaranteed that the credentials in COPY will never be visible to anyone with access to a Redshift cluster, regardless of their access rights?"
Amazon Redshift	"Re: Security of credentials used in COPY
You should use IAM roles to authenticate when performing COPY.

Since this is cross-account, you would need to give access to your bucket to the customer's AWS account. Their administrators would then need to grant access to that bucket to a role in their account which is attached to the cluster, and give permission to use that role to a database account you have credentials for. You would then be able to run COPY commands authorized via the IAM_ROLE clause (instead of CREDENTIALS), completely eliminating the risk of leaking AWS keys."
Amazon Redshift	"Re: Security of credentials used in COPY
Hi Petros,

Another place to check for exposed credentials would be any user activity audit logging that may be turned on.  All statements executed by any session are logged there for audit purposes.  

However, a more secure method would be to use an IAM service role instead to allow the Redshift cluster to get temporary credentials for any COPY, UNLOAD, or external table query (using Spectrum) statements that need S3 access.

I hope this helps,
-Kurt"
Amazon Redshift	"Table Size in Amazon Redshift
I have a text file which is 23M which I load into a table in Redshift.
When I run following command in Redshift , I see size to be 784.
SELECT “table”, size, tbl_rows FROM SVV_TABLE_INFO

table	size	tbl_rows
collateral_analysis_prepay_report	784	21884

Is the unit of size in above results in MB?"
Amazon Redshift	"Re: Table Size in Amazon Redshift
Yes.

https://docs.aws.amazon.com/redshift/latest/dg/r_SVV_TABLE_INFO.html

""size 	bigint 	Size of the table, in 1 MB data blocks."""
Amazon Redshift	"Re: Table Size in Amazon Redshift
If the table is much larger than you expected, it could be due to many things.
1. Are the columns encoded?
2. Do all columns have optimal datatypes, e.g. character and integer fields defined to be as small as possible?
3. Try running a vacuum and analyze."
Amazon Redshift	"ERROR:  Dist auto table is not supported.
When I issue a CREATE TABLE with DISTSTYLE AUTO, I receive the error message ""ERROR: Dist auto table is not supported."" EVEN, KEY, and ALL work as expected. We are on Version 1.0.6145 in us-east-1. How can I use AUTO?"
Amazon Redshift	"Re: ERROR:  Dist auto table is not supported.
The docs indicate AUTO should work, so you appear to have uncovered a bug.

You can still use AUTO by not specifying a diststyle at all.  The default style, in the absence of a user-specified style, is AUTO."
Amazon Redshift	"Re: ERROR:  Dist auto table is not supported.
It is not yet available in your region but you can try your test again after your next maintenance window to see if it has been enabled for you. 

Sorry for the confusion around this, but the feature has been enabled per region separately from the version upgrade to ensure that it can be closely monitored.

We expect the feature to be fully deployed soon and we will add a note in the announcement when it is available in all regions."
Amazon Redshift	"Re: ERROR:  Dist auto table is not supported.
If I don't specify a style, the database sets it to EVEN."
Amazon Redshift	"Re: ERROR:  Dist auto table is not supported.
Yes.

This is because (see Joe's post) this functionality is not yet active in your region."
Amazon Redshift	"Number of sortkey per table
Hi,

I'm getting the following error while creating a test table with multiple sortkey as follow:

create table test1(
	key1 varchar(32)not null distkey,
	key2 int not null sortkey,
	key3 int sortkey,
	key4 varchar(20),
	key5 int,
	key6 int sortkey
	);

ERROR: too many SORTKEY declarations at column ""key3"" of table ""test1"" SQL State=42601 

I wonder if multiple sortkey per table is supported?

Thanks,
Trung."
Amazon Redshift	"Re: Number of sortkey per table
Try this syntax instead:

CREATE TABLE logs (   
    brand_id        integer,   
    ts        timestamp
) SORTKEY( brand_id, ts );"
Amazon Redshift	"Re: Number of sortkey per table
It works, thanks for the help.

Thanks,
Trung."
Amazon Redshift	"Re: Number of sortkey per table
Just to point out: this syntax creates one composite sortkey. It sorts the columns by brand_id and then by ts. Not two sortkeys."
Amazon Redshift	"Re: Number of sortkey per table
So how do we create two sort keys rather than one sort key composed of two columns?"
Amazon Redshift	"Re: Number of sortkey per table
Redshift only allows a single ""key"", that being the sort key. 
You could create an interleaved key if you need to query by each column individually, but in my experience, interleaved keys are more trouble than they are worth due to the vacuuming requirements. However maybe for your use case it could work.
Otherwise you just need to choose the best possible single key for your data."
Amazon Redshift	"Re: Number of sortkey per table
So how do we create two sort keys rather than one sort key composed of two columns?

In Redshift, this is done by having mutiple copies of a table, each with a different sortkey.  Use the table you mean to use in your queries.

(In Vertica, there's one logical table, and then behind the scenes many instances of it - the optimizers selects the best fit table.  This is convenient, but in my experience completely unnecessary.  You know which queries need which sort orders, so just use the appropriate table.)

Addendum : interleaving is not a first-order Big Data method.  If you want to use it, make sure you genuinely understand it and how it will sort records, so you can actually know if you -do- want to use it.

I have a web-site with a good explanation, but I've just moved over to https and the mediawiki is confused.  I'll  post the link once I've sorted it out."
Amazon Redshift	"Late Binding Views behaving unexpectedly
Hi there,

I'm using late binding views as a part of my in-database transformation process. These late-binding views are really handy because i can rebuild the tables they select from atomically without drop...cascading, meaning that there is no impact to downstream query-ers. Great!

It looks to me however like there are a couple of really troubling issues with late binding views, and I was hoping someone can give me some insight into workarounds / best practices / or future plans for LBVs.

For context, `select version()` shows me:
PostgreSQL 8.0.2 on i686-pc-linux-gnu, compiled by GCC gcc (GCC) 3.4.2 20041017 (Red Hat 3.4.2-6.fc3), Redshift 1.0.5833

Querying pg_get_late_binding_view_cols() results in ""table 12345 dropped by concurrent transaction

This is hard to pin down, but I'm able to reproduce this in the following way:
1. Run a script that repeatedly creates and drops late binding views
2. Query pg_get_late_binding_view_cols() concurrently

You can find SQL queries that reproduce this here:
https://gist.github.com/drewbanin/98beec96edadfe25398a79a8274b2cdb

Effectively every time I run these queries, I can get Redshift to return:
ERROR:  table 6615484 dropped by concurrent transaction


Is there some known workaround for this? This query error manifests when a database user runs a drop statement at the same time as I'm querying pg_get_late_binding_view_cols. That doesn't sound like the correct behavior to me.

I'm not so well attuned to the machinations of Redshift, but it looks to me like this pseudo table/function is not locked (or snapshotted?) when it is queried? In this doc (https://docs.aws.amazon.com/redshift/latest/dg/c_serial_isolation.html) I see:

A database snapshot is also created in a transaction for any SELECT query that references a user-created table or Amazon Redshift system table (STL or STV). SELECT queries that do not reference any table will not create a new transaction database snapshot, nor will any INSERT, DELETE, or UPDATE statements that operate solely on system catalog tables (PG).


I tried adding a cross join to a user-created table with one row in it based on the advice above, but that didn't seem to help here. Any other ideas for why a drop statement (outside of a transaction) would result in this error?

Really troublingly, this query appears to occasionally cause Redshift to restart?? It's much harder for me to reproduce this, though I can continue to play around and see if there's some surefire way to replicate it. Anecdotally, I've heard from others that they've had similar issues with pg_get_late_binding_view_cols causing their db to restart, and I'm curious if this is what the patch in Amazon Redshift Maintenance (January 23rd - February 20th 2019) is regarding. It appears that my cluster is on version 1.0.5833, so I was hoping this would be patched. Can you share the details of that bugfix?

Last, and certainly less pressingly than the above issues, I think Redshift's LBVs are implemented in an error-prone way. Redshift allows you to create views with invalid logic, which seems incorrect to me. Other modern data warehouses will validate views at creation time, which is certainly useful for any sort of in-database development of new data transformations. You can find an example of this behavior here: https://gist.github.com/drewbanin/2c11b864875144aabcc97424ab8a9cb0

----

I guess the overarching context which may be helpful here is that I'm one of the maintainers of dbt (https://github.com/fishtown-analytics/dbt), an open-source utility for transforming data in the warehouse. As a maintainer, I do a lot of support for folks using Redshift with dbt, and LBVs are a topic which have been coming up pretty frequently these days! I'm interested in finding a resolution to these questions so that I can get dbt working reliably on Redshift for the many organizations out there that rely on dbt to for their BI processes!

I'm grateful to anyone who reads this and can offer advice, workarounds, or alternative approaches. Thanks!
-Drew"
Amazon Redshift	"Re: Late Binding Views behaving unexpectedly
Did you read this part of the docs?

""System catalog tables (PG) and other Amazon Redshift system tables (STL and STV) are not locked in a transaction; therefore, changes to database objects that arise from DDL and TRUNCATE operations are visible on commit to any concurrent transactions.

For example, suppose that table A exists in the database when two concurrent transactions, T1 and T2, start. If T2 returns a list of tables by selecting from the PG_TABLES catalog table, and then T1 drops table A and commits, and then T2 lists the tables again, table A is no longer listed. If T2 tries to query the dropped table, Amazon Redshift returns a ""relation does not exist"" error. The catalog query that returns the list of tables to T2 or checks that table A exists is not subject to the same isolation rules as operations against user tables.

Transactions for updates to these tables run in a read committed isolation mode. PG-prefix catalog tables do not support snapshot isolation."""
Amazon Redshift	"Re: Late Binding Views behaving unexpectedly
Thanks for the additional info! While this is interesting and good to know, I'm not sure it's germane to the ""table dropped by concurrent transaction"" question. I have no problem with the results of the pg_get_late_binding_view_cols function varying within a transaction depending on the actions of other concurrent query-ers. My problem is that the query returns an error if any user drops any late bound view while the pg_get_late_binding_view_cols function is executing. This behavior is not present for queries against other system catalog tables or Amazon Redshift system tables.

Really appreciate the additional info though -- thanks"
Amazon Redshift	"Re: Late Binding Views behaving unexpectedly
Hey FishtownDrew,

It's worth noting that LBVs were in part introduced to provide view support to tables in external catalogs queried via Spectrum.  I believe the behavior you are seeing is consistent with the definition of ""late binding"" with external catalogs where the statement can't be bound to the object metadata until execution time, especially when external catalogs contain metadata that Redshift can't control.  This obviously has some side effects that may not be welcome when LBVs are used on only Redshift stored user tables.

I hope this helps provide at least a little perspective.
-Kurt"
Amazon Redshift	"Re: Late Binding Views behaving unexpectedly
Hi Klarson, thanks for the additional perspective! Do you know if the ""Table dropped by concurrent transaction"" is something that can/will be fixed in the future?

Our ETL process frequently needs to find the columns in a table/view, and we've historically made use of the information schema tables plus functions like pg_get_late_binding_view_cols. 

If not, I think we'll have to do something suboptimal for LBVs like:
1) create a ""bound"" view that ""select *""s from the LBV
2) inspect the columns in the view
3) drop the view

I'd really prefer not to do something like this, but I suppose it would be our only recourse if we can't reliably call the pg_get_late_binding_view_cols function. Thanks!

Edited by: FishtownDrew on Feb 25, 2019 6:25 AM

Edited by: FishtownDrew on Feb 25, 2019 6:27 AM

Edited by: FishtownDrew on Feb 25, 2019 6:27 AM"
Amazon Redshift	"Re: Late Binding Views behaving unexpectedly
Hi FishtownDrew,

As I too am a Redshift customer, I can't speak authoritatively on the subject.  So, if that's what you're looking for and you have enterprise support I'd suggest the you engage your TAM or open a support case on the same.

However, from what I understand about LBV, IMO what you're asking for is not consistent with the ""late"" nature of the view binding.  It looks like your asking for information that you can gather prior to when the view binding would happen to be immutable or for the binding to remain consistent across the lifetime of a transaction.  My understanding is the ""late binding"" is sub-statement execution level, meaning the view dependencies can change during an executing statement, which is certainly less than a transaction lifetime.  I'd be happy from someone from Redshift to correct an errors in my understanding here, but so far I have seen nothing that would speak to that.

So, pending any corrections from Redshift, I don't think what you're asking for is consistent with the LBV design and further that your efforts might best be spent on coding a reasonable reaction to the possible errors instead of trying to get Redshift to change the feature design.  Along those lines it would be nice if Redshift were to clearly document what the behavior is and what errors SQL  programmers should be prepared to handle when LBVs are found to not be valid at execution time.

I know this is not what you wanted, but I hope this helps.
-Kurt"
Amazon Redshift	"varchar column and table size strange behavior.
Hi.
I did an experiment for understanding what is the best varchar column size to use in order to save table space. For that I created 6 tables and to all of then inserted the same 2 billion values. 
The largest value inserted was 918 characters but other values have been inserted too such as nulls.
The table script is as follows
create table  yoav_full_lzo (custom_parameters varchar(2048) encode lzo);
create table yoav_full_zstd (custom_parameters varchar(2048) encode zstd);
create table yoav_full_no (custom_parameters varchar(2048));

create table yoav_trim_lzo (custom_parameters varchar(920) encode lzo);
create table yoav_trim_zstd (custom_parameters varchar(920) encode zstd);
create table yoav_trim_no (custom_parameters varchar(920));

the insert was a deep copy (for example insert into yoav_trim_no select col1 from bigtable). 
then I run the table size calculation script from
https://github.com/awslabs/amazon-redshift-utils/blob/master/src/AdminScripts/table_info.sql

I found a very strange behavior that the ""trimmed"" tables took more  space than the ""full"" ones.
for example 
yoav_trim_lzo - 93120MB
yoav_full_lzo - 85430MB
----
yoav_full_zstd - 63372MB
yoav_trim_zstd - 63678MB
----
yoav_full_no - 92505MB
yoav_trim_no - 93004MB

Did anyone had that issue? can anyone explain the results?
thanks
Yoav.

Edited by: yoavsun on Feb 13, 2019 5:34 AM"
Amazon Redshift	"Re: varchar column and table size strange behavior.
Hi.

Hej.

I did an experiment for understanding what is the best varchar column size to use in order to save table space.

Interesting.

I would have expected the varchar maximum length to make no difference whatsoever.

I'm surprised and intrigued that it seems to do so.

For that I created 6 tables and to all of then inserted the same 2 billion values.

Okay.  Same sortkey and distkey?  and you VACUUMed and ANALYZed afterwards?

The largest value inserted was 918 characters but other values have been inserted too such as nulls.

Okay.

The table script is as follows
create table yoav_full_lzo (custom_parameters varchar(2048) encode lzo);
create table yoav_full_zstd (custom_parameters varchar(2048) encode zstd);
create table yoav_full_no (custom_parameters varchar(2048));
create table yoav_trim_lzo (custom_parameters varchar(920) encode lzo);
create table yoav_trim_zstd (custom_parameters varchar(920) encode zstd);
create table yoav_trim_no (custom_parameters varchar(920));

Ah, no specified distkey or sortkey.

I've just had a look but I can't find information in the docs about the default behaviour for distkey/diststyle (I suspect the default might be EVEN, but I'm guessing).

A table with no sortkey is not sorted.

Your tables are not necessarily storing their records in the same order.

This will influence data compression and could by itsself explain your findings.

If the default diststyle is EVEN, this would further contribute, by altering the distribution of records over nodes, which again influences compression outcomes.

Please remake the tables, but specify a distkey, diststyle and sortkey, and be sure to VACUUM (to 100%) and ANALYZE, and then again examine sizings."
Amazon Redshift	"Re: varchar column and table size strange behavior.
Thanks for the reply. 
You have gave an interesting idea. I will sort the tables and reply in the thread. 

Yoav."
Amazon Redshift	"Re: varchar column and table size strange behavior.
The sorting did the trick. 
thanks."
Amazon Redshift	"Re: varchar column and table size strange behavior.
Excellent"
Amazon Redshift	"Re: varchar column and table size strange behavior.
By the way, the new default diststyle is ALL until a table reaches a certain size then it is EVEN."
Amazon Redshift	"Re: varchar column and table size strange behavior.
Hey Toebs2,

To make a valid test there also needs to be a distribution key column that isn't changing, ie subject to the TRIM operation so that the calculation of the distribution remains constant as the data is varied, especially for DISTSTYLE KEY.  I suppose DISTSTYLE ALL wouldn't need an invariant distribution key column though.

-Kurt"
Amazon Redshift	"Revoke table definition privileges
Hello,

I created a user ""test_sree"" with password and gave access to specific tables table_f, table_g, table_h and so on in schema ""edw"". 

Now, the user logs in using DBeaver or DbVisualizer. 

In the left side on the explorer pane, he can see all tables such as table_a, table_b, table_c. He cannot SELECT from the other tables as he gets permission denied message. This is good.
+ table_a, table_b should not be VISIBLE to the user ""test_sree""?+ He can login from any tool including Business Objects or Tableau. 

Basically, business users of one department should not be able to see structures/definitions of tables from other departments. 
Please let me know what process I should follow to create user and revoke privileges.

Thanks!"
Amazon Redshift	"Re: Revoke table definition privileges
I may be wrong, but I believe what you are asking for is not possible.

Tables and views and so on are always visible to all users.

Access can of course be controlled, just as you have, but not visibility."
Amazon Redshift	"Re: Revoke table definition privileges
Hello,
In Oracle, I can create a user and give access to few tables. That user will only see those tables when he logs on using Oracle SQL Developer. However, in Redshift, all other tables are being visible, even when that user does not have access. 

Why doesn't it happen the same way in redshift?

Please let me know. Thanks!"
Amazon Redshift	"Re: Revoke table definition privileges
It doesn't happen in the same way, because Oracle and Postgresql (the basis for Redshift) were built with different frames of thinking. 
Oracle was always a commercial, closed-source system designed (and paid for) by business: stability, scaling, security and control are key components there. PostgreSQL was started as a university project at Berkeley, based on another university project (ingres) and was released as open source - so generally features and performance would be higher priorities. 

For AWS to use a commercial basis for Redshift would have cost waaaaay too much, and they chose to adapt an open source RDBMS  - therefore redshift has the core limitations of PostgreSQL, with custom distrubuted/scaling features (from Paraccel) bolted on. 

Maybe tighter UAC is on the roadmap, but I wouldn't hold my breath..."
Amazon Redshift	"Re: Revoke table definition privileges
Hello All, 

I still did not get the answer I am looking for. I am sure some of you must have encountered this issue. Please let me know how I can restrict a user to not see tables that he is not supposed to in drop-down or in the explorer.

Thank you,
Sreekanth"
Amazon Redshift	"Re: Revoke table definition privileges
Can you use a different schema for each group?"
Amazon Redshift	"Re: Revoke table definition privileges
Like toebs2 said above, this just isn't possible in Redshift. 
There is a workaround using a separate Postgres/RDS database as a middle-layer, where you create a user-specific database PostgreSQL database, and use  https://aws.amazon.com/blogs/big-data/join-amazon-redshift-and-amazon-rds-postgresql-with-dblink/ to create links to only the tables or views in Redshift that user should have access to. But then you'll come into other issues (multiple server management, performance hits, etc)..."
Amazon Redshift	"Re: Revoke table definition privileges
Hi samidala,

You can not revoke visibility of tables (or any object) in any PostgreSQL derivative data catalog, including Redshift which is a PostgreSQL 8.03 descendant.  This is inherent in the PostgreSQL design.  Other RDBMSs do behave differently in this regard, including Oracle.  

PostgreSQL applies permissions at the object level, including catalog tables.  What you're effectively asking for is row level permissions (or filtering) to a catalog table that is not available in PostgreSQL or Redshift.

Others have offered workarounds.

Regards,
-Kurt"
Amazon Redshift	"Unload is broken when a column contains certain characters
We are using the UNLOAD command to dump data to S3 in a TSV format and then read it using some other programs. Our UNLOAD command looks like the following:

UNLOAD ('SELECT column1, column2, column2 FROM some_table')
TO 's3://bucket/and/a/prefix/'
CREDENTIALS 'aws_access_key_id=<put_access_key_id_here>;aws_secret_access_key=<put_secret_access_key_here>'
ALLOWOVERWRITE
NULL AS '$$NULL$$'
ESCAPE
ADDQUOTES
DELIMITER '\t';


This works fine, but we noticed it breaks the output format when column1 contains the following string: ""1% G"".

Just to be clear:
column1 = ""some text 1% General more text""
column2 = 10
column3 = ""whatever""

We would expect to get a TSV with that looks like the following:
""some text 1% General more text""\t""10""\t""whatever""

Instead, we get:
""some text 1-NANeneral more text\t""10""\t""whatever""

Please note:
1. The TSV we get is broken - the first column isn't enclosed in quotes.
2. 1% G has been turned into 1-NAN."
Amazon Redshift	"Re: Unload is broken when a column contains certain characters
Sorry you've experienced this problem. I think you are currently on 1.0.6145. We identified the issue and are releasing a new version 1.0.6230 that resolves it. 

You can either upgrade in the console to 1.0.6230 or later or you can rollback to the previous version temporarily. https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html#rs-mgmt-cluster-version

Please contact AWS Support if you need further assistance."
Amazon Redshift	"Unload is broken in 1.0.6145 : Cause cluster restart
Good morning,

As part of our daily process, we are unloading data from an external Redshift cluster.
However, after that cluster switched to 1.0.6145 a few of our UNLOAD commands started to fail. 

We saw two type or errors : 
1 - For some rows with ADDQUOTES option, we got truncated columns with missing end quotes causing further import of generated files to fail. We got things like {""co,""col2"", ""col3""}.   At least this is what stl_load_errors is showing (cannot post the real line, there is sensitive data in it). In that case switching to FORMAT CSV solved the issue.
2 - Even Worse, for some export the UNLOAD command  failed and made the cluster restart... Even switching to CSV didn't help.

After discussing with the admins of that cluster, they rolledback the upgrade, and all the failing exports started to work again. So I believe the issue comes from the new release. I'm not sure how to proceed here, since it was not on our cluster / account directly. But this seems to be like a serious issue.

Thanks,"
Amazon Redshift	"Re: Unload is broken in 1.0.6145 : Cause cluster restart
Sorry you've experienced this problem. We identified the issue and are releasing a new version 1.0.6230 that resolves it. 

For other readers, if you experience this issue and are currently on 1.0.6145 you can either upgrade in the console to 1.0.6230 or later or rollback as mentioned. Please contact AWS Support if you need further assistance."
Amazon Redshift	"Redshift vacuum does not reclaim disk space of deleted rows
We are having a problem with disk space usage in our Redshift cluster.

Many of our pipelines into Redshift delete rows when updating tables. We have manually ran vacuums on tables:
vacuum full <TABLE> to 100 percent


However, the disk usage from SVV_TABLE_INFO does not change after running the vacuum. Similarly, inspecting SVV_VACUUM_SUMMARY indicates no data was freed.

At the moment, the only way we have of reclaiming space is by performing a deep copy: https://docs.aws.amazon.com/redshift/latest/dg/performing-a-deep-copy.html (truncate the table and re-insert the data). However, this is not a sustainable solution for us.

We would be grateful if someone could help us with this problem.

Thank you"
Amazon Redshift	"Re: Redshift vacuum does not reclaim disk space of deleted rows
If the size does not decrease it sounds like VACUUM has already been run on these tables.

Please use the following query to review what happened with the VACUUMs you asked for. This is based on the view we publish in our Utils: https://github.com/awslabs/amazon-redshift-utils/blob/master/src/AdminViews/v_get_vacuum_details.sql
SELECT vac_start.userid
     , vac_start.xid
     , vac_start.table_id
     , tab.schema_name        AS schema_name
     , tab.table_name         AS table_name
     , TRIM(vac_start.status) AS vac_type
     , TRIM(vac_end.status)   AS result
     , NVL(vac_end.""rows"",vac_start.""rows"")     AS end_rows
     , (end_rows - vac_start.""rows"")            AS rows_delta
     , NVL(vac_end.""blocks"",vac_start.""blocks"") AS end_blocks
     , (end_blocks - vac_start.""blocks"")        AS blocks_delta
     , vac_start.""eventtime""                    AS start_time
     , DATEDIFF(seconds,vac_start.""eventtime"",NVL(vac_start.""eventtime"",vac_end.""eventtime"")) AS duration
FROM stl_vacuum vac_start
LEFT JOIN stl_vacuum vac_end
    ON vac_start.userid = vac_end.userid
   AND vac_start.table_id = vac_end.table_id
   AND vac_start.xid = vac_end.xid
   AND (vac_end.status NOT LIKE 'Start%'
   AND  vac_end.status NOT LIKE 'Skip%')
LEFT JOIN (SELECT TRIM(pgn.nspname) AS schema_name
                , TRIM(name) AS table_name
                , tbl.id AS table_id
           FROM stv_tbl_perm tbl
           JOIN pg_class pgc ON pgc.oid = tbl.id
           JOIN pg_namespace pgn ON pgn.oid = pgc.relnamespace
           GROUP BY 1,2,3
          ) tab 
    ON tab.table_id = vac_start.table_id
WHERE vac_start.status LIKE 'Start%'
   OR vac_start.status LIKE 'Skip%'
ORDER BY start_time DESC
;"
Amazon Redshift	"Re: Redshift vacuum does not reclaim disk space of deleted rows
The output from that query for one of the tables in question is:
    vac_type: Started Delete Only (To 100%)
    result: Finished
    rows_delta: 0
    end_blocks: 2035
    end_rows: ~22M

The table currently has 12M rows. The 22M figure was the original number of rows before the DELETE operation was performed."
Amazon Redshift	"Re: Redshift vacuum does not reclaim disk space of deleted rows
Auto-maintenance is, when the cluster is idle (exactly what that means I do not know) constantly running VACUUM DELETE and then ANALYZE on all tables.

Possibly these tables have already been VACUUMed.

However, I think I may have run into something similar about six months ago, just as I finished a contract, and i was something which I did not get to the bottom of.

It may be related to the way VACUUM ""thinks"".

A problem I ran into originally, I think (again, not completely sure - not enough time to get really into it), was that the table had a lot of deleted records and very few actual records - and the actual records were in fact sorted.

From VACUUMs point of view, the table was 100% sorted - even though there were lots of deleted records - and it did nothing."
Amazon Redshift	"Re: Redshift vacuum does not reclaim disk space of deleted rows
Our VACUUM issue seems to have resolved itself. It seems like the automatic background vacuum was not running last week on our cluster. It ran successfully yesterday, and the disk space used by rows marked for deletion has been freed.
It is interesting that our efforts to manually VACUUM tables was not doing anything to free disk space before the auto-vacuum ran successfully. Does anyone know if this may be the case?"
Amazon Redshift	"Re: Redshift vacuum does not reclaim disk space of deleted rows
It appears that the ""TO x PERCENT"" parameter on a ""VACUUM FULL"" only provides the sort percent. It's likely that your table was already sorted, as Toebs2 pointed out. Next time try ""VACUUM DELETE ONLY <table> TO 100 PERCENT""."
Amazon Redshift	"Diststyle All produces incorrect query result
We are encountering a very peculiar Redshift error related to a WHERE in a query. The query (tables renamed) is:

SELECT
    customer_id, start_time
FROM
    table_a
    LEFT JOIN table_b on table_a.id = table_b.id


Manually searching the output of this query, for a particular customer_id, produces 7 rows. However, when we add a simple ""WHERE customer_id = xxx"", the query only produces 3 rows for that customer. 

We have table_a, and table_b set to DISTSTYLE ALL. I spent many hours getting to the bottom of this. It turns out that if I make an exact replica of table_a, except switching it to DISTSTYLE EVEN, the query with the WHERE clause produces the correct result of 7 rows.

I scoured the Redshift documentation, but could not find any reason why the distribution style should change the result of queries. Does anyone know why this is the case?

Edited by: eadan on Feb 11, 2019 7:40 AM"
Amazon Redshift	"Re: Diststyle All produces incorrect query result
Evening.

You are correct to think that the distribution style should have no effect on the logical results of the query.

I am curious about this : ""It turns out that if I make an exact replica of table_a"".

How are you making an exact replica?

Have you tried making that exact replica but with ALL?"
Amazon Redshift	"Re: Diststyle All produces incorrect query result
You should see if Build 1.0.6145's ""Fix for an issue with predicate pushdown handling for outer joins"" fixes this."
Amazon Redshift	"Reject copy command when received null value
Hi ,

Is there any option available in copy command to reject whole copy command when received NULL for any records for any columns ?
or any workaround ?

Thanks in Advance."
Amazon Redshift	"Re: Reject copy command when received null value
The COPY command cannot itself do this.

You could mark every column NOT NULL, in which case COPY would fail any row containing a NULL in any column.  That record would not be loaded.  Normally MAXERROR is 0 and so the COPY would fail, but you could set MAXERROR to a higher value (the maximum is 100,000) and so obtain a load.  I profoundly advise you not to do so, as it is problematic for ETL design.

The real solution to your problem is to pre-process your data before loading, to sanitize and validate.  I use Elastic Map-Reduce for this."
Amazon Redshift	"Re: Reject copy command when received null value
Have you tried setting NOT NULL on all of your staging table's columns? Redshift supposedly enforces this constraint."
Amazon Redshift	"Oracle to Redshift Migration
Hello all,

We are trying to start the walkthrough recommended by AWS for *Oracle to Redshift Migration*(https://docs.aws.amazon.com/dms/latest/sbs/CHAP_RDSOracle2Redshift.Steps.LaunchRDSwCloudFormation.html )

The link is provided on this page to download the Cloudformation template in order to create the Oracle & Redshift clusters


http://docs.aws.amazon.com/dms/latest/sbs/samples/dms-sbs-RDSOracle2Redshift.zip


However, this link is not working and we are unable to download the templates.

Pl. let us know if there is an alternate link to download it. 

We have already reported this problem to AWS Support but has not got any response yet.

Thanks in advance.

Edited by: eCloud on Feb 15, 2019 3:05 AM"
Amazon Redshift	"Re: Oracle to Redshift Migration
Seems to be working again."
Amazon Redshift	"Amazon Redshift Announcements
Watch this thread to receive notice of forum announcements. We will post notices here when announcements are updated. 

You can also subscribe to this RSS feed for Redshift Announcements: https://forums.aws.amazon.com/rss/rssannounce.jspa?forumID=155

Old notices do age out but a record of releases can be found at: http://docs.aws.amazon.com/redshift/latest/dg/doc-history.html"
Amazon Redshift	"Re: Amazon Redshift Announcements
Investigating an issue with BYTEDICT and large VARCHAR columns causing increased load times and space utilization.

https://forums.aws.amazon.com/ann.jspa?annID=2059"
Amazon Redshift	"Re: Amazon Redshift Announcements
Fix for the issue causing increased load times and space utilization: https://forums.aws.amazon.com/ann.jspa?annID=2063"
Amazon Redshift	"Re: Amazon Redshift Announcements
New Features: Resource Level IAM, CSV double quote support, CRC32, restore status and progress.

https://forums.aws.amazon.com/ann.jspa?annID=2064"
Amazon Redshift	"Re: Amazon Redshift Announcements
System maintenance and a new feature to replace invalid UTF-8 characters with a user-specified substitute character. Details at: https://forums.aws.amazon.com/ann.jspa?annID=2090"
Amazon Redshift	"Re: Amazon Redshift Announcements
JSON support, CURSORS, REGEX, logging for ACCEPTINVCHARS and several stability improvements. Please see details at: https://forums.aws.amazon.com/ann.jspa?annID=2114"
Amazon Redshift	"Re: Amazon Redshift Announcements
Upcoming Maintenance and new features, including LZO compression, LZOP input file compression, and TO_HEX() will be rolling out in maintenance windows this week. Please see the following for details:

https://forums.aws.amazon.com/ann.jspa?annID=2136"
Amazon Redshift	"Re: Amazon Redshift Announcements
New features and fixes. Daylight savings support in CONVERT_TIMEZONE, new system table for logging user changes, SPLIT_PART function and numerous fixes.

Details at: https://forums.aws.amazon.com/ann.jspa?annID=2159"
Amazon Redshift	"Re: Amazon Redshift Announcements
Some of the older URLs are now invalid - they 404."
Amazon Redshift	"Re: Amazon Redshift Announcements
Old notices do age out but a record of release can be found at:

http://docs.aws.amazon.com/redshift/latest/dg/doc-history.html"
Amazon Redshift	"Re: Amazon Redshift Announcements
Support for using manifest files to manage data loads & numerous fixes

https://forums.aws.amazon.com/ann.jspa?annID=2187"
Amazon Redshift	"Re: Amazon Redshift Announcements
UNLOAD manifests, four new builtin functions and stability and performance fixes.
https://forums.aws.amazon.com/ann.jspa?annID=2210"
Amazon Redshift	"Re: Amazon Redshift Announcements
Improvements to resize performance, single node cursors, numerous fixes and a recap of features announced at re:Invent in November.

https://forums.aws.amazon.com/ann.jspa?annID=2267"
Amazon Redshift	"Re: Amazon Redshift Announcements
Announcing new SSD-based Node Types for Amazon Redshift:

https://forums.aws.amazon.com/ann.jspa?annID=2317"
Amazon Redshift	"Re: Amazon Redshift Announcements
AWS CloudFormation adds support for Amazon Redshift

https://forums.aws.amazon.com/ann.jspa?annID=2341"
Amazon Redshift	"Re: Amazon Redshift Announcements
Lots of new features including ROW_NUMBER(), STRTOL() and LAST_DAY() as well as the ability to cancel sessions:

https://forums.aws.amazon.com/ann.jspa?annID=2346"
Amazon Redshift	"Re: Amazon Redshift Announcements
This week's maintenance has numerous fixes and stability improvements:

https://forums.aws.amazon.com/ann.jspa?annID=2379"
Amazon Redshift	"Re: Amazon Redshift Announcements
Numerous new features and fixes including COPY from JSON, Resize Progress, new views of explain plans, REGEX_SUBSTR() and fixes to improve performance and stability.

https://forums.aws.amazon.com/ann.jspa?annID=2416"
Amazon Redshift	"Re: Amazon Redshift Announcements
We will be patching your Amazon Redshift clusters during your system maintenance windows this week. This will require a database restart so you will experience a few minutes of downtime after which you can resume using your clusters. You can view or change your maintenance window settings from the AWS Management Console. After your cluster has been patched, the new cluster version will be 1.0779. The patch contains the following items:

New Features:

Increased maximum concurrent query slots to 50 from 15. See Defining query queues.
Added support for ECDHE-RSA and ECDHE-ECDSA cipher suites. See Configure Security Options for Connections.
Enabled  direct COPY from Amazon EMR clusters. 


Fixes:

Improved analyze compression when using Bytedict with wide columns
Fixed an issue where limit clauses were not being pushed down to underlying tables on union all queries
Improved catalog maintenance operations
Fixed issues that could lead to errors when loading data into fixed width columns
Fixed an issue with JSON ingestion when using the 'ignoreheader' option with GZIP
Fixed several items related to cluster stability and performance


Please visit the Developer Guide documentation history and the Management Guide documentation history for a complete list of changes."
Amazon Redshift	"Re: Amazon Redshift Announcements
We will be patching your Amazon Redshift clusters during your system maintenance windows this week. This will require a database restart so you will experience a few minutes of downtime after which you can resume using your clusters. You can view or change your maintenance window settings from the AWS Management Console. After your cluster has been patched, the new cluster version will be 1.0.789. The patch contains the following items:

New Features:

Three additional regular expressions functions: regexp_instr, regexp_replace, regexp_count

Unload data from multiple nodes to a single file

Fixes:

Fixed slv_query_summary to properly display scans of inner tables when processing merge joins 
Fixed an issue in which the WHERE clause not processing IS TRUE correctly under certain conditions 
Fixed a rare issue with very wide tables that could lead to stuck resizes 
Fixed an issue with JSON ingestion involving escaped backslash followed by double quote 
Added better error handling when listing files to COPY from Amazon Elastic MapReduce 
Fixed several items related to regular expressions 
A variety of fixes to improve system stability 


Please visit the Developer Guide documentation history and the Management Guide documentation history for a complete list of changes.

Edited by: Tina@aws on May 8, 2014 11:55 AM"
Amazon Redshift	"Re: Amazon Redshift Announcements
We will be patching your Amazon Redshift clusters during your system maintenance windows this week. This will require a database restart so you will experience a few minutes of downtime after which you can resume using your clusters. You can view or change your maintenance window settings from the AWS Management Console. After your cluster has been patched, the new cluster version will be 1.0.791. The patch contains the following items:
 
New Features:
You can now rename clusters. The cluster endpoint will change to use the new name after the rename finishes. Please see Amazon Redshift Clusters.

COPY supports a more streamlined process to load data from Amazon EMR called the bootstrap action, which configures Amazon EMR host instances to accept commands from Amazon Redshift nodes. See Loading data from Amazon EMR using the Amazon Redshift bootstrap action.

The Amazon Redshift Best Practices section has been expanded, reorganized, and moved to the top of the navigation hierarchy to make it more discoverable.



Fixes:

Improved Random() function when operating at very large scale
Made CSV support for COPY from Amazon EMR consistent with COPY from Amazon S3
Fixed an issue with JSON ingestion involving field names containing dots 
Added better error handling when creating JSON manifest for COPY from Amazon S3 
A number of fixes to improve system stability 


Please visit the Developer Guide documentation history and the Management Guide documentation history for a complete list of changes.

Edited by: Tina@aws on Jun 3, 2014 4:34 PM"
Amazon Redshift	"Re: Amazon Redshift Announcements
We have deployed an update to address the recently disclosed OpenSSL issue to Amazon Redshift data warehouse clusters. The update will take effect after a reboot, which is scheduled to occur during your  next cluster maintenance window. Please note that the reboot operation typically takes less than two minutes to complete and the cluster will be unavailable during that time.

To have the update take effect immediately, you can adjust you maintenance window settings from the AWS Management Console. <https://console.aws.amazon.com/redshift>

All new Amazon Redshift clusters deployed after 4:23pm PDT on June 5, 2014 already have the update applied.

For more information, please visit:
AWS Security Bulletin Page: https://aws.amazon.com/security/security-bulletins/
OpenSSL’s Official Advisory: https://www.openssl.org/news/secadv_20140605.txt"
Amazon Redshift	"Re: Amazon Redshift Announcements
We will be patching your Amazon Redshift clusters during your system maintenance windows this week. This will require a database restart so you will experience a few minutes of downtime after which you can resume using your clusters. You can view or change your maintenance window settings from the AWS Management Console. After your cluster has been patched, the new cluster version will be 1.0.794. The patch contains the following items:

Fixes:

Fixed a number of issues in the optimizer that could lead to cluster restarts under certain conditions, especially when dealing with complex queries with certain classes of merge join plans or with unsastisfiable where clauses.
Fixed an issue where an invalid time format would lead to cluster crashes.
UPPER and INITCAP can now handle non-ascii characters. 
A number of fixes to improve system stability and performance.


Please visit the Developer Guide documentation history and the Management Guide documentation history for a complete list of changes."
Amazon Redshift	"Re: Amazon Redshift Announcements
We will be patching your Amazon Redshift clusters during your system maintenance windows this week. This will require a database restart so you will experience a few minutes of downtime after which you can resume using your clusters. You can view or change your maintenance window settings from the AWS Management Console. After your cluster has been patched, the new cluster version will be 1.0.797. The patch contains the following items:

New Features:

You can now COPY data directly into Amazon Redshift from an Amazon S3 bucket or Amazon DynamoDB table that is not in the same region as the Amazon Redshift cluster. 
We've also launched new numeric SQL functions, greatest and least, as well as new window functions, percentile_cont and percentile_disc, for more advanced analytics. 


Fixes:

Significantly improved restart times with large stl_querytxt files
Fixed an issue where the sort key was not updated on compute nodes after dropping columns
Improved handling of out of disk conditions, commonly seen with temporary storage used for sorting or pipelining join steps
Fixed issues related to query complication and memory access when optimizing queries


Please visit the Developer Guide documentation history and the Management Guide documentation history for a complete list of changes."
Amazon Redshift	"Re: Amazon Redshift Announcements
We will be patching your Amazon Redshift clusters during your system maintenance windows this week. This will require a database restart so you will experience a few minutes of downtime after which you can resume using your clusters. You can view or change your maintenance window settings from the AWS Management Console. After your cluster has been patched, the new cluster version will be 1.0.805. The patch contains the following items:

New Features:
You can now COPY data directly into Amazon Redshift from an Amazon S3 bucket or Amazon DynamoDB table that is not in the same region as the Amazon Redshift cluster.
We've also launched new numeric SQL functions, greatest and least, as well as new window functions, percentile_cont and percentile_disc, for more advanced analytics.

Fixes:
Significantly improved restart times with large stl_querytxt files
Fixed an issue where the sort key was not updated on compute nodes after dropping columns
Improved handling of out of disk conditions, commonly seen with temporary storage used for sorting or pipelining join steps
Fixed a number of issues related to query compilation and memory access when optimizing queries

Please visit the Developer Guide documentation history and the Management Guide documentation history for a complete list of changes."
Amazon Redshift	"LIKE operator evaluating incorrectly
I was running a query to test for improperly-assembled URLs, 

=# select first_image_url, (first_image_url like 'http%http%') from my_table  ;
                                                        first_image_url                                                       | ?column?
 -----------------------------------------------------------------------------------------------------------------------------+----------
...snip...
  https:/assets.example.com/https://cdn.example.com/upload/162087/10_P1010128.JPG                                      | t
  https://assets.example.com/44/25/45fcd4054737bbf4285058536391/zzzzzzzzzzaaabbbbbbmaterials.jpeg                       | t
...snip...


Well that's wrong! The first row should say true but the second should say false.

Let's do a quick check:
=# select ('http://www.example.com/http://this.url.was.assembled.incorrectly' like 'http%http%') ;
 ?column?
----------
 t
(1 row)
 
=# select ('http://www.example.com/this/is/fine' like 'http%http%') ;
 ?column?
----------
 f
(1 row)


So it works correctly with literals, but not values from a table.

Needless to say this is a critical-severity bug (like any bug that produces incorrect results!).

My cluster version is 1.0.5833"
Amazon Redshift	"Re: LIKE operator evaluating incorrectly
Did you try the literal value test with the actual values from the table?"
Amazon Redshift	"Re: LIKE operator evaluating incorrectly
Yes I did. The values from the table that incorrectly produce a true result when queried, correctly produce a false result when passed as a literal."
Amazon Redshift	"Re: LIKE operator evaluating incorrectly
So, reproduced here:
drop table if exists test_like;
create temp table test_like as
  select 'a' as val
  union all select 'ab'
  union all select 'abc'
  union all select 'abcd'
  union all select 'abcde'
  union all select 'abcdef'
  union all select 'ababcdef'
  union all select 'abcdefab';
 
select val, val like 'a%a%', val like 'ab%ab%' from test_like;


AFAIK the false match occurs when there is an exactly repeated term in the LIKE criteria, the test string must be at least as long as the chars in the criteria (i.e. for `ab%ab%`, the string will match `abcd`, but not `abc`).  And as previously noted - string literals don't have this problem. But then, there are other cases where literals work totally differently to actual table values...
This is running on version 1.0.5749"
Amazon Redshift	"AWS Redshift Connectivity with OBIEE on Oracle Cloud
Hi All,

One of my clients have their EDW on AWS Redshift whereas their Enterprise Reporting tool OBIEE is hosted on Oracle Cloud.

We have a requirement to connect AWS Redshift with OBIEE on Oracle Cloud to create reports.

Please share your learnings for above scenario.

Thank you so much for your help.

BR,
Avinash."
Amazon Redshift	"Cannot save ""Defer Maintanence"" setting in Maintainance Window setting
We are having an issue where the ""defer maintenance"" checkbox in the Cluster Modify > maintenance window settings cannot be saved in the ""unchecked"" state.  The box is presently checked, then uncheck and press ""Modify"" button. 

Going back into the popup window screen shows the maintenance deferment still checked. This is preventing our cluster from entering the schedule window. 
Thank you for your time.

Edited by: lou-capece on Feb 18, 2019 6:34 AM"
Amazon Redshift	"Re: Cannot save ""Defer Maintanence"" setting in Maintainance Window setting
There is no deferment active on your clusters when I checked.

Please note that maintenance deferment is checked by default in the ""Modify cluster"" popup window. It is not indicative of an existing deferment. 

You can check the existing ""Defer Maintenance"" setting on the ""Cluster"" page ""Configuration"" tab. It is  under the Backup, Audit Logging, and Maintenance section. If you see ’Set it now’ it means there is no deferment set.

I have attached screenshots showing the dialogues and settings for my cluster with and without a deferment. You can see that box is ticked by default when no deferment is in place."
Amazon Redshift	"How to connect to AWS Redshift from node.js
I am building a Node.js API using LoopBack v4. Currently I am using the loopback-connector-postgresql:https://github.com/strongloop/loopback-connector-postgresql which provides the database client, connectivity, and pooling options. This client so far has worked for connecting and sending queries to Redshift.

What is the recommended approach to connect to RedShift from a node application? Should we use postgresql clients? I see documented approaches for Java and .NET using either JDBC or ODBC connections but none for Node.js."
Amazon Redshift	"Copy from S3, Format as Parquet. ERROR: XX000: S3 Query Excpt. (Fetch).
Hi. Good evening!

I'm trying to copy data formatted as parquet, from S3 to my Redshift cluster — (please, find below the details about the versions and similar) — . Using these same parquet files, I'm able to create an external table on the Athena or Glue catalog without a problem, and I'm also able to query such external table via an external schema on Redshift (Spectrum) with no issues at all. The problem comes when I try to copy this same data to a Redshift regular table. 

Note: according to the following URL, this ""copy parquet to Redshift"" process is supported:
Link: https://aws.amazon.com/about-aws/whats-new/2018/06/amazon-redshift-can-now-copy-from-parquet-and-orc-file-formats

Please, find below the steps I'm performing:

Table DDL (masking the real table and column names):
CREATE TABLE tmp_tbl_copy_from_parquet_01
(
col_a VARCHAR(16383) ENCODE lzo,
col_b VARCHAR(16383) ENCODE lzo,
...
col_n VARCHAR(16383) ENCODE lzo,
)
SORTKEY
(
	col_n
);


And then my copy command:

COPY tmp_tbl_copy_from_parquet_01
FROM 's3://****/date=20181105'
iam_role 'arn:aws:iam::********:role/****_role'
FORMAT AS PARQUET;


Error:
ERROR: XX000: S3 Query Exception (Fetch)


Note: There's nothing for this event, at the svl_s3log or stl_load_errors views. Just that single error line with no more description than that.

I've found similar cases, for example on Stack-Over-Flow, where mention that maybe it's the number of columns that don't match the parquet file vs the Redshift table. However, if I double-check the columns of my parquet files:

%matplotlib inline
import pandas as pd
my_dataframe=pd.read_parquet('parquet_sample_temp_file_read_snappy.parquet', engine='pyarrow')
list(my_dataframe.columns.values)


I confirm with the above's code that the number of columns is totally accurate with my table DDL. The problem is that the error is not showing any additional information, at least to know where it could be the problem.

Details about my environment:

Redshift Environment:
	Cluster TypeThe cluster's type: Multi Node
	Node TypeDetermines the compute and memory capacity of the Cluster: ds2.xlarge
	NodesThe cluster's type: 4
	ZoneThe EC2 Availability Zone that the cluster was created in: eu-west-1a
	Created TimeDate and time the cluster was created: November 27, 2018 at 4:03:09 PM UTC
	Maintenance TrackMaintenance track of this cluster: Current
	Cluster Version: 1.0.4852 


Client Environment:
	Aginity Workbench for Redshift
	Version 4.9.1.2686 (build 05/11/17)
	Microsoft Windows NT 6.2.9200.0 (64-bit)


Network:
	Connected to OpenVPN, via SSH Port tunneling. 
	This is a customer's Network requirement.
	The connection is not being dropped. This issue is only affecting the COPY command. The connection remains active. 


Command:
	COPY tmp_tbl_copy_from_parquet_01
	FROM 's3://****/date=20181105'
	iam_role 'arn:aws:iam::********:role/****_role'
	FORMAT AS PARQUET;


S3 Structure:
	attempt 1) 336 files of 20 MB average = Same error
	attempt 2) 200 files of 4 MB average = Same error


I'm not sure if it's relevant to mention, but I'm trying to do a basic performance benchmark, by comparing a query using S3 directly (external table via Spectrum), vs. copying-From-S3 and query the table.

Please, let me know if you need more information. 

Kind regards,
Carlos C."
Amazon Redshift	"Re: Copy from S3, Format as Parquet. ERROR: XX000: S3 Query Excpt. (Fetch).
I am having similar issues. Did you get around with it?

Edited by: Andy007 on Feb 14, 2019 8:25 AM"
Amazon Redshift	"How to determine libamazonredshiftodbc64.so ODBC Driver version?
Is it possible to determine the Redshift odbc driver version from the libamazonredshiftodbc64.so file? The 1.4.4 version is not working for me but an older version is, however I no longer have the driver associated with the .so file."
Amazon Redshift	"RedShift Performance
I have been trying to measure Redshift's performance. We have this table with approximate 3 billions rows with one dist key (on a column which has month id), and sortkey (column 1, column 2, column 3, column 4, column 5 ) , I had defined sortkeys ( sequence of columns) while creating the table in DDL, the distkey is also part of sortkey but has sequence 5 in sorting.
These columns are going to be in where clause.  However whenever I query where I dont have column 1 in where clause the query takes alot of time(minutes), but with column1 in where clause it takes less time(3-5 seconds). I have also used encoding based on the output of ANALYZE COMPRESSION  on table.  Even If I run a query with the column which is distkey that is also part of sort key , still query takes time."
Amazon Redshift	"Re: RedShift Performance
I have been trying to measure Redshift's performance.

Okay.

We have this table with approximate 3 billions rows with one dist key (on a column which has month id), and sortkey (column 1, column 2, column 3, column 4, column 5 ) , I had defined sortkeys ( sequence of columns) while creating the table in DDL, the distkey is also part of sortkey but has sequence 5 in sorting.

Well, the distkey is as such independent of the sortkey, but the sortkey imposes constraints on the queries you can efficiently issue, and if they involve joins of any kind then the distkey matters, to keep work local to a node, and so it's likely then that the distkey is in practise related to the sortkey.

A one month wide distkey is okay if your queries are going to work over multiple months a time.  If they work on shorter periods of time, you're going to be making single nodes do the work, rather than the whole cluster.

Also, your sortkey sounds possibly amiss.  If a month based value is fifth, it's probably has no meaningful effect, which makes me question the rest of the sortkey.

These columns are going to be in where clause. However whenever I query where I dont have column 1 in where clause the query takes alot of time(minutes), but with column1 in where clause it takes less time(3-5 seconds).

Yes.  That sounds absolutely right.

Can you post the query plans?

I have also used encoding based on the output of ANALYZE COMPRESSION on table.

That's probably wrong.  You need to understand how the query is going to work and then judge for yourself what compression to use.  However, this isn't going to make so much difference - the sort key ordering and the where clause are the main players.

Even If I run a query with the column which is distkey that is also part of sort key , still query takes time.

Yes.  The further a column is down the sorkey, the less its influence.

I made a web-site with some articles about this.

Compound sortkeys are here;

http://www.redshiftusersgroup.org/mediawiki/index.php?title=Compound_Sortkey

http://www.redshiftusersgroup.org/pages/compound%20with%20zone%20map%20and%20numbers%20and%20shading.html"
Amazon Redshift	"Copy from S3 parquet to Redshift table
Hi!
I tried to copy parquet files from S3 to Redshift table but instead I got an error:
  Invalid operation: COPY from this file format only accepts IAM_ROLE credentials 


I provide User credentials what should work according to that document: https://docs.aws.amazon.com/redshift/latest/dg/copy-usage_notes-copy-from-columnar.html 
 ""COPY command credentials must be supplied using an AWS Identity and Access Management (IAM) role as an argument for the IAM_ROLE parameter or the CREDENTIALS parameter."" 

My code:
COPY {table_name}
FROM {path_to_s3}
CREDENTIALS 'aws_access_key_id=...;aws_secret_access_key=...'
FORMAT PARQUET
;


Is it possible to load parquet data using user credentials ?"
Amazon Redshift	"Re: Copy from S3 parquet to Redshift table
""COPY command credentials must be supplied using an AWS Identity and Access Management (IAM) role as an argument for the IAM_ROLE parameter or the CREDENTIALS parameter."" 
You must use a role but you can declare the role to either the IAM_ROLE or CREDENTIALS parameters.
CREDENTIALS 'aws_iam_role=arn:aws:iam::<aws-account-id>:role/<role-name>'
IAM_ROLE 'arn:aws:iam::<aws-account-id>:role/<role-name'

I  recommend that you transition to using IAM roles for access control in general, rather than embedding keys in your processes."
Amazon Redshift	"Redshift - Performance - Interleaved Sort Key
I have a redshift  table with 5 billion rows that is used in tableau. I am working on performance improvement as the report takes ~2 minutes to load. The underlying query generated by tableau does not join any table. It contains about 3 - 5 filters and group by 4 columns.

I don't have a distribution key because my data is not evenly distributed across any columns. Hence my distribution key is set to EVEN. 
I have tried using compound sort keys but here's the problem:

There are 3 filters in the report that are used most of the time. Since tableau generates its own query, the filter order maybe different. Due to this compound sort key doesn't work at all times.
I decided to use interleaved sort key. However, its not working as expected. The performance is slower than compound key.

I did run vacuum on both the tables. The encoding for the 3 columns are text255. 
What am I missing here?

Thanks.
Dv

Edited by: dv21 on Jun 16, 2018 2:20 PM

Edited by: dv21 on Jun 16, 2018 2:21 PM"
Amazon Redshift	"Re: Redshift - Performance - Interleaved Sort Key
Hi.

I missed the question at the time - I could have given you a very nice answer!  I will do so now, but it's probably rather late.

I have a redshift table with 5 billion rows that is used in tableau.

That will not work in a timely manner.  Tableau issues crazy queries, and it has no idea whatsoever about sorting orders.

I am working on performance improvement as the report takes ~2 minutes to load. The underlying query generated by tableau does not join any table. It contains about 3 - 5 filters and group by 4 columns.

Okay.  You're doing well - as well as you can do; you've arranged the table so there's no join, which will protect from a whole subset of Tableau query insanity.

I don't have a distribution key because my data is not evenly distributed across any columns. Hence my distribution key is set to EVEN.

I think it will make no difference here, because you are doing no joins.  (If you want to get around this, maybe you can make an artifical column, which you can distribute upon?)

I have tried using compound sort keys but here's the problem: There are 3 filters in the report that are used most of the time. Since tableau generates its own query, the filter order maybe different. Due to this compound sort key doesn't work at all times.

Yes and no.

A compound sort key orders data, as you know.  This affects the zone map.  The zone map is always there.  The extent to which the data is such that each zone map block contains only a narrow range of values is the extent to which Redshift has to scan less blocks to find rows.

To be sure, the further down the compound sort key a column is, then in general, the wider the range of values in each block in the zone map for that column.

However, what you actually get all depends on your data.

I decided to use interleaved sort key.

Interleaved sort keys are not a first order big-data method.  They do not handle really large numbers of rows, because you still end up with a lot of rows to query.  Queries are not timely.

A second issue is that compound sort key when you vacuum has to sort once.  An interleaved sort key has to sort once per column in the sort key.  If you have a lot of rows, your vacuum time can be become very long.  Only one vacuum can run at a time on a cluster.  You only have 24 hours a day of vacuum time.

However, its not working as expected. The performance is slower than compound key.

That's quite possible.

If your data in each column in the interleaved sort key is evenly distribued across its value range, each sort key specified in the where clause of the query will halve the number of rows scanned.

So if you specify three columns in a query and all three are in the interleaved sort key, you will scan 12.5% of all rows.

12.5% of 5 billion rows is a lot.

You should get hold of the query plan from Tableau and post it here.  The problem is, the queries issued by Tableau are quite often not human readable - they are vast - and I fear the query plan will be equally impenetrable.  It will be logically correct, but insanely inefficient.

I did run vacuum on both the tables.

Just to check - on the interleaved table, you did run VACUUM REINDEX, not just VACUUM, right?  a normal vacuum, without reindex, does nothing at all to an interleaved table.

I have made a users group web-site for Redshift.

I have extensively and exactly described how interleaved sort keys work - and I mean in a way you can understand, not the BS explanations on the net about Z-orders.

Go have a read;

http://www.redshiftusersgroup.org/mediawiki/index.php?title=Interleaved_Sortkey

In particular look at this page;

http://www.redshiftusersgroup.org/pages/interleaved%20with%20zone%20map%20and%20numbers%20and%20shading.html"
Amazon Redshift	"Re: Redshift - Performance - Interleaved Sort Key
Thank you, Toebs2! That was very helpful."
Amazon Redshift	"self-joining CTE produces incorrect results
create table blergh ( id int, somefield varchar(100) ) ;
 
insert into blergh values ( 1, 'foo' ), ( 2, 'bar' ), ( 3, 'fizz' ), ( 1, 'buzz' ) ;
 
with x as ( select id, somefield from blergh )
select x1.id, x1.somefield, x2.id, x2.somefield
  from x x1
  join x x2 on(x1.id = x2.id) ;


Through any JDBC client (SQL Workbench, 0xDBE, GroovyConsole), that query produces the correct results:

id	somefield	id	somefield
1	foo	1	buzz
1	foo	1	foo
2	bar	2	bar
3	fizz	3	fizz
1	buzz	1	buzz
1	buzz	1	foo

But through any libpq client (psql, Navicat, psycopg2, Actual ODBC) it produces incorrect results, as if the join clause were not present:

 id | somefield | id | somefield
----+-----------+----+-----------
  1 | foo       |  1 | foo
  1 | foo       |  2 | bar
  1 | foo       |  3 | fizz
  1 | foo       |  1 | buzz
  2 | bar       |  1 | foo
  2 | bar       |  2 | bar
  2 | bar       |  3 | fizz
  2 | bar       |  1 | buzz
  1 | buzz      |  1 | foo
  1 | buzz      |  2 | bar
  1 | buzz      |  3 | fizz
  1 | buzz      |  1 | buzz
  3 | fizz      |  1 | foo
  3 | fizz      |  2 | bar
  3 | fizz      |  3 | fizz
  3 | fizz      |  1 | buzz
(16 rows)

(same results if you change join/on to join/using or a where clause equality)

Remove the CTE and you get correct results all the time:
select x1.id, x1.somefield, x2.id, x2.somefield
  from blergh x1
  join blergh x2 on(x1.id = x2.id) ;

or
select x1.id, x1.somefield, x2.id, x2.somefield
  from (select * from blergh) x1
  join (select * from blergh) x2 on(x1.id = x2.id)

or even
with x as ( select id, somefield from blergh )
, other_x as (select * from x)
select x1.id, x1.somefield, x2.id, x2.somefield
  from x x1
  join other_x x2 on(x1.id = x2.id) ;


Also it seems to only happen for ""real"" tables.  This query using literals works perfectly as expected:
with x(id, somefield) as 
( 
  select 1, 'foo' union all 
  select 2, 'bar' union all
  select 3, 'fizz' union all
  select 1, 'buzz' 
)
select x1.id, x1.somefield, x2.id, x2.somefield
  from x x1
  join x x2 on(x1.id = x2.id) ;


We have a lot of Navicat users, ODBC users, and a lot of python code here so this behavior is highly troubling.  Please advise."
Amazon Redshift	"Re: self-joining CTE produces incorrect results
This is a known issue and we're working on a fix. As a workaround, you could use view instead of CTE. Sorry for the inconvenience and thank you for taking the time to share your feedback."
Amazon Redshift	"Re: self-joining CTE produces incorrect results
Could I get an update or ticket number for this? I use CTEs a lot"
Amazon Redshift	"Re: self-joining CTE produces incorrect results
Has this issue been resolved? I'm seeing similar results when joining two tables by more than one column.

Nevermind, found the solution to my problem, not sure if it is related to the posted question.
The 2 fields in the dimension table were created with NOT NULL and they were not in the fact table. I re-created the dim without NOT NULL and the join worked as expected.

Edited by: CM on Jun 24, 2016 6:24 AM"
Amazon Redshift	"Re: self-joining CTE produces incorrect results
Yes this issue was resolved some time ago.

root@redshift/dev=# create table blergh ( id int, somefield varchar(100) ) ;
CREATE TABLE
Time: 739.042 ms
root@redshift/dev=#
insert into blergh values ( 1, 'foo' ), ( 2, 'bar' ), ( 3, 'fizz' ), ( 1, 'buzz' ) ;
INSERT 0 4
Time: 951.352 ms
root@redshift/dev=#
with x as ( select id, somefield from blergh )
select x1.id, x1.somefield, x2.id, x2.somefield
  from x x1
  join x x2 on(x1.id = x2.id) ;
 id | somefield | id | somefield
----+-----------+----+-----------
  3 | fizz      |  3 | fizz
  1 | foo       |  1 | buzz
  1 | foo       |  1 | foo
  2 | bar       |  2 | bar
  1 | buzz      |  1 | buzz
  1 | buzz      |  1 | foo
(6 rows)
 
Time: 116.475 ms"
Amazon Redshift	"Re: self-joining CTE produces incorrect results
So i know this thread has been dead for a long time, but i can confirm that the issue still persists or was only partially fixed. 

WITH superset AS (
        SELECT
          subset, special_id, SUM(counter_1) AS counter_1, SUM(counter_2) AS counter_2
        FROM blergh
        GROUP BY subset, special_id
        LIMIT 1000
        )
 SELECT subset1.special_id AS special_id, subset1.counter1 AS subset1_counter1, subset2.counter1 AS subset2_counter1, subset1.counter2 AS subset1_counter2, subset2.counter2AS subset2_counter2
      FROM superset subset2, superset subset1
      WHERE subset1.subset = 1
      AND subset2.subset = 0
      AND subset2. special_id = subset1.special_id
      ;
 


If i query only on  the set without join i get seperate values back. but with above query the subset1.counter1 and subset1.counter2 are overwritten with each other. the same goes for subset2, but with another value from the set 2.

So expected ouput would be something like:
subset1.counter1,subset1.counter2, subset2.counter1, subset2.counter2. 
0, 1, 2, 3

But returned results is: 
0,0,2,2

I've not figured out the logic yet about what redshift exactly does but it's a bug 100% sure, everytime counter1 and counter2 get made the same value per subset after doing a join."
Amazon Redshift	"HAS_TABLE_PRIVILEGE query failing
Hi,
I use the Redshift admin views for quite a lot, and noticed that at least one of them no longer works for some reason. I'm unsure if it's due to a recent maintenance period or something else. The view in question is: https://github.com/awslabs/amazon-redshift-utils/blob/master/src/AdminViews/v_get_tbl_priv_by_user.sql

Here's the error I get
ERROR:  3F000: schema ""pg_temp_118"" does not exist
LOCATION:  LookupExplicitNamespace, /home/ec2-user/padb/src/pg/src/backend/catalog/namespace.c:1338


I distilled the query down to this, and it still fails:
SELECT
        schemaname
        ,tablename
        ,HAS_TABLE_PRIVILEGE('admin', schemaname || '.' || tablename, 'select') AS sel
FROM pg_tables;

Removing the cross join (and almost everything else) didn't fix it. However, it works if I remove the HAS_TABLE_PRIVILEGE field, so it appears to be a problem with that function."
Amazon Redshift	"Re: HAS_TABLE_PRIVILEGE query failing
I tested this on numerous versions and am not able to reproduce the issue. 

Recommend dropping and recreating the view as a first step.

Also check that your user is a superuser or you have limited the results to items on which you have permissions."
Amazon Redshift	"Re: HAS_TABLE_PRIVILEGE query failing
Note that I tested a straight select on pg_tables, and it threw an error, regardless of the view, or what user ran the query.

However testing it now works, which is strange. It must have just been a temporary thing. Nothing appears to have changed."
Amazon Redshift	"Forum loosing posts
There's a design flaw with the forum.

Sometimes when you post, Amazon pops up a captcha, and when you complete it, you are taken to the home page of all forums, and the post you wrote is lost, unless you were writing for long enough that a draft copy was auto-saved."
Amazon Redshift	"Re: Forum loosing posts
Doh!  That's a bad end user experience."
Amazon Redshift	"Re: Forum loosing posts
Just lost another post.

Had to remake it.

Redshift guys reading this - the anti-spam stuff from Amazon has broken the forums.  Users can now lose posts from it.

Who do I speak to about this?  YOU NEED TO FIX IT.

You cannot a forum which randomly loses posts."
Amazon Redshift	"Re: Forum loosing posts
Thank you for bringing this to our attention. 

Can you please PM with an example (screenshots would be great) of what happens before the post is lost so we can try to reproduce the issue?"
Amazon Redshift	"Re: Forum loosing posts
I can try, but it's hard to do so because it's unpredictable.

It just happened again, however.

It's... so, to log in, sometimes first I need to prove I'm not a robot with a captcha, then I enter my username, then I need to enter another captcha to prove I'm not a robot, then I can enter my password, and then I can enter my OTP code.

And then when I'm posting on the forum, as I mentioned, sometimes when I submit I'll be asked to complete a captcha and this will lose the original post as it brings me back to the root of all forums.

There seem to be a number of independent mechanisms, which are not defering to the OTP sign-in, and so there's a barrage of verification."
Amazon Redshift	"ETL inside Redshift
Hello!
I have architectural question. I need to develop ETL flows on top of data in my Redshift cluster (sources and destination is Redshift table). I know that there are 2 general approaches to this solution:
1.) Use AWS Glue or AWS EMR to develop ETL processing
2.) Use partner solution (like Informatica or Talend)
First approach seems to be good one if I have big amounts of data to be processed. But It require to setup additional cluster which involves additional costs and adds startup time. Seems like quite overkill if relatively small data set (up to few GB) needs to be processed. Second one is not serverless approach and I'm getting  maintenance overhead. And obviously additional costs.

I'm thinking of adapting old school DWH approach to Redshift. 
1.) All ETL logic will be implemented in Redshift view
2.) Actual loading will be done with: insert DESTINATION_TABLE (...) select * from VIEW_WITH_ETL_LOGIC
3.) Execution of insert statements will be done with lambda functions over JDBC
4.) Orchestration of Lambda functions will be done in AWS Step Function

From my perspective this setup gives me fully serverless approach without need of additional costs (I'm paying for Redshift anyway).

Do you see any drawbacks of such solution?
Regards
Andrzej"
Amazon Redshift	"Re: ETL inside Redshift
I know that there are 2 general approaches to this solution:
1.) Use AWS Glue or AWS EMR to develop ETL processing

I've never used this, so I can't comment.

2.) Use partner solution (like Informatica or Talend)

I have seen these used, and they're appalling.

They all have SQL generators, and those SQL generator produce the most appalling SQL you can imagine - SQL which is bad enough anyway, but also in addition is profoundly hostile to sorted column-store relational databases, such as Redshift.

Redshift requires the sorting arrangement of tables to match up with the queries to be issued.

You cannot know what queries an SQL generator will issue, so you cannot make the tables correctly.

All software with SQL generators cannot be used with sorted column-store databases - if, that is, you are thinking to use the database for its intended purpose, of handling Big Data.  If you are not, this type of relational database should not be used, since it comes with many constraints and what you want to do can be handled by a conventional relational database.

First approach seems to be good one if I have big amounts of data to be processed. But It require to setup additional cluster which involves additional costs and adds startup time. Seems like quite overkill if relatively small data set (up to few GB) needs to be processed. Second one is not serverless approach and I'm getting maintenance overhead. And obviously additional costs.

Why are you using Redshift for a few GB?  it's as improper as using MySQL for a few terabytes.

I'm thinking of adapting old school DWH approach to Redshift.
1.) All ETL logic will be implemented in Redshift view
2.) Actual loading will be done with: insert DESTINATION_TABLE (...) select * from VIEW_WITH_ETL_LOGIC
3.) Execution of insert statements will be done with lambda functions over JDBC
4.) Orchestration of Lambda functions will be done in AWS Step Function

I've not used lambda functions, but I understand they have a 900 second maximum duration, and I vaguely remember people running into concurrency issues with Redshift?  but that may be a red herring.

If your insert ever exceeds 900 seconds, you will need to rewrite your ETL system."
Amazon Redshift	"Re: ETL inside Redshift
Hey Andrzej - we have a large number of transforms occurring within Redshift as well (landing raw data in Redshift and creating derived tables/views within Redshift).

Your solution will work, but the biggest problem I would anticipate is maintenance - when you (or whoever one day takes over for you at your company) want to change a script or add new scripts, will you be able to quickly and easily understand which scripts need to run in which order, etc?

To get to a middle ground between completely writing your own execution logic and relying on a partner solution, I would encourage you to take a look at dbt ( http://getdbt.com ) - an open-source tool for writing transform logic. There are a few jinja-templating tricks to it, but at it's core it is executing SQL that you wrote (as opposed to the generated SQL Toebs2 mentioned). If you use DBT's reference variables to connect your models to one another DBT will generate the DAG and execute your scripts in order.

The maintainers of DBT off a cloud-hosted version with a robust free version here: https://www.sinterdata.com - which makes this closer to a partner solution but also takes care of your ""serverless"" goal. There's also a vibrant Slack community providing a lot of support here: https://slack.getdbt.com/

Note: I'm not a paid rep of DBT - I'm a happy user and a paying Sinter customer. I was previously executing our SQL transforms via an Airflow server but found the barrier to entry was too high for other analysts at the company to get involved scheduling their own transforms; I've found DBT complements the rest of our ETL / ELT systems by giving us a simple, dedicated place to handle transformations within Redshift.


Brad Colbert

Outdoor Voices"
Amazon Redshift	"Re: ETL inside Redshift
Hello Toebs!
Thanks for your input. Answering your questions:

Why are you using Redshift for a few GB? it's as improper as using MySQL for a few terabytes.

We are talking about ETL like processing, every day new partitions will be added. If we have 50 tables and each growth few GB per day than we will get into TB scale in 2-3 days. If we want to store few years of data then Redshift seems to be right solution.

If your insert ever exceeds 900 seconds, you will need to rewrite your ETL system.

That is my concern also. For sure there where be INSERTs exceeding this duration. So if Lamba is not the right choice here (and in this situation I belive isn't) what AWS service should I choose? Issue seems to be relatively simple- I need a way where I can automatically execute Redshift scripts?

Regards
Andrzej"
Amazon Redshift	"Re: ETL inside Redshift
Hello Brad!
Thanks for your input. Sinter & Airflow seems to be nice scheduling tools (I worked with Airflow). But base on my understanding of AWS Setp Function it should be able to provide me with same functionality (job orchestration, scheduling and easy maitenance). The missing component here is how to execute Redshift SQL script from AWS Step Function. My idea was Lambda however, as noticed by Tobes 900s limit is serious limitation. That is why I'm looking for alternative solution.
Regards
Andrzej"
Amazon Redshift	"Re: ETL inside Redshift
Here are couple of options. as you know every use case is different. Please use your judgement.

If the source & dest are redshift, is it the same cluster & same DB? then adding additional layers add complexity for the small volume you mentioned.

Lot of tools that work with bigdata & redshift perform ELT. informatica, Matillion etc.. Tools realize ELT is best suited for bigdata workloads where cluster computing / distributing is involved.
For big teams & enterprise scale with tons of processes, airflow or similar tools makes sense.
A tiny rds postgresql instance with control flow, logic with dblink to redshift has been working great for us.

for orchestration, 
your approach on using step function + lambda will work. Just need a check in step fn to keep checking every x mins with script completed. due to lambda timeout issue for long running processes.
or use glue can be used to just call sql that executes on redshift, not for data processing using spark. at this point glue is just like a scheduler and you pay extra."
Amazon Redshift	"Re: ETL inside Redshift
Lot of tools that work with bigdata & redshift perform ELT. informatica, Matillion etc..

In my opinion, from my experience using it and my understanding of how Redshift is implemented and the technical requirements of working with Big Data, Matllion -cannot- be used with Redshift for Big Data.  It is impossible.  Matillion firstly uses an SQL generator, which is a fundamental problem, and secondly, by its very nature, inserts a thick layer between the mind of the developer and the actual data processing work being done, which is utterly hostile to the close matching up of table sorting orders and the queries issued upon them.  It's like trying to perform heart surgury wearing rubber gloves.

Moreover, I think in general graphical ETL tools are a profound blunder, because the graphical interface is not well suited to writing software.

The reason they get used is because non-technical managers, who do -not- know about coding, buy into them and purchase them and instruct dev teams that they have to use them.

For big teams & enterprise scale with tons of processes, airflow or similar tools makes sense.

I made one project with Airflow.  I thought it was a bad mistake.  In general I think open source software is a bad mistake, and it was for those reasons I thought Airflow was wrong - I have a little saying about it : ""open source, broken out of the box with no docs"".  Also, Airflow, like all these workflow tools, has it's own particular paradym.  If you step outside of that, your life becomes hell.

your approach on using step function + lambda will work. Just need a check in step fn to keep checking every x mins with script completed. due to lambda timeout issue for long running processes.

But what happens when the data load does take longer than the lambda function will allow?

or use glue can be used to just call sql that executes on redshift, not for data processing using spark.

Spark is appalling.  The docs are awful.  Last time I tried to intall it, in 2016, even the installer docs were not correct.

You can't make reliable, production quality systems with that."
Amazon Redshift	"Re: ETL inside Redshift
Full Disclosure: I am CTO of Matillion

@Andrzej, Implementing ETL transformation logic on Redshift (push down ELT style) was the original idea behind the Matillion ETL product so I can confirm that architecture you propose in your original question is valid. We have hundreds of customer using Matillion doing exactly that. You will need an orchestration layer and a scheduler at a minimum to tie the whole process together.

@Toebs2, I totally understand that graphical ETL tools are not for everyone and a lot of people prefer to hand code their pipelines. The issue we see very often with this is data transformation processes quickly become very complex and are difficult to maintain when scripted (often in a mixture of languages/frameworks Python+SQL, Air+Scala). In addition to the complexity, pipelines also need to flex with the changing needs of the business. This is a problem if all data transformation changes in an organisation are constrained by availability of developer resource. A GUI tool can open up the use of data to a wider range of people in an organisation. It's probably worth noting that the SQL generation in Matillion is heavily optimised for Redshift and is not generic, oh and you can hand write your SQL in the pipeline if you want to."
Amazon Redshift	"Re: ETL inside Redshift
Hej - I will be replying to this, but I'm waiting for some information from a previous employer, notes I made about using Matillion.  Most of it I think is not relevant here, but I want to make a fully considered reply, and that means checking that information for any important and relevant points."
Amazon Redshift	"Re: ETL inside Redshift
Hi All!
Thanks for your input. Architectural questions are typically difficult to answer (because every approach has advantages and disadvantages) so I'm happy to see how this discussion evolves.

Last few days I was checking different options and:
1.) I was able to execute INSERT from VIEW on Redshift using Glue. The first issue is that it requires quite a lot of weird stuff to do- including using custom pg8000 python library- probably only option to connect to Redshift from glue. But this is not a good idea- Glue requires a cluster which has long cold start so if we have ETL that lasts for 1-2 minutes it is completely unprofitable.
2.) Second option I checked was just using glue script with Redshift tables but lack of push down predicate seems important limitation. And again- cold start is quite significant.
3.) I checked AWS DataPipeline service but it seems to me much over complicated and available in only few Regions (mine was not on the list). My feeling was this service is slowly going to its end of live. Any opinion here?
4.) Executing script from Lambda and then periodical check for progress (to avoid Lambda timeout) seems to me over complicated

Base on above and all your comments I have feeling that my architecture assumptions were wrong from the begging. My goal is to perform DWH migration from on-prem to AWS cloud. My first though was it should be 1 to 1 copy of all layers to Redshift however it seems it is not very good idea. Now I'm thinking that right way of doing that is to implement DWH Stage layer as S3 buckets/directories and use Redshift to keep Star/Snowflake like data (& Data marts). This way ETL will be performed with Glue & EMR and orchestrated in AWS Step Function. As last step data will be copied to Redshift to serve it to different visualization tools for analysts.
I believe this should make more sense?

@edthompson1 Thanks for mentioning Matillion. If we decide to to go full-Redshift than we will take it into serious consideration because non of approaches we tried till now seems to be good enough.

@Toebs2 Thanks for your input. If you have any more hands-on on mentioned tech I would be happy to hear!

Regards
Andrzej"
Amazon Redshift	"Re: ETL inside Redshift
Full Disclosure: I am CTO of Matillion

Good afternoon, CTO of Matillion!

But don't worry, I still think of you as human despite being in management  

@Toebs2, I totally understand that graphical ETL tools are not for everyone and a lot of people prefer to hand code their pipelines. The issue we see very often with this is data transformation processes quickly become very complex and are difficult to maintain when scripted (often in a mixture of languages/frameworks Python+SQL, Air+Scala).

Software - any software - can end up being unmaintainable.  That is one of the fundamental challenges of writing software anyway, where requirements change over time and so tend to make the original design incorrect, and this is exacerbated in hierarchical organizations, because in such organizations typically the consumers of the software are ""above"" the producers of the software, and so the long-term costs and problems of software maintenance are not understood, and so not honoured, which leads to a progressively worsening problem.

All of this however is also true for graphical ETL tools.

In addition to the complexity, pipelines also need to flex with the changing needs of the business.

Yes.  ""Flex"" and ""changing needs"" here means that requirements change and so the ETL process also must change.  This is true regardless of the implementation platform, graphical or scripting.  The question then is which is platform should be chosen for any given work.

This is a problem if all data transformation changes in an organisation are constrained by availability of developer resource. A GUI tool can open up the use of data to a wider range of people in an organisation.

Absolutely and categorically no.

When people who are not software developers attempt to perform software development they produce unmaintainable garbage which emits the wrong results.

They have no idea and no experience of the nature of software development - no idea that everything is wrong until tested, no idea about testing, nothing.

Furthermore, what we found was that Matillion had a significant number of issues and idiosyncrasies, such that it was challenging for us to use, as experienced software developers.  We had one or two people who were explicitly not software developers, and never had been, using it.  They produced horrors, huge, complex, unmaintainable horrors which every time we looked at them had issues, and which we ended up unofficially doing our best to maintain.  The output from those things could never be correct.

Speaking for our own use, as experienced software developers, there were a very large number of issues, small and large, and we desperately wanted to move away from Matillion.  The choice of development tools however was politically sensitive.

For example, one of the major problems is that the output from Matillion, what we store in source control, is a large JSON file.  The contents change massively on every change to the Matillion project.  As as result, diff is not viable because there are thousands of lines of human unreadable change between each revision.  As a result of this, we cannot work in parallel, in the usual concurrent source control model.  We have to serialize work, with only one developer working on the Matillion project at a time.

A more fundamental issue is that graphical tools, by their very nature - two dimensional, on screen, strongly favouring the X axis - strongly impose a linear flow on the code path, and along with this, Matillion itself has no native data processing capability.  Natively, it can only issue instructions to Redshift.  It is normally not possible to use and only use an SQL database to provide ETL functionality.  You must be able to perform arbitrary logic, and often specifically outside of the database, where the flow of control is not constrained such that it can be represented two dimensionally on a screen, and perform actions which simply cannot be performed by an SQL database.  For example, we had to merge large numbers of files prior to loading because loading large numbers of files is not performant.  We couldn't even load the data into Redshift to get Matillion to work on it.  You need to be able to script.   We simply could not implement much of what we needed using Matillion.

Matillion supports scripting, but the support is weak and it breaks the Matillion paradym by being a black box.

We used Python, and embedded it into the Matillion project, and this was a major problem in two ways.  Firstly, Matillion was not very good with embedded Python.  We ran into a range of significant problems getting it to work, which took a lot of time and where for example some of these problems caused Matillion to lock up and require rebooting.  Secondly, we then had a black box in the project.  You could read the project and know what it was doing - right up until you hit one of the black boxes. Then you needed to become a scripter and read and comprehend some (long and fairly complex) scripts.

We had to script for a range of reasons, not least of which were wide tables.  You cannot use an interface which requires significant mousing and clicking per column with hundreds of columns, not unless you have interns and you hate them.  ""Configure this step in Matillion three hundred times!  bwahahahah!  and don't make any mistakes or you'll be demoted to JUNIOR intern!  BWAHAHAHA!!!""

If you're going to have to write significant parts of the project in a script anyway, why take on the innumerable and painful costs of using Matillion *anyway*?  why not just go fully scripted, where you can have developers work in parallel and have automated testing?  what are you getting from Matillion here?  what's the advantage in use?

We never found a way to perform automated testing on a graphical ETL platform.  You can't have a serious, professional ETL system without automated testing.

Working on Matillion was the least productive experience I've had in my life.  It got in the way, and it made us do a lot of extra work, and for inferior outcomes - and that was for us, as experienced software developers.  The one or two people we had using it who were not software developers were not coping, and simply producing garbage.  We avoided making changes where-ever possible, moved as much functionality as possible into scripting stages and basically used Matillion as a shell for running Python.

It's probably worth noting that the SQL generation in Matillion is heavily optimised for Redshift and is not generic,

Okay, here, I'm almost angry.

This is absolutely wrong and it is misleading.

So, Redshift.

You use it because you're using Big Data.

To use Big Data in Redshift, you have to operate Redshift correctly.

What this means is that the queries must match up with the sorting orders of the tables.

If they do not, your perform goes out the window and you wait a week for queries to return.

This leads to two primary concerns.

Firstly, when writing an ETL system, the SQL being executed and the table DDL must be absolutely in your face.  You have to see them, right in front of you, in full, so you can see if they match up and then make the changes to make them match up.

Matillion strongly hinders this key, central, critical, vital process.

Firstly, the SQL is buried.  To find the SQL, you must go to the correct step in the Matillion project, go to its properties, display the SQL, and have Matillion display it to you as it will be issued to the database.  You then for non-trivial queries must copy and paste it out of Matillion and into an editor of some kind, to fix the formatting so you can read it.

Then, once you have the SQL, you need to adjust it.

To adjust SQL in Matillion, you must operate Matillion.  You must return to the graphical user interface, and then figure out what you must do to Matillion to get it to make the changes you need in the SQL.

This is a long, slow, painful process.  Typing a few words in an SQL script takes seconds.  Adjusting Matillion takes at least a minute, if you know what to do to make the changes you need, and also if you're only changing one column.  If you do not know what to do, then you are then experimenting with adjusting Matillion, to make SQL changes which you already know how to make in SQL.  If you have many columns to change, a simple job in text becomes a painful, tedious, slow job because of extensive mousing and clicking.

The time taken for the fundamental development cycle of change/run/check, which is the keystone of productivity, becomes much slower, and more error-prone.

Matillion also for us had a habit of intermittantly disconnecting.  We would then have to log in again, figure out where we'd got to, and start over.

When you've completed work, when you want to look over a project, you want to be able to easily scan the SQL and compare it to the table DDLs.  In Matillion, this is problematic, because you have to manually make your way through all of the graphical projects.  There's no search functionality.  You have to mouse, point and click, a lot, and remember in your head where everything is.  Then bring up the SQL, then copy and paste it out of Matillion so you can reformat it to be readable, and then compare it with the table DDL.

It's like performing heart surgery wearing rubber gloves.

Secondly, it also means non-software developers CANNOT, EVER, produce systems on Redshift, because they have absolutely no clue about these issues.  They can never get it right, and the ones we had never did.  How could they?

The SQL generated by Matillion actually is fairly directly whatever you've set up in the graphical interface, because the graphical interface is a direct representation pretty much of SQL.  I saw no evidence whatsoever of Redshift specific behaviour.  The SQL seemed to be whatever you put on the graphical display.

Where the non-software developers using Matillion did not know about databases or SQL, they happily and unknowingly produced queries with for example cross-joins, and the performance was appalling and hammered the cluster for all users.  We had to then try to explain to them why this wasn't a good idea and that they really couldn't do it, which didn't work well, because they didn't really understand why it happened, and so they had no idea what not to do, or what to do instead.

They are then a burden and problem to the software teams, and a risk to the company, as they are producing garbage and also have no idea about backups, leading other parts of the company, consuming their output, to become dependent on a source of data which is not only incorrect but unreliable.  When their crazy systems fall over, who else can they turn to for help but the software teams, who then can have to drop what they're doing and scramble to try to resurrect - instantly, if not sooner - a completely crazy system which is only fit for replacement?

oh and you can hand write your SQL in the pipeline if you want to.

Yes, you can, but then you have a black box in the graphical project, with the drawbacks described earlier.  We did have SQL scripts in the project, we had to, and it made us wonder why we were using Matillion since we had to script anyway.

There are a lot of other issues with Matillion.  I've touched upon some of the major issues in this post.

These issues are not unique to Matillion.  The fundamental issues, for example forcing code paths into a two-dimensional representation which strongly favours the X axis - are true for all.  There are plenty of Matillion specific issues, but the other graphical ETL tools will also have plenty of their own specific issues.

If you as a company are going to write software, you need software developers.  You can't avoid that.  Graphical ETL tools are irrelevant to that need.  Let developers choose the tools they want to use, because they know what will work best.  I think graphical tools are a profound and fundamental blunder.  I may be wrong, flat out.  Or it may be other have found ways to use them well, where I (and to be fair, also the team I was in) have not.

In all things, read the evidence and reasoning, ask questions, think for yourself, and experiment.

On a more personal note, Ed, I'm sorry to be the bearer of so bad a review."
Amazon Redshift	"Re: ETL inside Redshift
FYI.. you probably saw 2 new options..
1 - Stored procedures in redshift. coming soon.
2 - AWS glue python shell. cheaper than AWS glue with spark"
Amazon Redshift	"Re: ETL inside Redshift
1 - Stored procedures in redshift. coming soon.

Say what?  Python UDF exists already, doesn't it?"
Amazon Redshift	"Re: ETL inside Redshift
Toebs2 wrote:
1 - Stored procedures in redshift. coming soon.

Say what?  Python UDF exists already, doesn't it?

UDFs and stored procedures generally have different purposes, but sometimes the terms are interchanged. I'll use what I'm most familiar with, MySQL's terminology.

UDFs generally refer to static, self-contained, deterministic wrappers for some custom logic that takes a specified input, does something with it, and outputs the result. For example, I have a UDF that takes a timestamp as input and returns the number of days in that month. This can be done in Python or SQL pretty easily.

Stored procedures are more complex, usually utilizing a procedural language to implement some logic, and most of the time can actually interact with tables/data. For example, you could use a stored procedure to generate and run some dynamic SQL, or loops through a dataset and conditionally executes some logic.

The lack of true stored procedures may have something to do with Redshift's lack of table-generating functions, e.g. the inability to generate a multi-row dataset from a single function call. Or maybe it simply the lack of a procedural language. I'm not sure.

I haven't heard any news that these kind of stored procedures are coming to Redshift, but I'd be happy to hear otherwise. They are useful at times."
Amazon Redshift	"Re: ETL inside Redshift
Python UDFs in Redshift only support scalar UDFs.
Procedural UDFs using pl/pgsql are coming to Redshift as announced at re:invent couple of months ago."
Amazon Redshift	"Re: ETL inside Redshift
That's excellent news. Looking forward to that!"
Amazon Redshift	"Re: ETL inside Redshift
I concur with your description and differentiation between UDF/SP."
Amazon Redshift	"Can Redshift save off or quarantine bad records on import
In the following command, 90 records import successfully, and one record fails:
user@host:~$ psql --host=redshiftcluster.amazonaws.com --username=user --dbname=dev --port=5439 -c ""COPY tmp_table FROM 's3://tmpbucket/test.json' iam_role 'tmp_role' FORMAT AS JSON 's3://tmpbucket/test.jsonpaths' TIMEFORMAT as 'auto' MAXERROR as 100000;""
Password for user user:
INFO:  Load into table 'tmp_table' completed, 90 record(s) loaded successfully.
INFO:  Load into table 'tmp_table' completed, 1 record(s) could not be loaded.  Check 'stl_load_errors' system table for details.
COPY

Is there a way to save off or quarantine (to a table, flat file, or something else) the one that did not import?  That way an analyst can inspect the records that failed, fix them, and then try importing them again."
Amazon Redshift	"Re: Can Redshift save off or quarantine bad records on import
The system table STL_LOAD_ERRORS will contain a line per failed record, and contain the first 1024 characters of the failed line.

That's it.  No other way that I know of to do what you want.

I would strongly advise you NOT to use MAXERRORS.  It is infinitely to set it to 0, fail all loads which have any errors, fix the errors, and then load the file.

If you have partial loads your ETL system and overall state is going to become complex and problematic.

Consider for a simple example that when MAXERRORS is not 0, you have three possible outcomes for a load - fully successful, partially successul, and failed.  When MAXERRORS is 0, you only have two states - success and failure."
Amazon Redshift	"Issue with COPY command through VPC
Hello,

Recently I started up a VPC and launched a redshift cluster.  I have data in S3 buckets but I am unable to run the copy command.  I have created an endpoint within the VPC that clearly shows it is for the S3 environment.  The endpoint creation seemed to work give the route was automatically created as well.  What else is needed?

Could it be that we do not use IAM and when you set up a redshift environment through a VPC you need to use IAM?  Or the same commands we used before should work as normal?  Just not sure where the issue stems from.

Thanks!"
Amazon Redshift	"Re: Issue with COPY command through VPC
Is the S3 bucket owned by same account as the cluster? If not does the cluster's account have access to the bucket?

IAM roles are generally the recommended way to enable a cluster to access other AWS resources now. 

I recommend carefully following the steps in the doc ""Authorizing COPY, UNLOAD, and CREATE EXTERNAL SCHEMA Operations Using IAM Roles"" https://docs.aws.amazon.com/redshift/latest/mgmt/copy-unload-iam-role.html"
Amazon Redshift	"Re: Issue with COPY command through VPC
Yes, sorry the S3 bucket is under the same account and same region FWIW"
Amazon Redshift	"Slow queries/performance after redshift maintenance?
We have noticed that our 3 clusters have been extremely slow for queries and general performance after there was amazon scheduled maintenance on Jan 23, 2019. We originally thought it was due to the version that was upgraded to, so we rolled back to 1.0.5463 yet are still seeing issues with queries/updates taking almost twice as long prior to the maintenance. However, cluster performance graphs seem to be fine? 

Everything keeps pointing back to that date that the issues started.... Any insight would be greatly appreciated.

Jan 23
12:00 AM
Maintenance started on Amazon Redshift cluster 'dev-xx' at 2019-01-23 05:00 UTC.
info
dev-xx
cluster
management
REDSHIFT-EVENT-2003
Jan 23
12:00 AM
Maintenance started on Amazon Redshift cluster 'prod-xx' at 2019-01-23 05:00 UTC.
info
prod-xx
cluster
management
REDSHIFT-EVENT-2003
Jan 23
12:00 AM
Maintenance started on Amazon Redshift cluster 'qa-xx' at 2019-01-23 05:00 UTC.
info
qa-xx
cluster
management
REDSHIFT-EVENT-2003"
Amazon Redshift	"Re: Slow queries/performance after redshift maintenance?
Sorry that you're having an issue. Please note that we cannot provide detailed investigations via this forum. 

If you have rolled back to the previous version and there was no change that suggests the issue is related to something other than the Redshift version - it's hard to say what without investigating directly. 

You say that the issue affects multiple clusters though so I strongly recommend raising this issue with AWS Support so that a Redshift specialist can investigate and provide further assistance.
https://console.aws.amazon.com/support/home#/"
Amazon Redshift	"Re: Slow queries/performance after redshift maintenance?
We see this is a regular pattern, i.e. after a maintenance upgrade the etl jobs runs slower then usual on the first run, then on the next run it's back to normal. Explanation has been that after an upgrade Redshift needs to recompile statements sent for execution. I don't know whether that's the whole story, as the increase in execution times seems excessive if recompilation is all that's required. But rolling back to the previous version probably just reproduces the same pattern."
Amazon Redshift	"Reshift Spectrum scale-out patterns
We are evaluating Redshift Spectrum against one of our data set. We observe some behavior that we don't understand.

We look at different amount of Partitions, all data files are Parquet snappy compressed.

Each day is a partition, and each partition has about 250 Parquet files and each file has roughly the same size. Across 365 days, each file per day ranges from 350MB to 500MB.

When we evaluate just 1 day/partition, our two Redshift cluster (A: 14 nodes ds2.8xlarge configuration performs about the same. B: 5 nodes dc2.8xlarge)

When we evaluate 30 days/partitions, both clusters performs about the same.

When we evaluate 365 days/partitions, cluster A with 14 nodes out-performed cluster B with 5 nodes by 3x.

The query plan shows the following:

XN HashAggregate (cost=300017018.00..300017018.50 rows=200 width=524)
-> XN Partition Loop (cost=300000000.00..300012018.00 rows=1000000 width=524)
-> XN Seq Scan PartitionInfo of REDACTED (cost=0.00..15.00 rows=5 width=0)
Filter: (((dt)::text <= '2018-01-01 00:00:00'::text) AND ((dt)::text >= '2017-01-01 00:00:00'::text))
-> XN S3 Query Scan s (cost=150000000.00..150002000.50 rows=200000 width=524)
-> S3 HashAggregate (cost=150000000.00..150000000.50 rows=200000 width=516)
-> S3 Seq Scan sherlock_pq.search s location:""REDACTED"" format:PARQUET (cost=0.00..100000000.00 rows=10000000000 width=516)

 userid | query  |   xid    |  pid  | segment | step |         starttime          |          endtime           |     elapsed      | aborted |           external_table_name           | is_partitioned | is_rrscan | s3_scanned_rows | s3_scanned_bytes | s3query_returned_rows | s3query_returned_bytes | files | files_max | files_avg | splits | splits_max | splits_avg | total_split_size | max_split_size | avg_split_size | total_retries | max_retries | max_request_duration | avg_request_duration | max_request_parallelism | avg_request_parallelism
--------+--------+----------+-------+---------+------+----------------------------+----------------------------+------------------+---------+-----------------------------------------+----------------+-----------+-----------------+------------------+-----------------------+------------------------+-------+-----------+-----------+--------+------------+------------+------------------+----------------+----------------+---------------+-------------+----------------------+----------------------+-------------------------+-------------------------
    000 | 000000 | 23139139 | 69900 |       0 |    1 | 2018-02-22 21:03:20.454431 | 2018-02-22 21:08:13.686283 |        293231852 |       0 | S3 Subquery REDACTED          | t              | f         |      3838364085 |      49507494388 |               2074674 |               58837703 | 91250 |      1271 |      1140 | 341933 |       4600 |       4274 |   41211766420355 |      150994253 |      120525852 |          2150 |           2 |             87067259 |               526010 |                      10 |                      10


The above row is from the 5 node cluster scanning 365 partitions, we are surprised by the number of bytes returned to redshift cluster and bytes scanner in s3. When we run an equivalent athena query, the bytes scanned is only 1.3GB vs 49GB, and Athena complete the query much faster than our 6 nodes cluster in Redshift."
Amazon Redshift	"Re: Reshift Spectrum scale-out patterns
This is a key difference between Redshift Spectrum and Athena. Each slice of your Redshift cluster is able to call up to 10 Spectrum query workers (if there are enough files involved in the query to use that many workers).

• 5 nodes of dc2.8xlarge has (16 * 5) 80 slices = up to 800 Spectrum workers
• 14 nodes of ds2.8xlarge has (16 * 14) 224 slices = up to 2,240 Spectrum workers

If your query involves significantly more than 800 files then you should expect the larger cluster to process it more quickly.

Regarding the query that processes more data (and takes longer) on Spectrum than Athena, it sounds like Spectrum did not optimize the query as well as it could have. Can you send me a private message with the query number, query SQL and cluster endpoint so we can investigate?"
Amazon Redshift	"Re: Reshift Spectrum scale-out patterns
Can you tell us what 

XN Partition Loop

is? There is no documentation on this. 

Thanks,
Drew"
Amazon Redshift	"Re: Reshift Spectrum scale-out patterns
Hey Joe,

There's one thing that would be helpful to clarify and/or document for all of us that are performing similar Redshift/Spectrum performance analyses.  Could you please clarify how and under what conditions Redshift makes decisions on the composition and boundary of the units of work that get passed to Spectrum nodes and how those decisions effect and hopefully maximize Spectrum layer parallelism?

For an external partitioned table using Parquet there are 4 possible places where Redshift could decide to split up the work assigned to the Spectrum nodes.  
1. The highest level would be at the partition level where a Spectrum node could be assigned a partition or list of partitions to process the table sub-query against.  This would require the Spectrum nodes to have access to the metadata in the external catalog to be able determine how to get the partition location and build it's own list of files to operate on.  Here Redshift could control the number of units of work to ensure efficient use of Spectrum nodes and maximal parallelism.
2. The next highest level would be to simply have Redshift do all the external catalog data location work and pass a simple list of files to a Spectrum node that could be a file list of a partition's files,  a file list of a multiply complete partitions' files, or an arbitrary file list that spans parts of one or more partitions.  This could also be used to could control the number of units of work to ensure efficient use of Spectrum nodes and maximal parallelism.
3. A mid level would be to simply pass a Spectrum node a single file to operate on.  This would likely be the simplest to implement, but the least flexible and difficult to make optimal because the number of units of work and the Spectrum parallelism is more controlled by the what ever writes the external table and files as opposed to Redshift deciding.
4. The lowest level could be for Redshift to pass Parquet file row group or list of row groups to a Spectrum node to process.  The similar sub-file construct in ORC, stripes, could do effectively the same. This would require Redshift to read metadata from Parquet (or ORC) files.  This could also be used to control the number of units of work and maximal parallelism and cover the degenerate case where there are not enough files to provide 10 x slices units of work to max out Spectrum parallelism.

Thanks,
-Kurt"
Amazon Redshift	"Re: Reshift Spectrum scale-out patterns
Thank you for the info. I think this is relevant information and it should be in the docs somehow.

When you say ""up to"", what are the factors for the spectrum engine to assign the maximum number of workers available? For example, I have 2xdc2.large cluster = 2 node * 2 slices * 10 workers = 40 workers available. However when check my spectrum query summary in the SVL_S3QUERY_SUMMARY table I can see the maximum request parallelism is 10, even though the query would clearly benefit from a higher degree of parallelism (I am querying a bucket with thousands of small files), why isn't spectrum exhausting all available 40 workers in this situation?"
Amazon Redshift	"Re: Reshift Spectrum scale-out patterns
The information in the field is per slice.
max_request_parallelism	- The maximum number of outstanding requests at one node slice for this Redshift Spectrum request.
https://docs.aws.amazon.com/redshift/latest/dg/r_SVL_S3QUERY_SUMMARY.html"
Amazon Redshift	"failed to build any 6-way joins;
https://forums.aws.amazon.com/(500310) Invalid operation: failed to build any 6-way joins;

While joining couple of tables in Redshift spectrum i am getting error like above.. Are there any fixes for this or workaround suggested by AWS team ?

Thanks in advance."
Amazon Redshift	"Re: failed to build any 6-way joins;
Hello,
Can you provide the query that generated this error message ? 
Thanks"
Amazon Redshift	"Re: failed to build any 6-way joins;
Here are the details.
My data exist in S3. I created external schema on this data (not created external tables, as I have many tables in that schema). 

So, now I have two ways to query the data, one through Athena (by creating Glue catalog) an second one is through Redshift spectrum (through external tables).

When I query using Athena I am not getting any errors, but when I query from Redshift External Schema then only getting error. 

My external sChema name is : spect_rxs_dataingest_pbmsys_new.

Sql:

select distinct 
p.nabp
,a.Pcn
,p.NcpdpChainCode
, bd.planNumber
,case when PharmacyExcluded.IsExcluded = true then 'Yes' else '' end as PharmacyExcluded
from spect_rxs_dataingest_pbmsys_new.dr_pcn a 
Inner Join spect_rxs_dataingest_pbmsys_new.dr_group b 
on a.PcnId = b.PcnId and a.BinId=1
Inner Join  spect_rxs_dataingest_pbmsys_new.dr_EntityBenefitDesign ed  
on ed.entityid=b.GroupId and ed.EntityTypeId=4
Inner Join  spect_rxs_dataingest_pbmsys_new.dr_BenefitDesign  bd 
on bd.BenefitDesignId = ed.BenefitDesignId
Inner Join  spect_rxs_dataingest_pbmsys_new.dr_Provider p 
on 1=1
left join ( 
select distinct n.name, np.NetworkProviderId, np.NcpdpChainCode, np.Nabp, np.IsExcluded, np.StartDate, np.EndDate, bda.PlanNumber
        from spect_rxs_dataingest_pbmsys_new.dr_BenefitDesign bda 
        inner join spect_rxs_dataingest_pbmsys_new.dr_BenefitDesignNetworkPricingSet bdnps  on bda.BenefitDesignId = bdnps.BenefitDesignId
        inner join spect_rxs_dataingest_pbmsys_new.dr_NetworkPricingSet nps  on nps.NetworkPricingSetId = bdnps.NetworkPricingSetId
        inner join spect_rxs_dataingest_pbmsys_new.dr_Network n  on nps.NetworkId = n.NetworkId
        inner join spect_rxs_dataingest_pbmsys_new.dr_NetworkProvider np  on np.networkid = n.networkid
        where IsExcluded=true 
           ) as PharmacyExcluded on PharmacyExcluded.Nabp = p.Nabp
   and PharmacyExcluded.PlanNumber = bd.PlanNumber
   limit 10"
Amazon Redshift	"Re: failed to build any 6-way joins;
Try turning the subquery into a CTE using WITH. That worked on one of our queries."
Amazon Redshift	"Urgent: Redshift cluster in unhealthy state
Last night, our cluster went into an unhealthy state and rebooted itself, which took over an hour to complete. After rebooting, the cluster was still periodically refusing connections. This morning I attempted to reboot it to resolve the connection issues, but it is now stuck again in an unhealthy state and not finished rebooting yet.

I need assistance with this ASAP. It is unacceptable that our cluster can be unreachable for this amount of time with no word from Amazon about what might be going wrong."
Amazon Redshift	"Re: Urgent: Redshift cluster in unhealthy state
Sorry that you had this issue. Please note that we cannot provide quick responses via this forum. 

I can see a cluster attached to your account that was automatically flagged for investigation by our operators during both periods.

For time critical response to issues I strongly recommend raising them with AWS Support who can investigate issues 24/7 and provide further assistance.
https://console.aws.amazon.com/support/home#/"
Amazon Redshift	"Spectrum scalability
I want to understand the statement ""Redshift Spectrum automatically scales out to thousands of instances if needed, so queries run quickly, whether processing a terabyte, a petabyte or an exabyte.""

This means that the performance is still limited on how big the Redshift cluster is, correct?"
Amazon Redshift	"Re: Spectrum scalability
I also do not understand that statement. In my Spectrum queries I get every time the same parallelism factor (10) even though the query would clearly benefit from a higher degree of parallelism... I does not look like I can adjust this via a service configuration either...
Did you find out something?

Edited by: dcereijo on Feb 2, 2019 10:09 AM"
Amazon Redshift	"Re: Spectrum scalability
Spectrum uses up to 10 nodes per Redshift cluster slice.

You cannot configure the number in use, or the maximum number."
Amazon Redshift	"Creating cluster redshift
I want to create a cluster of redshift nodes in one private subnet /28. But I'd like to know if there is a minimum CDIR length and if AWS allocate IP addresses internally.

Best regards,"
Amazon Redshift	"Unexpected behaviour with STDDEV_POP function
Hi, I'm having issues with the STDDEV_POP() function on Redshift - when running it as a window function, I'm getting nan for some of the rows, but this doesn't happen in Postgres 10 (not too sure about other versions). 

So I have this table:
create temp table table_test (
  cal_date date,
  checksave_bal float
);
 
INSERT INTO ""table_test""(""cal_date"",""checksave_bal"")
VALUES
('2018-01-13',492.130154252052),
('2018-01-14',NULL),
('2018-01-15',NULL),
('2018-01-16',NULL),
('2018-01-17',NULL),
('2018-01-18',NULL),
('2018-01-19',NULL),
('2018-01-20',NULL),
('2018-01-21',NULL),
('2018-01-22',NULL),
('2018-01-23',NULL),
('2018-01-24',NULL),
('2018-01-25',NULL),
('2018-01-26',NULL),
('2018-01-27',NULL),
('2018-01-28',NULL),
('2018-01-29',NULL),
('2018-01-30',NULL),
('2018-01-31',NULL),
('2018-02-01',3854.82998406887),
('2018-02-02',NULL),
('2018-02-03',NULL),
('2018-02-04',NULL),
('2018-02-05',NULL),
('2018-02-06',NULL),
('2018-02-07',NULL),
('2018-02-08',NULL),
('2018-02-09',NULL),
('2018-02-10',2995.87996828556);


And I want to run this query in order to calculate the rolling stddev_pop for each cal_date:
select
  cal_date
  , checksave_bal
  , stddev_pop(checksave_bal) over (order by cal_date rows between 14 preceding and 0 preceding)
from table_test
order by cal_date


Results:
""cal_date"",""checksave_bal"",""stddev_pop""
""2018-01-13"",492.130154252052,0
""2018-01-14"",,0
""2018-01-15"",,0
""2018-01-16"",,0
""2018-01-17"",,0
""2018-01-18"",,0
""2018-01-19"",,0
""2018-01-20"",,0
""2018-01-21"",,0
""2018-01-22"",,0
""2018-01-23"",,0
""2018-01-24"",,0
""2018-01-25"",,0
""2018-01-26"",,0
""2018-01-27"",,0
""2018-01-28"",,
""2018-01-29"",,
""2018-01-30"",,
""2018-01-31"",,
""2018-02-01"",3854.82998406887,0
""2018-02-02"",,0
""2018-02-03"",,0
""2018-02-04"",,0
""2018-02-05"",,0
""2018-02-06"",,0
""2018-02-07"",,0
""2018-02-08"",,0
""2018-02-09"",,0
""2018-02-10"",2995.87996828556,nan


The last row should have a stddev_pop value of ~429. After some testing, I'm only able to arrive at that value when I exclude the ""2018-01-13"" row from this query, but this just seems weird.

I think this is a bug, but if it isn't - I'd like to know how to use this properly. Would really appreciate it if someone could help out? Thanks!

Redshift Version
PostgreSQL 8.0.2 on i686-pc-linux-gnu, compiled by GCC gcc (GCC) 3.4.2 20041017 (Red Hat 3.4.2-6.fc3), Redshift 1.0.5671"
Amazon Redshift	"listagg functions with other distinct aggregate function not supported
Hi,

currently it seems this kind of aggregation is not possible :

select
vendor_id,
count(DISTINCT sale_id) AS sales_count,
listagg(DISTINCT product_name, ', ') as product_names
from orders
group by vendor_id

throws ""Using LISTAGG/PERCENTILE_CONT/MEDIAN aggregate functions with other distinct aggregate function not supported""

Does someone know when it will be available ?
I can't manage to find a trick that's not 50x times longer to execute or 50x times uglier (like doing a separate query for the listagg() in a WITH bloc before joining them)

Thanks a lot for any tip !

Félix

Edited by: felix-lrqdo on Jul 18, 2018 3:05 AM"
Amazon Redshift	"Re: listagg functions with other distinct aggregate function not supported
I encountered this same issue and figured out a reasonably simple workaround! It's super hacky and feels very silly, but it works.

You can use another invocation of listagg, strip out the actual results, and count the number of separators in the resulting string (plus one) to get a count of the original distinct values. If the value being counted might be null, you can coalesce the whole thing to zero so you get 0 instead of null if the count is supposed to be zero.

For example, in your case you could do the following:

select
vendor_id,
coalesce(len(regexp_replace(listagg(DISTINCT sale_id, ','), '[^,]', '')) + 1, 0) AS sales_count,
listagg(DISTINCT product_name, ', ') as product_names
from orders
group by vendor_id


As long as the maximum character length of sale_id times the max number of sales in any group is less than 65535 (the max number of characters permitted in a listagg result), then this should work.

Edited by: vergenzt on Jan 30, 2019 9:42 AM: Forgot the ""+ 1"" in sales_count!

Edited by: vergenzt on Jan 30, 2019 9:44 AM: Added note about coalescing to zero."
Amazon Redshift	"Re: listagg functions with other distinct aggregate function not supported
You can create your own listagg UDF in Python."
Amazon Redshift	"Redshift rejecting ARN of IAM roles that have a path
I'm finding that you cannot attach an IAM role to a Redshift cluster if that role is associated with a path (other than '/').  Here's steps to recreate:

1. Create a role policy file:

cat <<EOM > /tmp/assume-role-policy.json
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""Service"": [
          ""redshift.amazonaws.com""
        ]
      },
      ""Action"": [
        ""sts:AssumeRole""
      ]
    }
  ]
}
EOM


2: Create an IAM role using the default path of '/', save its ARN:

role1_arn=$(
  aws iam create-role \
    --role-name test_role_1 \
    --assume-role-policy-document file:///tmp/assume-role-policy.json |
  jq -r '.Role.Arn'
)


3: Create an IAM role using a custom path, save its ARN:

role2_arn=$(
  aws iam create-role \
    --role-name test_role_2 \
    --path '/mypath/' \
    --assume-role-policy-document file:///tmp/assume-role-policy.json |
  jq -r '.Role.Arn'
)


4. Try launching the cluster with the role that has a custom path:

aws redshift create-cluster \
  --cluster-identifier my-test-cluster \
  --cluster-type single-node \
  --node-type dc1.large \
  --master-username test \
  --master-user-password Test0123 \
  --iam-roles ""$role2_arn""


I get this error:

An error occurred (InvalidParameterValue) when calling the CreateCluster operation: Invalid IAM Role ARN format: arn:aws:iam::[REDACTED]:role/mypath/test_role_2


5. Try again, but use the role that uses the default path:

aws redshift create-cluster \
  --cluster-identifier my-test-cluster \
  --cluster-type single-node \
  --node-type dc1.large \
  --master-username test \
  --master-user-password Test0123 \
  --iam-roles ""$role1_arn""


This works fine.

Looks like there's a bug with the validation Redshift performs on IAM ARNs.  I've also replicated the same issue when modifying an existing cluster, using 'aws modify-cluster-iam-roles'.

Using aws-cli/1.10.41 - but I've also replicated the issue and seen the same behavior in a node.js script using the JavaScript SDK."
Amazon Redshift	"Re: Redshift rejecting ARN of IAM roles that have a path
I ran into this same issue and opened an AWS support ticket. This was the response I received:

I would like to inform you that this is a known bug and this issue has already been raised to the internal team, It would be fixed in one of the future releases which would be announced on the forum : https://forums.aws.amazon.com/forum.jspa?forumID=155 . 

Meanwhile, the workaround is to create the role without PATH , that should work normal until it’s incorporated in future release.

Edited by: DerekG on Jan 30, 2019 11:39 AM

Edited by: DerekG on Jan 30, 2019 11:39 AM"
Amazon Redshift	"Quote escaped quotes in external tables
I'm trying to create an external table in Redshift from a csv that has quote escaped quotes in it.

I get no errors but the final table has a null value where my string should be.

Any ideas how to solve this (that don't involve manipulating the csv file)?

Example csv & external table creation below.

rfc4180 CSV documentation https://tools.ietf.org/html/rfc4180:
If double-quotes are used to enclose fields, then a double-quote
appearing inside a field must be escaped by preceding it with
another double quote.  For example:
 
""aaa"",""b""""bb"",""ccc""


Example csv:
""some """"text"""""",some more text
""more, text"",and more


First field appears as null in the table when it should be
some ""text""


All other fields display as expected.

Create external table:
create external table spectrum2.spectrum_test_quote(
  a varchar(32),
  b varchar(32)
)
row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
with serdeproperties (
  'separatorChar' = ',',
  'quoteChar' = '\""',
  'escapeChar' = '\\'
)
stored as textfile
location 's3://';


Edited by: tommmm on Jan 30, 2019 9:32 AM"
Amazon Redshift	"COPY from Amazon S3 with prefix/bucket
If you put several new Amazon S3 objects into a bucket with a prefix and then immediately do a COPY and specify the prefix, will you be guaranteed to get all of the objects that you have previously put with that prefix?

This seems to be unclear. Does the COPY command simply do a dir listing, which would be eventual consistency and not necessarily load all of the new objects since they're referenced by prefix and not KEY, or does it do some kind of magic in S3 that only Amazon could do?

AFAIK, the only way to make sure you get all of the files is to build a manifest file if you want to load more than one file and be sure that all files are loaded, but I have heard others loading by prefix without any issue. I don't think that this is safe, but perhaps I am incorrect.

PUT /ObjectName1 HTTP/1.1
Host: BucketName.s3.amazonaws.com
...

PUT /ObjectName2 HTTP/1.1
Host: BucketName.s3.amazonaws.com
...

COPY table-name FROM 's3://BucketName/ObjectName' ...

Is a PUT and a COPY of an object by prefix considered a read after write consistency use case?

Thanks!"
Amazon Redshift	"Re: COPY from Amazon S3 with prefix/bucket
Hi bigdatamark,

I believe you are correct.  Eventual consistency of list after put is in play, and that the behavior depends on the time between the put of the data to S3 by some external process and the later list that Redshift COPY command does.  Also, AWS clearly documents that S3 eventual  consistency varies across region with some regions being more consistent that other and that within a region consistency can vary over time.  

Checks against the stl_load* tables in the Redshift catalog with or without a manifest is the acid test of whether or not the data you expected got loaded into Redshift.  The mainfest just sets a more concrete expectation in the COPY statement itself, that is assuming no other external process is modifying the manifest file after the external process the originally wrote it to S3.

I hope this helps,
-Kurt"
Amazon Redshift	"Re: COPY from Amazon S3 with prefix/bucket
Thanks Kurt. I'm going to assume that we're both correct until an official answer says otherwise."
Amazon Redshift	"Re: COPY from Amazon S3 with prefix/bucket
One other think that comes to mind that may or may not help.

If you happen to be doing this from a node of an EMR cluster, using EMRFS, on that cluster can help a lot by eliminating eventual consistency of list after put regardless of what region you're in.

-Kurt"
Amazon Redshift	"Automating the removal of a user across multiple clusters.
Hello, would you have suggestions on removing a user with ownership of multiple objects in multiple clusters?"
Amazon Redshift	"Re: Automating the removal of a user across multiple clusters.
I'd implement this with a shell script that uses psql to run a query to find all owned DB objects that builds another script that transfers ownership of all owned objects to another user and deletes the user.  I'd then wrap that in a script that either loops over an input list of clusters or finds the list of clusters from a CLI command.

Now that I think of it parts of this are already in the AWSLabs repos in GitHub.  Have a look there first a solution or at least parts of a solution.

-Kurt"
Amazon Redshift	"Unload with timestamp filter
I was attempting to unload all data prior to 2017 from a database and hit what seems to be a bug...

my query was: 

unload ('select * from traffic where timestamp < \'2017-01-01\' order by timestamp asc')
to 's3://xxx/export/unload/2019-01-22-3/' 
iam_role 'arn:aws:iam::xxx:role/xxx'
delimiter as ','
addquotes
escape
manifest;

what I ended up with was only data prior to 2015, which I assume is a result of the date being evaluated as subtraction and looking at the query in the inspector:

unload ('select * from traffic where timestamp < 2017-01-01') to 's3://xxx/export/unload/2019-01-22/' iam_role '' delimiter as ',' addquotes escape allowoverwrite manifest"
Amazon Redshift	"Re: Unload with timestamp filter
which I assume is a result of the date being evaluated as subtraction

(lols!!  I shouldn't laugh, but that is funny =-)

Maybe try using DATE_CMP() and seeing if the literal in the function is interprented correctly?

I would try it myself but I do not have access to a cluster."
Amazon Redshift	"Re: Unload with timestamp filter
It's likely that the backslash escape is being stripped by something that handles the query text before the query gets to Redshift.  

Try doubling up the quotes instead as that also works.
unload ('select * from traffic where timestamp < ''2017-01-01'' order by timestamp asc')…"
Amazon Redshift	"Re: Unload with timestamp filter
One general approach to avoid bugs or limitations in the select clause of the unload statement is to define a view where the view definition is what you want to unload, then refer to the view in the unload statement, e.g.:

create view #vname as
  select * from traffic where timestamp < 2017-01-01 order by timestamp asc
;

unload (select * from #vname) to ..
;

Edited by: karbjonn on Jan 29, 2019 12:38 AM
Removed the quotes around the date criteria, and the unload select clause, as on posting the original message AWS gets confused and do not display the statements correctly. Of course, in real life, quotes must be added."
Amazon Redshift	"Spectrum timestamp and int value in json file
Hi,

I have an issue with integer timestamps. Sample JSON looks like the following:

{""name"": ""foo"", time: 1540338652}

I store the schema on Glue, setting the time attribute to type ""timestamp"".
It works perfectly with Athena, but I get an error 15001 with Spectrum. I know that int64 and int96 are supported as timestamps for Spectrum in parquet format. Do you think Spectrum will support JSON int properties as timestamps soon? Is there a workaround?

Here is a sample error:

https://forums.aws.amazon.com/https://forums.aws.amazon.com/ https://forums.aws.amazon.com/(500310) Invalid operation: S3 Query Exception (Fetch)
Details:
  error:  S3 Query Exception (Fetch)
  code:      15001
  context:   Task failed due to an internal error. In file https://s3.amazonaws.com/.../file.json.gz declared column type TIMESTAMP for column INT is incompatible for path ('time'...."
Amazon Redshift	"Re: Spectrum timestamp and int value in json file
Hello, 
We understand your concern and it is on our roadmap. Meantime can you try using a conversion like below.  

# select timestamp 'epoch' + 1540338652 * interval '1 second' as tcol;
        tcol
---------------------
 2018-10-23 23:50:52
(1 row)


Thanks."
Amazon Redshift	"Re: Spectrum timestamp and int value in json file
Thanks for the quick response. Please keep us posted when the fix/feature is rolled out."
Amazon Redshift	"Re: Spectrum timestamp and int value in json file
I was wondering if there is an update on this issue?"
Amazon Redshift	"Enforcing Timeouts on queries vs COPY
What is the best way to enforce a timeout on a COPY command submitted through a JDBC driver? It seems like the setQueryTimeout() method will simply cause the client to stop listening for a response but the cluster is still executing the COPY.  Correct?  

On the other hand statement_timeout sounds like it would be enforced by the Redshift cluster itself, but does this include the time spent sitting in a queue?  If not, we'd need to find some way to enforce this timeout from the client side, and somehow cancel the executing process/queue when it expires.
http://docs.aws.amazon.com/redshift/latest/dg/r_statement_timeout.html

Is there any recommended practice around this??

Thanks"
Amazon Redshift	"Re: Enforcing Timeouts on queries vs COPY
Hello,

Did you find the answer to your question? I am specially worried about if statement_timeout or max_execution_time include the time sitting in the queue.

Thanks!"
Amazon Redshift	"Re: Enforcing Timeouts on queries vs COPY
1+ 

Hey Redshift is there an answer on whether or not the ""statement_timeout"" parameter includes the time a statement spends waiting in a queue for a slot to execute or does it only include the actual execution time once the statement is dispatched to a slot to run?"
Amazon Redshift	"Re: Enforcing Timeouts on queries vs COPY
You can change the `statement_timeout` setting with a query inside your connection to limit execution time. Note that the value is in milliseconds. https://docs.aws.amazon.com/redshift/latest/dg/r_statement_timeout.html

SET statement_timeout to 100; -- 0.1 sec
	-- SET
SELECT * FROM svv_table_info;
	-- ERROR:  Query (939819) cancelled on user's request
	-- Time: 225.950 ms
SET statement_timeout to 1000; -- 1 sec
	-- SET
SELECT * FROM svv_table_info;
	-- ERROR:  Query (939821) cancelled on user's request
	-- Time: 1123.200 ms (00:01.123)
SET statement_timeout to 10000; -- 10 secs
	-- SET
SELECT * FROM svv_table_info;
	-- ERROR:  Query (939822) cancelled on user's request
	-- Time: 10649.682 ms (00:10.650)
SET statement_timeout to 100000; -- 100 secs
	-- SET
SELECT * FROM svv_table_info;
	--   database  |     schema     | table_id | …
	-- ------------+----------------+----------+-…
	--  my_db      | my_schema      |   364365 | catalog_sales …
	-- {etc}
	-- Time: 50206.015 ms (00:50.206)"
Amazon Redshift	"Re: Enforcing Timeouts on queries vs COPY
Hi Joe,

Thanks for the reply.  Although it wasn't in this thread I was aware that the ""statement_timeout"" parameter can be dynamically set at the session level.

That leaves 2 remaining questions in this thread:

1. Does the ""statement_timeout"" parameter apply to COPY commands?  There's some ambiguity in the doc.  The ""statement_timeout"" indicates there's some level of equivalence between the ""statement_timeout"" and the WLM queue ""timeout"" documented here: 

https://docs.aws.amazon.com/redshift/latest/dg/cm-c-defining-query-queues.html?shortFooter=true#wlm-timeout

especially when their values are directly compared and the lower value applied.  However, the WLM queue ""timeout"" doc. specifically says it does not apply to COPY commands.  

2. Does the ""statement_timeout"" parameter include WLM queue waittime or put another way when does the ""statement_timeout"" timer start and end?  The  WLM queue ""timeout"" doc explicitly says it only includes statement execution time and does not include WLM queue wait time, nor does it include any execution time after the statement has reached ""returning"" state, i.e. once the statement has started returning a result to the SQL client.  Do they behave the same and preserve the equivalence of the 2 values?  Is this just a case the WLM queue ""timeout"" just getting documented a little better?

Clear understanding of timeout behavior will go a long way toward being able to predict Redshift behavior and code directly to it.

Thanks for helping put this one to bed! 
-Kurt"
Amazon Redshift	"Re: Enforcing Timeouts on queries vs COPY
Kurt,

Statement timeout does include queue time, WLM timeout does not.  Statement timeout does apply to COPY, VACUUM, and ANALYZE queries. 

If your COPY triggers secondary activities, for example STATUPDATE ON = ANALYZE, the secondary activity could be cancelled even though the COPY succeeded. 

Since VACUUMs are composed of steps, a cancelled VACUUM will not undo the steps already completed. 

Queries in the returning state are not cancelled by either of these timeouts because they are considered complete. However the QMR setting `query_execution_time` will force even returning queries to terminate - use with care."
Amazon Redshift	"Re: Enforcing Timeouts on queries vs COPY
Hi Joe,

Thanks for your crystal clear answer and explanation!!!

I think a lot of customers would benefit if the details you provided found their way into the Redshift doc.

Many thanks,
-Kurt"
Amazon Redshift	"New Feature: ANALYZE now runs automatically to update table statistics
Analyze operations now run automatically on your Amazon Redshift tables in the background to deliver improved query performance and optimal use of system resources.

Amazon Redshift's sophisticated query planner uses a table's statistical metadata to choose the optimal query execution plan for better query performance. The analyze operation generates or updates the table statistics. With this update, you no longer need to explicitly run the ANALYZE command. If you do run it as part of your extract, transform, and load (ETL) workflow, automatic analyze skips tables with up-to-date statistics. Similarly, an explicit ANALYZE skips tables with up-to-date table statistics.

For more information, see Automatic Analyze in the Amazon Redshift Database Developer Guide.
https://docs.aws.amazon.com/redshift/latest/dg/t_Analyzing_tables.html#t_Analyzing_tables-auto-analyze

Automatic Analyze is now available with the release version 1.0.5671 or higher in all AWS commercial regions. Refer to the AWS Region Table for Amazon Redshift availability. https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/

https://aws.amazon.com/about-aws/whats-new/2019/01/amazon-redshift-auto-analyze/"
Amazon Redshift	"Re: New Feature: ANALYZE now runs automatically to update table statistics
Woot!"
Amazon Redshift	"Suggestion, COPY support for column mapping based on Parquet column names
Today the documentation for columnar COPY specifies that:
https://docs.aws.amazon.com/redshift/latest/dg/copy-usage_notes-copy-from-columnar.html
""COPY inserts values into the target table's columns in the same order as the columns occur in the columnar data files. The number of columns in the target table and the number of columns in the data file must match.""

This causes a lot of pain because it tightly couples the database schema with the process that produces the data to be ingested. Today, in best case scenarios I can change the order of columns in the Parquet files to carefully match what Redshift requires, but worst-case I don't control the data creating process, and is forced into a clumsy solution with temporary staging tables in order to map columns.

Ideally I would like Redshift to present an option to map columns based on table column and Parquet column names, so the COPY command is invariant to the order of columns in the Parquet files.

Is support for column mapping in general, and specifically based on matching names of columns between Parquet files and table schema on AWS's roadmap?

Thanks,
Simon Ejsing"
Amazon Redshift	"Re: Suggestion, COPY support for column mapping based on Parquet column names
I have vaguish memories of seeing people writing about this for some time now - at least a year.

I think it's true for other column-store formats too, like AVRO.

(I'd need to verify both those claims to see if they're actually true, and it's bedtime, so maybe tomorrow).

I would guess though given the Parquet and AVRO support have been in for some time (two years or more?) it's not on the A list."
Amazon Redshift	"Re: Suggestion, COPY support for column mapping based on Parquet column names
+1 for this

Ought to be easy, given the existence of ""json 'auto'"" and the fact that Spectrum supports out-of-order Parquet columns."
Amazon Redshift	"Re: Suggestion, COPY support for column mapping based on Parquet column names
+1  This would only make more Parquet files, created by other tools for reasons other than loading into Redshift with COPY commands, loadable into Redshift.  Because sufficient metadata exists in both places a simple remap is all that's needed. 

Also, as an additional big data stack compatibility issue, it would be good if Redshift could support the same data process semantics as all the other big data stack tools for things like, no value means NULL where nullability is allowed.

This would make even more existing Parquet file compatible with Redshift.

Lastly, please do the same with ORC and Parquet.  WRT these asks there is no difference in available metadata in the 2 file formats.

-Kurt"
Amazon Redshift	"Re: Suggestion, COPY support for column mapping based on Parquet column names
Thank you for for this feature suggestion. We've made a note of this and will consider adding to our roadmap. 

When new features are released they are noted in our regular maintenance announcements at the top of the forum.

In the meantime, consider defining the files to be loaded as an external table and then using a query to manipulate the columns as required:
INSERT INTO target_tbl SELECT {{ data to be loaded }} FROM external_tbl;"
Amazon Redshift	"Re: Suggestion, COPY support for column mapping based on Parquet column names
Hi Joe,

Thanks for placing the ask under feature consideration and for the workaround using an external table.

As a matter of performance I wondering if you can comment on whether a CTAS like:

CREATE <temp_internal_tbl> AS SELECT ... FROM <external_tbl>

from the external table into an internal temp table in Redshift storage followed by an append operation like:

ALTER TABLE <perm_internal_tbl> APPEND FROM <temp_internal_tbl> 

statement sequence would tend to be faster than a row wise insert like:

 INSERT INTO <internal_tbl> SELECT ... FROM <external_tbl>

Thanks,
-Kurt"
Amazon Redshift	"Feature Request: Extended GROUP BY clause
When could we see support in GROUP BY ROLLUP / CUBE / GROUPING SETS / GROUPING / GROUPING_ID ?"
Amazon Redshift	"Re: Feature Request: Extended GROUP BY clause
Thank you for for this feature suggestion. We have definitely heard this request and it is under consideration for our roadmap. 

We do not comment on the timing of new features until they are announced but new feature releases are noted in our regular maintenance announcements at the top of the forum."
