{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv('Amazon S3.tsv', delimiter='\\t', encoding='utf-8')\n",
    "\n",
    "# punctuation = \"!\\\"#$%&()*+,-.:;<=>?@[\\]^_`{|}~\"\n",
    "\n",
    "cleantxt = []\n",
    "\n",
    "for i in text.description:\n",
    "    i = i.lower()\n",
    "    i = ' '.join(i.split())\n",
    "    # i = i.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "    cleantxt.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"s3 public access can not be set. i've edited the bucket policy, public access settings, but the bucket is not changed to public and access is denied. why do you see these symptoms? https://imgur.com/rv64vwd (edit public access settings) https://imgur.com/y3ppaqj (bucket policy)\", \"i can't figure out why s3 won't display my website on my domain error 403 and i am not sure how to troubleshoot the error or resolve it despite reading guides i've looked at a couple of forums and pages but they are either irrelevant or beyond my current understanding. i cannot figure out why i keep getting a 403. i have a public bucket policy which changed my previous error of not getting a connection to the site to a 403, just forbidding traffic. i am new to aws, what am i missing? i only have one html file in the bucket and when i hit 'make public' it said access denied. are there other objects that i have to find? i was able to get into the html file permissions and when i selected 'public access' 'read object' it said access denied. for whatever reason, when accessing the domain i purchased, it says 403. when i go to the amazon s3 version of my website it works.. huh. why doesn't it come up on my domain?\", \"re: i can't figure out why s3 won't display my website on my domain test, ignore\", \"my aws s3 account is suspended without any notification today when i was ready to some work on my website i found all the components which are stored on amazon s3 are missing. to check the issue i tried to log in to s3 console and found out that my account is suspended. i didn't find any email regarding the suspension and i have no payment dues in my billing options. i have also opened a support case with case id: 5866475801 and mailed the issue but still no reply. what the hell is happening ? please atleast tell and help me to resolve the issue so that i can work further.\", 'unable to read more than 1000 objects from a bucket i am using the rest apis (not through sdk) to read the list of objects from a bucket. the bucket has more then 1000 objects. as per https://docs.aws.amazon.com/amazons3/latest/api/restbucketget.html documentation, i am extracting the \"istruncated\" and \"nextmarker\" values from the response and if \"istruncated\" is true, i pass the value of \"nextmarker\" as \"marker\" parameter in the subsequent call. whenever, i add the \"marker\" to the query parameter i get \"signaturedoesnotmatch\" error. i specify other parameters like \"max-keys\", \"prefix\", \"delimiter\" and they does cause any error and the call goes through fine. of course, i did uri encode the parameters. its only the \"marker\" parameter that is causing the error. i also tried to use the v2 version, https://docs.aws.amazon.com/amazons3/latest/api/v2-restbucketget.html . i get the same \"signaturedoesnotmatch\" error when using the \"continuation-token\" in the subsequent call. the \"list-type\" and \"max-keys\" parameters were ok. any help is appreciated.', 'adding expires or cache-control header for folders in s3 the help page here is as always quite useless: https://docs.aws.amazon.com/amazoncloudfront/latest/developerguide/expiration.html#expiration-individual-objects it says that in a folder, i\\'d see \"properties\" and then \"metadata\". none of my folders in s3 have this \"metadata\". inside properties, there are cards like versioning, server logging, etc. where should i enter the cache-control setting? this is for cloudfront, which seems to be not getting this header from s3. i could set up this header in cloudfront, but that ui isn\\'t any more intuitive. thanks for any guidance.', 're: adding expires or cache-control header for folders in s3 found it in the \"change metadata\" for folders and files. the help should be updated, the documentation remains fairly hideous. found the solution on stack overflow. thanks.', 'where do i get the canonical user id hi, i am setting up a new bucket and need to assign specific iam users to the bucket. it is asking for the canonical user id. how do i get the canonical user id for all my iam users? this is so confusing. thanks', 're: where do i get the canonical user id hello nicolas_briant, thank you for your post, i trust you are well. regarding the canonical user id, you can find it in two ways: 1) logging in as root. in the top right of the console, choose your account name or number. then choose my security credentials. you will see the canonical user id in the account identifiers section. finding your account canonical user id: http://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#findingcanonicalid 2) by using a listbuckets api call. when you perform a get operation on the s3 service to get a listing of all the buckets you own, the response contains an owner id element which is your canonical user id. for example: aws s3api list-buckets --output text list-buckets: http://docs.aws.amazon.com/cli/latest/reference/s3api/list-buckets.html best regards. jayd j.', 're: where do i get the canonical user id thanks jayd. i was missing \"logging in as root\". regards, nicolas', \"re: where do i get the canonical user id i wanted to post the following, but i wasn't allowed because my forum account had just been created in the past hour. (some kind of spam prevention?) i'm pretty new to using aws. i can't find my canonical id and i don't understand your instructions for looking up my id. i have access to the aws console via my personal account. isn't there a way to see my own canonical id via the web interface, without using a cli? i've created an s3 bucket under my organization's account. i want to grant myself and another user access to the bucket, but i need our two canonical ids to do so. i'd appreciate step-by-step instructions for getting the ids. assume i know nothing about aws. thanks in advance. while i'd still like to have explicit step-by-step instructions for finding canonical ids, the delay in forum posting inspired me to try a different approach. or rather, the same approach over again. that is, when i was creating the s3 bucket, i had entered my email address as one to be granted access to it. the bucket was created, but i received an error message that i couldn't be granted access by email address for some reason. (it was unclear as to why.) for some reason, i decided to try using my email address again. this time it worked and my canonical id was automatically substituted! i entered the email address of the other person that i wanted to give access to the bucket, and his canonical id was filled in as well. now we both have access. however... when we view the permissions of the s3 bucket, we only see canonical ids. there's no indication to whom each of the ids refer! that's not a useful ui! it really looks lazy. so, how can i tell which canonical id goes with each person?\", 're: where do i get the canonical user id hello, how can i find another user canonical id? i created new iam user and want him to have access to s# bucket. thank you!', 're: where do i get the canonical user id every iam user will not have their own canonical id. there will be only one canonical id per aws account. the following link has step-by-step instructions on how to find the canonical id associated with your aws account --> https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#findingcanonicalid using obejct/bucket acls, you can grant/restrict access to an object/bucket to your aws account or to another aws account. you cannot restrict access to bucket/object to an iam user using object/bucket acl. you will have to use bucket policy if you want to provide/restrict access to bucket/object to an iam user in your account then you will have to use s3 bucket policy instead of acl. bucket policy examples --> https://docs.aws.amazon.com/amazons3/latest/dev/example-bucket-policies.html', 'bug in permission evaluation? to reproduce: create a role: testrole create a bucket, we\\'ll call it testbucket here create a user: testuser add permissions to testuser to sts:assumerole to testrole, and to the trust relationship of testrole to allow the user to assume it add an inline policy to testrole to allow actions s3:* to testbucket and testbucket/* add an inline policy to testuser to allow actions s3:* to testbucket and testbucket/* upload a file to the bucket (via ui is fine) create a bucket policy on testbucket: { \"version\": \"2012-10-17\", \"statement\": [ { \"sid\": \"deny-others\", \"effect\": \"deny\", \"notprincipal\": { \"aws\": [ \"arn:aws:iam::<acct id>:role/service-role/testrole\", \"arn:aws:iam::<acct id>:root\", \"arn:aws:iam::<acct id>:user/testuser\", ] }, \"notaction\": [ \"s3:putobject\", \"s3:putobjectacl\" ], \"resource\": \"arn:aws:s3:::<bucket name eg testbucket>/*\" } ] } the bucket policy is taken from an anonymous upload use case, but in essence its deny\\'ing access to everyone but root, our test role, and a test user to everything other than putobject and putobjectacl. so i\\'d expect that from testuser if i assumed the role testrole, that i\\'d be able to, for example, call s3:getobject on the file i uploaded. but this gets access denied. meanwhile if i do the same thing with testuser, it is allowed, despite both of these having the exact same set of policies. please help! thanks. edited by: davidericksonfn on mar 6, 2019 7:27 pm', \"accidentally expired entire bucket contents so yesterday one of the files in our bucket wouldn't delete. i set a lifecycle rule to expire the object, at least i thought i did. what i actually did was name the rule the object i wanted to expire and set the scope to the entire bucket. is there any way to recover the contents? edited by: _bd on mar 6, 2019 11:46 am\", 'pentest of aws cloud application - s3 hello, the new policy for penetrationtests permits the penetrationtest of a list of aws services, as seen on this site: https://aws.amazon.com/security/penetration-testing/ s3 is not in the list. since publicly accessible files on a s3 bucket are often a security relevant issue it should be part of a security assessment of a cloud infrastructure. i wonder if it is necessary to obtain a special permission of aws to issue s3-related requests in the context of a penetration test or is it generally forbidden for a pentester to examine s3 buckets belonging to a tested application? generally speaking: what am i allowed to do as a pentester performing a pentest in the aws cloud regarding the s3 bucket? kind regards michael', 'preview large files in s3 i have several thousand image files that are 1g in size each storaged in s3. i access them locally via storage gateway. paging through these image files locally is slow and cumbersome. i am looking for a way to allow me to preview these files without having to download them one at a time to view them. is there any kind of viewer available in aws that will allow me to view these files without downloading them locally?', 'unexplained error setting up policy for s3 cross-region replication hi, i\\'m following the instructions on this page for setting up the roles for crr: https://docs.aws.amazon.com/amazons3/latest/dev/setting-repl-config-perm-overview.html i have a role (we\\'ll call it replrole) with the exact trust policy listed on that page, and an access policy that looks like this: { \"version\": \"2012-10-17\", \"statement\": [ { \"effect\": \"allow\", \"action\": [ \"s3:getreplicationconfiguration\", \"s3:listbucket\", \"s3:getobjectversion\", \"s3:getobjectversionacl\", \"s3:getobjectversiontagging\", \"s3:replicateobject\", \"s3:replicatedelete\", \"s3:replicatetags\" ], \"resource\": [ \"arn:aws:s3:::our-s3-bucket-prefix-*\", \"arn:aws:s3:::our-s3-bucket-prefix-*/*\" ] } ] } it doesn\\'t look exactly like the policy provided, but the main difference is that i use wildcards for the resources specified. i also group the actions together. but if i understand iam access policies correctly, this should suffice for the purpose. s3 allows me to create the cross-region replication rule, but then i get this error message: the crr rule is saved, but it might not work. there was an error with setting up the iam policy for the selected iam role gobscrossregionreplicationrole. ensure that you have set up the correct policy, or select another role. what did i do wrong?', 're: unexplained error setting up policy for s3 cross-region replication what i found was that the cross-region replication was actually working, despite the error message. when i checked back the next morning, the objects were successfully replicated to the backup bucket.', \"can't export cloudwatch logs to s3 i'm trying to export a cloudwatch log group to s3 but i keep getting this error every time i click export data on the cloudwatch side: the acl permission for the selected bucket is not correct. the amazon s3 bucket must reside in the same region as the log data that you want to export. the cloudwatch log is in us-east-1. i created a bucket in each of the two us east regions: n. virginia and ohio but i still get this error when i try to export to either one of them. why?\", \"re: can't export cloudwatch logs to s3 please respond. this is very important and time sensitive.\", \"re: can't export cloudwatch logs to s3 i had the exact same problem. it turns out that i neglected to populate the 's3 bucket prefix' field under the 'advanced' section of the export dialog. edited by: lbrooks on nov 30, 2018 12:17 am\", \"re: can't export cloudwatch logs to s3 this unfortunately still doesn't work for me either. what exactly did you enter as the prefix? why can't we simple save the logs to any bucket that we own as a user aka admin?\", \"re: can't export cloudwatch logs to s3 did you ever solve this? how do i tell which region my cloudwatch logs are in? i am trying to do the same thing and am getting the same error.\", \"re: can't export cloudwatch logs to s3 i had the same problem. the solution i found was to select the bucket in the web portal, and along the top you then have a number of buttons: overview properties permissions management in permissions you can set the bucket policy to allow cloudwatch exports as detailed here: https://docs.aws.amazon.com/amazoncloudwatch/latest/logs/s3exporttasks.html hope this helps\", 're: can\\'t export cloudwatch logs to s3 it took me 30min to figure the error. their example policy is wrong the region in the example is the one you have to replace simply replace it by: \"service\": \"logs.us-east-1.amazonaws.com\" do it in both fields below. { \"version\": \"2012-10-17\", \"statement\": [ { \"action\": \"s3:getbucketacl\", \"effect\": \"allow\", \"resource\": \"arn:aws:s3:::my-exported-logs\", \"principal\": { \"service\": \"logs.us-east-1.amazonaws.com\" } }, { \"action\": \"s3:putobject\" , \"effect\": \"allow\", \"resource\": \"arn:aws:s3:::my-exported-logs/random-string/*\", \"condition\": { \"stringequals\": { \"s3:x-amz-acl\": \"bucket-owner-full-control\" } }, \"principal\": { \"service\": \"logs.us-east-1.amazonaws.com\" } } ] }', 'amazon s3 slow on large buckets hi, i got a big amazon s3 bucket around 8 gb and now things started to get slow. uploading files (with iam user, so the api) takes forever. now i recreated a new bucket with the same properties and settings and api uploading to this bucket is way faster. is there a connection between a large bucket and slow api uploading? do i have to make new buckets all the time? time to write an image of around 300 kb with api to large bucket: imageget: 1260ms imagewrite: 67683ms to a newly created bucket time: imageget: 1275ms imagewrite: 822ms on https://docs.aws.amazon.com/amazons3/latest/dev/bucketrestrictions.html it is stated: there is no limit to the number of objects that can be stored in a bucket and no difference in performance whether you use many buckets or just a few. you can store all of your objects in a single bucket, or you can organize them across several buckets. for me this does not seem the case, anyone? regards, dick goosen edited by: dickgoosen on feb 1, 2019 12:49 am edited by: dickgoosen on feb 1, 2019 1:22 am', 're: amazon s3 slow on large buckets hi there! i have the same issue. the small image takes 2 min by uploading to s3 from ec2.', 'unable to set the acl to public read properly on objects stored in bucket the bucket permissions are set to public, and when i upload image files from the angular web application, i include in the header of the put request \"x-amz-acl\": \"public-read\" as described by the aws documentation. when i attempt to view the images through the s3 link to the file however, a 403 forbidden error is returned. if i go into the bucket and examine the image, it shows there are no access permissions, and if i try to make the image public, i get an \"access denied\" message. what can i do to resolve this issue? i\\'ve attached some images to the post, the first one shows the headers included in the put request for uploading, the second shows the bucket policy, and the third image shows the current state of permissions for an image i uploaded.', 'password protect s3 mount using storage gateway, we\\'ve created an s3 mount, which we connect to using the recommended: \"mount -o nolock...\" command. i know that we can limit the policy so that only certain iam users have access to the buckets, and we can then mount the buckets using something like tntdrive and enter the iam credentials that way to access the buckets, but are there any alternatives to tntdrive that would allow us to do this? that is, when we set up a mount, is there any command like \"mount -o nolock -username=\"username\" password=\"password\" that would allow us to protect the mount?', '\\'access denied\\' when access s3 from angular app with cognito user pool i have s3 bucket which i configured to manage access using cognito user pool, as described here https://docs.amazonaws.cn/en_us/iam/latest/userguide/reference_policies_examples_s3_cognito-bucket.html: { \"version\": \"2012-10-17\", \"statement\": [ { \"effect\": \"allow\", \"principal\": \"*\", \"action\": \"s3:listbucket\", \"resource\": \"arn:aws:s3:::<bucket-name>\", \"condition\": { \"stringlike\": { \"s3:prefix\": \"cognito/<app-name>/\" } } }, { \"effect\": \"allow\", \"principal\": \"*\", \"action\": [ \"s3:putobject\", \"s3:getobject\", \"s3:deleteobject\" ], \"resource\": [ \"arn:aws:s3:::<bucket-name>/cognito/<app-name>/${cognito-identity.amazonaws.com:sub}*\", \"arn:aws:s3:::<bucket-name>/cognito/<app-name>/${cognito-identity.amazonaws.com:sub}/*\" ] } ] } i have angular web app which authenticate users with cognito user pool, and i\\'m using s3 client to get object. i see a call to cognito service (https://cognito-identity.eu-central-1.amazonaws.com/) is made successfully, and an identity is returned as a response, but the immediate call afterwards to s3 is failing with status code 403: <error><code>accessdenied</code><message>access denied</message><requestid>aac2b5fc5c74c971</requestid><hostid>l8aoygybut+y1qhtjhydrj9uxc97elsz+l6h2rqlnglpquzrqqpw532u6pixil7ypz4ugpreoss=</hostid></error> here\\'s my code setting aws creds: buildcognitocreds(idtokenjwt: string) { let url = \\'cognito-idp.\\' + cognitoutil._region.tolowercase() + \\'.amazonaws.com/\\' + cognitoutil._user_pool_id; if (environment.cognito_idp_endpoint) { url = environment.cognito_idp_endpoint + \\'/\\' + cognitoutil._user_pool_id; } let logins: cognitoidentity.loginsmap = {}; logins[url] = idtokenjwt; let params = { identitypoolid: cognitoutil._identity_pool_id, /* required */ logins: logins }; let serviceconfigs = <awsservice.serviceconfigurationoptions>{}; if (environment.cognito_identity_endpoint) { serviceconfigs.endpoint = environment.cognito_identity_endpoint; } let creds = new aws.cognitoidentitycredentials(params, serviceconfigs); this.setcognitocreds(creds); return creds; } what am i missing? no matter what i try, i\\'m getting access denied.', 's3 console shows error in access row also it can\\'t generate an url for \\'download as\\'. it shows \"an error occurred generating the download link for this object.\". this happens for all aws accounts we have and for different people.', \"re: s3 console shows error in access row hi, i understand that you are seeing error in the in the s3 console and 'download as' option is throwing error. this usually happens when you do not proper permissions to access the bucket or object. please ensure that you have all the necessary permissions. if you still see the same issue, please share the bucket name, object name and requester iam arn with me over private message to troubleshoot further. thanks,\", \"re: s3 console shows error in access row rgumber, i've sent requested information via pm several days ago.\", \"re: s3 console shows error in access row any news? i don't have any issue or restriction when i use aws cli. it only happens with aws console. edited by: deniskot on feb 28, 2019 2:28 am\", 'amazon s3 : \"error retrieving access type\", can\\'t use bucket hello, i have a problem concerning my s3 buckets. every bucket that i create (or that is created automatically by elastic beanstalk) show a \"error\" in the \"access\" column (short for \"error retrieving access type\"). i am unable to do anything with the console : i can\\'t delete the bucket, or download/upload any file. elastic beanstalk as well is unable to use s3 and it makes impossible the deployment of new version of any web application. using the cli, i can perform any operation i want. there is no iam on the account. i log directly with root access. do you have any idea what the problem can be ? thank you, edouard', \"can't delete empty s3 bucket (cf-templates) i can't delete a s3 bucket that • is empty • has no versioning • has no policy this s3 bucket has been created with some other service. when i attempt deleting the bucket in the aws console, i get an error with no message. and then i tried to delete via aws cli with below command, aws s3 rb s3://bucketname --force it doesn't work and shows error msg like this, fatal error: an error occurred (nosuchbucket) when calling the listobjects operation: the specified bucket does not exist remove_bucket failed: unable to delete all objects in the bucket, bucket will not be deleted. i searched on forum and tried to edit bucket policy, but it also shows like this, error data not found hope anyone could help me with this issue. thanks for and have a nice day!\", 'copy from source bucket to dest bucket - getobject() stream problem hi all, my name is eliran and i new in aws. my goal is to copy from one bucket (ireland) to another bucket (n.virginia) i will explain my work flow: 1) i use the cli command - aws s3 sync s3://sorce-bucket/ s3://dest-bucket/ --exclude \"logs/*\". the sync completed after some time... 2)in my app in .net i use the aws sdk and use the command getobject() like this: amazons3client s3client = new amazons3client(globals.awsaccesskey,globals.awssecretkey, regionendpoint.useast1); stream rs = s3client.getobject(new getobjectrequest { bucketname = sourcecontainer, key = key}).responsestream; its work fine but the responestream for some objects is md5stream (that what i need) and for some objects is cachingwarpperstream.... (its not good for me) if i use the source bucket from ireland so all the request with getobject on the same objects (like above) will return responestream md5stream! *the settings and policies is the same in both buckets. what goes wrong? and how i can get always md5stream from my new bucket in n.virginia. thanks a lot, eliran kasif.', 'stockholm s3 endpoint issue hello, i\\'m trying to connect to an s3 bucket in the newly available eu north 1 region (stockholm) through two mac s3 compatible apps (forklift and chronosync) without success. i\\'ve used \"s3.eu-north-1.amazonaws.com\" and \"s3-eu-north-1.amazonaws.com\" endpoints to no avail. i get the following error: the authorization header is malformed; the region \\'us-east-1\\' is wrong; expecting \\'eu-north-1\\' is anyone experiencing the same issue? thanks. regards.', \"re: stockholm s3 endpoint issue i answer to myself: one of the mentioned apps (forklift) has been updated recently and now it works with eu north 1 region s3 buckets. i suppose the same will happen soon with chronosync app. it's actually a matter of the specific app that needs to be updated to support new regions.\", 'issue reading s3 buckets (xml parsing error) when i access my list of buckets in the web browser, i get the following error in the console: xml parsing error: no root element found location: https://us-east-1.console.aws.amazon.com/s3/proxy line number 1, column 1: i believe this is coming from a few buckets i had deleted but are stuck in my account. if i try to delete them again, nothing happens visually, and in the console, this error is hit again. is this something that will resolve itself in time? or does something need to be done to resolve it? thanks!', \"strange issue granting unexpected permission to single object, how to fix? i have two objects in a bucket, and getobject should 403 for both for role a, except inexplicably one object is accessible. the bucket configuration is: blank bucket policy, all 4 options under public access settings are true, acl is defaults role config grants no permissions to s3 permissions for both objects appear identical (at least in console). iam policy simulator indicates both objects will be denied, but again, in reality one object is allowed. i'm performing the getobject from javascript aws sdk using federated identities & cognito pool. role a is associated with the logged in user's group. i've tried running cloudtrail but getobject is not being recorded when executed via aws sdk. it seems to log the event however when i manually download via console. please help! edited by: davegravy on feb 22, 2019 5:36 am\", 'does amazon s3 or glacier has build-in fixity checking? hi see https://dltj.org/article/oclc-digital-archive-vs-amazon-s3/ claimed that amazon s3 does not provide fixity check. see https://www.slideshare.net/amazonwebservices/deep-dive-on-archiving-and-compliance page 12 from aws presentation said that it has built-in fixity checking. i can\\'t find anything mention about fixity by simply searching \"fixity\" in https://docs.aws.amazon.com/s3/index.html#lang/en_us https://docs.aws.amazon.com/glacier/index.html#lang/en_us so, does amazon s3/glacier provides fixity checking or not?', \"need suggestions on how to look up existing objects on s3 hi, we have incoming files everyday, and we upload them to s3. the files can be duplicates from earlier days, so we need to check if they already exist before uploading. our old way is to save the filename to sdb after uploading a new file, so we can use sdb query to look up existing files. we recently want to change it to use s3 head object api to check existence. we need suggestions: (1) if there's better way beside sdb and s3 head api? any new s3 api to check existence in bulk? (3) is s3 head api good enough for our use case (we need look up ~200 filenames every hour during the day) thanks in advance!\", \"re: need suggestions on how to look up existing objects on s3 hi, i use s3 pretty extensively but i'm not employed by amazon so take this as it is. that amount of head requests per hour will be totally fine, it should not be anywhere close to stressing out the service. there isn't really a better way to check for existence of a random key. however, if you have a lot of keys at once and the keys share a path-like structure, you could do better by executing a list request on the common prefix and checking the contents. keep in mind that s3 may not be immediately consistent. you should read the docs to fully understand the impact to your particular use case, but a couple things stick out: 1) if you do a get/head prior to uploading, then put, then get/head -- that response will be eventually consistent i.e. not guaranteed to return that the object does exist 2) list requests are eventually consistent given that and that your expectations for number of objects to check, i would recommend keeping it simple and just doing head - maybe with time-based retries to clear up the eventual consistency issue. if you make the wrong decision, you simply re-upload a duplicate, which is not data loss but just extra cost to you, so if this happens every once in a while, shouldn't be too big of a deal. hope this helps!\", 're: need suggestions on how to look up existing objects on s3 thank you. i agreed to keep it simple is the right way to start. your suggestions are very much appreciated.', 'can\\'t delete object and its deletion marker from both cli and console. hi, while playing with s3 bucket that i own i uploaded some text files that are several kilobytes, and put a deletion marker afterwards. however, when i tried to remove the object version 1 (original file) and version 2 (deletion marker), the console just failed saying \"delete object: total objects:2, successful: 0(0%). i also tried cli command aws s3api delete-object --bucket storage.ik1ne --version-id nbyr0gs5mabrxhwhhgqqosojac0ocznia --key (filename).txt on both its version 1(upload) and 2(deletion marker), but it just outputs json output {\"versionid\": \"version_i_previously_specified\"}. i tried this with account that has administratoraccess but failed. also, all modification i did to bucket policy was public/private settings(i.e. did not specify deletion policy, etc). even the \"empty bucket\" command on console also fails. just in case, the filename was \"면접질문.txt\" and \"면접질문리스트.txt\", which is 2-bit character. (yes, actually i have two files with same symptoms). what am i missing? to make it sure, i uploaded another file to the same bucket and tried to reproduce the issue but that file works out exactly expected(both file and deletion marker gets deleted).', \"re: can't delete object and its deletion marker from both cli and console. hi, i see that your bucket is now empty. in case you still face the same issue, please share the bucket name and object name with me over private message and i will be able to assist you further. also, you can setup a lifecycle policy to expire all (or filter) the objects in your s3 bucket. https://docs.aws.amazon.com/amazons3/latest/dev/object-lifecycle-mgmt.html thanks,\", \"re: can't delete object and its deletion marker from both cli and console. i tried this on chrome and it worked, so i think it was just safari webkit and mac terminal encoding bug. thank you.\", \"unable to delete s3 event notification hi, i'm trying to delete an s3 event notification but i get this error: unable to validate the following destination configurations. not authorized to invoke function [arn:aws:lambda:eu-west-1:xxxxxxxx:function:mylambdafunction]. (arn:aws:lambda:eu-west-1:xxxxxxxx:function:mylambdafunction, null) i was using this s3 event as a trigger for a lambda function that i have since deleted. i thought that might be the issue and recreated the lambda function (with the same name), but this did not solve the issue. thank you.\", \"re: unable to delete s3 event notification hi, i understand that you are not able to delete s3 event notification and you are getting the above mentioned error. from the error message, it looks like you have another event rule with the destination as 'arn:aws:lambda:eu-west-1:xxxxxxxx:function:mylambdafunction' and your s3 bucket does not have the permission to invoke that lambda function. i would suggest you to please verify the same and if the issue persists, please share the bucket name and complete error message with me over private message and i would be able to troubleshoot further. thanks,\", \"s3 bucket with versioning: lifecycle expiration rules not applied we have an s3 bucket with replication and versioning enabled, about 60k. a rule should delete previous versions older than 3 days, but i can still see weeks old versions on this source bucket. the same rule applied to the destination bucket works fine. the rule has been applied weeks ago to both buckets. is there anything obvious i might have missed related to replication, versioning etc? see attached configuration for expiration. no path set, entire bucket selected. thanks, luigi edited by: lclemente on jan 29, 2019 12:42 am after i posted this message the bucket lifecycle worked. i don't know if aws fixed it.\", 're: s3 bucket with versioning: lifecycle expiration rules not applied hi luigi, i understand that lifecycle rule was not executed for your bucket but it worked now. please note that when an object reaches the end of its lifetime, amazon s3 queues it for removal and removes it asynchronously. there may be a delay between the expiration date and the date at which amazon s3 removes an object. you are not charged for storage time associated with an object that has expired. to find when an object is scheduled to expire, use the head object https://docs.aws.amazon.com/amazons3/latest/api/restobjecthead.html or the get object api https://docs.aws.amazon.com/amazons3/latest/api/restobjectget.html operations. these api operations return response headers that provide this information. please refer to the following document for more details: https://docs.aws.amazon.com/amazons3/latest/dev/lifecycle-expire-general-considerations.html thanks,', 's3 presigned post url fails with `405 client error: method not allowed for` i\\'ve been struggling all day to try and get the \"presigned post url\" feature working with s3 but keep running into errors. i have no problem making the request, but it seems that the response from the api is pointing at a domain that doesn\\'t allow posts.', 're: s3 presigned post url fails with `405 client error: method not allowed for` hi, i would like to inform you that you cannot make post request to object endpoints eg. (https://bucketname.s3.amazonaws.com/object.ext). in order to make post request, you will need to make post request to bucket endpoint (eg. https://bucketname.s3.amazonaws.com/) and specify the object key name and other properties in the multipart/form-data encoded message body. please refer to the following document for more details: https://docs.aws.amazon.com/amazons3/latest/api/restobjectpost.html please refer to the following document for an example on browser-based upload using http post: https://docs.aws.amazon.com/amazons3/latest/api/sigv4-post-example.html thanks,', 'glacier to physical medium does amazon offer any service that would transfer some of our glacier content to a physical, encrypted medium such as tape and ship it to us? or are there any third party companies that might?', 'aws lambda write image to s3 access denied i’m trying to get a deeplens lambda function to upload an image to s3: response = s3.put_object(acl=\\'public-read\\', body=jpg_data.tostring(),bucket=‘my-bucket-name’,key=file_name) however, i keep getting the error: error in face detection lambda: an error occurred (accessdenied) when calling the putobject operation: access denied i made an iam role and attached it to the deeplens lambda function and attached the following policies: awsdeeplenslambdafunctionaccesspolicy, awslambdaexecute, awsdeeplensservicerolepolicy, amazons3fullaccess, and a custom policy with the following json: { \"version\": \"2012-10-17\", \"statement\": [ { \"effect\": \"allow\", \"action\": [ \"s3:putobject\", \"s3:putobjectacl\" ], \"resource\": [ \"arn:aws:s3:::my-bucket-name”, \"arn:aws:s3:::my-bucket-name/*\" ] } ] } i even gave the bucket public access through the access control list and made the bucket policy public: { \"version\": \"2012-10-17\", \"id\": \"policy1534108093104\", \"statement\": [ { \"sid\": \"stmt1534108083533\", \"effect\": \"allow\", \"principal\": \"*\", \"action\": \"s3:*\", \"resource\": \"arn:aws:s3:::my-bucket-name” } ] } but i’m still getting the accessdenied error.', 're: aws lambda write image to s3 access denied did you ever get this to work? i am having the same issue. i got my function to write to the s3 bucket by change the public policy of \"block new public acls and uploading public objects\" to false but this is not an ideal setup.', 's3 throws error with latest curl in amz1 when getobject greetings! since updating curl to version curl-7.61.1-7.91.amzn1.i686 getobject(sdk) throws the following error when retrieving a file with content-encoding=utf-8 and content-type=text/html. does not happen with previous version curl-7.53.1-16.86.amzn1.i686 which works flawless: ..... exception \\'aws\\\\s3\\\\exception\\\\s3exception\\' with message \\'error executing \"getobject\" on \"xxxxxxxx\"; aws http error: curl error 61: unrecognized content encoding type. libcurl understands deflate, gzip content encodings. (see http://curl.haxx.se/libcurl/c/libcurl-errors.html) (server): 200 ok (request-id: cb1f86a65abdff78) - \\' .... exception \\'guzzlehttp\\\\exception\\\\requestexception\\' with message \\'curl error 61: unrecognized content encoding type. libcurl understands deflate, gzip content encodings. (see http://curl.haxx.se/libcurl/c/libcurl-errors.html) .....', \"re: s3 throws error with latest curl in amz1 when getobject i suffer form the same issue. i succeeded in getting this issue work by using 'http' => ['decode_content' => false], in the s3 client constructor, like $client = new s3client([ 'region' => 'us-west-2', 'version' => 'latest', 'http' => ['decode_content' => false], ]);\", 're: s3 throws error with latest curl in amz1 when getobject thanks for the tip, it worked.', 'sync/copy objects from different cloud providers hi, is there any tool that i can use to copy objects from other cloud providers for example ibm cloud object store to aws s3? afaik rclone is one of the tool. i wanted to check if we still can achieve this using aws cli or s3cmd thanks, nithin', \"storage help i am helping set up storage for our municipal document scanning and archiving department. we aren't big and i'm not the best. amazon offers the storage i want but the interface is more than i'm trained on or have time to learn. i haven't found any good third party apps that i can use to transfer the data. i know asking this here is likely not the best, commercial plugs are likely not appreciated, but can anyone (maybe an amazon employee) point me to a good secure app or a service for me to use? scott\", 'need suggestions on renaming large amount of objects hi, we need to rename ~10 million objects (~2.5tb in total) on s3 in the same location, same bucket. is there any better way than copying them to new names and deleting the original ones? thanks in advance! edited by: xpli on feb 19, 2019 8:55 am', \"extending the date on s3 object lock hi, the documentation is unclear on how you can extend the object lock date with the api. i think the api guides have not been updated maybe? i've tried: 1) http post on object with new x-amz-object-lock-retain-until-date header. this is disallowed right off the bat, no post allowed 2) http put with no length/data this is disallowed, length required (i don't have the source data anymore) i see that in general, object metadata can be changed with a copy request. i'm not sure if this applies to object lock metadata, but that isn't quite what we want; our use case is to extend retention on objects frequently even if the original retention has not yet expired. we only want to pay for one versions' worth of storage, of course... any pointers are appreciated!\", \"s3 - how to properly exeed days that file is avaiable in s3 in lifecycle ru hello, i have s3 bucket with lifecycle rule that transists objects with proper tag to amazon glacier some days after creation. after object upload, i attach this tag to it. i would like to exeed that number of days for single object. what is the most efficient way of achiving this? can i create lifecycle rule that is applied not for days after creation but let's say days after last touch/tag creation ? do i need to create lifecycle rule for each day configuration? edited by: wyci on feb 15, 2019 12:15 am\", \"account suspended & reactivation for billing? hi - my account was suspended for billing purposes, we've paid the outstanding invoices. i opened a support ticket yesterday morning (> 24hrs ago) requesting re-instatement, but haven't heard back. what do we need to do to get this turned back on?\", 're: account suspended & reactivation for billing? hi i have escalated your concerns to the billing department. you will receive an update on your account status via the support ticket. i do apologize for any inconveniences caused. regards, francois', 're: account suspended & reactivation for billing? francisco, the same thing is happening to me. it\\'s been over 24 hours and haven\\'t heard back. it won\\'t let me access the account because it is \"suspended\" and my website is down, yet it still seems to be charging me for using it... and even worse, my support ticket has been unassigned for over 24 hours! please help! saul', 're: account suspended & reactivation for billing? hi saul, i sincerely apologize for the delay in responding to your support case. i confirm that the account has been reinstated. please respond via support case 1348555031 if you have any questions or concerns. best regards, kuda', \"re: account suspended & reactivation for billing? same issue, i've opened a service case for it. still waiting for reply.\", \"re: account suspended & reactivation for billing? same issue, i've opened a service case for it. still waiting for reply.\", 're: account suspended & reactivation for billing? hi neciboliks, i have replied to you support case # 1354368401, should you have any further queries relating to the this please reply to the case directly. your account has successfully been reinstated. have a good day.', 're: account suspended & reactivation for billing? hi, it\\'s been three days since i made all required payments to unsuspend my account, but i\\'m still unable to login aws console. in support case # 1354368401 it is stated that my account is activated and all services will be available in 30 minutes, however i still cant login to aws console and services are not working for three days. the following message displayed when i try to login aws console: \"authentication failed because your account has been suspended. please contact aws customer support.\"', 're: account suspended & reactivation for billing? can someone please help me with my account. i have paid all the bills and the account is still suspended. i urgently need it up and running. i have made several requests since tuesday and still no response from anyone. i really appreciate it. thanks', 're: account suspended & reactivation for billing? can someone please help me with my account. i have paid all the bills and the account is still suspended. i urgently need it up and running. i have made several requests since tuesday and still no response from anyone. i really appreciate it. thanks', \"re: account suspended & reactivation for billing? hello, the same thing happened to me as well, we haven't heard from amazon, we opened two tickets, it is extremely important for us to reenable the account. can you help, please? our case number is 1360905041. thanks!\", 're: account suspended & reactivation for billing? same deal, please expedite the enabling of my account case# 1361705261', 're: account suspended & reactivation for billing? i will be leaving aws for encountering a similar such delay. 3 support tickets and 2 days later, yet still locked out of my account. way to fail miserably at customer support amazon, if you ever get around to reading this message!', 're: account suspended & reactivation for billing? hello, the same thing happened to me as well, extream urgent can you help, please? our case number is 1390544761', 're: account suspended & reactivation for billing? hi concern, my account has been suspended, i cleared the bill and opened a ticket to reinstate my account. since it is a mail server all our mails were down from two days. please helpout asap. thanks & regards, bharath', 're: account suspended & reactivation for billing? i have paid all the bills and the account is still suspended. i urgently need it up and running. 676733943884 please,help me! thanks adriana', \"re: account suspended & reactivation for billing? i have the same problem. it's been more than 24 hours. my case number is 1394053931. thank you\", 're: account suspended & reactivation for billing? hi, i have the same problem. i cant acces to my account, i already made the payment, i need the service a soon as posible', 'account login .', 're: account suspended & reactivation for billing? hi, i am having the same issue and having an open ticket since friday night. can somebody please help me?', 're: account suspended & reactivation for billing? hello all, i have my account suspended with case id #1516007081. could someone please take a look at this?', 're: account suspended & reactivation for billing? same here, we have all our services down and we are wating for the reactivation. we have already paid all unpayed bills. our case is: 1556450571 please, we need a solution, we are losing money. thank you.', 're: account suspended & reactivation for billing? hi we had a bad credit card on file, we have since since updated billing couple days ago but account is still suspended but , opened few tickets and no response ? account id: 552859475310 account name: mobile', \"re: account suspended & reactivation for billing? hi my account was suspended because the credit card failed to process, but i now payed the outstanding amount. can you please reactivate my account asap? it's been over 6 hours and i still haven't heard back (case 1635447631, account nr: 361721873029) thanks edited by: coolasdf on jan 27, 2016 7:10 pm edited by: coolasdf on jan 27, 2016 7:11 pm\", \"re: account suspended & reactivation for billing? the same thing happens with me. we've paid the outstanding invoices. i opened a support ticket requesting re-instatement, but haven't heard back. my ticket support/case id is 1640316161. what do we need to do to get this turned back on?\", \"is s3 the correct service for my project? hello lads! i'm currently working on a schoolproject where i basically want to send data from a pi to a cloud storage once every eight hours (approximately). the data is pretty primitive ( probably just an array and a affiliated timestamp). furthermore the data should be stored in the cloud and it should just be possible for the sender of the data to look at the data. (probably from an mobile app i'll add on later) so basically my question is: is amazon s3 the correct service for those requirements? do i need another service to implement these functionalities? what is the simplest way to do so? thank you for your advice in advance.\", 're: is s3 the correct service for my project? hello from my understanding of what you want to do, you will need 1. api gateway 2. lambda 3. s3 and/or dynamodb api gateway receives your api call and passes that to a lambda function which writes the data to s3 or dynamodb. if you save the data in s3, each time you write somethign, it will become an object. if you save it to dynamodb, it will be a new entry in the db which is faster and easier to retrieve but has extra cost. to read, it will be api gateway, lambda, and read from s3 requires knowing the object name to get to the content. to read from dynamo, you need the key to get it or get all entries in the db (which costs more) hope this helps, rt', \"re: is s3 the correct service for my project? ok so i've set up my dynamodb and can add entries with a java program. can you tell me how i can make an api which downloads certain entries using lambda and api gateway? googled but couldnt really find an solution. edited by: bautista on feb 14, 2019 1:05 pm\", 'unable to check whether the bucket is public using api call hello, i am testing api operations on aws s3 buckets and by calling get bucketpolicystatus api operation (https://docs.aws.amazon.com/amazons3/latest/api/restbucketgetversion.html) using postman i am not able to get response saying whether my bucket is public or not. i tried these 2 requests: request 1: get https://bucket-name.s3.us-east-1.amazonaws.com/?policystatus response 1: <error> <code>nosuchbucketpolicy</code> <message>the bucket policy does not exist</message> <bucketname>bucket-name</bucketname> <requestid>...</requestid> <hostid>...</hostid> </error> request 2: get https://bucket-name.s3.us-east-1.amazonaws.com/bucket-name?policystatus response 2: <error> <code>nosuchkey</code> <message>the specified key does not exist.</message> <key>bucket-name</key> <requestid>...</requestid> <hostid>...</hostid> </error> could you please explain me what am i doing wrong? thanks in advance.', 're: unable to check whether the bucket is public using api call hello the request should be get /<bucket-name>?policystatus http/1.1 host: <bucket-name>.s3.amazonaws.com x-amz-date: <thu, 15 nov 2016 00:17:21 gmt> authorization: <signaturevalue> notice that there is no slash \"/\" before the ?policystatus here is the link for the example https://docs.aws.amazon.com/amazons3/latest/api/restbucketgetpolicystatus.html hope this helps rt', 're: unable to check whether the bucket is public using api call hi rt-jaws, the request 2 seems pretty much the same to a request you provided, and that request throws an error for me.', 're: unable to check whether the bucket is public using api call hello from the documentation, the request is a little bit different this is your original request https://bucket-name.s3.us-east-1.amazonaws.com/bucket-name?policystatus the doc shows https://bucket-name.s3.amazonaws.com/bucket-name?policystatus notice that it does not use the region in the call https://docs.aws.amazon.com/amazons3/latest/api/restbucketgetpolicystatus.html rt', 're: unable to check whether the bucket is public using api call i have tried it without the region as you suggested and i received nosuchkey error just like i described in request 2. so unless you managed to get the proper response with that request, then either i am wrong or the documentation is not correct in this case. edited by: xidex on feb 13, 2019 5:39 am', 'what is any other used for \"log delivery groups\" acl hello. i found document what \"log delivery group\" acl is used for s3 access logging. document url is https://docs.aws.amazon.com/amazons3/latest/dev/acl-overview.html but, elb access logs,cloudtrail logs and vpc flow logs does not used \"log delivery group\"acl? i can\\'t found what is \"log delivery group\" acl is used for other. tell me please.']\n"
     ]
    }
   ],
   "source": [
    "print(cleantxt[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
