{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv('Amazon S3.tsv', delimiter='\\t', encoding='utf-8')\n",
    "\n",
    "punctuation = \"!\\\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "\n",
    "cleantxt = []\n",
    "\n",
    "for i in text.description:\n",
    "    i = i.lower()\n",
    "    i = i.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "    i = i.strip()\n",
    "    i = i.replace(\"\\r\\n\", \" \")\n",
    "    cleantxt.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"s3 public access can not be seti've edited the bucket policy public access settings but the bucket is not changed to public and access is deniedwhy do you see these symptomshttps//imgurcom/rv64vwdedit public access settingshttps//imgurcom/y3ppaqjbucket policy\", \"i can't figure out why s3 won't display my website on my domainerror 403 and i am not sure how to troubleshoot the error or resolve it despite reading guidesi've looked at a couple of forums and pages but they are either irrelevant or beyond my current understanding i cannot figure out why i keep getting a 403 i have a public bucket policy which changed my previous error of not getting a connection to the site to a 403 just forbidding traffic i am new to aws what am i missingi only have one html file in the bucket and when i hit 'make public' it said access denied are there other objects that i have to findi was able to get into the html file permissions and when i selected 'public access' 'read object' it said access deniedfor whatever reason when accessing the domain i purchased it says 403 when i go to the amazon s3 version of my website it works huh why doesn't it come up on my domain\", \"re i can't figure out why s3 won't display my website on my domaintest ignore\", \"my aws s3 account is suspended without any notificationtoday when i was ready to some work on my website i found all the components which are stored on amazon s3 are missing to check the issue i tried to log in to s3 console and found out that my account is suspendedi didn't find any email regarding the suspension and i have no payment dues in my billing optionsi have also opened a support case with case id 5866475801 and mailed the issue but still no replywhat the hell is happening  please atleast tell and help me to resolve the issue so that i can work further\", 'unable to read more than 1000 objects from a bucketi am using the rest apis not through sdk to read the list of objects from a bucket the bucket has more then 1000 objects as per https//docsawsamazoncom/amazons3/latest/api/restbucketgethtml  documentation i am extracting the istruncated and nextmarker values from the response and if istruncated is true i pass the value of nextmarker as marker parameter in the subsequent callwhenever i add the marker to the query parameter i get signaturedoesnotmatch error i specify other parameters like maxkeys prefix delimiter and they does cause any error and the call goes through fine of course i did uri encode the parameters its only the marker parameter that is causing the errori also tried to use the v2 version https//docsawsamazoncom/amazons3/latest/api/v2restbucketgethtml  i get the same signaturedoesnotmatch error when using the continuationtoken in the subsequent call the listtype and maxkeys parameters were okany help is appreciated', \"adding expires or cachecontrol header for folders in s3the help page here is as always quite useless https//docsawsamazoncom/amazoncloudfront/latest/developerguide/expirationhtmlexpirationindividualobjectsit says that in a folder i'd see properties and then metadata none of my folders in s3 have this metadata inside properties there are cards like versioning server logging etc where should i enter the cachecontrol setting this is for cloudfront which seems to be not getting this header from s3 i could set up this header in cloudfront but that ui isn't any more intuitive thanks for any guidance\", 're adding expires or cachecontrol header for folders in s3found it in the change metadata for folders and files the help should be updated the documentation remains fairly hideous found the solution on stack overflow thanks', 'where do i get the canonical user idhii am setting up a new bucket and need to assign specific iam users to the bucketit is asking for the canonical user idhow do i get the canonical user id for all my iam usersthis is so confusingthanks', 're where do i get the canonical user idhello nicolasbriant thank you for your post i trust you are well regarding the canonical user id you can find it in two ways1 logging in as root in the top right of the console choose your account name or number then choose my security credentials you will see the canonical user id in the account identifiers section finding your account canonical user idhttp//docsawsamazoncom/general/latest/gr/acctidentifiershtmlfindingcanonicalid2 by using a listbuckets api call when you perform a get operation on the s3 service to get a listing of all the buckets you own the response contains an owner id element which is your canonical user id for exampleaws s3api listbuckets output textlistbuckets http//docsawsamazoncom/cli/latest/reference/s3api/listbucketshtmlbest regards jayd j', 're where do i get the canonical user idthanks jayd i was missing logging in as rootregardsnicolas', \"re where do i get the canonical user idi wanted to post the following but i wasn't allowed because my forum account had just been created in the past hour  some kind of spam preventioni'm pretty new to using aws  i can't find my canonical id and i don't understand your instructions for looking up my id  i have access to the aws console via my personal account  isn't there a way to see my own canonical id via the web interface without using a clii've created an s3 bucket under my organization's account  i want to grant myself and another user access to the bucket but i need our two canonical ids to do soi'd appreciate stepbystep instructions for getting the ids  assume i know nothing about awsthanks in advancewhile i'd still like to have explicit stepbystep instructions for finding canonical ids the delay in forum posting inspired me to try a different approach  or rather the same approach over againthat is when i was creating the s3 bucket i had entered my email address as one to be granted access to it  the bucket was created but i received an error message that i couldn't be granted access by email address for some reason  it was unclear as to whyfor some reason i decided to try using my email address again  this time it worked and my canonical id was automatically substituted  i entered the email address of the other person that i wanted to give access to the bucket and his canonical id was filled in as well  now we both have accesshowever  when we view the permissions of the s3 bucket we only see canonical ids  there's no indication to whom each of the ids refer  that's not a useful ui  it really looks lazyso how can i tell which canonical id goes with each person\", 're where do i get the canonical user idhellohow can i find another user canonical idi created new iam user and want him to have access to s bucketthank you', 're where do i get the canonical user idevery iam user will not have their own canonical id there will be only one canonical id per aws account the following link has stepbystep instructions on how to find the canonical id associated with your aws account  https//docsawsamazoncom/general/latest/gr/acctidentifiershtmlfindingcanonicalidusing obejct/bucket acls you can grant/restrict access to an object/bucket to your aws account or to another aws account you cannot restrict access to bucket/object to an iam user using object/bucket aclyou will have to use bucket policy if you want to provide/restrict access to bucket/object to an iam user in your account then you will have to use s3 bucket policy instead of aclbucket policy examples  https//docsawsamazoncom/amazons3/latest/dev/examplebucketpolicieshtml', \"bug in permission evaluationto reproducecreate a role testrolecreate a bucket we'll call it testbucket herecreate a user testuseradd permissions to testuser to stsassumerole to testrole and to the trust relationship of testrole to allow the user to assume itadd an inline policy to testrole to allow actions s3 to testbucket and testbucket/add an inline policy to testuser to allow actions s3 to testbucket and testbucket/upload a file to the bucket via ui is finecreate a bucket policy on testbucket    version 20121017    statement                     sid denyothers            effect deny            notprincipal                 aws                     arnawsiamacct idrole/servicerole/testrole                    arnawsiamacct idroot                    arnawsiamacct iduser/testuser                                        notaction                 s3putobject                s3putobjectacl                        resource arnawss3bucket name eg testbucket/            the bucket policy is taken from an anonymous upload use case but in essence its deny'ing access to everyone but root our test role and a test user to everything other than putobject and putobjectaclso i'd expect that from testuser if i assumed the role testrole that i'd be able to for example call s3getobject on the file i uploaded  but this gets access denied  meanwhile if i do the same thing with testuser it is allowed despite both of these having the exact same set of policiesplease helpthanksedited by davidericksonfn on mar 6 2019 727 pm\", \"accidentally expired entire bucket contentsso yesterday one of the files in our bucket wouldn't delete i set a lifecycle rule to expire the object at least i thought i did what i actually did was name the rule the object i wanted to expire and set the scope to the entire bucket is there any way to recover the contentsedited by bd on mar 6 2019 1146 am\", 'pentest of aws cloud application  s3hellothe new policy for penetrationtests permits the penetrationtest of a list of aws services as seen on this site https//awsamazoncom/security/penetrationtesting/s3 is not in the list since publicly accessible files on a s3 bucket are often a security relevant issue it should be part of a security assessment of a cloud infrastructure i wonder if it is necessary to obtain a special permission of aws to issue s3related requests in the context of a penetration test or is it generally forbidden for a pentester to examine s3 buckets belonging to a tested applicationgenerally speaking what am i allowed to do as a pentester performing a pentest in the aws cloud regarding the s3 bucketkind regardsmichael', 'preview large files in s3i have several thousand image files that are 1g in size each storaged in s3  i  access them locally via storage gateway  paging through these image files locally is slow and cumbersome  i am looking for a way to allow me to preview these files without having to download them one at a time to view them  is there any kind of viewer available in aws that will allow me to view these files without downloading them locally', \"unexplained error setting up policy for s3 crossregion replicationhi i'm following the instructions on this page for setting up the roles for crr https//docsawsamazoncom/amazons3/latest/dev/settingreplconfigpermoverviewhtmli have a role we'll call it replrole with the exact trust policy listed on that page and an access policy that looks like this    version 20121017    statement                     effect allow            action                 s3getreplicationconfiguration                s3listbucket                s3getobjectversion                s3getobjectversionacl                s3getobjectversiontagging                s3replicateobject                s3replicatedelete                s3replicatetags                        resource                 arnawss3ours3bucketprefix                arnawss3ours3bucketprefix/                        it doesn't look exactly like the policy provided but the main difference is that i use wildcards for the resources specified i also group the actions together but if i understand iam access policies correctly this should suffice for the purposes3 allows me to create the crossregion replication rule but then i get this error messagethe crr rule is saved but it might not workthere was an error with setting up the iam policy for the selected iam role gobscrossregionreplicationrole ensure that you have set up the correct policy or select another rolewhat did i do wrong\", 're unexplained error setting up policy for s3 crossregion replicationwhat i found was that the crossregion replication was actually working despite the error message when i checked back the next morning the objects were successfully replicated to the backup bucket', \"can't export cloudwatch logs to s3i'm trying to export a cloudwatch log group to s3 but i keep getting this error every time i click export data on the cloudwatch sidethe acl permission for the selected bucket is not correct the amazon s3 bucket must reside in the same region as the log data that you want to exportthe cloudwatch log is in useast1 i created a bucket in each of the two us east regions n virginia and ohio but i still get this error when i try to export to either one of them why\", \"re can't export cloudwatch logs to s3please respond this is very important and time sensitive\", \"re can't export cloudwatch logs to s3i had the exact same problem it turns out that i neglected to populate the 's3 bucket prefix' field under the 'advanced' section of the export dialogedited by lbrooks on nov 30 2018 1217 am\", \"re can't export cloudwatch logs to s3this unfortunately still doesn't work for me either what exactly did you enter as the prefix why can't we simple save the logs to any bucket that we own as a user aka admin\", \"re can't export cloudwatch logs to s3did you ever solve this  how do i tell which region my cloudwatch logs are ini am trying to do the same thing and am getting the same error\", \"re can't export cloudwatch logs to s3i had the same problemthe solution i found was to select the bucket in the web portal and along the top you then have a number of buttonsoverviewpropertiespermissionsmanagementin permissions you can set the bucket policy to allow cloudwatch exports as detailed here https//docsawsamazoncom/amazoncloudwatch/latest/logs/s3exporttaskshtmlhope this helps\", \"re can't export cloudwatch logs to s3it took me 30min to figure the error their example policy is wrongthe region in the example is the one you have to replacesimply replace it by                 service logsuseast1amazonawscomdo it in both fields below    version 20121017    statement                 action s3getbucketacl          effect allow          resource arnawss3myexportedlogs          principal  service logsuseast1amazonawscom                       action s3putobject           effect allow          resource arnawss3myexportedlogs/randomstring/          condition  stringequals  s3xamzacl bucketownerfullcontrol            principal  service logsuseast1amazonawscom\", 'amazon s3 slow on large bucketshii got a big amazon s3 bucket around 8 gb and now things started to get slow uploading files with iam user so the api takes forever now i recreated a new bucket with the same properties and settings and api uploading to this bucket is way faster is there a connection between a large bucket and slow api uploading do i have to make new buckets all the timetime to write an image of around 300 kb with api to large bucketimageget 1260msimagewrite 67683msto a newly created bucket timeimageget 1275msimagewrite 822mson https//docsawsamazoncom/amazons3/latest/dev/bucketrestrictionshtml it is statedthere is no limit to the number of objects that can be stored in a bucket and no difference in performance whether you use many buckets or just a few you can store all of your objects in a single bucket or you can organize them across several bucketsfor me this does not seem the case anyoneregardsdick goosenedited by dickgoosen on feb 1 2019 1249 amedited by dickgoosen on feb 1 2019 122 am', 're amazon s3 slow on large bucketshi therei have the same issue the small image takes 2 min by uploading to s3 from ec2', \"unable to set the acl to public read properly on objects stored in bucketthe bucket permissions are set to public and when i upload image files from the angular web application i include in the header of the put request xamzacl publicread as described by the aws documentation when i attempt to view the images through the s3 link to the file however a 403 forbidden error is returned if i go into the bucket and examine the image it shows there are no access permissions and if i try to make the image public i get an access denied message what can i do to resolve this issue  i've attached some images to the post the first one shows the headers included in the put request for uploading the second shows the bucket policy and the third image shows the current state of permissions for an image i uploaded\", \"password protect s3 mountusing storage gateway we've created an s3 mount which we connect to using the recommended mount o nolock command  i know that we can limit the policy so that only certain iam users have access to the buckets and we can then mount the buckets using something like tntdrive and enter the iam credentials that way to access the buckets but are there any alternatives to tntdrive that would allow us to do this  that is when we set up a mount is there any command like mount o nolock usernameusername passwordpassword that would allow us to protect the mount\", \"'access denied' when access s3 from angular app with cognito user pooli have s3 bucket which i configured to manage access using cognito user pool as described here https//docsamazonawscn/enus/iam/latest/userguide/referencepoliciesexampless3cognitobuckethtml    version 20121017    statement                     effect allow            principal             action s3listbucket            resource arnawss3bucketname            condition                 stringlike                     s3prefix cognito/appname/                                                        effect allow            principal             action                 s3putobject                s3getobject                s3deleteobject                        resource                 arnawss3bucketname/cognito/appname/cognitoidentityamazonawscomsub                arnawss3bucketname/cognito/appname/cognitoidentityamazonawscomsub/                        i have angular web app which authenticate users with cognito user pool and i'm using s3 client to get object i see a call to cognito service https//cognitoidentityeucentral1amazonawscom/ is made successfully and an identity is returned as a response but the immediate call afterwards to s3 is failing with status code 403errorcodeaccessdenied/codemessageaccess denied/messagerequestidaac2b5fc5c74c971/requestidhostidl8aoygybuty1qhtjhydrj9uxc97elszl6h2rqlnglpquzrqqpw532u6pixil7ypz4ugpreoss/hostid/errorhere's my code setting aws creds    buildcognitocredsidtokenjwt string         let url  'cognitoidp'  cognitoutilregiontolowercase  'amazonawscom/'  cognitoutiluserpoolid        if environmentcognitoidpendpoint             url  environmentcognitoidpendpoint  '/'  cognitoutiluserpoolid                let logins cognitoidentityloginsmap          loginsurl  idtokenjwt        let params                 identitypoolid cognitoutilidentitypoolid / required /            logins logins                        let serviceconfigs  awsserviceserviceconfigurationoptions        if environmentcognitoidentityendpoint             serviceconfigsendpoint  environmentcognitoidentityendpoint                let creds  new awscognitoidentitycredentialsparams serviceconfigs        thissetcognitocredscreds        return creds    what am i missingno matter what i try i'm getting access denied\", \"s3 console shows error in access rowalso it can't generate an url for 'download as' it shows an error occurred generating the download link for this object this happens for all aws accounts we have and for different people\", \"re s3 console shows error in access rowhii understand that you are seeing error in the in the s3 console and 'download as' option is throwing error this usually happens when you do not proper permissions to access the bucket or objectplease ensure that you have all the necessary permissions if you still see the same issue please share the bucket name object name and requester iam arn with me over private message to troubleshoot furtherthanks\", \"re s3 console shows error in access rowrgumber i've sent requested information via pm several days ago\", \"re s3 console shows error in access rowany news i don't have any issue or restriction when i use aws cli it only happens with aws consoleedited by deniskot on feb 28 2019 228 am\", \"amazon s3  error retrieving access type can't use buckethelloi have a problem concerning my s3 buckets every bucket that i create or that is created automatically by elastic beanstalk show a error in the access column short for error retrieving access type i am unable to do anything with the console  i can't delete the bucket or download/upload any file elastic beanstalk as well is unable to use s3 and it makes impossible the deployment of new version of any web applicationusing the cli i can perform any operation i wantthere is no iam on the account i log directly with root accessdo you have any idea what the problem can be  thank youedouard\", \"can't delete empty s3 bucket cftemplatesi can't delete a s3 bucket that• is empty• has no versioning• has no policythis s3 bucket has been created with some other servicewhen i attempt deleting the bucket in the aws console i get an error with no message and then i tried to delete via aws cli with below commandaws s3 rb s3//bucketname forceit doesn't work and shows error msg like thisfatal error an error occurred nosuchbucket when calling the listobjects operation the specified bucket does not existremovebucket failed unable to delete all objects in the bucket bucket will not be deletedi searched on forum and tried to edit bucket policy but it also shows like this errordata not foundhope anyone could help me with this issuethanks for and have a nice day\", 'copy from source bucket to dest bucket  getobject stream problemhi allmy name is eliran and i new in awsmy goal is to copy from one bucket ireland to another bucket nvirginiai will explain my work flow1 i use the cli command  aws s3 sync s3//sorcebucket/ s3//destbucket/ exclude logs/the sync completed after some time2in my app in net i use the aws sdk and use the command getobject like thisamazons3client s3client  new amazons3clientglobalsawsaccesskeyglobalsawssecretkey regionendpointuseast1stream rs  s3clientgetobjectnew getobjectrequestbucketname  sourcecontainerkey  keyresponsestreamits work fine but the responestream for some objects is md5stream that what i need and for some objects is cachingwarpperstream its not good for meif i use the source bucket from ireland so all the request with getobject on the same objects like above will return responestream md5stream the settings and policies is the same in both bucketswhat goes wrong and how i can get always md5stream from my new bucket in nvirginiathanks a loteliran kasif', \"stockholm s3 endpoint issuehelloi'm trying to connect to an s3 bucket in the newly available eu north 1 region stockholm through two mac s3 compatible apps forklift and chronosync without successi've used s3eunorth1amazonawscom and s3eunorth1amazonawscom endpoints to no avail i get the following error the authorization header is malformed the region 'useast1' is wrong expecting 'eunorth1'is anyone experiencing the same issue thanksregards\", \"re stockholm s3 endpoint issuei answer to myself one of the mentioned apps forklift has been updated recently and now it works with eu north 1 region s3 bucketsi suppose the same will happen soon with chronosync app it's actually a matter of the specific app that needs to be updated to support new regions\", 'issue reading s3 buckets xml parsing errorwhen i access my list of buckets in the web browser i get the following error in the console xml parsing error no root element foundlocation https//useast1consoleawsamazoncom/s3/proxyline number 1 column 1i believe this is coming from a few buckets i had deleted but are stuck in my account if i try to delete them again nothing happens visually and in the console this error is hit againis this something that will resolve itself in time or does something need to be done to resolve itthanks', \"strange issue granting unexpected permission to single object how to fixi have two objects in a bucket and getobject should 403 for both for role a except inexplicably one object is accessiblethe bucket configuration isblank bucket policy all 4 options under public access settings are true acl is defaultsrole config grants no permissions to s3permissions for both objects appear identical at least in console iam policy simulator indicates both objects will be denied but again in reality one object is allowedi'm performing the getobject from javascript aws sdk using federated identities  cognito pool role a  is associated with the logged in user's group i've tried running cloudtrail but getobject is not being recorded when executed via aws sdk it seems to log the event however when i manually download via console please helpedited by davegravy on feb 22 2019 536 am\", \"does amazon s3 or glacier has buildin fixity checkinghisee https//dltjorg/article/oclcdigitalarchivevsamazons3/ claimed that amazon s3 does not provide fixity checksee https//wwwslidesharenet/amazonwebservices/deepdiveonarchivingandcompliance page 12 from aws presentation said that it has builtin fixity checking i can't find anything mention about fixity by simply searching fixity in https//docsawsamazoncom/s3/indexhtmllang/enushttps//docsawsamazoncom/glacier/indexhtmllang/enusso does amazon s3/glacier provides fixity checking or not\", \"need suggestions on how to look up existing objects on s3hi we have incoming files everyday and we upload them to s3 the files can be duplicates from earlier days so we need to check if they already exist before uploadingour old way is to save the filename to sdb after uploading a new file so we can use sdb query to look up existing fileswe recently want to change it to use s3 head object api to check existencewe need suggestions1 if there's better way beside sdb and s3 head api any new s3 api to check existence in bulk3 is s3 head api good enough for our use case we need look up 200 filenames every hour during the daythanks in advance\", \"re need suggestions on how to look up existing objects on s3hii use s3 pretty extensively but i'm not employed by amazon so take this as it is that amount of head requests per hour will be totally fine it should not be anywhere close to stressing out the servicethere isn't really a better way to check for existence of a random key however if you have a lot of keys at once and the keys share a pathlike structure you could do better by executing a list request on the common prefix and checking the contents keep in mind that s3 may not be immediately consistent you should read the docs to fully understand the impact to your particular use case but a couple things stick out1 if you do a get/head prior to uploading then put then get/head  that response will be eventually consistent ie not guaranteed to return that the object does exist2 list requests are eventually consistentgiven that and that your expectations for number of objects to check i would recommend keeping it simple and just doing head  maybe with timebased retries to clear up the eventual consistency issue if you make the wrong decision you simply reupload a duplicate which is not data loss but just extra cost to you so if this happens every once in a while shouldn't be too big of a dealhope this helps\", 're need suggestions on how to look up existing objects on s3thank you i agreed to keep it simple is the right way to start your suggestions are very much appreciated', \"can't delete object and its deletion marker from both cli and consolehi while playing with s3 bucket that i own i uploaded some text files that are several kilobytes and put a deletion marker afterwardshowever when i tried to remove the object version 1 original file and version 2 deletion marker the console just failed saying delete object total objects2 successful 00i also tried cli command aws s3api deleteobject bucket storageik1ne versionid nbyr0gs5mabrxhwhhgqqosojac0ocznia key filenametxt on both its version 1upload and 2deletion marker but it just outputs json output versionid versionipreviouslyspecifiedi tried this with account that has administratoraccess but failed also all modification i did to bucket policy was public/private settingsie did not specify deletion policy etceven the empty bucket command on console also failsjust in case the filename was 면접질문txt and 면접질문리스트txt which is 2bit character yes actually i have two files with same symptomswhat am i missing to make it sure i uploaded another file to the same bucket and tried to reproduce the issue but that file works out exactly expectedboth file and deletion marker gets deleted\", \"re can't delete object and its deletion marker from both cli and consolehii see that your bucket is now empty in case you still face the same issue please share the bucket name and object name with me over private message and i will be able to assist you furtheralso you can setup a lifecycle policy to expire all or filter the objects in your s3 buckethttps//docsawsamazoncom/amazons3/latest/dev/objectlifecyclemgmthtmlthanks\", \"re can't delete object and its deletion marker from both cli and consolei tried this on chrome and it worked so i think it was just safari webkit and mac terminal encoding bug thank you\", \"unable to delete s3 event notificationhii'm trying to delete an s3 event notification but i get this errorunable to validate the following destination configurations not authorized to invoke function arnawslambdaeuwest1xxxxxxxxfunctionmylambdafunction arnawslambdaeuwest1xxxxxxxxfunctionmylambdafunction nulli was using this s3 event as a trigger for a lambda function that i have since deleted i thought that might be the issue and recreated the lambda function with the same name but this did not solve the issuethank you\", \"re unable to delete s3 event notificationhii understand that you are not able to delete s3 event notification and you are getting the above mentioned error from the error message it looks like you have another event rule with the destination as 'arnawslambdaeuwest1xxxxxxxxfunctionmylambdafunction' and your s3 bucket does not have the permission to invoke that lambda functioni would suggest you to please verify the same and if the issue persists please share the bucket name and complete error message with me over private message and i would be able to troubleshoot furtherthanks\", \"s3 bucket with versioning lifecycle expiration rules not appliedwe have an s3 bucket with replication and versioning enabled about 60k a rule should delete previous versions older than 3 days but i can still see weeks old versions on this source bucket the same rule applied to the destination bucket works fine the rule has been applied weeks ago to both bucketsis there anything obvious i might have missed related to replication versioning etcsee attached configuration for expiration no path set entire bucket selectedthanksluigiedited by lclemente on jan 29 2019 1242 amafter i posted this message the bucket lifecycle worked i don't know if aws fixed it\", 're s3 bucket with versioning lifecycle expiration rules not appliedhi luigii understand that lifecycle rule was not executed for your bucket but it worked nowplease note that when an object reaches the end of its lifetime amazon s3 queues it for removal and removes it asynchronously there may be a delay between the expiration date and the date at which amazon s3 removes an object you are not charged for storage time associated with an object that has expired  to find when an object is scheduled to expire use the head object https//docsawsamazoncom/amazons3/latest/api/restobjectheadhtml or the get object api https//docsawsamazoncom/amazons3/latest/api/restobjectgethtml operations these api operations return response headers that provide this information please refer to the following document for more details https//docsawsamazoncom/amazons3/latest/dev/lifecycleexpiregeneralconsiderationshtmlthanks', \"s3 presigned post url fails with 405 client error method not allowed fori've been struggling all day to try and get the presigned post url feature working with s3 but keep running into errors i have no problem making the request but it seems that the response from the api is pointing at a domain that doesn't allow posts\", 're s3 presigned post url fails with 405 client error method not allowed forhii would like to inform you that you cannot make post request to object endpoints eg https//bucketnames3amazonawscom/objectext in order to make post request you will need to make post request to bucket endpoint eg https//bucketnames3amazonawscom/ and specify the object key name and other properties in the multipart/formdata encoded message body please refer to the following document for more details https//docsawsamazoncom/amazons3/latest/api/restobjectposthtmlplease refer to the following document for an example on browserbased upload using http post https//docsawsamazoncom/amazons3/latest/api/sigv4postexamplehtmlthanks', 'glacier to physical mediumdoes amazon offer any service that would transfer some of our glacier content to a physical encrypted medium such as tape and ship it to us  or are there any third party companies that might', \"aws lambda write image to s3 access deniedi’m trying to get a deeplens lambda function to upload an image to s3response  s3putobjectacl'publicread' bodyjpgdatatostringbucket‘mybucketname’keyfilenamehowever i keep getting the errorerror in face detection lambda an error occurred accessdenied when calling the putobject operation access deniedi made an iam role and attached it to the deeplens lambda function and attached the following policies awsdeeplenslambdafunctionaccesspolicy awslambdaexecute awsdeeplensservicerolepolicy amazons3fullaccess and a custom policy with the following json    version 20121017    statement                     effect allow            action                 s3putobject                s3putobjectacl                        resource                 arnawss3mybucketname”                arnawss3mybucketname/                        i even gave the bucket public access through the access control list and made the bucket policy public    version 20121017    id policy1534108093104    statement                     sid stmt1534108083533            effect allow            principal             action s3            resource arnawss3mybucketname”            but i’m still getting the accessdenied error\", 're aws lambda write image to s3 access denieddid you ever get this to work i am having the same issuei got my function to write to the s3 bucket by change the public policy of block new public acls and uploading public objects to false but this is not an ideal setup', \"s3 throws error with latest curl in amz1 when getobjectgreetingssince updating curl to version curl7611791amzn1i686 getobjectsdk throws the following error when retrieving a file with contentencodingutf8 and contenttypetext/html does not happen with previous version curl75311686amzn1i686 which works flawlessexception 'awss3exceptions3exception' with message 'error executing getobject on xxxxxxxx aws http error curl error 61 unrecognized content encoding type libcurl understands deflate gzip content encodings see http//curlhaxxse/libcurl/c/libcurlerrorshtml  server 200 ok requestid cb1f86a65abdff78  'exception 'guzzlehttpexceptionrequestexception' with message 'curl error 61 unrecognized content encoding type libcurl understands deflate gzip content encodings see http//curlhaxxse/libcurl/c/libcurlerrorshtml\", \"re s3 throws error with latest curl in amz1 when getobjecti suffer form the same issue i succeeded in getting this issue work by using 'http'     'decodecontent'  false in the s3 client constructor like client  new s3client    'region'   'uswest2'    'version'  'latest'    'http'     'decodecontent'  false\", 're s3 throws error with latest curl in amz1 when getobjectthanks for the tip it worked', 'sync/copy objects from different cloud providershiis there any tool that i can use to copy objects from other cloud providers for example ibm cloud object store to aws s3afaik rclone is one of the tool i wanted to check if we still can achieve this using aws cli or s3cmdthanksnithin', \"storage helpi am helping set up storage for our municipal document scanning and archiving department we aren't big and i'm not the best amazon offers the storage i want but the interface is more than i'm trained on or have time to learn i haven't found any good third party apps that i can use to transfer the data i know asking this here is likely not the best commercial plugs are likely not appreciated but can anyone maybe an amazon employee point me to a good secure app or a service for me to usescott\", 'need suggestions on renaming large amount of objectshi we need to rename 10 million objects 25tb in total on s3 in the same location same bucketis there any better way than copying them to new names and deleting the original onesthanks in advanceedited by xpli on feb 19 2019 855 am', \"extending the date on s3 object lockhithe documentation is unclear on how you can extend the object lock date with the api i think the api guides have not been updated maybei've tried1 http post on object with new xamzobjectlockretainuntildate header this is disallowed right off the bat no post allowed2 http put with no length/datathis is disallowed length required i don't have the source data anymorei see that in general object metadata can be changed with a copy request i'm not sure if this applies to object lock metadata but that isn't quite what we want our use case is to extend retention on objects frequently even if the original retention has not yet expired we only want to pay for one versions' worth of storage of courseany pointers are appreciated\", \"s3  how to properly exeed days that file is avaiable in s3 in lifecycle ruhelloi have s3 bucket with lifecycle rule that transists objects with proper tag to amazon glacier some days after creationafter object upload i attach this tag to iti would like to exeed that number of days for single objectwhat is the most efficient way of achiving thiscan i create lifecycle rule that is applied not for days after creation but let's say days after last touch/tag creation do i need to create lifecycle rule for each day configurationedited by wyci on feb 15 2019 1215 am\", \"account suspended  reactivation for billinghi my account was suspended\\xa0for billing purposes we've paid the outstanding invoices  i opened a support ticket yesterday morning  24hrs ago requesting reinstatement but haven't heard backwhat do we need to do to get this turned back on\", 're account suspended  reactivation for billinghii have escalated your concerns to the billing department you will receive an update on your account status via the support ticketi do apologize for any inconveniences causedregardsfrancois', \"re account suspended  reactivation for billingfranciscothe same thing is happening to me it's been over 24 hours and haven't heard back it won't let me access the account because it is suspended and my website is down yet it still seems to be charging me for using it and even worse my support ticket has been unassigned for over 24 hoursplease helpsaul\", 're account suspended  reactivation for billinghi  sauli sincerely apologize for the delay in responding to your support case i confirm that the account has been reinstatedplease respond via support case 1348555031 if you have any questions or concernsbest regardskuda', \"re account suspended  reactivation for billingsame issue i've opened a service case for it still waiting for reply\", \"re account suspended  reactivation for billingsame issue i've opened a service case for it still waiting for reply\", 're account suspended  reactivation for billinghi neciboliksi have replied to you support case  1354368401 should you have any further queries relating to the this please reply to the case directlyyour account has successfully been reinstatedhave a good day', \"re account suspended  reactivation for billinghiit's been three days since i made all required payments to unsuspend my account but i'm still unable to login aws console in support case  1354368401 it is stated that my account is activated and all services will be available in 30 minutes however i still cant login to aws console and services are not working for three days the following message displayed when i try to login aws console authentication failed because your account has been suspended please contact aws customer support\", 're account suspended  reactivation for billingcan someone please help me with my account  i have paid all the bills and the account is still suspended  i urgently need it up and running  i have made several requests since tuesday and still no response from anyone  i really appreciate it  thanks', 're account suspended  reactivation for billingcan someone please help me with my account  i have paid all the bills and the account is still suspended  i urgently need it up and running  i have made several requests since tuesday and still no response from anyone  i really appreciate it  thanks', \"re account suspended  reactivation for billinghello the same thing happened to me as well we haven't heard from amazon we opened two tickets it is extremely important for us to reenable the account can you help please our case number is 1360905041thanks\", 're account suspended  reactivation for billingsame deal please expedite the enabling of my account case 1361705261', 're account suspended  reactivation for billingi will be leaving aws for encountering a similar such delay 3 support tickets and 2 days later yet still locked out of my account way to fail miserably at customer support amazon if you ever get around to reading this message', 're account suspended  reactivation for billinghello the same thing happened to me as well extream urgent can you help please our case number is 1390544761', 're account suspended  reactivation for billinghi concern               my account has been suspended i cleared the bill and opened a ticket to reinstate myaccount since it is a mail server all our mails were down from two days please helpout asapthanks  regardsbharath', 're account suspended  reactivation for billingi have paid all the bills and the account is still suspended i urgently need it up and running676733943884pleasehelp methanksadriana', \"re account suspended  reactivation for billingi have the same problem it's been more than 24 hours my case number is 1394053931 thank you\", 're account suspended  reactivation for billinghi i have the same problem  i cant acces to my account i already made the payment i need the service a soon as posible', 'account login', 're account suspended  reactivation for billinghi i am having the same issue and having an open ticket since friday night can somebody please help me', 're account suspended  reactivation for billinghello alli have my account suspended with case id 1516007081could someone please take a look at this', 're account suspended  reactivation for billingsame herewe have all our services down and we are wating for the reactivation we have already paid all unpayed bills our case is 1556450571please we need a solution we are losing moneythank you', 're account suspended  reactivation for billinghi we had a bad credit card on file we have since since updated billing couple days ago but account is still suspended but  opened few tickets and no response account id552859475310 account namemobile', \"re account suspended  reactivation for billinghimy account was suspended because the credit card failed to process but i now payed the outstanding amount can you please reactivate my account asap it's been over 6 hours and i still haven't heard back case 1635447631 account nr 361721873029thanksedited by coolasdf on jan 27 2016 710 pmedited by coolasdf on jan 27 2016 711 pm\", \"re account suspended  reactivation for billingthe same thing happens with me we've paid the outstanding invoices i opened a support ticket requesting reinstatement but haven't heard back my ticket support/case id is 1640316161 what do we need to do to get this turned back on\", \"is s3 the correct service for my projecthello ladsi'm currently working on a schoolproject where i basically want to send data from a pi to a cloud storage once every eight hours approximately the data is pretty primitive  probably just an array and a affiliated timestamp furthermore the data should be stored in the cloud and it should just be possible for the sender of the data to look at the data probably from an mobile app i'll add on laterso basically my question isis amazon s3 the correct service for those requirements do i need another service to implement these functionalities what is the simplest way to do so thank you for your advice in advance\", 're is s3 the correct service for my projecthellofrom my understanding of what you want to do you will need1 api gateway 2 lambda3 s3 and/or dynamodbapi gateway receives your api call and passes that to a lambda function which writes the data to s3 or dynamodb  if you save the data in s3 each time you write somethign it will become an object  if you save it to dynamodb it will be a new entry in the db which is faster and easier to retrieve but has extra costto read it will be api gateway lambda and read from s3 requires knowing the object name to get to the content  to read from dynamo you need the key to get it or get all entries in the db which costs morehope this helpsrt', \"re is s3 the correct service for my projectok so i've set up my dynamodb and can add entries with a java program can you tell me how i can make an api which downloads certain entries using lambda and api gateway googled but couldnt really find an solutionedited by bautista on feb 14 2019 105 pm\", 'unable to check whether the bucket is public using api callhelloi am testing api operations on aws s3 buckets and by calling get bucketpolicystatus api operation https//docsawsamazoncom/amazons3/latest/api/restbucketgetversionhtml using postman i am not able to get response saying whether my bucket is public or noti tried these 2 requestsrequest 1get https//bucketnames3useast1amazonawscom/policystatusresponse 1error    codenosuchbucketpolicy/code    messagethe bucket policy does not exist/message    bucketnamebucketname/bucketname    requestid/requestid    hostid/hostid/errorrequest 2get https//bucketnames3useast1amazonawscom/bucketnamepolicystatusresponse 2error    codenosuchkey/code    messagethe specified key does not exist/message    keybucketname/key    requestid/requestid    hostid/hostid/errorcould you please explain me what am i doing wrongthanks in advance', 're unable to check whether the bucket is public using api callhellothe request should beget /bucketnamepolicystatus http/11host bucketnames3amazonawscomxamzdate thu 15 nov 2016 001721 gmtauthorization signaturevaluenotice that there is no slash / before the policystatushere is the link for the examplehttps//docsawsamazoncom/amazons3/latest/api/restbucketgetpolicystatushtmlhope this helpsrt', 're unable to check whether the bucket is public using api callhi rtjawsthe request 2 seems pretty much the same to a request you provided and that request throws an error for me', 're unable to check whether the bucket is public using api callhellofrom the documentation the request is a little bit differentthis is your original requesthttps//bucketnames3useast1amazonawscom/bucketnamepolicystatusthe doc showshttps//bucketnames3amazonawscom/bucketnamepolicystatusnotice that it does not use the region in the callhttps//docsawsamazoncom/amazons3/latest/api/restbucketgetpolicystatushtmlrt', 're unable to check whether the bucket is public using api calli have tried it without the region as you suggested and i received nosuchkey error just like i described in request 2 so unless you managed to get the proper response with that request then either i am wrong or the documentation is not correct in this caseedited by xidex on feb 13 2019 539 am', \"what is any other used for log delivery groups aclhelloi found document what log delivery group acl is used for s3 access loggingdocument url is https//docsawsamazoncom/amazons3/latest/dev/acloverviewhtmlbut elb access logscloudtrail logs and vpc flow logs does not used log delivery groupacli can't found what is log delivery group acl is used for othertell me please\"]\n"
     ]
    }
   ],
   "source": [
    "print(cleantxt[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
