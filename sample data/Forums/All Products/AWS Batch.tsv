label	description
AWS Batch	"Determine which instances can be used in the compute environment
I'm trying to find which instances I can use in the batch compute environment. 

When trying to create a computer environment with for example t3.small using the cli I get the error:

 An error occurred (ClientException) when calling the CreateComputeEnvironment operation: Error executing request, Exception : Instance type can only be one of [r3.8xlarge, r3, r4, r5, optimal, r5d.24xlarge, etc....


How can I find these instances types out before?"
AWS Batch	"Job Queue priorities not working as documented
We have a number of job queues with different priorities in us-east-2 that share a compute environment.  These queues all have runnable jobs.  All jobs have equivalent vcpu and memory requirements.  When resources become available, jobs are not assigned to resources based on the priority of their job queue.  We cannot determine any logic to the assignment ordering.

Example 1:
Queues A, B and C have increasing priorities: pA > pB > pC
Queues A, B and C share a single compute environment
Queues A, B and C all have both running and runnable jobs.
A spot instance spins up.
Some jobs in queues B and C start running.
Afterwards, A, B and C all still have runnable jobs.

Example 2:
Queues A, B and C have increasing priorities: pA > pB > pC.
Queues A, B and C share a single compute environment.
Queues A and B both have running and runnable jobs.
A new job arrives in queue C.
A running job finishes.
The new job from queue C is scheduled ahead of both A and B.

Many other examples.  Every scheduling interval brings new examples of unexpected behavior."
AWS Batch	"Re: Job Queue priorities not working as documented
Hi,

Are you seeing this when there's no additional room in the CE? That is, desired is equal to or close to max vCPU?"
AWS Batch	"Re: Job Queue priorities not working as documented
Yes.  We haven't seen a situation where priority-based assignment appears to work as documented.  Regarding the saturation level of the CE, we are seeing this behavior both (1) when a CE is not saturated - desired much below max vCPUs - and new resources become available to a CE (e.g. example 1 from the original post where the CE had none or insufficient resources and many otherwise identical jobs were submitted to job queues with different priorities, the jobs assigned to these new resources are not selected based on the priority of their job queues) and also (2) when there is no additional room in the CE (e.g. example 2 in the original post where the CE is completely saturated - at max vCPUs - and runnable jobs are available in job queues with different priorities.  Once a running job finishes there is space for another - that next job is not often selected from the highest priority queue with runnable jobs).

Just to make sure there isn't a misunderstanding... this isn't something that is usually working as expected and occasionally behaves differently.   We have yet to see any evidence that it works as documented in any situation.  The CE has all the same machine types. The jobs themselves all use the same job definition and have identical vCPU and memory requirements.  The only relevant difference between jobs is the job queue to which they have been assigned.  These queues all share the same CE and all have different queue priorities."
AWS Batch	"""CannotPullContainerError: error pulling image configuration"" Error
Hello,

My team and I have been running batch jobs for a while now and just in the past week I have been receiving this error randomly:

CannotPullContainerError: error pulling image configuration: Get https://prod-us-east-1-starport-layer-bucket.s3.amazonaws.com/9e9f-970539778329-ceace685-8632-b8f9-cc32-a3bbf2dfdfaa/aa64a117-6148-4133-8e60-25a4d0e9b5ac

We run a job at least once an hour and this error seems to happen once a day for the past 4 or 5 days. Any help or insight into why this started happening would be very helpful. Thanks!

Job ID's are:

bfece7ba-9f9e-4683-98ee-ca8da45de5af
f3e1501f-e67c-4d69-82dd-888473d639b8"
AWS Batch	"Re: ""CannotPullContainerError: error pulling image configuration"" Error
Hey there, 

Did you ever find a solution to this issue?"
AWS Batch	"Re: ""CannotPullContainerError: error pulling image configuration"" Error
Similar issue during a docker-compose pull:

$ docker-compose pull app
Pulling app ... pull complete
 
ERROR: for app  error pulling image configuration: Get https://prod-us-east-1-starport-layer-bucket.s3.amazonaws.com/.../...?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=...&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=...%2F20190221%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=...: EOF"
AWS Batch	"Long running ec2 instances are slower than fresh instances.
Hello 

I have many jobs that should roughly take about 10mins. Initially, on a fresh ec2 instance they do take 10 mins, but as that instance is reused for hours, jobs start taking 30mins. When I increased the compute environment size and new instances are created, their jobs are back down to 10 mins.

When looking at old vs new instances, the new instances use about 90% of the cpu, while the older on is maybe 20%.

Any idea, what is going on? and how I can fix this without shutting down running instances?"
AWS Batch	"Container Security Issue (CVE-2019-5736) and AWS Batch
Just wanted to double-check my interpretation of what I need to do to fix AWS Batch for this issue (https://aws.amazon.com/security/security-bulletins/AWS-2019-002/). The brief states that ""An updated Amazon ECS Optimized AMI is available as the default Compute Environment AMI. As a general security best practice we recommend that Batch customers replace their existing Compute Environments with the latest AMI available.""

When I create my compute environment for AWS batch I let AWS manage the AMI i.e. I don't check the ""Enable user-specified Ami ID"" option. As per the wording next to the option, ""By default, AWS Batch managed compute environments use a recent, approved version of the Amazon ECS-optimized AMI for compute resources.""

So am I right to assume that if I just create a new compute environment for my queue that it will use an AMI with the fix to CVE-2019-5736 automatically applied?

Edited by: GraemeLudwig on Feb 19, 2019 8:52 AM"
AWS Batch	"Startup time?
Any thoughts on how to make the job startup time more consistent?

Here's the result of a super quick experiment:

2CPU, 2000MB of RAM with 1 free host and no pending jobs
Created at 10:40:35 pm 12/15/16
Started at 10:40:49 pm 12/15/16

2 CPU, 2000MB of RAM with 1 free host and no pending jobs

Created at 10:49:27 pm 12/15/16
Started at 10:51:07 pm 12/15/16

Why'd it take 1.5min to start in the second case?"
AWS Batch	"Re: Startup time?
Thank you for contacting us!

You are seeing behavior that is a result of the scheduling frequency of AWS Batch jobs.  The AWS Batch Scheduler periodically evaluates jobs in the queue and moves them forward as appropriate. As you submit more jobs, you will see that the AWS Batch Scheduler evaluates and operates upon many jobs at once with each scheduling interval.  

For example, if you submit a hundred jobs to AWS Batch, the Scheduler will transition all of these from SUBMITTED to RUNNABLE or PENDING in about a minute.  RUNNABLE jobs should transition to STARTING and RUNNING fairly quickly assuming you have sufficient resources in your compute environment.

Best regards,

Jamie"
AWS Batch	"Re: Startup time?
Thanks, that's useful. Is there any way to increase that frequency?"
AWS Batch	"Re: Startup time?
Not at this time.  Could you share some information regarding your use case and desired scheduling frequency?

Thanks,

Jamie"
AWS Batch	"Re: Startup time?
Jamie,
It's been a few months since this issue came up and I'm a different user, but I have the same concern. I have one particular type of batch job that only takes maybe 10 seconds per job. If the scheduling frequency is once per minute, the instances are idle for the majority of the time. Is there a way to increase the scheduling frequency, or is this just not a good use case for Batch?

Thanks,
Tim"
AWS Batch	"Re: Startup time?
I am having the same issue here.  My cluster is about 10% utilized as jobs aren't started fast enough but the cluster is fully scaled out based on the number of submitted jobs.  Any thoughts here on how to better fully utilize the cluster?"
AWS Batch	"Re: Startup time?
I got complete silence from this post for 2 months so I just decided to pool my short-running jobs together into longer-running jobs. We first did some profiling to determine how long each type of job took, then put them together into groups that run about a half hour per job. It seems like Batch is more suited for jobs that are much longer than the scheduling interval (60 sec?).

It meant restructuring our workflow a little, but we are getting better utilization now. I just wish we could get more insight into the scheduling, or have some settings to tweak how it works."
AWS Batch	"Re: Startup time?
Hello, i have similar issues with job startup time. 
A job that take a few seconds to complete in my old setup ( ecs + sqs, see my post https://forums.aws.amazon.com/thread.jspa?threadID=258172&tstart=0), now in batch takes ~30 seconds or more. The task is executed in same times in both setup, but in aws batch it takes too much time to get executed from the job queue.
My jobs can takes from a few seconds to a few minutes to complete. For first case this configuration of aws batch is not good, and i need faster startup time.
I hope new settings are added to the job queue for better configure this aspect of aws batch.
In my previous setup i have a task that periodically fetch from sqs queues to start jobs, and i can tweak this aspect changing the parameter WaitTimeSeconds of sqs.receiveMessage() function."
AWS Batch	"Re: Startup time?
Greetings from the AWS Batch team,

Addressing both of the topics that you raised…

1. Time to begin execution of jobs.

First, it is important to understand that we are constantly making enhancements to AWS Batch in order to improve compute resource utilization, the speed at which jobs are scheduled, and the scale at which you can utilize the service. For example, when we launched the service, we would schedule jobs every 1 minute. We now perform these operations every 10 seconds. It is however important to differentiate that this delay is seen only if there are no Submitted jobs in the JobQueue. Batch continues to transition and schedule jobs with no delay until the JobQueue has no Submitted or Runnable jobs. Thus, AWS Batch may take longer to schedule an individual job when Jobs are submitted infrequently.  At scale, Batch can schedule jobs far more quickly.

Further, it is important to note that the AWS Batch resource scaling decisions occur on a different frequency. Upon receiving your first job submission, AWS Batch will launch an initial set of compute resources. After this point Batch re-evaluates resource needs approximately every 10 minutes. By making scaling decisions less frequently, we avoid scenarios where AWS Batch would scale up too quickly and complete all RUNNABLE jobs, leaving a large number of unused instances with partially consumed billing hours. Over time, we will continue to enhance the periodicity and algorithms used to scale compute resources in response to customer feedback and the observed efficiency of compute resource utilization.

2. Batch doesn’t schedule jobs as quickly as expected.

In order to schedule each AWS Batch Job, we currently need to invoke an API (RegisterTaskDefinition) from ECS that limits the pace at which we can schedule your jobs.  We have worked with the ECS team to address this limitation and are actively addressing the overall scheduling throughput of jobs with a goal of significantly increasing job scheduling throughput. Stay tuned for more updates. We will post to this forum as these enhancements become available.

Hope that clarifies the behaviors you have been observing!

-CH"
AWS Batch	"Re: Startup time?
Hi Guys and Girls, 

Quick idea, maybe an enhancement to the API would be to have an API call that can define a bunch of jobs 

E.g the following workflow: 
1) Call batch_submitted_started, with a ID ""project1"" 
2) Submitted the jobs with a ref to ""project1""
3) Call batch_submitted_complete with a ref to ""project1""

Once batch_submitted is called you can figure out the optimal stack, and run the jobs

Also it would be nice if there was a API call the start the jobs explicitly 

Thanks 
@macarthy"
AWS Batch	"Re: Startup time?
Today, we announced that AWS Batch is now optimized for jobs lasting as short as a few seconds.  The announcement is published at https://forums.aws.amazon.com/ann.jspa?annID=4908. 

We welcome your feedback!

Jamie

Edited by: Jamie@AWS on Aug 21, 2017 9:42 PM"
AWS Batch	"Re: Startup time?
Hi,
I've been trying aws batch - the fetch and run tutorial specifically, and it consistently takes 2-3 minutes from job scheduling to job start. What do I need to do to reduce the start delay sufficiently to make short run jobs practical? 

All I have tried so far is submitting from the AWS job console either via create or clone and using a managed compute environment.

Thanks
Alistair"
AWS Batch	"Re: Startup time?
Only way I have been able to reduce the startup time is by using a spot fleet with a minimum of 1vcpu  so that the instance stays warm. The jobs it is running on that queue can be retried if there is a spot termination.

We are looking at moving the jobs that take just a minute or less to lambda to avoid the startup costs."
AWS Batch	"Re: Startup time?
chaws wrote:
After this point Batch re-evaluates resource needs approximately every 10 minutes. By making scaling decisions less frequently, we avoid scenarios where AWS Batch would scale up too quickly and complete all RUNNABLE jobs, leaving a large number of unused instances with partially consumed billing hours. 

Hi there,

We've been trying to figure out why Batch jobs that trigger cluster expansion from zero instances to one instance sometimes happen quickly and other times take up to fifteen minutes. We've figured out that if a new job comes in shortly after the last instance in a compute environment has shut down it takes over 10 minutes for Batch to decide to add a new instance to the CE. From what you've written above this is intentional behaviour to save billing hours, however EC2 now bills by the second instead of the hour. Can we get the frequency of scale up events increased?

Thanks,
Max

Edited by: reconfigureio on Mar 19, 2018 9:55 AM"
AWS Batch	"Re: Startup time?
Second that. Or perhaps, allow a Compute-Environment-level setting that allows us to increase the frequency of resource re-scaling. It would help us both on the way up (wrt responsiveness) as well as down (wrt wasted)."
AWS Batch	"Re: Startup time?
Responding well after the fact, but have you considered moving to Lambda? For short running jobs it seems preferable."
AWS Batch	"Re: Startup time?
10 minutes is pretty ridiculous for recalculating after a job is running. 1 min would make a lot more sense if you really want to support short running jobs. It takes 10 minutes to figure out it needs to spin up more instances if the additional jobs get queued even 30 seconds after the first one. This results in 10 minute waits for 10 second jobs while there are plenty of idle instances waiting to get spun up."
AWS Batch	"Re: Startup time?
I'd like to highlight this for the posts that came after this comment: this is current Batch behavior and we're working on a fix to change it. Right now you're seeing 10 - n, where n is the time period after your last job exited. If a job comes in shortly after the last jobs have exited, Batch should make an instant scaling decision rather than its normal 10 minute evaluation."
AWS Batch	"Re: Startup time?
Hi, is there any update on this? Surely it must be possible to schedule this delay to be shorter than 10 minutes, or even allow the user to be responsible for setting that delay?
Best, B."
AWS Batch	"CannotInspectContainerError: Could not transition to inspecting; timed out
I'm running into an intermittent issue with my AWS Batch jobs where they are seemingly running just fine, but Batch marks them as FAILED with the error CannotInspectContainerError: Could not transition to inspecting; timed out

There are no errors or issues that I see from within the docker container where my job is run so this looks like some kind of issue with the orchestration/monitoring happening with AWS Batch.  The jobs themselves are quite simple and download a bit of data from S3, do some processing/transformations, and then put the modified data back to S3.  I'm using all managed Batch compute environments on Spot instances with very basic settings.

Is there a way to resolve this problem?  Despite the jobs working properly it's highly disruptive to have them marked as FAILED"
AWS Batch	"AWS Batch + Docker with cap-add and devices
I have a docker image that I run with an added device and linux capabilities, i.e. docker run --device <device name> --cap-add SYS_ADMIN <image>. I would like to run the image in AWS batch, but I don't see a way of passing eithe rthe --device or --cap-add options to AWS batch. Is there a way to do so?

Thanks very much for any help!
Larry."
AWS Batch	"Managed compute enviroment - Update user data
Hi all,

Is it possible to update the user data created by default on a managed compute environment ?

Appreciate some one's help."
AWS Batch	"AWS Batch - Container override - How to pass a sequence of statements
Hi,
I am running my first batch job through console using amazon/linux as my image. 

Docker image has command set as CMD ""/bin/bash""
How do I pass a script through command override during Job submission.

When I pass command as yum -y install unzip aws-cli --aws s3 ls  I get an error also I tried using && no luck. How do I combine multiple statements and pass via command override to run it in a sequence.

Appreciate some ones help.

Edited by: Sara99 on Feb 4, 2019 11:05 AM"
AWS Batch	"AWS Batch Memory/Swap Handling
1) I have a use case that calls for a moderate amount of memory and a large amount of swap space to be allocated for each job.

According to the documentation, the Batch Job Definition ""Memory"" parameter maps to the Docker hard process memory limit (upon hitting the limit the docker process is immediately killed)  

There does not appear to be any current mechanism to allow a larger virtual memory/swap space an cap the physical memory usage.  (i.e. a mapping to the Docker memory-swap parameter, etc).  Is this really not possible or did I miss something in the documentation.

2) How does the memory parameter play into the scheduling/scaling algorithm?  If I specify a Job Memory parameter a N and a EC2 instance type that has N of memory, with this mean one allocated machine per job instance?  If I schedule a large value for N, will the ""optimal"" selection automatically select a memory optimized instance over something with more compute resources?"
AWS Batch	"Re: AWS Batch Memory/Swap Handling
Bump."
AWS Batch	"Re: AWS Batch Memory/Swap Handling
Same problem here."
AWS Batch	"Using S3 from a batch container?
Has anyone tried using S3 from within a batch container?  Is there any easy way to do this that does not require starting with a barebone linux, installing the CLI and giving it my credentials?  Seems like the credentials are exposed if I save them in the image but also if I give them to the batch as environment variables?

Any suggestions for a good way to do this?

p.s. The idea is to have a batch container that reads a file from S3, does some analysis, and then posts a result back to S3"
AWS Batch	"Re: Using S3 from a batch container?
Yes, the only way appears to be to install AWS CLI or an AWS SDK for your favorite language. As to credentials, you can pass them through a role that you assign to the job definition. Baking them in or passing as environment variables is much less secure. Permissions assigned through roles will be successfully picked up by CLI/SDK as environment variables only visible inside the container, and they handle key rotation etc so you don't need to worry about them.

We've successfully created containers that pipe data from S3 to an application's standard input, and stream the results back to S3 to its standard output, all using AWS CLI. This way (for applications that support ""streaming"" I/O), you don't need any external storage attached to that container.

That being said, if you need  to run just a shell script job without any external dependencies, I believe it comes pre-installed with AWS CLI and you don't need to create a container for that.

Edited by: denvlad on Jan 25, 2017 3:53 PM"
AWS Batch	"Re: Using S3 from a batch container?
Greetings from the AWS Batch team!

Denvlad's response is correct.  It is indeed possible to make calls to S3, DynamoDB, and any other AWS service from within your AWS Batch jobs.  As Denvlad suggests, you can do this by submitting an AWS Batch job that references a container image with the AWS CLI installed.  By specifying an IAM role for your job at submission, your job will gain access to temporary credentials that enable it to call other AWS services such as S3.  In this manner, you can submit AWS Batch jobs without having to embed your credentials within your container image, parameters, or environment variables.

You can refer to the following ECS document for more information on this topic:
http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html

Best regards,

Jamie"
AWS Batch	"Re: Using S3 from a batch container?
I have the same concern, I need to sync huge amount of server access logs from an s3 bucket, parse them and upload it back and I haven't found better way than schedule these parses using AWS batch on its ec2 instances. What is the other way you are writing about?"
AWS Batch	"Jobs stuck in runnable
Like a lot of other threads on this forum, I've a problem with jobs being stuck in Runnable status.

A job was stuck over the weekend, on infrastructure that has run jobs successfully in the past.

I deleted that setup and defined a new one, via a Terraform script, with identical parameters.

I've checked the usual things (VPC internet access, EC2 limits etc) but all looks ok. An EC2 instance is launched, but the job does not execute.

Job definition is:

{
    ""jobDefinitionName"": ""redacted-manifest-dev"",
    ""jobDefinitionArn"": ""arn:aws:batch:us-east-1:219829223284:job-definition/redacted-manifest-dev:2"",
    ""revision"": 2,
    ""status"": ""ACTIVE"",
    ""type"": ""container"",
    ""parameters"": {},
    ""containerProperties"": {
        ""image"": ""219829223284.dkr.ecr.us-east-1.amazonaws.com/redacted-dev:latest"",
        ""vcpus"": 1,
        ""memory"": 1024,
        ""command"": [],
        ""jobRoleArn"": ""arn:aws:iam::219829223284:role/RedactedBatchJobRole"",
        ""volumes"": [],
        ""environment"": [
            {
                ""name"": ""spring.profiles.active"",
                ""value"": ""qa""
            }
        ],
        ""mountPoints"": [],
        ""ulimits"": []
    }
}"
AWS Batch	"Re: Jobs stuck in runnable
Sorry you've hit this issue. Are you still experiencing this?"
AWS Batch	"AWS jobs running but remain in starting state
Hi,

I can see that jobs I submit are actually running and they move from one state to another as expected.

But after they reach starting, they don't show state as running even though they are actually running. Once finished they move to failure/success but takes lot longer to do so even after finishing.

Also, I have noticed that it has stopped running these jobs in the creation order and now they seem to just run randomly.

Can someone please help?

Thanks"
AWS Batch	"Re: AWS jobs running but remain in starting state
Hi! We're looking into this issue. Although I don't have details for your particular case, this appears to be the result of an issue we have identified and are working on a fix for shortly. I will update when we have more details. In the meantime, please file a support ticket for tracking purposes."
AWS Batch	"Re: AWS jobs running but remain in starting state
Thanks awsatstephen.

I can see that issue with state transition is better now. Today we ran 6 jobs and they all transitioned through different states properly.

Problem with ordering of jobs is still ongoing though. I submitted 6 jobs but it ran in this order  1,3,6,2,5,4

Thanks for your help."
AWS Batch	"Re: AWS jobs running but remain in starting state
One thing to note: Batch doesn't enforce strict FIFO in all cases, especially if your jobs are of different size - sometimes it takes longer to download the image for one job than others. Of course, Batch will respect your dependencies and make sure they are processed appropriately."
AWS Batch	"Jobs stuck in RUNNABLE state after EBS volumes are KMS encrypted
My Requirement is to create a AWS Batch Compute Environment and run jobs with docker image.

Of course, I have created the following

Job Definition
Job Queue
Docker Image is pushed to ECR 
Batch Service Role
EC2 Instance Role

and hooked them all up.

I'm using AMI ""amzn-ami-2018.03.j-amazon-ecs-optimized (ami-06bec82fb46167b4f)""
But we have a requirement to have All EBS volume be KMS encrypted, so I had to copy AMI ""amzn-ami-2018.03.j-amazon-ecs-optimized"", and check ""Encrypt target EBS snapshots"" and create a new AMI to get EBS volume KMS encrypted.

If I specify AMI ""amzn-ami-2018.03.j-amazon-ecs-optimized (ami-06bec82fb46167b4f)"", it works.
If I specify copied AMI with EBS volume KMS encryption enabled, then all jobs are stuck in ""RUNNABLE"" state in the batch job queue.
If I recreate Compute Environment with AMI ""amzn-ami-2018.03.j-amazon-ecs-optimized (ami-06bec82fb46167b4f)"", then again it works.
Am using my own KMS key not the default KMS key that AWS provides for aws/ebs.

Can anyone help me with this?

Edited by: ashokr on Jan 22, 2019 6:18 AM"
AWS Batch	"Using AMI for aws batch
Hi,

I was trying to get started with AWS Batch. I am using the free tier, so am restricted to t3.micro (1 CPU, 1GB). I created an AMI using one of the aws optimized images (https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html), and then use this when I create the Compute Environment (https://docs.aws.amazon.com/batch/latest/userguide/create-compute-environment.html). I then tried to run Batch, but it has been stuck in the 'runnable' state and doesn't seem to progress. What am I doing wrong?

Let me know if there is any other information that I can provide.

thanks!"
AWS Batch	"AWS Batch Job Definitions Missing from Dropdown Menu
Hi, just recently I've noticed an issue in which the job submission page's job definition drop down menu is missing several values from the list despite them being present in the job definitions. The only way to start jobs that are missing from that page is to open the job definitions screen, selecting the job definition, and then opening the action menu to click submit job. Has anyone else experienced this?"
AWS Batch	"Container Overrides error
Hi,
I'm receiving Container Overrides length must be at most 8192 while running one of my Batch Job, even though I haven't provided any container-override options. At first I thought that length of command was exceed the mentioned size after parameters are replaced, but its not, it adds up to 1200 characters. This error comes with a specific set of parameters only, rest seems to work fine.
I'm not able to understand the exact issue. Can someone help me here?
I'm triggering job via botocore3 on python."
AWS Batch	"CLIENT_ERROR - Instance profile arn does not exist
Hi,
I am getting invalid compute environment with error as ""CLIENT_ERROR - Instance profile arn does not exist"". The role also exists. Previously, create compute environment api was accepting ""role"" arn in instance profile field. Was anything changed recently?
If I create new compute environment using GUI, I now can see ""instance-profile"" arn in the instance role field.

Thanks,
Nithin Venugopal"
AWS Batch	"AWS Batch can't run 2 different compute environments at the same time
I've been struggling to find out why my batch operations will sit for 10 minutes at a time with jobs RUNNABLE but ""Desired vCPUs"" of 0. Looking through CloudTrail logs it seems to me that Batch is waiting for all instances of one compute environment to be terminated before starting anything in another compute environment. 

I have 2 compute environments. One for GPU instances, and one for CPU-only. My process starts off by submitting a job to the GPU environment, and then immediately 10 jobs to the CPU-only environment. There is no dependency between these jobs. 

As expected the Batch dashboard shows 1 RUNNABLE GPU job and 10 RUNNABLE CPU jobs. Desired vCPUs for the GPU environment immediately jumps to 8 but the CPU environment stays at 0. Desired vCPUs for the CPU compute environment doesn't change until about 10 minutes after the GPU job has finished (when auto scaling finally shuts everything down). 

In CloudTrail this is what I see.

GPU job is submitted:
4:17:30 SubmitJob
4:17:35 RunTask
Followed by the CPU-only jobs:
4:17:44 SubmitJob
...x 10
4:17:45 RunTask
...x 10
The GPU environment starts up:
4:17:58 UpdateAutoScalingGroup
4:18:26 RunInstances
4:18:58 CreateTags
4:21:02 RegisterContainerInstance

Up to this point RunTask is repeated 11 times (for 11 jobs) every 30 seconds and failing with 
InternalFailure (""An unknown error occurred:). As soon as RegisterContainerInstance happens the RunTask for the single GPU job finally succeeds:
4:21:07 RunTask
4:21:08 BatchGetImage
4:22:29 CreateLogStream

At this point the GPU job finally runs. It's just a test job and so completes in 2 seconds. Meanwhile in CloudTrail there are still the 10 RunTask attempts every 30 seconds for the CPU jobs. Those attempts keep happening again and again until finally the auto scaling group is created:
4:29:02 UpdateAutoScalingGroup
As concurrently the GPU compute environment is terminated:
4:29:03 DeregisterContainerInstance
4:29:03 TerminateInstanceInAutoScalingGroup
4:29:04 TerminateInstances
And then the CPU environment starts to come alive:
4:29:17 RunInstances
4:29:49 CreateTags
4:30:14 RegisterContainerInstance
4:30:17 RegisterContainerInstance
4:30:22 BatchGetImage
...x 10
4:30:58 CreateLogStream
...x 10

And finally, at 4:30:59, 13 minutes after they were submitted, these jobs start. 

I also submit groups of jobs where the GPU jobs depend on the CPU ones – so the CPU environment starts first, and the GPU environment should start once the CPU jobs complete. But again the same things happens: the CPU jobs finish and then around 10 minutes later when the CPU environment is terminated the GPU environment finally begins to start up.

What's going on here? I can't find any AWS Batch or AutoScaling limits that might be affecting this. Since the CPU and GPU environments use different instance types I'm not hitting an instance type limit. 

Any suggestions?"
AWS Batch	"Re: AWS Batch can't run 2 different compute environments at the same time
Hi! Are you still seeing this issue?

Can you help me out a little with your configuration? Are you using Spot or On-Demand? And are both of these CEs attached to the same job queue?"
AWS Batch	"AWS Batch doesn't respect Max vCPU setting
hi,

i'm using a cluster configured for 16 vCPUs however, as can be seen in the images, AWS Batch decided to use 24 vCPUs.

link to evidences: https://imgur.com/a/waXPiWv

the cluster is created via CloudFormation.

what is going on? I expected Max. vCPUs to be an upper limit to compute usage.

looking forward to the explanation!"
AWS Batch	"AWS Batch - DockerTimeoutError: Could not transition to created;
Hi,

We've scheduled a lot of batch processes (4000+) under managed environment with C5 instance type.
We start saw a lot of failed jobs (80%) with this error:

DockerTimeoutError: Could not transition to created; timed out after waiting 4m0s


What should we do to solve this issue?

Thank you,
Enrico"
AWS Batch	"Re: AWS Batch - DockerTimeoutError: Could not transition to created;
Dear  Enrico,

The error ""DockerTimeoutError: Could not transition to created; timed out after waiting 4m0s"" occurs mostly when there is high I/O on the EBS volumes used by the container instance. I would suggest you to monitor the volumes and consider trying the Provisioned IOPS SSD (io1) volume type that has Highest-performance SSD volume for mission-critical low-latency or high-throughput workloads. 

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html

Cheers,
Nitheesha"
AWS Batch	"AWS Batch - Non Docker Approach
Hi,

I see on AWS batch help page somewhere, it was mentioned that we can provide a our application as a zip file in a job and our application will be directly executed inside an EC2 instance without the need of a docker instance.
I have not seen a single example of such kind on internet. Is anyone aware if there is a way to use AWS batch without using a docker image?

Definition of Job from AWS official documentation:
A unit of work (such as a shell script, a Linux executable, or a Docker container image) that you submit to AWS Batch. 

-
Thanks,
Sid

Edited by: Siddharthvij on Nov 25, 2018 11:29 PM"
AWS Batch	"Re: AWS Batch - Non Docker Approach
Dear Sid,

AWS Batch supports the job that can executed as a Docker container. AWS Batch uses Amazon ECS to execute containerized jobs by making use of  ECS Agent installed on the EC2 instance(Container instance). 

For example- when you create a Batch Compute environment, an ECS cluster is created in the backend. AWS Batch only supports containerized applications. It is not possible to run non-containerized workloads on AWS Batch. 
https://aws.amazon.com/batch/faqs/

For you use-case of uploading an application bundle and running your app on EC2 instances, please consider looking into our Elastic beanstalk service-
https://aws.amazon.com/elasticbeanstalk/

Cheers,
Nitheesha

Edited by: Nitheeshaataws on Dec 12, 2018 10:34 AM"
AWS Batch	"Batch Job definition revision limit
Hi,

We're planning on using Batch for a process that can get updated multiple times a day. We'd like each update to create a new Batch job definition revision. What is the the limit on job definition revisions? We're not seeing one in the documentation.

Thank you!
Andrey"
AWS Batch	"Re: Batch Job definition revision limit
Dear Andrey,

Batch uses ECS in the backend to create job/task definitions and revisions. According to ECS documentation, the limit on number of revisions per task definition family is 1,000,000. It is a hard limit and cannot be increased. 

https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service_limits.html

Cheers,
Nitheesha"
AWS Batch	"Delete Compute Environment failed - Invalid State - Access Denied
I just want to force delete the ComputeEnvironments.

I set up a couple of compute environments through a cloud formation stack, and didn't quite understand what I was doing when I was cleaning up resources, so have probably erroneously removed some dependent resource.

Essentially, I have 2 invalid compute environments that I can't remove through CloudFormation, or manually through the Batch dashboard. When I click Delete, it moves to status deleting, and then back to Invalid, with: CLIENT_ERROR - Access denied

When I try to update, I select Create Service Role, I get the error, so maybe that's related: There are unresolved issues with your inputs. Please review the form and try again."" The VPC Id is blank, and I get a console error saying ""Cannot read property 'vpcId' of undefined"
AWS Batch	"Re: Delete Compute Environment failed - Invalid State - Access Denied
Hi there,

You are right that the CE is in invalid due to missing service role. The error ""CLIENT_ERROR - Access denied"" appears when the service role that is associated with your CE is deleted before the CE is deleted.

Perform a describe operation as follows-
aws batch describe-compute-environments --compute-environments <your-compute-env-name> --region <region>

Note down the service role name from the output and create a service role in your IAM with same name with trust relationship and permissions as described in the document below-
https://docs.aws.amazon.com/batch/latest/userguide/service_IAM_role.html

After the service role is created, validate the compute environment and proceed to deletion.

Cheers,
Nitheesha

Edited by: Nitheeshaataws on Dec 12, 2018 10:12 AM"
AWS Batch	"If parent job fails is there any way to run child job in aws batch
Hi ,

In AWS batch if parent job fails then automatically child job also fails, Is there any way to make child job success if parent fails."
AWS Batch	"No space left on device
I'm experiencing a very strange ""No space left on device"" error when using a custom AMI for AWS Batch. 

The AMI was created starting from ECS-Optimized Amazon Linux AMI 2017.03 to which was added a third EBS volume of 1000GB. The Docker storage has been extended as explained here http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-ami-storage-config.html i.e.: 
sudo vgextend docker /dev/xvdb
sudo lvextend -L+1000G /dev/docker/docker-pool


However when launching a few jobs I get quite immediately the following error message: 
.command.run.1: line 50: cannot create temp file for here-document: No space left on device
tee: .command.err: No space left on device
mkdir: cannot create directory ‘fastqc_SRR3192434_logs’: No space left on device


Logging in the instance it seems to be enough space: 

$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda1      7.8G  1.2G  6.5G  16% / 
devtmpfs         32G   96K   32G   1% /dev
tmpfs            32G     0   32G   0% /dev/shm
 
$ sudo vgs
  VG     #PV #LV #SN Attr   VSize    VFree  
  docker   2   1   0 wz--n- 1021.99g 224.00m
 
$ sudo lvs
  LV          VG     Attr       LSize    Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  docker-pool docker twi-aot--- 1021.73g             2.46   11.87  
 
$ docker info 
Containers: 8
 Running: 3
 Paused: 0
 Stopped: 5
Images: 3
Server Version: 17.03.2-ce
Storage Driver: devicemapper
 Pool Name: docker-docker--pool
 Pool Blocksize: 524.3 kB
 Base Device Size: 10.74 GB
 Backing Filesystem: ext4
 Data file: 
 Metadata file: 
 Data Space Used: 28 GB
 Data Space Total: 1.097 TB
 Data Space Available: 1.069 TB
 Metadata Space Used: 3.031 MB
 Metadata Space Total: 25.17 MB
 Metadata Space Available: 22.13 MB
 Thin Pool Minimum Free Space: 109.7 GB
 Udev Sync Supported: true
 Deferred Removal Enabled: true
 Deferred Deletion Enabled: true
 Deferred Deleted Device Count: 0
 Library Version: 1.02.135-RHEL7 (2016-11-16)
Logging Driver: json-file
Cgroup Driver: cgroupfs
Plugins: 
 Volume: local
 Network: bridge host macvlan null overlay
Swarm: inactive
Runtimes: runc
Default Runtime: runc
Init Binary: docker-init
containerd version: 4ab9917febca54791c5f071a9d1f404867857fcc
runc version: 54296cf40ad8143b62dbcaa1d90e520a2136ddfe
init version: 949e6fa
Security Options:
 seccomp
  Profile: default
Kernel Version: 4.9.51-10.52.amzn1.x86_64
Operating System: Amazon Linux AMI 2017.09
OSType: linux
Architecture: x86_64
CPUs: 16
Total Memory: 62.91 GiB
Name: ip-172-30-2-110
ID: VEEF:VQDY:Z72J:NY25:YIMO:BG7Z:J5EH:ZBXU:IKLX:OJN2:F7GM:EY5Q
Docker Root Dir: /var/lib/docker
Debug Mode (client): false
Debug Mode (server): false
Registry: https://index.docker.io/v1/
Experimental: false
Insecure Registries:
 127.0.0.0/8
Live Restore Enabled: false


Any idea what's wrong ?

Edited by: paulecci on Oct 28, 2017 5:00 AM"
AWS Batch	"Re: No space left on device
Could you check /etc/sysconfig/docker-storage in regards to DOCKER_STORAGE_OPTIONS ?

We've increased container size using e.g. --storage-opt dm.basesize=65G ."
AWS Batch	"Re: No space left on device
The current content of file /etc/sysconfig/docker-storage is: 

DOCKER_STORAGE_OPTIONS=""--storage-driver devicemapper --storage-opt dm.thinpooldev=/dev/mapper/docker-docker--pool --storage-opt dm.use_deferred_removal=true --storage-opt dm.fs=ext4 --storage-opt dm.use_deferred_deletion=true""


Edited by: paulecci on Oct 30, 2017 5:21 AM"
AWS Batch	"Re: No space left on device
Adding the `--storage-opt dm.basesize=200GB` option solve the problem. Thanks for pointing in the right direction.

Edited by: paulecci on Oct 30, 2017 1:32 PM"
AWS Batch	"Re: No space left on device
Hello,

I am using a p3.8xlarge instance.
I could not find  /etc/sysconfig/docker-storage.
Could you help me find it? 

Thanks.

Edited by: Aldebaran33 on Dec 10, 2018 6:43 PM"
AWS Batch	"Can't terminate pending jobs or queue
I have 87 pending jobs in a queue that I cannot terminate or cancel.  I've tried just deleting the queue and now it's stuck in a deleting state.    I have basic AWS support and therefore can't file a support ticket.   

The job queue name is ""dna-pipeline"" and the account number is:  623119755067"
AWS Batch	"Re: Can't terminate pending jobs or queue
Greetings from the AWS Batch team!

We have investigated the issue, identified the root cause, and are working on fix that will prevent this error from happening again. Please DM me with your email address  and I can provide further information. Alternatively, I can email the address associated with your account.

-Jamie"
AWS Batch	"Re: Can't terminate pending jobs or queue
Jamie,

Has there been any update on this?  I still can't delete the pending jobs. 

Thanks."
AWS Batch	"Re: Can't terminate pending jobs or queue
Jamie and the Batch team helped clear up my issue."
AWS Batch	"Re: Can't terminate pending jobs or queue
I have this same issue right now. Can someone help me out?"
AWS Batch	"Re: Can't terminate pending jobs or queue
jmeran85,

I took a look at your account and don't see any RUNNABLE or RUNNING jobs.  Could you open a support case with a more detailed description of the behavior that you are seeing?  Alternatively, you can DM me so that we can follow-up directly.

Best regards,

Jamie"
AWS Batch	"Re: Can't terminate pending jobs or queue
Thanks Jamie,
I sent you a PM. The issue I am having is the my job queue is permanently in a DELETING state and has two jobs in the STARTING state. It seems like I can't create any new Batch Compute Environments as well."
AWS Batch	"Re: Can't terminate pending jobs or queue
How was this issue cleared up? I have had it happen twice in the last week and I have not been given any direction about how to prevent this issue from happening again.
Can one of your engineers explain what is happening?"
AWS Batch	"Re: Can't terminate pending jobs or queue
I am also having trouble canceling some jobs. For example, 0e5db34c-5214-49fc-b4e3-4cce46ae87cb is currently in RUNNABLE and cancel-job gives no errors nor effect.

Is this a known AWS issue?

Thanks.

Edited by: gregfr on May 29, 2018 10:24 PM"
AWS Batch	"Re: Can't terminate pending jobs or queue
Greetings from the AWS Batch team,

The initial issue with jobs stuck in PENDING has been fixed since about two months now. We have monitored a few accounts to ensure this has not occurred again. We apologize for the inconvenience this has caused.
The issue itself was caused due to an edge case triggered when jobs with dependencies were submitted close to each other (under 100 milliseconds apart from each other).

Please let us know if you still have a job stuck in PENDING and we can take a look at it. Ideally, if dependencies have been satisfied a job should not be stuck in PENDING.

Regarding your issue, do you have other jobs in the queue that were submitted before this job? The Batch scheduler handles cancellations in a FIFO order, and therefore it's possible that your job has been stuck in RUNNABLE, behind other jobs that have not been cancelled.

Regards,
CH"
AWS Batch	"Re: Can't terminate pending jobs or queue
Thanks for your reply.

I just experienced this issue again and I do in fact have jobs that are in a STARTING submitted before the jobs I cancelled. However, the array job ""thinks"" the child jobs are STARTING (i.e. the tabs on https://us-west-2.console.aws.amazon.com/batch/home?region=us-west-2#/jobs/6f979bdf-699c-4df3-97fd-b1ddedb397aa say 4 child jobs are STARTING) but when I try to drill down (i.e. click on the tabs) the jobs are actually RUNNING.  (See attached screenshot)

So maybe this is a bug with array job status which is preventing later jobs from being cancelled?"
AWS Batch	"Re: Can't terminate pending jobs or queue
When describeJobs is invoked for an array job, the statusSummary attribute in ArrayProperties is eventually consistent data and provides a delayed view of the overall status of the array job. When listJobs is invoked for the same array job for a particular job status, the jobs returned are a more accurate representation of the number of child jobs in that status. The console uses describeJobs to show the overall status of the job, and it uses listJobs when a particular status is clicked.

https://docs.aws.amazon.com/batch/latest/APIReference/API_DescribeJobs.html
https://docs.aws.amazon.com/batch/latest/APIReference/API_ListJobs.html"
AWS Batch	"Re: Can't terminate pending jobs or queue
Thanks for the clarification. Is this preventing the jobs from being cancelled?"
AWS Batch	"Re: Can't terminate pending jobs or queue
I have this same issue right now...

I have 536 pending jobs in a queue that I cannot terminate or cancel.
I've tried just deleting the queue and now it's stuck in a deleting state.
I have basic AWS support and therefore can't file a support ticket.

The account number is: 314913702427

Edited by: ktakai on Jul 2, 2018 2:18 AM"
AWS Batch	"Re: Can't terminate pending jobs or queue
currently having the same issue, i filed a support ticket we'll see."
AWS Batch	"Re: Can't terminate pending jobs or queue
I imagine this is the correct place to pile on with, ""I have this problem too."".

I'm seeing the same issues described above. I have 23 jobs that have been running for 5 days now. The last logs in any of the jobs were from 5 days ago. I cannot terminate or cancel the jobs. And the queue has been stuck in ""deleting"" for ~24 hours. 

There's been 64 vCPU's set as ""desired"" for the entire time so I imagine that I'm paying for compute that isn't happening.

Similarly with the above comment, I am not allowed to create a support ticket. Although I will be creating a billing ticket to get these charges removed.

I've seen jobs stuck in running before but killing the queue has resolved the issue in a few minutes, but not this time."
AWS Batch	"Re: Can't terminate pending jobs or queue
After ""only"" 7 days. The queue finally deleted."
AWS Batch	"Integration of AWS License Manager and Batch
Hi,
Is there any integration with AWS License Manager and Batch and if so how can we use this? Basically jobs should not be scheduled when we are out of licenses

Thanks,
Nithin"
AWS Batch	"How to set shm_size for batch jobs?
ECS has added the shm_size option, to change what kind of /dev/shm size the docker container has access to.

How do I do this for aws batch jobs?

For context:  I'm trying to optimize a single node intel MPI job that uses shm as a communication strategy between MPI processes.  This job crashes with the default 64M /dev/shm

I spun up an ECS optimized instance and ran the same fetch_and_run job with a larger shm and it does not crash."
AWS Batch	"Re: How to set shm_size for batch jobs?
I have one work around for the time being.  But running docker in privileged mode is not a best practice as far as I can tell.

Set the batch job privileged option to true

In a script once docker is running:
umount shm
mount -t tmpfs -o rw,nosuid,nodev,noexec,relatime,size=2G shm /dev/shm"
AWS Batch	"Batch Jobs stuck in runnable suddenly
So we have set up a lambda that uses two dockers, until a few days ago everything worked fine. No on Frankfurt suddenly all jobs in Batch are stuck in Runnable. Not sure what has happened, has there been a change or something? Or is the cluster heavily used now or something? Any answers would help. This system uses AMI, could that be the problem? I uses AMI to mount a volume but is otherwise created from the proper Instance type as per documentation.

Thanks in advance

Edited by: patbj363 on Nov 1, 2018 3:05 AM"
AWS Batch	"Re: Batch Jobs stuck in runnable suddenly
I am also facing the same issue. Runs that were moving through rather quickly with 50% spot instance seem to be stuck in runnable now. I've kicked off two runs and will let them run over night. I'll post back the results."
AWS Batch	"Tagging for Spot instances in managed environments
Hi Team,

I've noticed that tagging is currently not supported when creating managed compute environments with Spot provisioning model. Applying tags could help us better distinguish various instances, e.g. when accessing them over SSH. Is it something that could be enabled in the future?

Thank you"
AWS Batch	"Re: Tagging for Spot instances in managed environments
+1"
AWS Batch	"Re: Tagging for Spot instances in managed environments
I would also like to see this feature for the purpose of keeping track of billing to various projects."
AWS Batch	"Re: Tagging for Spot instances in managed environments
Thanks Denvlad.

We recognize the need for this and have this on our product roadmap.

-Jamie"
AWS Batch	"Re: Tagging for Spot instances in managed environments
Hi Jamie,

When can we expect this feature?

Best,
Nithin Venugopal"
AWS Batch	"Re: Tagging for Spot instances in managed environments
Support for tagging of Spot instances was announced in October 2017.

Ref: https://aws.amazon.com/about-aws/whats-new/2017/10/tag-your-aws-batch-spot-managed-compute-environments/"
AWS Batch	"AWS Batch Computer Hardware and auto submit job
Hi. I'm quite new to batch systems in general, but writing a little paper on pricing, AWS Batch compared to Azure Batch and these cloud based systems compared to buying hardware and running batch jobs locally. It's for a university subject in cloud computing. 

I can't seem to find out what hardware the cheapest computers to run basic batch jobs use.
Also, if I were to run a batch job on a local batch system and the workload got so big I wanted to let AWS Batch take care of some of it, would it be possible to schedule a batch job automatically, or do I need to write a configuration for each time a job ought to run in AWS Batch?

Sorry if it was unclear what I meant. Again, I'm quite new to using AWS Batch. Thanks.

Edited by: Kolbein on Nov 6, 2018 4:35 AM"
AWS Batch	"AWS Batch instance not scaling down
I have been running with 48 max vCPUs. AWS created 3 ec2 instance ""A""-m4.large(2cpus), ""B""-m4.xlarge(4cpus), ""C""-c4.2xlarge(8cpus) for my jobs. For the last 3 jobs I have left running, 2 jobs are running in A, 1 job in running in B. I suppose C should shut itself down since it has no task, however it didn't. 

I don't see any error in ecs-agent log, just don't know why the desired vCPUs stucked at 14 and didn't scale down. Any inputs/suggestions will be appreciated, thank you.

Edited by: Roo on Jul 10, 2018 6:33 AM"
AWS Batch	"Re: AWS Batch instance not scaling down
Revisiting my post, surprisingly seems no one ever had similar issues? 

In short, the questions is simple, when will the ""Desired vCPUs"" value to changed/triggered to reflect the latest ""number of jobs"" to be processed? 
Thanks in advance for any inputs !!

Edited by: Roo on Oct 25, 2018 7:14 PM

Edited by: Roo on Oct 25, 2018 7:29 PM"
AWS Batch	"Re: AWS Batch instance not scaling down
similar issues been posted by someone else 6 months ago.
https://forums.aws.amazon.com/thread.jspa?messageID=832525&tstart=0"
AWS Batch	"Re: AWS Batch instance not scaling down
I'm having the same issue. Are you using the latest ECS agent?"
AWS Batch	"Re: AWS Batch instance not scaling down
Tks, now I got company. I can't provide the exact agent version I am using since I only use Batch service once a while.  But it is fairly new since I rebuild it with latest ECS optimized AMI few months ago."
AWS Batch	"Batch Spot Fleet issue
I think there may be a bug or I am misunderstanding how Batch interacts with Spot Fleet. When choosing ""optimal"" as the compute type. Batch selects a couple of instances that it thinks suit the workload. However even with the bid percentage set to 100% of the On Demand price, the Max price  of my spot fleet request appears to be set to the On Demand price of the first instance in the list of Instance Types it requests

That means it fails to get any capacity for the dozen or so instance types it tries first with a launchSpecUnusable error.
Spot bid price is less than Spot market price`

It then manages to launch an instance (the cheapest one)
Which is not necessarily the most suitable instance. Does anyone have any ideas? Obviously I could specify an exact instance type rather than ""optimal"" but the requirements of the workload change a lot and we get better performance with the ""optimal"" strategy rather than hardcoding an instance type.

I have attached some images from the Console that illustrate what is happening.

I suppose my question is:

Is it possible to use the ""optimal"" compute environment instances setting in conjunction with Spot Fleet?

Thanks!"
AWS Batch	"Monitoring, Telemetry and logs for batch jobs
Hi Everyone,
I was wondering and very curious to know how you/your company monitor batch containers? how do you collect telemetry data? how/where do you collect/transfer logs produced by the batch jobs?

Cloudwatch unified agent seems to be a good option, but has anyone tried running from a container?"
AWS Batch	"compute environment invalid and not-deletable
I have a compute environment that no longer functions. It cannot be deleted and any jobs I start no longer run.

It says:

INTERNAL_ERROR - Failed compute environment workflow

edit: problem with the queue"
AWS Batch	"Re: compute environment invalid and not-deletable
Try and delete it via the AWS CLI. I had this problem once and deleting it from there worked fine."
AWS Batch	"Re: compute environment invalid and not-deletable
deleting the queue and remaking it worked"
AWS Batch	"Limit concurrent execution for a job definition
Hi,

is there a way to limit concurrent execution of jobs based on their job definition.
In my case, I would like at most one instance of a given job definition to run at a same time.

Is there a way to do that ?

Best regards,

David"
AWS Batch	"FIFO job queues
When I submit more jobs than there are available available compute resources the jobs go into a RUNNABLE status while waiting for compute resources to become available.

When compute resources become available, the jobs don't appear to be selected for execution on a first in first out basis (selection appears to be random).

First in first out selection is important to us.  Are there other AWS batch users who would like first in first out selection for jobs?  Are there any plans to add first in first out selection, or is there a mechanism for doing so already?"
AWS Batch	"Re: FIFO job queues
Craig,

Batch runs Jobs in approximately the order in which they are submitted as long as all dependencies on other jobs have been met.  Are your jobs being scheduled completely randomly, or are you seeing slight deviations from pure FIFO scheduling?

http://docs.aws.amazon.com/batch/latest/userguide/job_scheduling.html

-Jamie"
AWS Batch	"Re: FIFO job queues
I submitted 14 batch jobs over a 5 minute period.

14    c1573ec1-6feb-4cf0-8431-16dff2d5c9c1    test RUNNABLE 03:57:28 pm 09/14/17
13    29ecc81a-379a-48c9-b10d-2f8ca1c86605    test RUNNABLE 03:57:13 pm 09/14/17
12    d11c067e-09a6-40c5-9c19-7b967c3ff166    test RUNNABLE 03:56:52 pm 09/14/17
11    4d867656-5221-4821-bd20-52a11df0306d    test RUNNABLE 03:56:45 pm 09/14/17
10    af9e2b34-b6d8-45b8-9dda-a63a23e80506    test RUNNABLE 03:56:39 pm 09/14/17
9      489a8a6d-c45a-4ba6-b5ae-6995b74abf49    test RUNNABLE 03:56:33 pm 09/14/17
8      3e506002-da27-4c5e-acac-725dbd67afd4    test RUNNABLE 03:56:27 pm 09/14/17
7      dc807c56-85ef-4a44-90dd-0e59116ac250    test RUNNABLE 03:56:19 pm 09/14/17
6      cce320ad-383a-4978-af2f-082e08eafd46    test RUNNABLE 03:56:13 pm 09/14/17
5      16b5db1c-4d87-4dc6-962f-e4151a6328dc    test RUNNABLE 03:54:43 pm 09/14/17
4      512bf840-d8e6-4dcd-bdd1-5b62802da7ff    test RUNNABLE 03:54:36 pm 09/14/17
3      b1fa27c0-6ccd-4a4a-ab7d-6df97506ddc1    test RUNNABLE 03:54:29 pm 09/14/17
2      b3e105d8-7b66-453a-84c9-17508067c385    test RUNNABLE 03:52:49 pm 09/14/17
1      968736be-6ad0-4aa5-b2cb-e32b9f0fc45f    test RUNNABLE 03:52:13 pm 09/14/17

All were clones bar one.

I configured compute resources to run only two jobs at a time.

Execution order was:

12, 11, 6, 13, 4, 10, 1, 7, 9, 8, 2, 3, 5, 14

Doesn't look anything like FIFO to me.

In a second test submitting another 18 batch jobs I got the following execution order:

18, 5, 10, 12, 6, 13, 8, 16, 4, 14, 11, 9, 15, 2, 1, 7, 3, 17

Edited by: craigj on Sep 14, 2017 5:28 PM

Edited by: craigj on Sep 14, 2017 6:52 PM"
AWS Batch	"Re: FIFO job queues
We are experiencing the same issue. The execution order of jobs submitted to a batch queue is far from FIFO. 
Is it possible to configure batch to use a FIFO queue? Or are there other options to work around this limitation?"
AWS Batch	"Mounting Elastic File System in Docker Container compute environment.
Hi, First, I'm sorry if you see this dup post in SO. I'm currently at odds with deciding which is the better help tool. AWS Forums or SO. Anyway...

I'm trying to get my elastic file system (EFS) to be mounted in my docker container so it can be used with AWS batch. Here is what I did:

 1. Create a new AMI that is optimized for Elastic Container Services (ECS). I followed this guide( https://docs.aws.amazon.com/batch/latest/userguide/create-batch-ami.html) to make sure it had ECS on it. I also put the mount into /etc/fstab file and verified that my EFS was being mounted (/mnt/efs) after reboot. 

 2. Tested an EC2 instance with my new AMI and verified I could pull the docker container and pass it my mount point via 

docker run --volume /mnt/efs:/home/efs -it mycontainer:latest


Interactively running the docker image shows me my data inside efs

3. Set up a new compute enviorment (https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html) with my new custom AMI that mounts my EFS on boot. 

4. Create a JOB definition File:

        {
        ""jobDefinitionName"": ""MyJobDEF"",
        ""jobDefinitionArn"": ""arn:aws:batch:us-west-2:#######:job-definition/Submit:8"",
        ""revision"": 8,
        ""status"": ""ACTIVE"",
        ""type"": ""container"",
        ""parameters"": {},
        ""retryStrategy"": {
            ""attempts"": 1
        },
        ""containerProperties"": {
            ""image"": ""########.ecr.us-west-2.amazonaws.com/mycontainer"",
            ""vcpus"": 1,
            ""memory"": 100,
            ""command"": [
                ""ls"",
                ""/home/efs"",
            ],
            ""volumes"": [
                {
                    ""host"": {
                        ""sourcePath"": ""/mnt/efs""
                    },
                    ""name"": ""EFS""
                }
            ],
            ""environment"": [],
            ""mountPoints"": [
                {
                    ""containerPath"": ""/home/efs"",
                    ""readOnly"": false,
                    ""sourceVolume"": ""EFS""
                }
            ],
            ""ulimits"": []
        }
    }

5. Run Job, view log

It does not list anything in my EFS. That tells me that the container is mounting it in the folder, but nothing is in the EFS. Is the Host container not mounting my drive before it runs the docker container?"
AWS Batch	"Batch leaves jobs in ""Starting"" state
For the past two days, most of the batch jobs I submit are stuck in the ""Starting"" state and never proceed to Running/Succeeded/Failed.

I logged into the ECS instance and saw in the log file:
2018-10-11T17:23:20Z [WARN] DockerGoClient: failed to pull image XXXX: write /var/lib/docker/tmp/GetImageBlob1111: no space left on device

Previously, when the instance ran out of disk space to pull an image, it would move the job to the ""Failed"" state with a fairly accurate error message.  For whatever reason, this is no longer happening and I just get a buildup of jobs in ""Starting""....

At the moment, my workaround is to create a custom AMI with more disk space, but I can't figure out why the jobs don't just move to ""Failed"" like they used to.  Any ideas why I don't get an error message on the batch console?

Edited by: robertctx on Oct 11, 2018 10:52 AM"
AWS Batch	"Enable docker override --memory constraint [Batch team needed]
Hi!

We're moving from our custom cluster to AWS Batch, and so far it's been working good, but there is some aspect that is stopping us:

When, in the job definition, we specify a Memory value, we understand it's an approximate value so Batch can decide wich instance we need. In our case, a certain task needs ""commonly"" 32 Gigs of RAM. However, this value is passed to docker as a constraint, so if Docker ""sees"" that any process is exceeding the max memory, it kills the process. So far so good, but the tricky part is that we're using 3rd party software and it ""sees"" that there is much more memory available. In this specific case, 8 of this jobs we're launched simultaneously, so Batch decided to use a p3.16xlarge instance, which has 488 GB of RAM, but nearly half the RAM can't be used, and even worse, when if the 3rd party program surpasses the 32 Gigs, it's killed and does not send any death signal to Batch, so even if it's not working, the instance remains up.

We would need an option like ""--oom-kill-disable"" to be added in the docker run, or another solution that enables docker container to use all the RAM available in the instance, not only the specified in the memory parameter of the job definition. Another way of solving it can be to separate memory and max memory usage. We've tried to use ulimits, but it's overriden by the --memory option of the docker run that's called by the ECS agent.

To sum up, the problem resides in this 3rd party software seeing a lot of RAM available, but it's killed by docker when it exceeds the limit which turns out to be the Memory that we put in the job definition.

Please, a quick answer will help us a lot as we would need to migrate as soon as posible."
AWS Batch	"AWS Batch tag volumes on instance creation
I am presently configuring a Compute Environment that automatically tags instances spawned for jobs. I have been able to do this successfully by setting Tags in the Compute Resources configuration, however I am noticing that volumes created for these instances are not tagged like the instances themselves. 

Is there a way to achieve this through Batch? Or will I need to look into something like a CloudWatch-based solution?

Thanks in advance for your help!"
AWS Batch	"Connection reset by peer
I am using Batch to launch multiple jobs that read the same S3 json files. I am using python3 smart_open library with botocore to access the files. Even though the jobs are identical, some of them fail, with the following errors:
 File ""/root/anaconda3/lib/python3.6/site-packages/smart_open/s3.py"", line 246, in readline
self._fill_buffer(self._buffer_size)
File ""/root/anaconda3/lib/python3.6/site-packages/smart_open/s3.py"", line 268, in _fill_buffer
raw = self._raw_reader.read(size=self._buffer_size)
File ""/root/anaconda3/lib/python3.6/site-packages/smart_open/s3.py"", line 134, in read
binary = self._body.read(size)
File ""/root/anaconda3/lib/python3.6/site-packages/botocore/response.py"", line 76, in read
chunk = self._raw_stream.read(amt)
File ""/root/anaconda3/lib/python3.6/site-packages/botocore/vendored/requests/packages/urllib3/response.py"", line 243, in read
data = self._fp.read(amt)
File ""/root/anaconda3/lib/python3.6/http/client.py"", line 449, in read
n = self.readinto(b)
File ""/root/anaconda3/lib/python3.6/http/client.py"", line 493, in readinto
n = self.fp.readinto(b)
File ""/root/anaconda3/lib/python3.6/socket.py"", line 586, in readinto
return self._sock.recv_into(b)
File ""/root/anaconda3/lib/python3.6/ssl.py"", line 1009, in recv_into
return self.read(nbytes, buffer)
File ""/root/anaconda3/lib/python3.6/ssl.py"", line 871, in read
return self._sslobj.read(len, buffer)
File ""/root/anaconda3/lib/python3.6/ssl.py"", line 631, in read
v = self._sslobj.read(len, buffer)
ConnectionResetError: [Errno 104] Connection reset by peer


It only occured when multiple jobs was running at the same time. 
I was getting the errors yesterday, but have not been able to reproduce it today. Also, today the communication with s3 seems to be quicker.

Any ideas, how the reset can occur? How can it be prevented/ recovered from?

Edited by: miromancer on Sep 27, 2018 3:03 PM"
AWS Batch	"AWS Batch and Windows Exe
Hi to All,

I'm new to AWS and I'm looking for the right cloud service that can be able to launch concurrently  many Windows Executables, that do some intensive computation. The Windows Exe is the same, but processes will use different input files. I'd like to assign each Exe to a single vCPU, because it runs intensive computation. I think that AWS Batch could suits my purpose, but I understood that AWS Batch can't execute Windows Executable or a docker containing a windows application, is this right? If yes, can you suggest me which AWS service I can use to accomplish this job?

Best Regards,

Giovanni

Edited by: gipe on Sep 25, 2018 12:29 AM"
AWS Batch	"AWS Batch Logs Naming
I am having really hard time trying to create a sensible log names for my batch jobs.
Our job setup is this way
an incoming event is parsed and based on the event a job is built from a generic job definition by changing the command and other container values and executed , all this happens in lambda.

The issue is all jobs are going to the log stream with name created from the job definition arn as group. I tried to override job definition name and job name. But, logs still go to log group from job definition arn and its hard for us to separate the job logs like this: 
ex: job def arn: arn:aws:batch:us-east-1:XXXXXXXX:job-definition/appcode-region-generic-data-prcs:4
and all logs are going to 'appcode-region-generic-data-prcs/default/stream'  in cloud watch. Is there a way to change this?
Ideally i want it based on jobname or at least job definition name that i can customize.

ex: in   /aws/batch/jobappcode-region-generic-data-prcs/default/stream
Some logical naming for either jobappcode-region-generic-data-prcs or a folder underneath this like
jobappcode-region-generic-data-prcs/20170801-reg1-xyz/default/stream
OR 
jobappcode-region-generic-data-prcs/20170801-reg1-xyz/stream
OR
jobappcode-region-generic-data-prcs-20170801-reg1-xyz/default/stream

Edited by: vashdevl on Sep 23, 2018 10:00 AM"
AWS Batch	"Moving docker-storage to ephemeral instance store (due to DockerTimeoutErr)
Dear all,

I have faced the DockerTimeoutError on our batch jobs due to the partition xvdc running out of IOPS. I'm using the amzn-ami-2017.09.l-amazon-ecs-optimized Image, by the way.
Since I'm usind c5d instances, I thought, let's just place the docker storage on the nvme SSD ephemeral storage available for these instance types. Based on the documentation about the ECS Image (https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-ami-storage-config.html), the docker-storage-setup utility is used (runs on instance launch) to setup the docker storage.

So I stopped the docker daemon, edited the /etc/sysconfig/docker-storage-setup file with the volume I'd like to use (which is /dev/nvme1n1 in this case), removed /var/lib/docker and the volume group ""docker"". I then made an Image of this stage. When I now manually run docker-storage-setup everything works as expected. I can start the docker daemon again and it uses the ephemeral instance storage for the docker stuff.
However, when I start a spot Instance from the Image I made, it doesn't work automatically. It seems, that the docker-storage-setup utility fails at the step where it creates the volume group (a partition gets created, but not the vg).
This is where I'm stuck now and I don't even know where to look for relevant logs to identify my problem. Anyone ever done something similar? Is it possible at all?

Any help would be highly appreciated!"
AWS Batch	"Re: Moving docker-storage to ephemeral instance store (due to DockerTimeoutErr)
Dear all,

the problem solved itself automagically. Just started an instance from my image from last week and it worked flawlessly. Don't know what went wrong last week.
It also works like a charm with AWS batch, my workload reaches around 7'500-8'000 IOPS and the nvme SSD instance store doesn't complain. I'm not quite sure why I have such high IOPS on the docker storage volume though (fairly big Image of 1.2 GB and many short jobs of around 30 sec. duration running in parallel probably).
So if anyone is facing the DockerTimeoutErr due to docker storage volume running out of IOPS, this is one possible solution.

Cheers"
AWS Batch	"Support container instance security group
Hi,

afaik, there is actually no way to specify a security group on a job definition. This implies that all batches running in AWS Batch will share the security group of the host. As a matter of security, I think it could be better.

Are there any plan to support security group per job definition ? 
It's actually possible to define it on task definition in ECS so I hope it will be the case for AWS Batch job definition in a near future ?

Regards,

David"
AWS Batch	"ECS setting from AWS Batch
When AWS Batch brings up a ECS instance, how do I pass in configuration variables? For instance, we want to set ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION, but we are not explicitly bringing up the ECS instance, it's AWS Batch that does that."
AWS Batch	"AWS batch job stuck in RUNNABLE state
Hi, I have below queries with AWS batch job. Plz check and suggest.

Query 1: We are using aws batch to run some jobs....needs 100GB diskspace.
I have created Compute environment with my own AMI steps as mentioned below https://landontclipp.github.io/2018/08/21/Amazon-AWS-Batch-Extended-AMI.html

But when i submit job its in RUNNABLE state forever. Neither started not exited
My aim is to run bigger jobs with 100GB diskspace . Is there anyway i can run this job without creating user defined AMI? or Why its stuck in Runnable state ?
Could you please check and help? .

Query 2:My understanding is that EC2 instances will run only when job is submitted to queue and terminated once job status is SUCCESS or FAILURE But i could see extra running instances in EC2 dashboard though there were NO jobs in queue. Plz correct me if i am missing some configuration?. 

Query3: How batch pricing woks? Will be calculated only for job running time or not? 
If EC2 instance is created as soon as compute environment is created then price will be calculated though no jobs submitted? or price will be calculated only for job execution time?

Thanks in advance"
