label	description
Amazon DynamoDB	"DocumentDB - 2dsphere indexes
Are there any plans to include 2dsphere indexes in future DocumentDB updates? This is required for geospatial querying.

https://docs.aws.amazon.com/documentdb/latest/developerguide/mongo-apis-index.html#mongo-apis-indexes"
Amazon DynamoDB	"Re: DocumentDB - 2dsphere indexes
Thank you for the feedback. Please continue to check the AWS What's New page for future releases: https://aws.amazon.com/new/"
Amazon DynamoDB	"Bug in DynamoDb Update Item with Condition Check
I have ran into some weird behavior by dynamodb when I try to update an attribute and I include a conditional check for that same attribute.

I attempt to update an attribute value, from ACTIVE to PAUSED
SET attributeName = attributeValue

, with the condition that the attribute is not EXPIRED
attributeName (enter less/large then symbols here) :attributeCheckValue


The expected behavior is to update the value, with no exceptions thrown.

The actual behavior that the value IS successfully updated, BUT a condition exception is also thrown:
com.amazonaws.services.dynamodbv2.model.ConditionalCheckFailedException: The conditional request failed (Service: AmazonDynamoDBv2; Status Code: 400; Error Code: ConditionalCheckFailedException; Request ID: Q3R06LPMHOJ88OCCKO6IDHC9VNVV4KQNSO5AEMVJF66Q9ASUAAJG)

I have repeated this many times, and I keep getting the same behavior.

Perhaps AWS could look into this by referring to the above request id.

Edited by: JOHN TSIOKOS on Feb 26, 2019 1:54 AM

Edited by: JOHN TSIOKOS on Feb 26, 2019 1:55 AM

Edited by: JOHN TSIOKOS on Feb 26, 2019 1:56 AM"
Amazon DynamoDB	"Exporting Table to CSV without AWS Data Pipeline
My home AWS Region is eu-north-1 (Stockholm), and I have a DynamoDB table with the following statistics. As you can see, the average record size is only ~100 bytes.

Table name	BreakoutMoves
Primary partition key	OriginalState (String)
Primary sort key	Move (Number)
Read/write capacity mode	Provisioned
Provisioned read capacity units	30 (Auto Scaling Disabled)
Provisioned write capacity units	50 (Auto Scaling Disabled)
Storage size (in bytes)	1.74 GB
Item count	11,320,384
Region	EU North (Stockholm)

I would like to export table contents to S3 with the ultimate aim of training a SageMaker machine learning model with it. For this reason, CSV export format would be preferred, but JSON is also ok. I am planning to use XGBoost algorithm, so libsvm format is also an option.

With these data sizes, I believe creating a AWS Data Pipeline would be the recommended option. However, AWS Data Pipeline service is unavailable in my region, as is SageMaker. I plan to use eu-west-1 region for SageMaker service. At this stage, a one-time export is enough, I am not planning on setting up a regular data transfer schedule just yet.

Question: Given these item counts, data sizes and RCUs, what option do you recommend for getting the data out and preparing it for model training?"
Amazon DynamoDB	"Re: Exporting Table to CSV without AWS Data Pipeline
For those who may be interested, here are some statistics for the brute force approach.

With a single-threaded java program performing scan operation on the forementioned table, I was able to scan (load) 2521 items per second, which translates to data rate of 400 kB per second. Throttling is the main constraint here; the throughput was remarkably better for the first 30 seconds, when the throughput burst to ten times the steady-state rate.

The table was downloaded in an hour and a half. This performance was feasible for the one-time operation, but I am leaving the question unanswered, as this option may not be satisfactory in all situations."
Amazon DynamoDB	"Sporadic no response from dynamodb request
Hi dynamodb experts 

I have an infrequent issue:
Sometimes (one of thousands or more of requests) a dynamodb call (no matter which table or what kind of request, get, query, put, del, etc...) is not responding (in time).

I call dynamodb from my lambda functions and track it like this (I track the calls with iopipe):
  const ddb = new AWS.DynamoDB.DocumentClient({ region: settings.region });
 
  const params = {
    TableName: 'my-table-for-user-projects',
    Key: {
      projectId: projectId,
      userId: userId
    }
  };
 
  settings.track('database').start();
  ddb.get(params, (err, ret) => {
    settings.track('database').end();
    if (err) return callback(err);
    callback(null, ret.Item);
  });


in iopipe I see ""NO TRACE END DETECTED"" for my custom metric and also for the dynamodb call, i.e:

REQUEST HEADERS
Authorization
AWS4-HMAC-SHA256 Credential=ASIA************G3Y/20190222/eu-west-1/dynamodb/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date;x-amz-security-token;x-amz-target, Signature=ca368c**************************************************bb6ac670
Content-Length
160
Content-Type
application/x-amz-json-1.0
Host
dynamodb.eu-west-1.amazonaws.com
User-Agent
aws-sdk-nodejs/2.406.0 linux/v8.10.0 exec-env/AWS_Lambda_nodejs8.10 callback
X-Amz-Content-Sha256
132956**************************************************220b3d9f
X-Amz-Date
20190222T145758Z
X-Amz-Target
DynamoDB_20120810.GetItem
x-amz-security-token
FQoGZXIvY*****************************************************xsfm9zKPPyv+MF
No Response Headers


Has there been any internal changes to dynamodb? Or any known network issues? Not resolving the correct dns name or similar?

Maybe other useful hints: Have these timeouts since 1-2 weeks and I switched the dynamodb plan to on-demand since last month. And also important: seems there are no big workload peaks when the timeouts are triggered (no special dynamodb capacity increase and no throttled request).

I hope anyone can help, because my customers starts to notice it. 

Have a nice day."
Amazon DynamoDB	"Re: Sporadic no response from dynamodb request
Hi adrai,

I am not sure if this is the case for your but we have encountered a similar problem recently. Basically, we were experiencing an unexpected delay (100ms to 300ms, depending on the memory allocated for the corresponding Lambda) for some dynamodb calls we made in our Python function and this was happening randomly. 

In our case since we had a bigger timeout value set in our Lambda, functions didn't timeout. However, we were using Thundra to monitor our Lambda function and it has an AWS SDK integration shows the individual dynamodb call's duration. We saw in the Thundra console that some random dynamodb calls experience an extra latency.

We eventually found out the reason was the connection timeout value that AWS SDK uses while making API calls. It was set to the 60s by default for Python AWS SDK. Basically, whenever there is a gap between two dynamodb calls greater than the 60s, AWS SDK creates a new connection and that was causing unexpected latency I have mentioned above. As long as the time between two dynamodb calls does not exceed the 60s, AWS SDK uses the same connection, so there is no extra latency for creating a new connection. Also, note that this is not something related to cold-start. Even if your Lambda is warm, once the connection that AWS SDK uses to make API calls is timed out; same latency is occurring again.

I don't know if this was the exact issue you faced with, but maybe that gives you a clue about one of the possible causes. Also, I suggest you have a look at Thundra (https://www.thundra.io). It helps a lot to deal with that kind of problems."
Amazon DynamoDB	"Re: Sporadic no response from dynamodb request
Thanks for your reply.
I see the same stuff you mentioned for thundra also in iopipe...
See screenshot here: https://twitter.com/adrirai/status/1099224045365669888
My setup with lambda + dynamodb is like for more than 2 years now... and the timeouts just appeared a couple of weeks ago... (but as said, really just very rare. Perhaps some dynamodb nodes react differently?)"
Amazon DynamoDB	"Increasing capacity unit settings takes long (>30min) times
Setting provisioned capacity, RCU:7000, WCU:4000 takes more than 30 minutes.
The table status keeps ""Updating""
Is it usual, intended behavior?"
Amazon DynamoDB	"Dynamo Db issue with migrating the existing table to global table
Hi,

Can anybody help me with an issue in regards to dynamodb recently launched global tables?

Use case:

I have an existing dynamodb table approx 8gb in size, i have want convert it into global table without any downtime to my application. Is it possible and how?

Thanks in advance!!"
Amazon DynamoDB	"Allow DynamoDB create_table to support tags during creation
Right now the Dynamodb API does not allow the creation of tags when invoking `create_table`.

I was hoping that this would be able to be implemented.

Thanks.

API Reference: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_CreateTable.html"
Amazon DynamoDB	"Re: Allow DynamoDB create_table to support tags during creation
Hi,

I can confirm that it is not possible to tag resources at the time of table creation[1] and that this is not an anomaly, but a known restriction on tagging for DynamoDB. 

A recommended way to tag the resources is using the TagResource[2] API call currently. Having said that I have also reached out to our internal team to check the feasibility of adding tags while table creation time. 

Having said, I will not be able to comment on whether this feature will be implemented or provide any timeline around this, but this will make sure that Development teams are aware of our customer requirements.

Hope this helps. Let us know if you have any additional questions. 

Reference:

1. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TaggingRestrictions.html
2. https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_TagResource.html"
Amazon DynamoDB	"DynamoDB Adaptive Capacity on older tables
We have some tables that were created prior to adaptive capacity being added to DynamoDB. Specifically, we have a table that was created in 2014. Is adaptive capacity enabled for these tables? Or is the functionality only available for tables created after a specific date? If the latter, what was the cutoff date?"
Amazon DynamoDB	"Re: DynamoDB Adaptive Capacity on older tables
Hi mostman1043,

Adaptive capacity is enabled by default for all DynamoDB tables. There is no cutoff date.

-Kai"
Amazon DynamoDB	"DynamoDB's replication type
What is the replication type of DynamoDB? Is it peer to peer, master slave, multi master, or something else?"
Amazon DynamoDB	"Re: DynamoDB's replication type
Hi whaatxd2,

DynamoDB global tables use active-active replication. In other words, any changes made to any item in any replica table will be replicated to all of the other replicas within the same global table.

To learn more, please see the DynamoDB documentation: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_HowItWorks.html

-Kai"
Amazon DynamoDB	"Updates in DynamoDB
Using Spring Data I am trying to perform an update to DB. we are using the same modal object for the CRUD operation. The problem here is I need to ignore some fields and those with null values during update. I couldnt find some out of the box solution so I tried using DynamoDBMapper. 
Below is the code I want to return entity like repo.save  I dont like to load it . is there any better approach ? 
JSON or Modal Object.
{
    ""id"": ""123"",
    ""displayName"": ""P123"",
    ""frmsId"": ""PFRMSL123"",
    ""eventsId"": ""PEVENTS123"",
    ""harmonyId"": ""PHA123"",
    ""uniteId"": ""PUNITE123"",
    ""kyloUrl"": ""https://kylo1.epsilon.com"",
    ""CategoryID"": ""d5914c54-a27e-419f-ba39-568d549052f0"",
    ""NiId"": ""0e1d6d50-43d4-476e-9b99-0147c6bba081"",
    ""nifiBuildUrl"": ""https://nifi1.build.epsilon.com"",
    ""nifiExecUrl"": ""https://nifi1.exec.epsilon.com"",
    ""active"": true,
    ""createts"": 1550684180829,
    ""createby"": null,
    ""updatets"": 1550684180834,
    ""updateby"": null
}

	@Override
	public TenantInfo update(TenantInfo entity) {		
		entity.setUpdatets(new Date().getTime());
              // assigned null to ignore it during the update
		entity.setCategoryID(null);
		entity.setNiId(null);
		AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard().build();
		DynamoDBMapperConfig mapperConfig = new DynamoDBMapperConfig.Builder()
				.withSaveBehavior(DynamoDBMapperConfig.SaveBehavior.UPDATE_SKIP_NULL_ATTRIBUTES).build();
		DynamoDBMapper mapper = new DynamoDBMapper(client, mapperConfig);
		mapper.save(entity);
		return mapper.load(TenantInfo.class, entity.getId());
	}

Edited by: snatarajan on Feb 20, 2019 12:56 PM"
Amazon DynamoDB	"DynamoDB reserved capacity still billing even switched to pay-per-request?
I purchased DynamoDB reserved capacity on last October, then switched all the tables to use pay-per-request type, however I'm seeing the reserved capacity is still charging for my tables.

For example, this is the latest bills recorded in AWS Service Charges.

Amazon DynamoDB PayPerRequestThroughput

$0.25 per million read request units (N. Virginia) 82,554 ReadRequestUnits $0.02
$1.25 per million write request units (N. Virginia) 21,758 WriteRequestUnits $0.03


Amazon DynamoDB TimedStorage-ByteHrs

USD 0.000016 hourly fee per Amazon DynamoDB, Reserved Read Capacity used this month 67,200 ReadCapacityUnit-Hrs $1.08
USD 0.000081 hourly fee per Amazon DynamoDB, Reserved Write Capacity used this month 67,200 WriteCapacityUnit-Hrs $5.44


Since no tables are running in reserved capacity, I assume the correct pricing would be $0.05, but why reserved read/write capacity is billed?
It would be great AWS team clarify whether it is a duplicated charge."
Amazon DynamoDB	"Re: DynamoDB reserved capacity still billing even switched to pay-per-request?
Hi tomodian,

Thanks for reaching out. I have private messaged you a response to your billing question.

-Kai"
Amazon DynamoDB	"How to use put_item in lambda to pass variables to dynamoDB
I'm trying to learn how to pass a variable to dynamoDB. I'm trying to use the following code to pass ""sum"" where sum just equals 10, but I can't seem to get it working.

import boto3

def lambda_handler(event, context):
    sum = 10
    # this will create dynamodb resource object and
    # here dynamodb is resource name
    client = boto3.resource('dynamodb')

    # this will search for dynamoDB table 
    # your table name may be different
    table = client.Table(""AverageTest"")
    print(table.table_status)

    table.put_item(Item= {'Value': {'N': 'sum'}})"
Amazon DynamoDB	"DynamoDB SDK for dotnet..
So I'm developing an interface for extracting data from a dynamoDB table, everything is fine but two things.
I'm using query for finding the values I need but I'm concerned about two things:
1. I've read somewhere that there's a limit of 1MB per query, and that the query results return a LastEvaluatedKey that can be used to finish the query, But I haven't seen any use example of that and I have no clue on how to use it, Is there any example i'm missing?
2. I want to limit the number of results that the query returns, I'm aware of the limit parameter but from what I've read it limits the items to query not the actual items that it returns, I'm I wrong here?
Is there a way to achieve what i want?

I'm using the dotnet sdk for c# if that matters

Edited by: Arturo on Feb 8, 2019 9:43 AM"
Amazon DynamoDB	"Re: DynamoDB SDK for dotnet..
bumpity bump. Im mostly interested on how to use the lastevaluatedkey"
Amazon DynamoDB	"AccessDeniedException:while trying to get item from Dynamo using aws-amplif
Hello,
I am following the instruction described in https://aws.amazon.com/blogs/mobile/building-fine-grained-authorization-using-amazon-cognito-user-pools-groups/ and have a test table called UserProfile (It is a simple table with paritionkey userID(set to cognito sub).

I am using aws-amplify sample code to get the item

    Auth.currentCredentials()
      .then(credentials => {
        const dynamodb = new AWS.DynamoDB({
          apiVersion: '2011-12-05',
          credentials: Auth.essentialCredentials(credentials)
        });

        var params = {
          Key: { /* required */
            HashKeyElement: { /* required */
              S: 'XXXX'
            },
            RangeKeyElement: {
              N: '1'
            }
          },
          TableName: 'UserProfile', /* required */
          AttributesToGet: [
            'userId',
            'updatedAt'
          ],
          ConsistentRead: false
        };
        dynamodb.getItem(params, function(err, data) {
          if (err) console.log(err, err.stack); // an error occurred
          else     console.log(data);           // successful response
        });

My IAM policy has following
{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Action"": [
                ""dynamodb:GetItem"",
                ""dynamodb:PutItem"",
                ""dynamodb:Query""
            ],
            ""Resource"": [
                ""arn:aws:dynamodb:us-west-2:XXX:table/UserProfile""
            ],
            ""Condition"": {
                ""ForAllValues:StringEquals"": {
                    ""dynamodb:LeadingKeys"": [
                        ""${cognito-identity.amazonaws.com:sub}""
                    ]
                }
            }
        }
    ]
}

When i execute the above i get the following error

AccessDeniedException: User: arn:aws:sts::xxx:assumed-role/awsreactAuth_Role/CognitoIdentityCredentials is not authorized to perform: dynamodb:GetItem on resource: arn:aws:dynamodb:us-west-2:xxx:table/UserProfile

I am trying to look for any logs in cloudtrail and cloudwatch and don;t see any logs. Can someone please provide pointers on how to debug this issue?

Thanks
-Ckm"
Amazon DynamoDB	"Re: AccessDeniedException: Row-Level Access to DynamoDB Based on Cognito ID
If i remove the ""Condition"" from IAM policy I am able to getItem, I am not sure if i am missing any settings in Cognito. Can someone provide any hints on what settings to check or enable logs (400 error code with AccessDenied Exception is not very useful)

I have the following policy in IAM with correct values for REGION, Account number and TableName
{
     ""Version"": ""2012-10-17"",
     ""Statement"": [
         {
             ""Effect"": ""Allow"",
             ""Action"": [
                 ""dynamodb:DeleteItem"",
                 ""dynamodb:GetItem"",
                 ""dynamodb:PutItem"",
                 ""dynamodb:Query"",
                 ""dynamodb:UpdateItem""
             ],
             ""Resource"": [
                 ""arn:aws:dynamodb:<REGION>:<ACCOUNTNUMBER>:table/<TABLE-NAME>""
             ],
             ""Condition"": {
                 ""ForAllValues:StringEquals"": {
                     ""dynamodb:LeadingKeys"": [
                         ""${cognito-identity.amazonaws.com:sub}""
                     ]
                 }
             }
         }
     ]
 }"
Amazon DynamoDB	"Re: AccessDeniedException:while trying to get item from Dynamo using aws-amplif
https://stackoverflow.com/questions/38731723/how-to-use-dynamodb-fine-grained-access-control-with-cognito-user-pools

Has details, key thing i missed was that i was creating entries with sub instead of IdentityID"
Amazon DynamoDB	"Re: AccessDeniedException:while trying to get item from Dynamo using aws-amplif
Could you give a code example? That would help greatly."
Amazon DynamoDB	"Dynamodb local image - Current sha256  is empty
Hello,

The current sha256 of the frankfurt/dynamodb_local_latest.tar.gz image is empty.

This link can be found here: 

https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.DownloadingAndRunning.html 

This is the empty link:
https://s3.eu-central-1.amazonaws.com/dynamodb-local-frankfurt/dynamodb_local_latest.tar.gz.sha256"
Amazon DynamoDB	"An AttributeValue may not contain an empty string
Why DynamoDB doesn't allow empty strings for attributes other than keys? and how do you suggest I handle an empty string"
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
AttributeValues and attribute names may not contain empty strings, whether they are part of the primary key or not.  More details are available in this documentation.  Were you able to insert an item with an empty string in any place in an item?  If so, which client library were you using?  Did our documentation seem to imply otherwise?  If you would prefer not to discuss the specifics of the data in your table on the forum, I would be happy to follow up with you off thread.

As a broader question, do you have a specific use case for having an empty string in an AttributeValue?  If so, we are interested in hearing more about it.  As a workaround, would it be possible to encode an empty string as some other string?  Perhaps a space character “ “?"
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
I am using the Java SDK. I would expect the auto marshalling of String would know how to handle null/empty string by it self.

I did manage to make a workaround for my self by simply use a EMPTY_STRING constant like you suggested. 

Thanks."
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
I don't understand why an empty string isn't allowed. An empty string has a different meaning than null. You are saying that we must differentiate the two by using a special constant?"
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
I think DynamoDBMapper not knowing how to handle null or empty string values is a huge design flaw.  The mapper should know that if DynamoDb doesn't have a value for that specific attribute for a given key, to set it as null.  Also, the mapper should know that to save an item, if an attribute is empty or null to skip it entirely.

The reason that this sucks:  let's say I have a class that represents a person...

class Person {
  String firstName;
  String middleName;
  String lastName;

  @DynamoDbAttribute(attributeName = ""firstName"")
  public String getFirstName() {
  ...
}

Now, when I go to serialize a person to DynamoDB -- if they don't have a middle name, it fails with this error message: ""AWS Error Code: ValidationException, AWS Error Message: Supplied AttributeValue is empty, must contain exactly one of the supported datatypes""

So now, the workaround is to start using some ugly constant instead of the natural null or empty strings OR change the mapper code myself -- neither of which are ideal."
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
I just ran into this same problem.  I'm not using the mapper. I'm using AttributeValue maps.  I'm using the DynamoClient directly. The way I got around this one. Is to check for empty string value, and not add it to the map.

(Java)

if ( !StringUtils.isBlank(attrVal.getS()))
	attrValMap.put(baseName, attrVal);"
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
The error message is abismal. You might want to state WHICH attribute value? Duh."
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
Just figured out my error. DynamoDB doesn't like an empty Set either. Very annoying bug / limitation. It's a common Java practice to initialize a variable to an empty set so as not to have to check for Null or get NullPointerExceptions. Especially when a bean has getters/setters for a set. They way you add to the set is getMySet().add(""myValue""). AWS makes for awkward code. 

PLEASE FIX THIS!

Where do I file a bug?"
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
maybe u could try removing the column instead of setting a null value to it:

if (currentOpponents.size() == 0){
        updateItems.put( ""current_opponent"",
                        new AttributeValueUpdate().withAction(AttributeAction.DELETE) );
} else {
        updateItems.put( ""current_opponent"",
	                new AttributeValueUpdate().withValue( new AttributeValue().withSS( currentOpponents ) ) );
 
}"
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
I'd like to upvote a change in this behaviour. Erring for an empty set is highly undesirable, and not concordant with Java semantics."
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
We have a use case:

We store information in Dynamo that is actually override data for another data store.
For instance we have in our DB nickName = ""Mr. Jenkins"".

We then would want an override of nickName = """" with retaining the original (which isn't stored in Dynamo).

An empty string here would be useful and semantically correct.

Echoing below, """" = String, not null."
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
One can deal with replacing a special character when the empty string is in top level attribute . While saving a JSON document as a M data type it does the marshaller in PHP AWS SDK does not even work if there is an empty string """" at any level of a JSON. I could recursively replace all of them and put the """" back while doing a get but this would be expensive in case of complex nested JSON structures."
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
That's true. this should be consider a bug not limitation. It has no reason to error on just empty string. Even null should just be treat as removing (so should just remove the update REMOVE keyword and let people set that value to null)

This is design flawed actually

However dynamoDB is such a whole pile of design flawed that amazon just leash us with the bait of free and auto replicate DB from the start anyway"
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
+1 for this. Having to get around this issue by using an empty character is a huge mess. It forces us to do exception at various places of our code to check if it's an empty string, which kinda break semantic. Please consider this as a bug!"
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
+1 Please change this. It's an unnecessary constraint and very awkward to write these exceptions in the consuming application."
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
+1

This is disappointing, and counter to the marketing of DynamoDB as a general purpose database. What database doesn't support empty strings??"
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
+1 for getting this fixed"
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
+1"
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
+1 do not understand the design"
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
I just ran into the same error - this is a maintenance nightmare. """" is not null. This is clearly a bug and not a feature."
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
This is a bug. You can call a bug ""intended behavior"" but it's still a bug. When I see things like this, I wonder what other bugs are lurking in AWS that they refuse to fix."
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
+1"
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
+1 indeed"
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
+1; this is silly, unless someone can explain why?"
Amazon DynamoDB	"Re: An AttributeValue may not contain an empty string
This is a really bad flaw that makes me want to use a different DB. It breaks compatibility with simple JSON objects like {""a"":""""}"
Amazon DynamoDB	"how are GSIs billed?
If I have a table and I create 3 GSIs does that automatically make that table more expensive than if I had created none even if I never use the GSIs to perform a query?
Or am I only charged for when I use that index to perform a query or get?

In other words does that index cost me money when I write items to the table since those items also need to be copied over into the index? 

Is the process of maintaining the index something that consumes WCUs and something you are charged for or are you charged only when you explicitly access the index?"
Amazon DynamoDB	"Re: how are GSIs billed?
Hi,

I understand that you have some questions relating to GSI pricing for DynamoDB. 

I've done some research into this and I can confirm that Global Secondary Indexes are not billed as an individual line item. Instead, any queries or scans on a global secondary index consume capacity units which will then contribute to your overall read/write capacity billing for DynamoDB.

For more information about working with GSIs, please see the following link: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html

Since GSI is essentially a DynamoDB table, it is charged the same way as a DynamoDB table is charged which is dependent on the region of use.

Pricing on DynamoDB can be confirmed in the link below:
https://aws.amazon.com/dynamodb/pricing/

I hope this information helps. Have a AWSome day"
Amazon DynamoDB	"DynamoDB UpdateItem error with array attributes
I am getting the following error while invoking update for the following object. 

""ValidationException: The document path provided in the update expression is invalid for update""

Update Expression Looks like this:

""set testArr[4].attrib3 = :s""

The object in DB looks like this.

{
  ""testArr"": [
    {
      ""attrib1"": ""0"",
      ""attrib2"": ""LOW"",
      ""attrib3"": ""Test"",
      ""attrib4"": {}
    },
    {
      ""attrib1"": ""1"",
      ""attrib2"": ""LOW"",
      ""attrib3"": ""Test"",
      ""attrib4"": {}
    },
    {
      ""attrib1"": ""2"",
      ""attrib2"": ""LOW"",
      ""attrib3"": ""Test"",
      ""attrib4"": {}
    },
    {
      ""attrib1"": ""3"",
      ""attrib2"": ""LOW"",
      ""attrib3"": ""Test"",
      ""attrib4"": {}
    },
    {
      ""attrib1"": ""4"",
      ""attrib2"": ""LOW"",
      ""attrib3"": ""Test"",
      ""attrib4"": {}
    },
    {
      ""attrib1"": ""5"",
      ""attrib2"": ""LOW"",
      ""attrib3"": ""Test"",
      ""attrib4"": {}
    }
  ],
  ""userId"": ""test123""
}

Basically I am trying to update the value of attrib3 in the 5th element in the testArr array.

Any pointers?"
Amazon DynamoDB	"DynamoDb Java - ScanRequest not returning all results and hangs
DynamoDB table has ~7500 items in it.

The following code snippet gets endlessly caught in a loop, logger output shows it is stuck after 1895 items:

2019-02-07 13:10:50 defaulthttps://forums.aws.amazon.com/  13:10:50.057 https://forums.aws.amazon.com/ DEBUG com.spotonresponse.webservices.GetData - Reading DB entries - count: 1895
2019-02-07 13:10:50 defaulthttps://forums.aws.amazon.com/  13:10:50.710 https://forums.aws.amazon.com/ DEBUG com.spotonresponse.webservices.GetData - Reading DB entries - count: 1895
2019-02-07 13:10:51 defaulthttps://forums.aws.amazon.com/  13:10:51.298 https://forums.aws.amazon.com/ DEBUG com.spotonresponse.webservices.GetData - Reading DB entries - count: 1895
...
...

What am I missing???

And here is my code snippet:
BasicAWSCredentials awsCreds = new BasicAWSCredentials(aws_access_key_id, aws_secret_access_key);

        client = AmazonDynamoDBClientBuilder.standard()
                .withCredentials(new AWSStaticCredentialsProvider(awsCreds))
                .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(amazon_endpoint, amazon_region))
                .build();
        myDynamoDB = new DynamoDB(client);

        // Get all the keys and store them in an array
        Map<String, String> currentItems = new HashMap<String, String>();
        ScanResult scanResult = null;
        Map<String, AttributeValue> lastEvaluatedKey = null;
        do {
            ScanRequest scanRequest = new ScanRequest()
                    .withTableName(DynamoDBTableName);
            scanResult = client.scan(scanRequest);

            if (scanResult != null) {
                scanRequest.setExclusiveStartKey(scanResult.getLastEvaluatedKey());
            }

            scanResult = client.scan(scanRequest);

            for (Map<String, AttributeValue> item : scanResult.getItems()) {
                currentItems.put(item.get(""title"").getS(), item.get(""md5hash"").getS());
            }
            lastEvaluatedKey = scanResult.getLastEvaluatedKey();
            logger.debug(""Reading DB entries - count: "" + currentItems.size());
        } while (lastEvaluatedKey != null);"
Amazon DynamoDB	"Trying to connect to DynamoDB through VPC from Lambda/Python/Boto3
Hello all,

I am following a few forum posts and documents to try and get this done, but no matter what I do, I keep timing out when creating the connection to DynamoDB.

The forum posts/documents I am reading are:
https://forums.aws.amazon.com/message.jspa?messageID=705389
https://aws.amazon.com/blogs/aws/new-vpc-endpoints-for-dynamodb/
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html

The code to connect to DynamoDB is:
dynamoDbClient = boto3.client('dynamodb', region_name='us-east-1')
    paginator = dynamoDbClient.get_paginator('query')
    response_iterator = paginator.paginate(
        TableName='[TABLE_NAME]',
        IndexName='[INDEX_NAME]',
        KeyConditionExpression = '[COLUMN_NAME] = :consKey AND [COLUMN_NAME] BETWEEN :minTS AND :maxTS',
        ExpressionAttributeValues={"":consKey"":{""S"":""[VALUE]""}, "":minTS"":{""N"":""[VALUE]""}, "":maxTS"":{""N"":""[VALUE]""}}
    )


The error I keep getting is:
2019-02-05 18:00:12,158 [INFO] Starting new HTTPS connection (1): dynamodb.us-east-1.amazonaws.com
18:00:21
END RequestId: 1cc51a0c-f866-4316-9c71-b05922e0818f
18:00:21
REPORT RequestId: 1cc51a0c-f866-4316-9c71-b05922e0818f	Duration: 10010.29 ms	Billed Duration: 10000 ms Memory Size: 128 MB	Max Memory Used: 100 MB
18:00:21
2019-02-05T18:00:21.529Z 1cc51a0c-f866-4316-9c71-b05922e0818f Task timed out after 10.01 seconds


I can confirm that my endpoint is created and seemingly mapped correctly:
aws ec2 describe-vpc-endpoints
{
    ""VpcEndpoints"": [
        {
            ""VpcEndpointId"": ""[VPCE_ID]"",
            ""VpcEndpointType"": ""Gateway"",
            ""VpcId"": ""[VPC_ID]"",
            ""ServiceName"": ""com.amazonaws.us-east-1.dynamodb"",
            ""State"": ""available"",
            ""PolicyDocument"": ""{\""Version\"":\""2008-10-17\"",\""Statement\"":[{\""Effect\"":\""Allow\"",\""Principal\"":\""*\"",\""Action\"":\""*\"",\""Resource\"":\""*\""}]}"",
            ""RouteTableIds"": [
                ""[RTB_ID1]"",
                ""[RTB_ID2]"",
                ""[RTB_ID3]""
            ],
            ""SubnetIds"": [],
            ""Groups"": [],
            ""PrivateDnsEnabled"": false,
            ""NetworkInterfaceIds"": [],
            ""DnsEntries"": [],
            ""CreationTimestamp"": ""2019-02-05T17:47:03.000Z""
        }
    ]
}


I can get this call to work by opening my VPC to all incoming ports and IP addresses, but this is clearly not desirable.  Please help."
Amazon DynamoDB	"DynamoDB Local now supports Transactions
2019-02-04

  * Add on-demand implementation
  * Add support for 20 GSIs (up from 5)
  * Add transaction API implementation
  * Update AWS SDK for Java to version 1.11.475

Nice!"
Amazon DynamoDB	"DynamoDBLocal maven repository broken?
Trying to load DynamoDBLocal using maven as described here: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.Maven.html 

Getting errors like this:

[ERROR] Failed to execute goal on project yummly-mobile-api: Could not resolve dependencies for project com.yummly:yummly-mobile-api:jar:1.0.12-SNAPSHOT: Failure to find com.amazonaws:DynamoDBLocal:jar:1.11.475 in https://s3-us-west-2.amazonaws.com/dynamodb-local/release was cached in the local repository, resolution will not be reattempted until the update interval of dynamodb-local-oregon has elapsed or updates are forced -> [Help 1]


The maven-metadata.xml says 1.11.475 but there are no artifacts for this version in the repo.

Pom attached."
Amazon DynamoDB	"Re: DynamoDBLocal maven repository broken?
This has been fixed"
Amazon DynamoDB	"Re: DynamoDBLocal maven repository broken?
Looks like DynamoDB Local now supports the new transaction API!!!!

👍🏻"
Amazon DynamoDB	"Stream Reading and throttling
Reading into DynamoDB stream document and it says ""No more than 2 processes at most should be reading from the same Streams shard at the same time. Having more than 2 readers per shard may result in throttling"" 

As far as I know, the stream doesn't consume the provisioned throughputs so what exactly is getting throttled?

Edited by: aac on Feb 4, 2019 10:08 PM"
Amazon DynamoDB	"DynamoDB Global Table Replication System
Hi everyone, 
I am working on Benchmarking Dynamodb's performance as part of a project at the university and have been looking for more details on the replication system when setting up Global tables as i want to understand its impact on latency / Throughput. 
I end up by finding 2 confusing Concept, Regions and Availability zones. From what i understood here:
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.CrossRegionRepl.html 
By Creating 2 Tables, one in Frankfurt and one in Ireland let's say, This means that i now have 
2 multi-master read/write Replicas. 

But then i found those links: 
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.Partitions.html
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html
https://aws.amazon.com/blogs/aws/new-for-amazon-dynamodb-global-tables-and-on-demand-backup/
explaining that the data is stored and automatically replicated across multiple Availability Zones in an AWS region but not mentioning the number of replicas and whether they can be used for read / write requests and are also multi-master or slaves or just for recovery purposes.
From what i understood here if going back the example i am using (Frankfurt / Ireland)
I will be having:
3 multi-master read/write Replicas in Frankfurt 
3 multi-master read/write Replicas in Ireland 

Please let me know which one is correct. Thanks in Advance"
Amazon DynamoDB	"multiplayer game data model design
Here is my initial table design for a game in pending state that has two players. Game state can change form pending -> live -> over

PK      SK      State     Name
gid1	state	pending	
gid1	pid1              alex
gid1	pid2              john


Now I want to query what pending games specific user is part of. In order to do so I have to duplicate Game State to each game player item and create GSI like this.

GSIPK  GSISK   PK
alex   pending gid1
john   pending gid1


This game state duplication works fine until I want to update game state to live. That means I must update all game's users State as well and if game has a lot of players that might be slow and expensive.

Is there a better design to model that parent/child relation where child depends on parent's attribute value?

Edited by: aleru on Jan 31, 2019 11:37 AM

Edited by: aleru on Jan 31, 2019 11:44 AM"
Amazon DynamoDB	"Support for RangeKeyConditions in dynamodb-local
The docker hub image amazon/dynamodb-local released about 5 months ago does not support this feature.
Are there plans to release dynamodb-local supporting this? When a release could be expected?"
Amazon DynamoDB	"DynamoDB stream - Clean stream
Hi everybody

I have linked a lambda function (LAMBDA STREAM) to my DynamoDB stream of a table. 
When I want to disable the trigger I use the following command with AWS CLI:
aws lambda update-event-source-mapping --uuid UUID_LAMBDA --function-name DynamoDbStreamTest --no-enabled

But, when I want to enable the trigger with the command:
aws lambda update-event-source-mapping --uuid UUID_LAMBDA --function-name LAMBDA_STREAM --enabled

the lambda function is triggered with events happened in the meanwhile it was disabled.

I tried to use a different approach with the commands:
aws lambda create-event-source-mapping --function-name LAMBDA_STREAM --event-source-arn arn:aws:dynamodb:REGION:ACCOUNT_ID:table/TABLE_NAME/stream/*???*
aws lambda delete-event-source-mapping --function-name LAMBDA_STREAM

But I don't understand how to define the correct stream arn:
arn:aws:dynamodb:REGION:ACCOUNT_ID:table/TABLE_NAME/stream/*???*


I don' know which stream label to put(I know it is a datetime string), ideally I want to connect the lambda with all the future streams, not old one. How to get this stream label?

Any idea?

Edited by: EduBic on Jan 30, 2019 5:56 AM"
Amazon DynamoDB	"Number vs UUID for PK in DynamoDB
hi,
  I am new to DynamoDB and have a very basic question - 

what is the best practice in choosing the datatype for the primary key?
should I be storing numbers like (1,2,3, etc) in a NUMBER datatype or UUIDs in a STRING datatype?


Jay"
Amazon DynamoDB	"Re: Number vs UUID for PK in DynamoDB
Choice of data type for partition key is completely based on your use case and type of data you want to store within your table. Whether you want to store UUID as string or number will completely depends on your requirements. (Also, Partition key can only have scalar data types)

As a best practice its recommended to have high cardinality of values for partition key on your DynamoDB table. 

If you can share more details about your use case, I can make recommendations for data types maybe"
Amazon DynamoDB	"Trying to select data with multiple parameters from DynamoDB
Hi, I have just started working on Dynamodb, i want to create a page on the basis of category, sub category and name using dynamodb and nodeJs However not able to do.

MYSQL Query: Select * from test Where category=""1"" and sub_category=""2"" and name=""user"";

however i don't know how to create this query on dynamodb, whenever I tried to define 2 primary key it return me an error so i have created local secondary key for same below is the table structure.

var params = {
    TableName : ""test"",
    KeySchema: [       
        { AttributeName: ""name"", KeyType: ""HASH""},  //Partition key
        { AttributeName: ""sub_category"", KeyType: ""RANGE""}  //Sort key
    ],
    AttributeDefinitions: [       
        { AttributeName: ""name"", AttributeType: ""S"" },
        { AttributeName: ""sub_category"", AttributeType: ""S"" },
        { AttributeName: ""category"", AttributeType: ""S"" },
    ],
    ProvisionedThroughput: {       
        ReadCapacityUnits: 10, 
        WriteCapacityUnits: 10
    },
    LocalSecondaryIndexes: [ // optional (list of LocalSecondaryIndex)
        { 
            IndexName: 'name_category',
            KeySchema: [ 
                { // Required HASH type attribute - must match the table's HASH key attribute name
                    AttributeName: 'name',
                    KeyType: 'HASH',
                },
                { // alternate RANGE key attribute for the secondary index
                    AttributeName: 'category', 
                    KeyType: 'RANGE', 
                }
            ],
            Projection: { // required
                ProjectionType: 'ALL', // (ALL | KEYS_ONLY | INCLUDE)
            },
        },
        // ... more local secondary indexes ...
    ],
};

DynamoDB Query:

Curriculum.findtest = function(https://forums.aws.amazon.com/, callback){
     var params = {
                TableName: ""test"",
                IndexNames: ""name_category"",
                KeyConditionExpression:""#name= :name and #sub_category = :sub_category and #category = :category"",
                ExpressionAttributeNames:{
                    ""#category"": ""category"",
                    ""#sub_category "": ""sub_category"",
                    ""#name"": ""name""
                },
                ExpressionAttributeValues: {
                    "":category"": category,
                    "":sub_category"": sub_category,
                    "":name"": name
                }

            };

     docClient.query(params, function(err, data) {  

        if (err) {
            console.error(""Unable to read item. Error JSON:"", JSON.stringify(err, null, 2));  
        } else {
            callback(null, data.Items);
            return true;
     }

    });
}

However getting an error in console :

Unable to read item. Error JSON: {
""message"": ""Conditions can be of length 1 or 2 only"",
""code"": ""ValidationException"",
""time"": ""2019-01-18T06:17:04.470Z"",
""requestId"": ""78406675-5485-4610-bb59-1ddc7976ab17"",
""statusCode"": 400,
""retryable"": false,
""retryDelay"": 32.41354621763527
}

I don't know what am i doing wrong. Kindly assist me.

Thanks in advance.

Edited by: DynamoDev on Jan 18, 2019 12:58 AM"
Amazon DynamoDB	"Re: Trying to select data with multiple parameters from DynamoDB
You can use KeyConditionExpression parameter to provide a specific value for the partition key. You can optionally narrow the scope of the Query operation by specifying a sort key value and a comparison operator in KeyConditionExpression. ( scenario when you have both partition key as well as sort key)

Now, reason you are facing error 'Conditions can be of length 1 or 2 only' is because you are specifying 3 conditions within KeyConditionExpression. Please specify only partition key and sort key. In order to further refine your Query you can optionally provide a FilterExpression on other attributes. A FilterExpression determines which items within the results should be returned to you. All of the other results are discarded.

You can read more about same here: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Query.html

Lastly, you dont need to create a LSI for this purpose. You can perform same operation on original table

Hope this helps"
Amazon DynamoDB	"Global Table replication stream event issues
We are encountering a situation where if we have a table globally replicated but also use the tables stream to process records on insert/update we are seeing double events in our stream for the region the insert/update was performed.

E.g. We have a table replicated between eu-west-1 and us-east-1 with a lambda connected to each tables stream in their region. 
When an update is performed in the eu table the eu lambda is invoked with the updated record information then invoked again with the `aws::` updated fields. With the us lambda only invoked once with the record update and the `aws::` fields. 

We have implemented logic on our lambdas to drop the `aws::` specific events but we're wondering if this is the expected behaviour for using streams on globally replicated tables? 

It seems that due to this logic not only are we being charged for the global replication but also the cost of our lambda functions being invoked twice per insert/update"
Amazon DynamoDB	"Re: Global Table replication stream event issues
Yes this is an expected behavior. Global Table uses DynamoDB streams to replicate records from one region to another.

When you use Global Table, you will see two types of records within DynamoDB streams:
1> One stream record is created when application performs Put/Update/Delete to item (same as regular tables)
2> Second stream record is created when Global Tables internal replication engine either adds or updates special attributes ‘aws:rep:deleting’, ‘aws:rep:updatetime and ‘aws:rep:updateregion’ on item

Stream record produced by replication will always have different values for aws:rep:updatetime attribute in the old and new images on the stream. So if you discard the stream records for which the aws:rep:updatetime attributes are different, then you will be left with the Put/Update/Delete records that you would have in the stream for regular DynamoDB tables. You can skip these records while processing records using lambda function 

From cost perspective for Global tables, you should be charged based on details specified in this page: https://aws.amazon.com/dynamodb/pricing/provisioned/

Hope it helps. Let us know if you have any additional questions"
Amazon DynamoDB	"How to create groups to show data to only users registered to that group?
Hey all,

I am making an app using Xamarin and AWS. I have successfully set up a user pool to enable users to sign up and login to my app. I have also created an identity pool to enable users to gain an authentication token to be able to consume additional resources. Next I set up a dynamoDB table that once the user is authenticated can perform CRUD operations on the table. 

However where I am stuck is how can I set up a group system to have multiple users subscribe to a single group and what ever data is posted to that group should only be visible by users registered to that group. I have been searching for a few days to try and find information but have been unsuccessful so far. Essentially I would like to achieve something like this:

https://wordpress.org/plugins/groups/

Any help will be much appreciated.
Thank you in advance.
Regards."
Amazon DynamoDB	"DynamoDB local image with GSI limit to 20
Hi,
The webservice now supports 20 GSI but the local image supports only 5. Is there a plan to update the GSI on DDB local?"
Amazon DynamoDB	"Re: DynamoDB local image with GSI limit to 20
Yes, there is a plan to update the DynamoDB local image with GSI limit to 20 soon."
Amazon DynamoDB	"DynamoDB list_append work in console get error in node.js
In my case when I have execute this in my console:
aws dynamodb update-item --endpoint-url http://localhost:8000 --table-name testing --key '{""id"": {""N"": ""4""}}' --update-expression 'SET cosas = list_append(cosas,:s)' --expression-attribute-values '{"":s"":{""L"":[{""N"":""8778789""}]}}'


works with out any problem, but when I try the same in node with the code:
var params = {
      ""TableName"": ""testing"",
      ""Key"": {
        ""id"": {
            ""N"": ""4""
        }
      },
      ""UpdateExpression"": ""SET #attrName = list_append(#attrName,:attrValue)"",
      ""ExpressionAttributeNames"" : {
        ""#attrName"" : ""cosas""
      },
      ""ExpressionAttributeValues"": {
        "":attrValue"": {
            ""L"": [
                {
                    ""N"": ""1489401606520""
                }
            ]
        }
     }
    };


I get this error:
    Unable to add log G030JF053195HW66:SINGLE . Error JSON: {
      ""message"": ""Invalid attribute value type"",
      ""code"": ""ValidationException"",
      ""time"": ""2017-03-15T09:03:45.642Z"",
      ""requestId"": ""f04532d2-44c5-48a7-8758-3f7012aaae22"",
      ""statusCode"": 400,
      ""retryable"": false,
      ""retryDelay"": 0
    }"
Amazon DynamoDB	"Re: DynamoDB list_append work in console get error in node.js
1. Is cosas list ? 

2. "":attrValue"": event.valPass,

Pass valPass input from node

This might help"
Amazon DynamoDB	"Re: DynamoDB list_append work in console get error in node.js
Hi,
Here's what worked for me: -
You don't need to specify types explicitly e.g. ""L"" or ""M"" when using the aws-sdk in nodejs. For the expression attribute values, aws-sdk automatically infers them using the type of value being passed through, e.g. if you pass an array, it assumes that it is a list, if you pass in an {}, it assumes that it is a map. aws-sdk will process your current params in a very convoluted way. Tip: When in doubt, enable logging in aws-sdk like this:
AWS.config.logger = console;.


Therefore, in your specific case, these would be the modified params:-
var params = {
      ""TableName"": ""testing"",
      ""Key"": {
        ""id"": 4
      },
      ""UpdateExpression"": ""SET #attrName = list_append(if_not_exists(#attrName, :empty_list), :attrValue)"",
      ""ExpressionAttributeNames"" : {
        ""#attrName"" : ""cosas""
      },
      ""ExpressionAttributeValues"": {
        "":attrValue"": [1489401606520],
        "":empty_list"": []
     }
    };

These params will tell AWS to:

Create an empty list if ""cosas"" list does not exist
Add the value 1489401606520 to the cosas list


Good luck.

Edited by: antman on Jan 13, 2019 9:40 PM

Edited by: antman on Jan 13, 2019 9:42 PM"
Amazon DynamoDB	"Transactions support for AWS SDK JS in Lambda environment?
Hi,

Current AWS SDK for JavaScript version in Lambda is 2.290.0 - https://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html.
DynamoDB transactions were added in version 2.365.0 and DynamoDB document client support was added in version 2.373.0.

This means that it isn't possible to call DynamoDB transactions from node.js Lambda functions without including the latest SDK in the function's bundle.

Any plans to update the AWS SDK for JavaScript version in Lambda's execution environment soon?

Thanks,
Boris

Edited by: borisirota on Jan 13, 2019 1:18 AM

Edited by: borisirota on Jan 13, 2019 1:18 AM"
Amazon DynamoDB	"AWS DynamoDB streams does not update with new information
I am using the DynamoDB database on AWS. I have a table that I have enabled its stream. From what I understand the stream should capture any activity regarding this table.

However, there are times at which the activity of the table, usually a modification of some records do not get recorded in the stream. I know for a fact that the new information was written to the table because I can see this activity in the table metrics.

Why would this happen? It is not something rare, it happens several times a day. Is there any way of refreshing the stream of something like that? I know the data in the shards of the stream is updated and refreshed on its own but still, is there any way of doing that?

I use this method to go through the shards of the stream:
private void processShards(AmazonDynamoDBStreamsClient sC, DescribeStreamResult dsr, String arn) {
                String lastEvaluatedShardId = null;
 
                final List<Shard> shards = dsr.getStreamDescription().getShards();
                do {
                    // Process each shard on this page
                    for (Shard shard : shards) {
                        final String shardId = shard.getShardId();
 
                        if (!shardMap.containsKey(shardId)) {
                            shardMap.put(shardId, new String[2]);
                            shardMap.get(shardId)[0] = """";
                            shardMap.get(shardId)[1] = ""open"";
                            System.out.println(""Shards map contains "" + shardMap.size() + "" shards."");
                        }
 
                        if (shardMap.get(shardId)[1].matches(""open"")) {
                            GetShardIteratorRequest getShardIteratorRequest = null;
                            if (shardMap.get(shardId)[0].matches("""")) {
                                getShardIteratorRequest = new GetShardIteratorRequest()
                                        .withStreamArn(arn)
                                        .withShardId(shardId)
                                        .withShardIteratorType(ShardIteratorType.TRIM_HORIZON);
                            } else {
                                getShardIteratorRequest = new GetShardIteratorRequest()
                                        .withStreamArn(arn)
                                        .withShardId(shardId)
                                        .withShardIteratorType(ShardIteratorType.AFTER_SEQUENCE_NUMBER)
                                        .withSequenceNumber(shardMap.get(shardId)[0]);
                            }
                            final GetShardIteratorResult sir = sC.getShardIterator(getShardIteratorRequest);
                            String currentShardIter = sir.getShardIterator();
                            while (currentShardIter != null) {
                                final GetRecordsResult getRecordsResult = sC.getRecords(new GetRecordsRequest().withShardIterator(currentShardIter));
                                final List<Record> records = getRecordsResult.getRecords();
                                if (!records.isEmpty()) {
                                    for (Record record : records) {
                                        final StreamRecord sr = record.getDynamodb();
 // process record ..... do something
                                        shardMap.get(shardId)[0] = sr.getSequenceNumber();
                                    }
                                    currentShardIter = getRecordsResult.getNextShardIterator();
                                } else {
                                    break;
                                }
                            }
 
                            final String endSeq = shard.getSequenceNumberRange().getEndingSequenceNumber();
                            if (endSeq != null) { 
                                shardMap.get(shardId)[1] = ""closed"";
                            }
                        } else {
                            // do nothing shard is closed
                        }
                    }
 
                    lastEvaluatedShardId = dsr.getStreamDescription().getLastEvaluatedShardId();
 
                } while (lastEvaluatedShardId != null);
            } 


shardMap is -->  HashMap <String, String[]> shardMap = new HashMap <String, String[]>(); 


When processing all the shards I can see that new information sometimes does not exist in there.

What could be the issue? Thank you for your help."
Amazon DynamoDB	"Re: AWS DynamoDB streams does not update with new information
Hello,

It seems that your code is iterating through the shards in the stream just once. According to the below documents, getRecords operation is recommended to be run in a loop since it can take multiple calls to reach a portion of shard and retrieve all records:
https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_streams_GetRecords.html
https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-sdk.html#kinesis-using-sdk-java-get-data-getrecords

(Please note that DynamoDB stream and Kinesis stream behavior is similar)


Juhi"
Amazon DynamoDB	"Conditional Update fails when attribute doesn't exist yet
Perhaps I misread the documentation, but is it expected for a conditional update to fail if the attribute it is checking against doesn't exist (yet)?

I expected it to treat the missing value as zero, for comparison purposes.

For example, the following fails when myUpdateTime is not present on the item:
aws dynamodb update-item  --table-name MyTable \
--key=""{\""myPK\"":{\""S\"":\""edsf\""}}"" \
--update-expression=""SET #myUpdateTime = :time"" \
--expression-attribute-values=""{\"":time\"":{\""N\"":\""201\""}}"" \
--expression-attribute-names=""{\""#myUpdateTime\"":\""myUpdateTime\""}"" \
--condition-expression=""#myUpdateTime <= :time""


However, if I change the condition expression to the following, it succeeds:
""#myUpdateTime <= :time OR attribute_not_exists(#myUpdateTime}""

Is that the best way to solve this, or is there a better way to handle when the attribute hasn't been created yet?"
Amazon DynamoDB	"Re: Conditional Update fails when attribute doesn't exist yet
Hello,

If the attribute does not exist yet but is used in the condition expression, the condition will be evaluated to false as a result of null response. 

Your approach to handling this scenario is efficient. 


Juhi"
Amazon DynamoDB	"Unity Android:  not receiving Async callbacks, when app in background
I am developing a Unity application that performs some tasks in the background (i.e. when the user has navigated away from the app for whatever reason).  When the app is in the background, an Android foreground service makes its way back into my Unity C# code, which periodically calls into the AWS Mobile SDK for Unity, specifically DynamoDB interactions.

While the Android app is in the background, these DynamoDB calls (e.g. QueryAsync) never seem to complete (i.e. no async callbacks are executed).  To be clear, my Android service runs just fine in the background otherwise, performing additional desired activities in my Unity C# code, besides trying to interact with the database.

Is there a setting, of which I'm unaware, to allow background database activity, or is there some manual pumping call I can make periodically to 'drain' the callbacks, or . . .?

Edited by: PistolShrimp on Jan 10, 2019 10:21 AM"
Amazon DynamoDB	"Transactions for DAX/DynamoDB Local?
Near the bottom of the DynamoDB Developer Guide section on transactions (https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html) it states:

""Transactional operations are not supported by DynamoDB Accelerator (DAX), DynamoDB local, or DynamoDB Mapper at this time"". 

With the advent of transactions, it seemed that one of the biggest issues with using DynamoDB with one of our apps had finally been solved; I'm eager to refactor some problematic code! 

The issue is that across our org, tests for our apps that leverage DynamoDB make extensive use of DynamoDB Local and its accompanying Java DynamoDBEmbedded fixture, and we have been closely examining possible use of DAX as well for certain other problematic use cases. 

When I manually browse the DynamoDB Local repository (https://s3-us-west-2.amazonaws.com/dynamodb-local/) I see that the newest version of DynamoDB Local (as of this writing) is 1.11.119, which seems to have been released/updated on 2018-04-12. 

Additionally, the v2 SDK (https://github.com/aws/aws-sdk-java-v2) development is well underway, and the interfaces and package names have changed. However, the existing DynamoDBEmbedded implementation uses the current v1 SDK interface, which means that at it stands now, this incredibly useful fixture will not be available for use when the v2 SDK comes out. 

As such, I have a number of questions which are really important for the development path of our organization.

1. Is DynamoDB Local still in active development? 
2. Is DynamoDB transaction support planned for DynamoDB local? 
3. Is DynamoDB transaction support planned for the DynamoDB Mapper in the v1.x SDK? 
4. Is DynamoDB transaction support planned for the DynamoDB Mapper in the v2 SDK? 
4. Is DynamoDB transaction support planned for DAX? 
5. Is AWS SDK v2 support planned for DynamoDB Local? 
6. With the current lay of the land (no compatible local fixtures) what is the expected development and test model when a team desires to use DynamoDB Transactions? 

We have multiple teams working with DynamoDB, and each team member must test their code which interfaces with DynamoDB.
Are tests to set up and tear down DynamoDB tables dynamically between tests? 
What if tests fail or computers crash? Do we manually troll and delete orphaned tables? The DynamoDB dashboard lumps all tables together in a single list. This is risky!


Thanks for any help or light you can shed on this situation.

Edited by: RyonDay on Dec 6, 2018 10:23 AM"
Amazon DynamoDB	"Re: Transactions for DAX/DynamoDB Local?
4. Is DynamoDB transaction support planned for DAX? 
I can only speak to DAX, but transaction support is definitely in development and will be coming soon."
Amazon DynamoDB	"Re: Transactions for DAX/DynamoDB Local?
Hi @RyonDay,

Thanks for the questions. Yes - DynamoDB local is still being actively developed. In fact, we made DynamoDB local available as downloadable docker image recently: https://aws.amazon.com/about-aws/whats-new/2018/08/use-amazon-dynamodb-local-more-easily-with-the-new-docker-image/.  We are working on adding more features to DynamoDB local to make easier to develop and test your applications locally. If you have a prioritized list, please let us know (the SDK v2 request below is noted)!

Transactions are not available for DynamoDB local, DynamoDB Mapper, or DAX at this time. Please keep an eye on our documentation and follow us on Twitter at dynamodb@ for the latest info on new feature announcements. Right now, you will have do dev and test using the DynamoDB cloud service. I will check with one our dev evangelists to see if they can recommend a good blog post with some table management best practices for dev/test workloads.

AH"
Amazon DynamoDB	"Re: Transactions for DAX/DynamoDB Local?
Thank you!"
Amazon DynamoDB	"Re: Transactions for DAX/DynamoDB Local?
Hey @RyonDay,

Wanted to confirm you saw that we launched support transactional APIs in DAX recently: https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-dynamodb-accelerator-adds-support-for-dynamodb-transactions/"
Amazon DynamoDB	"Re: Transactions for DAX/DynamoDB Local?
@ArturoAtAWS
Can you update us when DynamoDB Local supports transactions?
Thanks"
Amazon DynamoDB	"Unknown parameter in input: ""BillingMode""
My old DynamoDB script that works:

{
    ""AttributeDefinitions"": [
        {
            ""AttributeName"": ""userId"",
            ""AttributeType"": ""S""
        }
    ],
    ""TableName"": ""DEV_Users"",
    ""KeySchema"": [
        {
            ""AttributeName"": ""userId"",
            ""KeyType"": ""HASH""
        }
    ],
    ""ProvisionedThroughput"": {
        ""ReadCapacityUnits"": 1,
        ""WriteCapacityUnits"": 1
    }
}


My two attempts at updating this script to work for on-demand:

{
    ""AttributeDefinitions"": [
        {
            ""AttributeName"": ""userId"",
            ""AttributeType"": ""S""
        }
    ],
    ""TableName"": ""TEMP_Users"",
    ""KeySchema"": [
        {
            ""AttributeName"": ""userId"",
            ""KeyType"": ""HASH""
        }
    ],
    ""BillingMode"": ""PAY_PER_REQUEST""
}


And also

{
    ""AttributeDefinitions"": [
        {
            ""AttributeName"": ""userId"",
            ""AttributeType"": ""S""
        }
    ],
    ""TableName"": ""TEMP_Users"",
    ""KeySchema"": [
        {
            ""AttributeName"": ""userId"",
            ""KeyType"": ""HASH""
        }
    ],
    ""BillingModeSummary"": [
		{
			""BillingMode "": ""PAY_PER_REQUEST""
		}
    ]
}


Both attempts result in:

Parameter validation failed:
Missing required parameter in input: ""ProvisionedThroughput""
Unknown parameter in input: ""BillingMode"", must be one of: AttributeDefinitions, TableName, KeySchema, LocalSecondaryIndexes, GlobalSecondaryIndexes, ProvisionedThroughput, StreamSpecification, SSESpecification


Am I doing anything wrong? Looks like I am complying with the create-table CLI? https://docs.aws.amazon.com/cli/latest/reference/dynamodb/create-table.html"
Amazon DynamoDB	"Re: Unknown parameter in input: ""BillingMode""
I believe you might be using an older version of aws cli and hence its not able to recognize recently added parameter 'BillingMode'. Can you update aws cli using following command 'sudo pip3 install --upgrade awscli' to latest version and then retry."
Amazon DynamoDB	"Re: Unknown parameter in input: ""BillingMode""
Thank you - this was the problem."
Amazon DynamoDB	"Re: Unknown parameter in input: ""BillingMode""
I am glad issue is resolved."
Amazon DynamoDB	"DynamoDB Auto-Scaling only scales down when there is table activity
From looking at my DynamoDB auto-scaling patterns, it appears that the auto-scaling service will only scale when table activity (read or write) occurs.

For example, if you have a table set to autoscale from 0 to 1000, and every day there is a bulk write for 20 minutes that causes it to scale up to 1000 then no further writes until the next day, it appears that this table will not ever scale down.

I believe the root cause of this is due to how missing data is treated in CloudWatch. Autoscaling sets up low activity alarms that trigger scaling down, however they are set to treat missing data as missing, and only send a notification when it's alarm. However, if there are no writes, that means it's treated as missing data, no alarm is triggered, and no notification is sent. 

On a related note, is there any way to change the scale-up and scale-down duration? Currently it appears it will scale up after 5 minutes of the high alarm being consecutively triggered, and scale down after 15 minutes of the low alarm being consecutively triggered. Is there any way of configuring that?

Thank you"
Amazon DynamoDB	"Re: DynamoDB Auto-Scaling only scales down when there is table activity
Some images to demonstrate the point.

This table is configured to scale writes between 5 and 50, however it is only used for occasional bulk writes - which initially seems like a perfect use case for auto-scaling. Unfortunately, it's stuck at 50."
Amazon DynamoDB	"Re: DynamoDB Auto-Scaling only scales down when there is table activity
I experience the same problem.  I usually have to just manually set the way to 1 when I know it won't be in use for a while."
Amazon DynamoDB	"Re: DynamoDB Auto-Scaling only scales down when there is table activity
+1

This is a deal breaker for us. We have tons of bursty workflows and the scale down hangs at higher capacity when read/writes drop away. We need a way to treat missing data as 0's.

Does anyone know if this is on the roadmap?"
Amazon DynamoDB	"Re: DynamoDB Auto-Scaling only scales down when there is table activity
+1

Am just running across this issue as well.  Seems like a major oversight on the part of AWS (and it sure as heck wouldn't be the first)!

Edited by: _flp on Nov 2, 2017 4:15 PM"
Amazon DynamoDB	"Re: DynamoDB Auto-Scaling only scales down when there is table activity
+1. Can't do bulk loads into DynamoDB until this is fixed!!!"
Amazon DynamoDB	"Re: DynamoDB Auto-Scaling only scales down when there is table activity
i am experiencing the same behaviour.
as a work-around i created a lambda function which is triggered every 15 minutes (cloudwatch event) and reads/writes a dummy event to all my tables, to generate very low traffic. This allows the downscale mechanism to kick in."
Amazon DynamoDB	"Re: DynamoDB Auto-Scaling only scales down when there is table activity
From https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html: ""Currently, auto scaling does not scale down your provisioned capacity if your table’s consumed capacity becomes zero. As a workaround, you can send requests to the table until auto scaling scales down to the minimum capacity, or change the policy to reduce the maximum provisioned capacity to be the same as the minimum provisioned capacity."" 

We will check back with the thread when we have an even better solution. Can't say when that will be, but we will ping the thread at that time.

Craig"
Amazon DynamoDB	"Re: DynamoDB Auto-Scaling only scales down when there is table activity
+1"
Amazon DynamoDB	"Re: DynamoDB Auto-Scaling only scales down when there is table activity
Hello everyone,

The awaited change has been deployed. DynamoDB now scales down for 0 traffic on the table. The public facing documents will be updated to reflect this change soon.

Juhi"
Amazon DynamoDB	"Re: DynamoDB Auto-Scaling only scales down when there is table activity
Glad to see that AWS has finally deployed a fix in DynamoDB so that it will now scale down for 0 traffic on the table. However, we had to learn this the hard way back in September when it did not scale after first enabling DynamoDB on our account. We ran a price estimate and it came nowhere close to the amount we were actually billed. (Let's just say our bill came out to be 1000% greater than what we expected to pay). This was all due to our table's being written to in bursts and DynamoDB did not autoscale back down when there was no table activity.

I'm sharing this information to hopefully prevent others from having to go through the same nightmare. We opened a case in the Support Center to request a bill adjustment back in early October and it still has not been resolved. Every time I request an update or call in, I get a different person from the AWS support team that has no background knowledge of our case and I have to re-explain he issue all over again.

Beyond frustrated at this point and not sure what else to do."
Amazon DynamoDB	"Re: DynamoDB Auto-Scaling only scales down when there is table activity
Hello rmaxon,

We understand your concern. Please share your case and account details as requested (via private message). We will review the details and get the case to resolution as quickly as possible. 

Regards,
Phanindra"
Amazon DynamoDB	"partial replication in dynamoDB global tables
DynamoDB documentation on global tables is not very clear when it says this,

""With a global table, each replica table stores the same set of data items. DynamoDB does not support partial replication of only some of the items.""

Ref - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_HowItWorks.html

What does this mean exactly?

Could someone explain this with an example?

Thanks!"
Amazon DynamoDB	"Re: partial replication in dynamoDB global tables
Lets say you have a Global table with replicas in region 'us-east-1' and 'us-west-2'. You have performed write operation (100 items) on replica in region 'us-east-1'. So by default all 100 items will be replicated to table in us-west-2. You cant restrict or configure your Global table to only partially replicate only few items (lets say 5 out of 100). 

This is what is mentioned in developer docs you are referencing

Hopefully it helps. Let us know if you have any other questions"
Amazon DynamoDB	"Re: partial replication in dynamoDB global tables
Thanks for the reply Ankur. That seems to be what they have said.
Why would anyone want to do that though?"
Amazon DynamoDB	"Re: partial replication in dynamoDB global tables
shantanus wrote:
Thanks for the reply Ankur. That seems to be what they have said.
Why would anyone want to do that though?
For example, some customers have policies, regulations, etc. that require certain data not leave home regions.

-Jeff"
Amazon DynamoDB	"How to find out the partitionKey using a sortKey
hi,
  I am new to DynamoDB and have a basic question. I have a table called CUSTOMERS with partnerId as the partition key and customerId as the sort key. The idea is that every Partner may have many Customers each so I've chosen the keys as such.
  The question is, for a given customerId I want to find out the partnerId (which happens to be the partition key). I am writing python code in AWS Lambda for the above. I'd appreciate if someone could help with this. Thanks.

p.s. some sample python code would really help.


J"
Amazon DynamoDB	"Re: How to find out the partitionKey using a sortKey
You can create a Global secondary index  (GSI) on top of this table with partition key and sort key interchanged that is partnerId as the sort key and customerId as the partition key. Now, you can query Global secondary index using a customerId and you will get all partnerId. You can also project all or only selected attributes from main table to GSI

Refer this developer doc for more details:  https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes.html

Hope it helps"
Amazon DynamoDB	"Re: How to find out the partitionKey using a sortKey
Thank you @ankur2626, that works like a charm 


J"
Amazon DynamoDB	"Re: How to find out the partitionKey using a sortKey
I am glad it worked. Let us know if you have any other questions"
Amazon DynamoDB	"What should be the read requests per second with 6 read capacity units
If I have a read capacity units of 6 request capacity units than the number of transactional read requests which can be performed per second with this capacity should be 12kb per second. As one reads can be 4kb and we can perform 3 transactional read requests with 6 capacity units. But on documentation of Read/Write capacity mode in DynamoDB section it is written as 3kb per second.

https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html

How has this calculation been made? Is 3kb per second is right answer or it should be 12kb per second as per my calculation?"
Amazon DynamoDB	"Re: What should be the read requests per second with 6 read capacity units
Digvijay437 wrote:
If I have a read capacity units of 6 request capacity units than the number of transactional read requests which can be performed per second with this capacity should be 12kb per second. As one reads can be 4kb and we can perform 3 transactional read requests with 6 capacity units. But on documentation of Read/Write capacity mode in DynamoDB section it is written as 3kb per second.

https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html

How has this calculation been made? Is 3kb per second is right answer or it should be 12kb per second as per my calculation?

Hi Digvijay437,

It sounds like you are using a table with provisioned capacity and using the transactional APIs.  The Amazon DynamoDB pricing page (https://aws.amazon.com/dynamodb/pricing/provisioned/) explains it as such:

""Each API call to read data from your table is a read request. Read requests can be strongly consistent, eventually consistent, or transactional. For items up to 4 KB in size, one RCU can perform one strongly consistent read request per second. Items larger than 4 KB require additional RCUs. For items up to 4 KB in size, one RCU can perform two eventually consistent read requests per second. Transactional read requests require two RCUs to perform one read per second for items up to 4 KB. For example, a strongly consistent read of an 8 KB item would require two RCUs, an eventually consistent read of an 8 KB item would require one RCU, and a transactional read of an 8 KB item would require four RCUs. See Read Consistency for more details.""

To summarize, if you need to do 3 transactional reads of items up to 4KB in size, then you would require 6 read capacity units. 

-Jeff"
Amazon DynamoDB	"DynamoDB Local GSI / LSI Issue
I don't know if the issue I'm running into is related to an issue that was posted a year ago here, but it seems likely.  While searching the forums for my issue, I came across this unanswered thread:  https://forums.aws.amazon.com/thread.jspa?messageID=821514󈤊.  I ran into this very same problem myself.

That said, I'm running into a different issue now, but it seems like it might be related.  I have a table with 3 GSIs.  When I attempt to create the table against DynamoDB Local, I get this error:

""An error occurred (ValidationException) when calling the CreateTable operation:  No more than 20 attributes per table can be projected into Local Secondary Indices.""

The key part that confuses me is the fact that the error is saying that there is a problem with an LSI...except I'm creating a GSI.  I have no LSIs defined at all.  Is DynamoDB Local treating GSIs as LSIs?  That would seem like that's what it's doing based on the error that I got, and the previously-mentioned thread."
Amazon DynamoDB	"Filtering on the Contents of a Map Within A List
I would like to filter the results of a query by the contents of a map which may be contained within a list.  Here is an example of the structure of our data (hash key EventType, range key EventTime):
{
    'EventType': 'git/push'
    'EventTime': 1416251010.918,
    'commits': [
        {
            'id': '29d02aff...',
            'subject': 'Add the thing to the place'
        },
        {
            'id': '9d888fec...',
            'subject': 'Spelling errors'
        },
        ...
    ]
}

I am attempting to write a filter that filters the result of a query to one(s) that contains a specific commit id.  Is is possible to create a filter expression that correctly filters the query like this?"
Amazon DynamoDB	"Re: Filtering on the Contents of a Map Within A List
afaik there isn't a way to do this unless the length of the list is bounded and small enough to do something like:
""(commits[0].id = :val) or (commits[1].id = :val) or ..."" with :val = 'specific-id'

or you know the subject and can do:
""contains(commits, :val)"" with :val = {'id': 'specific-id', 'subject': 'subject-for-that-id'}

one solution could be to add a separate string set/list or map attribute that only contains the commit ids then use ""contains"" or ""attribute_exists"""
Amazon DynamoDB	"Re: Filtering on the Contents of a Map Within A List
Hi Glen,

This functionality isn't natively supported by the current API's expression language, but it's possible to implement it client side as shown by cfier:

Iterating over a list attribute isn't supported so (without changing your item model) you'll need to exactly match through the path or the Map you want (the first two suggestions by cfier).

If you're ok with changing your item model, you could break up the commits into something like:

'Commits': ['29d02aff...', '9d888fec...', ...etc.],
'CommitData': [
  {'subject': 'Add the thing to the place'},
  {'subject': 'Spelling errors'},
  ...etc.
]


This would allow you to filter with ""contains"" and could also save on storing the 'id' attribute name without losing the ordering and support for non-unique commit ids in the previous model.

We always value customer feedback, so please share any types of expressions you'd like to see in the future!

Thanks,
Johnny"
Amazon DynamoDB	"Re: Filtering on the Contents of a Map Within A List
I spent quite a bit of time trying to apply the same type of query before I found this.  Would be really useful to be able to filter by the contents of the Map attributes, especially given how easy it is to get Json dynamodb Document objects into the db

Edited by: seethruhead on Mar 25, 2015 10:22 PM"
Amazon DynamoDB	"Re: Filtering on the Contents of a Map Within A List
(think I should have replied to JohnnyW ... )  I spent quite a bit of time trying to apply the same type of filter before I found this. Would be really useful to be able to filter by the contents of the Map attributes, especially given how easy it is to get Json dynamodb Document objects into the db

This is the stackoverflow question I asked, and describes how I attempted to use the FilterExpression in a way that made sense using a path to the children to search: http://stackoverflow.com/questions/29270702/dynamodb-filter-expressions-in-java-cant-match-multiple-json-paths-at-once

Edited by: seethruhead on Mar 25, 2015 10:25 PM

Edited by: seethruhead on Mar 25, 2015 10:34 PM"
Amazon DynamoDB	"Re: Filtering on the Contents of a Map Within A List
Hello @seethruhead,
did you get the solutin for this? I am getting the same issue.

thanks,"
Amazon DynamoDB	"Re: Filtering on the Contents of a Map Within A List
Any solution on this issue ?"
Amazon DynamoDB	"Re: Filtering on the Contents of a Map Within A List
Hello everyone, I hope this answer is not too late.

The following code is  a implementation using the Java SDK for AWS; I used the second suggestion of cfier

final Map<String, AttributeValue> expressionAttributeValues = new HashMap();
final Map<String, AttributeValue> commit = new HashMap();
commit.put(""id"", new AttributeValue().withS(_ID_ARGUMENT));
commit.put(""subject"", new AttributeValue().withS(_SUBJECT_ARGUMENT));
expressionAttributeValues.put("":anyCommit"", new AttributeValue().withM(commit));
 
ScanRequest scanRequest = new ScanRequest()
        .withTableName(repository.getTableName())
        .withExpressionAttributeNames(expressionAttributesNames)
        .withFilterExpression(""contains(commits, :anyCommit)"")
        .withProjectionExpression(""commits"")
        .withExpressionAttributeValues(expressionAttributeValues)
        .withLimit(100);//Optional: you could limit the total of result
logger.info(""Scan Request: "" + scanRequest.toString());
ScanResult result = repository.scan(scanRequest);
logger.info(""Total: "" + result.getCount());
logger.info(""Scan Result : "" + scanRequest.toString());

GraphQL Query Resolver
   {
    ""version"" : ""2017-02-28"",
    ""operation"" : ""Scan"",
    ""filter"" : {
        ""expression"": ""contains(commits, :anyCommit)"",
        ""expressionValues"" : {
            "":anyCommit"": {
            	""M"": { ""id"": { ""S"": ""${ctx.args.commitId}"" }, ""subject"": { ""S"": ""${ctx.args.commitSubject}"" } }
                }
        }
    }
}


Regards!"
Amazon DynamoDB	"Re: Filtering on the Contents of a Map Within A List
Did AWS implement anything which would solve the original question? The question was asked 4 years ago. In my opinion, not having this feature kinda defeats the whole purpose of document management and nosql."
Amazon DynamoDB	"getting AccessDeniedException (client)
I am trying to use write some data to my dynamodb table with putItem in php. I am using IAM. 
AccessDeniedException (client): User: arn:aws:sts::497961711695:assumed-role/AmazonLightsailInstanceRole/i-015f853dce1665d1c is not authorized to perform: dynamodb:PutItem on resource: arn:aws:dynamodb:us-east-1:497961711695:table/historical-activities - 

I have tried many different things without success. I finally removed all the policies for this user and have just one inline policy below.
{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Sid"": ""ListAndDescribe"",
            ""Effect"": ""Allow"",
            ""Action"": [
                ""dynamodb:List*"",
                ""dynamodb:DescribeReservedCapacity*"",
                ""dynamodb:DescribeLimits"",
                ""dynamodb:DescribeTimeToLive""
            ],
            ""Resource"": ""*""
        },
        {
            ""Sid"": ""SpecificTable"",
            ""Effect"": ""Allow"",
            ""Action"": [
                ""dynamodb:BatchGet*"",
                ""dynamodb:DescribeStream"",
                ""dynamodb:DescribeTable"",
                ""dynamodb:Get*"",
                ""dynamodb:Query"",
                ""dynamodb:Scan"",
                ""dynamodb:BatchWrite*"",
                ""dynamodb:CreateTable"",
                ""dynamodb:Delete*"",
                ""dynamodb:Update*"",
                ""dynamodb:PutItem""
            ],
            ""Resource"": ""arn:aws:dynamodb:::table/historical-activities""
        }
    ]
}

MFA is not enabled for this user and he is not a member of any groups. Any idea why the putItem is failing?"
Amazon DynamoDB	"Re: getting AccessDeniedException (client)
Hi,

In order to overcome the AccessDenied error here, you need to provide permission to IAM role ""AmazonLightsailInstanceRole"" (and not to any user) to perform dynamodb:PutItem on your DynamoDB table.

Please add the below policy to IAM role ""AmazonLightsailInstanceRole"" and then perform PutItem on table:

{
""Version"": ""2012-10-17"",
""Statement"": [
{
""Sid"": ""Sample"",
""Effect"": ""Allow"",
""Action"": [
""dynamodb:List*"",
""dynamodb:Describe*:,
""dynamodb:PutItem""
],
""Resource"": ""arn:aws:dynamodb:us-east-1:497961711695:table/historical-activities""
}

I hope it help you. Please feel free to get back to us if you have any further concerns. I'd be happy to help you."
Amazon DynamoDB	"Scheduling?
I was wondering what's the best service or combination of services to build a durable, reliable scheduling system?

It's targeted for users which means that users are able to add some kind of predefined events with future timestamp and I have to make sure that events happen at that time. It doesn't have to be extremely accurate, 1 minute accuracy is good enough. I prefer serverless solution.


What I would do with the experience I have:

DynamoDB Table
TargetDay (partition key) - example 31-12-2018 
TargetTimestamp (sort key) - example 1546257600                
other attributes...

If it's very busy service, single partition might get too hot. 
In that case, partition key should be more cranual, e.g add full hours (00-31-12-2018 to 23-31-12-2018).

Lambda + CloudWatch
1. Run Lambda function once every minute using CloudWatch Schedule (can't go lower than that).
2. Query DDB using current day as partition key (or more cranual) and sort key to get all items older than now.
3. Process all the events or pass them to other functions/services.
4. Delete all processed messages from DynamoDB.


Any better ideas?"
Amazon DynamoDB	"Dynamodb primary key and filter performance
Question 1
I have a table with primary key ""id"" and secondary key ""createdAt"".
I created another index with primary key ""type"" and secondary key ""createdAt"".
However, this index was not used much. I want to delete it and save money.
Can I use the id index with id != null and filter expression ""type"" to do the same thing?
How is the performance?

Question 2
If the capacity read is 1kb, each item is 1kb, the query/scan perform for 10 items with one item output, I will get the result in 1 second, not 10 seconds?"
Amazon DynamoDB	"Re: Dynamodb primary key and filter performance
I sent your questions to the DynamoDB team, and here's Daniel's answers: 

""1. Not recommended. You would essentially need to scan the table and performance would be low.
2.  You should expect low millisecond latency from a query, regardless of the number of items. You should not use scan with anything that is latency sensitive.""

Craig"
Amazon DynamoDB	"Re: Dynamodb primary key and filter performance
Thank you. I know how to use."
Amazon DynamoDB	"DynamoDB On-Demand via cloudformation breaks on consecutive runs
for multiple updates through cloud-foramtion as part of pipeline, I am receiving following error:

One or more parameter values were invalid: The BillingMode for the table will not change. The requested value equals the current value. Current BillingMode: PAY_PER_REQUEST. Requested BillingMode: PAY_PER_REQUEST (Service: AmazonDynamoDBv2; Status Code: 400; Error Code: ValidationException;

I have removed ProvisionedThroughput and also all scaling policy for the table. The template updates the billing mode first time. However consecutive run tries to update it again even though BillingMode is same as last time and hence fails. I have talked to AWS support and they agree it's a bug however no ETA. It's a tricky situation for us as our entire pipeline is useless because of this issue and we don't really want to go for Provisioned capacity as temporary solution.

Edited by: NirajBhatt on Dec 10, 2018 1:59 PM"
Amazon DynamoDB	"Re: DynamoDB On-Demand via cloudformation breaks on consecutive runs
Hi NirajBhatt,

Thank you for bringing this matter to our attention. We are investigating and will keep this thread updated as soon as we have more information.

-Kai"
Amazon DynamoDB	"Re: DynamoDB On-Demand via cloudformation breaks on consecutive runs
Hi!

I have encountered the same issue. Firstly deployed cloudformation with a provisioned capacity, changed it to on-demand via the console and afterwards the cloudformation could not deploy anymore with that same error. 

One or more parameter values were invalid: The BillingMode for the table will not change. The requested value equals the current value. Current BillingMode: PAY_PER_REQUEST. Requested BillingMode: PAY_PER_REQUEST (Service: AmazonDynamoDBv2; Status Code: 400; Error Code: ValidationException;"
Amazon DynamoDB	"using Sessions with Dynamo DB
So I have a maven / spring application running on tomcat 8. I'm playing around with storing the sessions in dynmao db. There are a few reasons why I want to do this but i'll spare you the details.

I've been following this guide pretty religiously Link: https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/java-dg-tomcat-session-manager.html#java-dg-tomcat-sess-manage-with-ddb but my data does not seem to be being sent to the dynamoDB table I set up. 

So what i've done.

First I downloaded this jar:

aws-dynamodb-session-tomcat-2.0.4.jar


I renamd it

 AmazonDynamoDBSessionManagerForTomcat-2.0.4.jar 


to match the docs

and moved it to my lib folder.

Then I set up my context.xml like:
<Manager 
 
    className=""com.amazonaws.services.dynamodb.sessionmanager.DynamoDBSessionManager""
            awsAccessKey=""mykey""
            awsSecretKey=""mysecertKey""
            regionId=""us-east-1""
            createIfNotExist=""true"" />


These apps are on EC2 instances so I skiped the ECB step. Next I set up a DBB table that looks like:

 Table name	Tomcat_SessionState
    Primary partition key	sessionId (String) 


But when I restart my app and try and login I don't see anything geting posted there..

I've been tailing my catalina.out but no luck there either. Another note on this I don't see anything about DBB in my catalina.out strange.

Am I missing a common step here?

UPDATE:

When I start my app it creates the needed table. Just can't seem to get it to send the session id's out there. I wonder if a code change needs to be made to support this feature? I thought it supported any forum of sessions. 
Edited by: dennis93 on Mar 8, 2018 2:13 PM

Edited by: dennis93 on Mar 8, 2018 2:35 PM"
Amazon DynamoDB	"Re: using Sessions with Dynamo DB
The aws-dynamodb-session-tomcat project, unfortunately, has been archived and is no longer maintained by AWS. 
https://github.com/amazon-archives/aws-dynamodb-session-tomcat

The document team has corrected the user guide and removed managing Tomcat sessions in DynamoDB example as that tool is no longer supported.
https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/document-history.html

We apologize for the time that you have spent on dealing with the tool."
Amazon DynamoDB	"Re: using Sessions with Dynamo DB
Hi Louisa,

What do you recommend for tomcat distributed session management now ?"
